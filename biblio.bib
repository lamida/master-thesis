
@article{00036307,
  title = {00036307},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\4TA4KTVM\\00036307.pdf}
}

@article{abadiControlFlowIntegrityPrinciples2005,
  title = {Control-{{Flow Integrity Principles}}, {{Implementations}}, and {{Applications}}},
  author = {Abadi, Mart{\'i}n and Cruz, Santa and Budi{\'u}, Mihai and Erlingsson, Ulfar and Ligatti, Jay},
  year = {2005},
  abstract = {Current software attacks often build on exploits that subvert machine code execution. The enforcement of a basic safety property, Control-Flow Integrity (CFI), can prevent such attacks from arbitrarily controlling program behavior. CFI enforcement is simple, and its guarantees can be established formally, even with respect to powerful adversaries. Moreover, CFI enforcement is practical: it is compatible with existing software and can be done efficiently using software rewriting in commodity systems. Finally, CFI provides a useful foundation for enforcing further security policies, as we demonstrate with efficient software implementations of a protected shadow call stack and of access control for memory regions.},
  file = {D\:\\GDrive\\zotero\\Abadi\\abadi_2005_control-flow_integrity_principles,_implementations,_and_applications.pdf;D\:\\GDrive\\zotero\\Abadi\\abadi_2005_control-flow_integrity_principles,_implementations,_and_applications2.pdf;D\:\\GDrive\\zotero\\Mart´\\mart´_control-flow_integrity_principles,_implementations,_and_applications.pdf},
  keywords = {D24 [Software Engineering]: Software/Program Ver-ification General Terms: Security; Languages; Verification Additional Key Words and Phrases: Binary rewriting; control-flow graph; inlined reference moni-tors; vulnerabilities,D24 [Software Engineering]: Software/Program Verification,D34 [Programming Languages]: Processors,D46 [Operating Systems]: Security and Protection,D46 [Operating Systems]: Security and Protection General Terms Security; Languages; Verification Keywords Binary Rewriting; Control-Flow Graph; Inlined Reference Moni-tors; Vulnerabilities}
}

@article{abadiControlflowIntegrityPrinciples2009,
  title = {Control-Flow Integrity Principles, Implementations, and Applications},
  author = {Abadi, Mart{\'i}n and Budiu, Mihai and Erlingsson, {\'U}lfar and Ligatti, Jay},
  year = {2009},
  month = oct,
  volume = {13},
  pages = {1--40},
  issn = {1094-9224, 1557-7406},
  doi = {10.1145/1609956.1609960},
  abstract = {Current software attacks often build on exploits that subvert machine-code execution. The enforcement of a basic safety property, control-flow integrity (CFI), can prevent such attacks from arbitrarily controlling program behavior. CFI enforcement is simple and its guarantees can be established formally, even with respect to powerful adversaries. Moreover, CFI enforcement is practical: It is compatible with existing software and can be done efficiently using software rewriting in commodity systems. Finally, CFI provides a useful foundation for enforcing further security policies, as we demonstrate with efficient software implementations of a protected shadow call stack and of access control for memory regions.},
  file = {D\:\\GDrive\\zotero\\Abadi et al\\abadi_et_al_2009_control-flow_integrity_principles,_implementations,_and_applications2.pdf},
  journal = {ACM Transactions on Information and System Security},
  language = {en},
  number = {1}
}

@techreport{abadiTensorFlowLargeScaleMachinea,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
  abstract = {TensorFlow [1] is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition , computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  file = {D\:\\GDrive\\zotero\\Abadi\\abadi_tensorflow.pdf;D\:\\GDrive\\zotero\\Abadi\\abadi_tensorflow2.pdf}
}

@techreport{aberaCFLATControlFlowAttestation2016,
  title = {C-{{FLAT}}: {{Control}}-{{Flow Attestation}} for {{Embedded Systems Software}}},
  author = {Abera, Tigist and Asokan, N and Davi, Lucas and Ekberg, Jan-Erik and Nyman, Thomas and Paverd, Andrew and Sadeghi, Ahmad-Reza and Tsudik, Gene},
  year = {2016},
  abstract = {Remote attestation is a crucial security service particularly relevant to increasingly popular IoT (and other embedded) devices. It allows a trusted party (verifier) to learn the state of a remote, and potentially malware-infected, device (prover). Most existing approaches are static in nature and only check whether benign software is initially loaded on the prover. However, they are vulnerable to runtime attacks that hijack the application's control or data flow, e.g., via return-oriented programming or data-oriented exploits. As a concrete step towards more comprehensive runtime remote attestation, we present the design and implementation of Control-FLow ATtestation (C-FLAT) that enables remote attestation of an application's control-flow path, without requiring the source code. We describe a full prototype implementation of C-FLAT on Raspberry Pi using its ARM TrustZone hardware security extensions. We evaluate C-FLAT's performance using a real-world embedded (cyber-physical) application, and demonstrate its efficacy against control-flow hijacking attacks.},
  file = {D\:\\GDrive\\zotero\\Abera et al\\abera_et_al_2016_c-flat.pdf;D\:\\GDrive\\zotero\\Abera et al\\abera_et_al_2016_c-flat2.pdf},
  keywords = {control-flow attacks,embedded system security,remote attestation}
}

@article{aberaDIATDataIntegrity2019,
  title = {{{DIAT}}: {{Data Integrity Attestation}} for {{Resilient Collaboration}} of {{Autonomous Systems}}},
  author = {Abera, Tigist and Bahmani, Raad and Brasser, Ferdinand and Ibrahim, Ahmad and Sadeghi, Ahmad-Reza and Schunter, Matthias},
  year = {2019},
  doi = {10.14722/ndss.2019.23420},
  abstract = {Networks of autonomous collaborative embedded systems are emerging in many application domains such as vehicular ad-hoc networks, robotic factory workers, search/rescue robots, delivery and search drones. To perform their collaborative tasks the involved devices exchange various types of information such as sensor data, status information, and commands. For the correct operation of these complex systems each device must be able to verify that the data coming from other devices is correct and has not been maliciously altered. In this paper, we present DIAT-a novel approach that allows to verify the correctness of data by attesting the correct generation as well as processing of data using control-flow attestation. DIAT enables devices in autonomous collaborative networks to securely and efficiently interact, relying on a minimal TCB. It ensures that the data sent from one device to another device is not maliciously changed, neither during transport nor during generation or processing on the originating device. Data exchanged between devices in the network is therefore authenticated along with a proof of integrity of all software involved in its generation and processing. To enable this, the embedded devices' software is decomposed into simple interacting modules reducing the amount and complexity of software that needs to be attested, i.e., only those modules that process the data are relevant. As a proof of concept we implemented and evaluated our scheme DIAT on a state-of-the-art flight controller for drones. Furthermore, we evaluated our scheme in a simulation environment to demonstrate its scalability for large-scale systems.},
  file = {D\:\\GDrive\\zotero\\Abera\\abera_diat.pdf},
  isbn = {189156255X}
}

@article{aberaSADANScalableAdversary2019,
  title = {{{SADAN}}: {{Scalable Adversary Detection}} in {{Autonomous Networks}}},
  shorttitle = {{{SADAN}}},
  author = {Abera, Tigist and Brasser, Ferdinand and Gunn, Lachlan J. and Koisser, David and Sadeghi, Ahmad-Reza},
  year = {2019},
  month = dec,
  abstract = {Autonomous collaborative networks of devices are emerging in numerous domains, such as self-driving cars, smart factories and critical infrastructure, generally referred to as IoT. Their autonomy and self-organization makes them especially vulnerable to attacks. Thus, such networks need a dependable mechanism to detect and identify attackers and enable appropriate reactions. However, current mechanisms to identify adversaries either require a trusted central entity or scale poorly. In this paper, we present SADAN, the first scheme to efficiently identify malicious devices within large networks of collaborating entities. SADAN is designed to function in truly autonomous environments, i.e., without a central trusted entity. Our scheme combines random elections with strong but potentially expensive integrity validation schemes providing a highly scalable solution supporting very large networks with tens of thousands of devices. SADAN is designed as a flexible scheme with interchangeable components, making it adaptable to a wide range of scenarios and use cases. We implemented an instance of SADAN for an automotive use case and simulated it on large-scale networks. Our results show that SADAN scales very efficiently for large networks, and thus enables novel use cases in such environments. Further, we provide an extensive evaluation of key parameters allowing to adapt SADAN to many scenarios.},
  archiveprefix = {arXiv},
  eprint = {1910.05190},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Abera et al\\abera_et_al_2019_sadan.pdf;C\:\\Users\\Admin\\Zotero\\storage\\CJFKUBSZ\\1910.html},
  journal = {arXiv:1910.05190 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@article{AbgazEmbeddingCreativity,
  title = {Abgaz et al. - {{Embedding}} a {{Creativity Support Tool}} within {{Compute}}},
  file = {D\:\\GDrive\\zotero\\undefined\\abgaz_et_al.pdf}
}

@article{abgazEvaluationAnalogicalInferences2016,
  title = {Evaluation of {{Analogical Inferences Formed}} from {{Automatically Generated Representations}} of {{Scientific Publications ARIS}}-{{Analoical Reasoning}} for {{Implementations}} and {{Specifications View}} Project},
  author = {Abgaz, Yalemisew and O'donoghue, Diarmuid P and Hurley, Donny},
  year = {2016},
  doi = {10.13140/RG.2.2.12924.54405},
  abstract = {The user has requested enhancement of the downloaded file.},
  file = {D\:\\GDrive\\zotero\\Abgaz\\abgaz_2016_evaluation_of_analogical_inferences_formed_from_automatically_generated.pdf}
}

@techreport{abouOrganizationBasedAccess2003,
  title = {Organization Based Access Control},
  author = {Abou, Anas and Kalam, El and El Baida, Rania and Balbiani, Philippe and Benferhat, Salem and Cuppens, Fr{\'e}d{\'e}ric and Deswarte, Yves and M{\`i}, Alexandre and Saurel, Claire and Trouessin, Gilles},
  year = {2003},
  abstract = {None of the classical access control models such as DAC, MAC, RBAC, TBAC or TMAC is fully satisfactory to model security policies that are not restricted to static permissions but also include contextual rules related to permissions, prohibitions, obligations and recommendations. This is typically the case of security policies that apply to the health care domain. In this paper, we suggest a new model that provides solutions to specify such contextual security policies. This model, called Organization based access control, is presented using a formal language based on first-order logic.},
  file = {D\:\\GDrive\\zotero\\Abou\\abou_2003_organization_based_access_control.pdf}
}

@book{abreuAutomaticSoftwareFault2008,
  title = {Automatic {{Software Fault Localization}} Using {{Generic Program Invariants}} *},
  author = {Abreu, Rui and Gonz{\'a}lezgonz{\textasciiacute}gonz{\'a}lez, Alberto and Zoeteweij, Peter and Van Gemund, Arjan J C},
  year = {2008},
  abstract = {Despite extensive testing in the development phase, residual defects can be a great threat to dependability in the operational phase. This paper studies the utility of low-cost, generic invariants ("screeners") in their capacity of error detectors within a spectrum-based fault localization (SFL) approach aimed to diagnose program defects in the operational phase. The screeners considered are simple bit-mask and range invariants that screen every load/store and function argument/return program point. Their generic nature allows them to be automatically instrumented without any programmer-effort, while training is straightforward given the test cases available in the development phase. Experiments based on the Siemens program set demonstrate diagnostic performance that is similar to the traditional, development-time application of SFL based on the program pass/fail information known beforehand. This diagnostic performance is currently attained at an average 14\% screener execution time overhead, but this overhead can be reduced at limited performance penalty.},
  file = {D\:\\GDrive\\zotero\\Abreu\\abreu_2008_automatic_software_fault_localization_using_generic_program_invariants.pdf},
  isbn = {978-1-59593-753-7},
  keywords = {Black box diagnosis,Error detection,Fault localization,Program invariants,Program spectra}
}

@article{aciicmezAnotherMicroArchitecturalAttacks2007,
  title = {Yet Another {{MicroArchitectural}} Attacks: {{Exploiting I}}-Cache},
  author = {Acii{\c c}mez, Onur},
  year = {2007},
  pages = {11--18},
  issn = {15437221},
  doi = {10.1145/1314466.1314469},
  abstract = {MicroArchitectural Attacks (MA), which can be considered as a special form of Side-Channel Analysis, exploit microarchitectural functionalities of processor implementations and can compromise the security of computational environments even in the presence of sophisticated protection mechanisms like virtualization and sandboxing. This newly evolving research area has attracted significant interest due to the broad application range and the potentials of these attacks. Cache Analysis and Branch Prediction Analysis were the only types of MA that had been known publicly. In this paper, we introduce Instruction Cache (I-Cache) as yet another source of MA and present our experimental results which clearly prove the practicality and danger of I-Cache Attacks. \textcopyright{} 2007 ACM.},
  file = {D\:\\GDrive\\zotero\\Aciiçmez\\aciiçmez_2007_yet_another_microarchitectural_attacks.pdf},
  isbn = {9781595938909},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Instruction cache,MicroArchitectural analysis,Modular exponentiation,Montgomery multiplication,RSA,Side Channel Analysis}
}

@article{aciicmezPredictingSecretKeys2007,
  title = {Predicting Secret Keys via Branch Prediction},
  author = {Ac{\i}i{\c c}mez, Onur and Ko{\c c}, {\c C}etin Kaya and Seifert, Jean Pierre},
  year = {2007},
  volume = {4377 LNCS},
  pages = {225--242},
  issn = {16113349},
  doi = {10.1007/11967668_15},
  abstract = {This paper announces a new software side-channel attack \textemdash{} enabled by the branch prediction capability common to all modern high-performance CPUs. The penalty paid (extra clock cycles) for a mispredicted branch can be used for cryptanalysis of cryptographic primitives that employ a data-dependent program flow. Analogous to the recently described cache-based side-channel attacks our attacks also allow an unprivileged process to attack other processes running in parallel on the same processor, despite sophisticated partitioning methods such as memory protection, sandboxing or even virtualization. In this paper, we will discuss several such attacks for the example of RSA, and experimentally show their applicability to real systems, such as OpenSSL and Linux. Moreover, we will also demonstrate the strength of the branch prediction side-channel attack by rendering the obvious countermeasure in this context (Montgomery Multiplication with dummy-reduction) as useless. Although the deeper consequences of the latter result make the task of writing an efficient and secure modular exponentiation (or scalar multiplication on an elliptic curve) a challenging task, we will eventually suggest some countermeasures to mitigate branch prediction side-channel attacks.},
  file = {D\:\\GDrive\\zotero\\Acıiçmez\\acıiçmez_2007_predicting_secret_keys_via_branch_prediction.pdf},
  isbn = {9783540693277},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Branch prediction,Modular exponentiation,Montgomery multiplication,RSA,Side channel analysis,Simultaneous multithreading},
  number = {January}
}

@book{acmspecialinterestgroupforalgorithmsandcomputationtheory.SemanticsX86CCMultiprocessor2008,
  title = {The {{Semantics}} of X86-{{CC Multiprocessor Machine Code}}},
  author = {{ACM Special Interest Group for Algorithms and Computation Theory.} and {ACM Special Interest Group on Programming Languages.}},
  year = {2008},
  publisher = {{Association for Computing Machinery}},
  abstract = {"ACM order number 549091"--Page ii.},
  file = {D\:\\MEGA\\zotero\\ACM Special Interest Group for Algorithms and Computation Theory.\\acm_special_interest_group_for_algorithms_and_computation_theory._2008_the_semantics_of_x86-cc_multiprocessor_machine_code.pdf},
  isbn = {978-1-60558-379-2}
}

@techreport{adamsAspectIdiombasedException2007,
  title = {An {{Aspect}} for {{Idiom}}-Based {{Exception Handling}} (Using Local Continuation Join Points, Join Point Properties, Annotations and Type Parameters)},
  author = {Adams, Bram and De Schutter, Kris},
  year = {2007},
  abstract = {The last couple of years, various idioms used in the 15 MLOC C code base of ASML, the world's biggest lithography machine manufacturer , have been unmasked as crosscutting concerns. However, finding a scalable aspect-based implementation for them did not succeed thusfar, prohibiting sufficient separation of concerns and introducing possibly dangerous programming mistakes. This paper proposes a concise aspect-based implementation in Aspicere2 for ASML's exception handling idiom, based on prior work of join point properties, annotations and type parameters, to which we add the new concept of (local) continuation join points. Our solution takes care of the error value propagation mechanism (which includes aborting the main success scenario), logging, resource cleanup, and allows for local overrides of the default aspect-based recovery. The highly idiomatic nature of the problem in tandem with the aforementioned concepts renders our aspects very robust and tolerant to future base code evolution.},
  file = {D\:\\GDrive\\zotero\\Adams\\adams_an_aspect_for_idiom-based_exception_handling_(using_local_continuation_join.pdf},
  keywords = {D1m [Programming Tech-niques]: Aspect-oriented programming,D25 [Testing and De-bugging]: Error handling and recovery,D27 [Software Engin-eering]: Restructuring; reverse engineering; and reengineering,D32 [Programming Lan-guages]: Prolog,D32 [Programming Languages]: C,D3m [Programming Languages]: Local con-tinuation join point General Terms Design; Languages; Reliability}
}

@article{adcockAdvancesQuantumMachine2015,
  title = {Advances in Quantum Machine Learning},
  author = {Adcock, Jeremy and Allen, Euan and Day, Matthew and Frick, Stefan and Hinchliff, Janna and Johnson, Mack and {Morley-Short}, Sam and Pallister, Sam and Price, Alasdair and Stanisic, Stasja},
  year = {2015},
  month = dec,
  abstract = {Here we discuss advances in the field of quantum machine learning. The following document offers a hybrid discussion; both reviewing the field as it is currently, and suggesting directions for further research. We include both algorithms and experimental implementations in the discussion. The field's outlook is generally positive, showing significant promise. However, we believe there are appreciable hurdles to overcome before one can claim that it is a primary application of quantum computation.},
  file = {D\:\\GDrive\\zotero\\Adcock\\adcock_2015_advances_in_quantum_machine_learning.pdf}
}

@article{adepuAssessingEffectivenessAttack2018,
  title = {Assessing the Effectiveness of Attack Detection at a Hackfest on Industrial Control Systems},
  author = {Adepu, Sridhar and Mathur, Aditya},
  year = {2018},
  volume = {3782},
  publisher = {{IEEE}},
  issn = {23318422},
  doi = {10.1109/tsusc.2018.2878597},
  abstract = {A hackfest named SWaT Security Showdown (S3) has been organized consecutively for two years. S3 has enabled researchers and practitioners to assess the effectiveness of methods and products aimed at detecting cyber attacks launched in real-time on an operational water treatment plant, namely, Secure Water Treatment (SWaT). In S3 independent attack teams design and launch attacks on SWaT while defence teams protect the plant passively and raise alarms upon attack detection. Attack teams are scored according to how successful they are in performing attacks based on specific intents while the defense teams are scored based on the effectiveness of their methods to detect the attacks. This paper focuses on the first two instances of S3 and summarizes the benefits of hackfest and the performance of an attack detection mechanism, named Water Defense, that was exposed to attackers during S3.},
  file = {D\:\\GDrive\\zotero\\Adepu\\adepu_2018_assessing_the_effectiveness_of_attack_detection_at_a_hackfest_on_industrial.pdf},
  journal = {arXiv},
  keywords = {Attack Detection,Capture-The-Flag (CTF),Cyber Security,Cyber-physical Attacks,Cyber-Physical Systems,Hackfest,Industrial Control Systems,Water Defense,Water Treatment Plant},
  number = {c}
}

@article{adepuDistributedAttackDetection2021,
  title = {Distributed {{Attack Detection}} in a {{Water Treatment Plant}}: {{Method}} and {{Case Study}}},
  author = {Adepu, Sridhar and Mathur, Aditya},
  year = {2021},
  volume = {18},
  pages = {86--99},
  issn = {19410018},
  doi = {10.1109/TDSC.2018.2875008},
  abstract = {The rise in attempted and successful attacks on critical infrastructure, such as power grid and water treatment plants, has led to an urgent need for the creation and adoption of methods for detecting such attacks often launched either by insiders or state actors. This paper focuses on one such method that aims at the detection of attacks that compromise one or more actuators and sensors in a plant either through successful intrusion in the plant's communication network or directly through the plant computers. The method, labelled as Distributed Attack Detection (DAD), detects attacks in real-time by identifying anomalies in the behavior of the physical process in the plant. Anomalies are identified by using monitors that are implementations of invariants derived from the plant design. Each invariant must hold either throughout the plant operation, or when the plant is in a given state. The effectiveness of DAD was assessed experimentally on an operational water treatment plant named SWaT that is a near-replica of commercially available large treatment plants. The method used in DAD was found to be effective in detecting stealthy and coordinated attacks.},
  file = {D\:\\GDrive\\zotero\\Adepu\\adepu_2021_distributed_attack_detection_in_a_water_treatment_plant.pdf},
  journal = {IEEE Transactions on Dependable and Secure Computing},
  keywords = {coordinated attacks,Cyber physical systems,cyber security,distributed attack detection,industrial control systems,invariants,scada,water treatment plant},
  number = {1}
}

@article{adepuGeneralizedAttackerAttack2016,
  title = {Generalized {{Attacker}} and {{Attack Models}} for {{Cyber Physical Systems}}},
  author = {Adepu, Sridhar and Mathur, Aditya},
  year = {2016},
  volume = {1},
  pages = {283--292},
  publisher = {{IEEE}},
  issn = {07303157},
  doi = {10.1109/COMPSAC.2016.122},
  abstract = {An attacker model is proposed for Cyber Physical Systems (CPS). The attack models derived from the attacker model are used to generate parameterized attack procedures and functions that target a specific CPS. The proposed models capture both physical and cyber attacks and unify a number of existing attack models into a common framework useful for researchers in the experimental assessment of attack detection techniques. The generality of the models is shown by mapping a broad variety of existing attack models to the models proposed here, as well as generating attacks that are not found in the CPS design literature. The models have been used extensively in understanding the impact of cyber attacks on a water treatment system and in the design and assessment of detection mechanisms.},
  file = {D\:\\GDrive\\zotero\\Adepu\\adepu_2016_generalized_attacker_and_attack_models_for_cyber_physical_systems.pdf},
  isbn = {9781467388450},
  journal = {Proceedings - International Computer Software and Applications Conference},
  keywords = {Attacker model,Cyber and Physical Attacks,Cyber Physical Systems,Cyber Security,Secure Water Treatment system,Single-and multi-stage attacks}
}

@article{adepuInvestigationResponseWater2016,
  title = {An {{Investigation}} into the {{Response}} of a {{Water Treatment System}} to {{Cyber Attacks}}},
  author = {Adepu, Sridhar and Mathur, Aditya},
  year = {2016},
  volume = {2016-March},
  pages = {141--148},
  publisher = {{IEEE}},
  issn = {15302059},
  doi = {10.1109/HASE.2016.14},
  abstract = {An experimental investigation was undertaken to understand the impact of single-point cyber attacks on a Secure Water Treatment (SWaT) system. Cyber attacks were launched on SWaT through its SCADA server that connects to the Programmable Logic Controllers (PLCs) that in turn are connected to sensors and actuators. Attacks were designed to meet attacker objectives selected from a novel attacker model. Outcome of the experiments led to a better understanding of (a) the propagation of an attack across the system measured in terms of the number of components affected and (b) the behavior of the water treatment process in SWaT in response to the attacks. The observed response to various attacks was then used to propose attack detection mechanisms based on various physical properties measured during the treatment process.},
  file = {D\:\\GDrive\\zotero\\Adepu\\adepu_2016_an_investigation_into_the_response_of_a_water_treatment_system_to_cyber_attacks.pdf},
  isbn = {9781467399128},
  journal = {Proceedings of IEEE International Symposium on High Assurance Systems Engineering},
  keywords = {Attack detection,attack model,attacker model,cyber attacks,Cyber Physical Systems,cyber security,Secure Water Treatment testbed}
}

@techreport{adveLLVALowlevelVirtual2003,
  title = {{{LLVA}}: {{A Low}}-Level {{Virtual Instruction Set Architecture}}},
  author = {Adve, Vikram and Lattner, Chris and Brukman, Michael and Shukla, Anand and Gaeke, Brian},
  year = {2003},
  abstract = {A virtual instruction set architecture (V-ISA) implemented via a processor-specific software translation layer can provide great flexibility to processor designers. Recent examples such as Crusoe and DAISY, however , have used existing hardware instruction sets as virtual ISAs, which complicates translation and optimization. In fact, there has been little research on specific designs for a virtual ISA for processors. This paper proposes a novel virtual ISA (LLVA) and a translation strategy for implementing it on arbitrary hardware. The instruction set is typed, uses an infinite virtual register set in Static Single Assignment form, and provides explicit control-flow and dataflow information, and yet uses low-level operations closely matched to traditional hardware. It includes novel mechanisms to allow more flexible optimization of native code, including a flexible exception model and minor constraints on self-modifying code. We propose a translation strategy that enables offline translation and transparent offline caching of native code and profile information , while remaining completely OS-independent. It also supports optimizations directly on the representation at install-time, runtime, and offline between executions. We show experimentally that the virtual ISA is compact, it is closely matched to ordinary hardware instruction sets, and permits very fast code generation, yet has enough high-level information to permit sophisticated program analyses and optimizations.},
  file = {D\:\\GDrive\\zotero\\Adve et al\\adve_et_al_llva.pdf}
}

@misc{AdviceReadingAcademic,
  title = {Advice on {{Reading Academic Papers}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\EDDKUXYA\\2012-02-15-advice-on-reading-academic-papers.html},
  howpublished = {https://www.cc.gatech.edu/\textasciitilde akmassey/posts/2012-02-15-advice-on-reading-academic-papers.html}
}

@phdthesis{adviserEmulatorSpeedupUsing2010,
  title = {Emulator {{Speed}}-up {{Using JIT}} and {{LLVM}}},
  author = {Adviser, Wennborg and Walfridsson, Krister and Skeppstedt, Jonas},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Adviser\\adviser_2010_emulator_speed-up_using_jit_and_llvm.pdf;D\:\\GDrive\\zotero\\Wennborg\\wennborg_2010_emulator_speed-up_using_jit_and_llvm.pdf}
}

@article{adyaFastKeyvalueStores,
  title = {Fast Key-Value Stores : {{An}} Idea Whose Time Has Come and Gone},
  author = {Adya, Atul and Grandl, Robert and Myers, Daniel and Qin, Henry},
  file = {D\:\\GDrive\\zotero\\Adya\\adya_fast_key-value_stores.pdf},
  isbn = {9781450367271},
  keywords = {2019,acm reference format,an idea whose time,atul adya,caches,daniel myers and henry,distributed systems,fast key-value stores,has come and gone,key-value stores,qin,robert grandl}
}

@phdthesis{adyaWeakConsistencyGeneralized1999,
  title = {Weak {{Consistency}}: {{A Generalized Theory}} and {{Optimistic Implementations}} for {{Distributed Transactions Chairman}}, {{Departmental Committee}} on {{Graduate Students}}},
  author = {Adya, Atul and Liskov, Barbara H and Smith, Arthur C},
  year = {1999},
  file = {D\:\\GDrive\\zotero\\Adya\\adya_1999_weak_consistency.pdf}
}

@techreport{aggarwalAnalysisPrivateBrowsing,
  title = {An {{Analysis}} of {{Private Browsing Modes}} in {{Modern Browsers}}},
  author = {Aggarwal, Gaurav and Bursztein, Elie and Jackson, Collin and Boneh, Dan},
  abstract = {We study the security and privacy of private browsing modes recently added to all major browsers. We first propose a clean definition of the goals of private browsing and survey its implementation in different browsers. We conduct a measurement study to determine how often it is used and on what categories of sites. Our results suggest that private browsing is used differently from how it is marketed. We then describe an automated technique for testing the security of private browsing modes and report on a few weaknesses found in the Firefox browser. Finally , we show that many popular browser extensions and plugins undermine the security of private browsing. We propose and experiment with a workable policy that lets users safely run extensions in private browsing mode.},
  file = {D\:\\GDrive\\zotero\\Aggarwal\\aggarwal_an_analysis_of_private_browsing_modes_in_modern_browsers.pdf},
  keywords = {ss}
}

@techreport{agrawalDynamicProgramSlicing1990,
  title = {Dynamic {{Program Slicing}}},
  author = {Agrawal, Hiralal and Horgan, Joseph R},
  year = {1990},
  abstract = {The conventional notion of a program slice|the set of all statements that might aaect the value of a variable occurrence|is totally independent of the program input values. Program debugging, however, involves analyzing the program behavior under the spe-ciic inputs that revealed the bug. In this paper we address the dynamic counterpart of the static slicing problem||nding all statements that really aaected the value of a variable occurrence for the given program inputs. Several approaches for computing dynamic slices are examined. The notion of a Dynamic Dependence Graph and its use in computing dynamic slices is discussed. We i n troduce the concept of a Reduced Dynamic Dependence Graph whose size does not depend on the length of execution history, which is unbounded in general, but whose size is bounded and is proportional to the number of dynamic slices arising during the program execution.},
  file = {D\:\\GDrive\\zotero\\Agrawal\\agrawal_dynamic_program_slicing.pdf}
}

@techreport{aguileraManyFacesConsistency2016,
  title = {The Many Faces of Consistency},
  author = {Aguilera, Marcos K and Terry, Douglas B},
  year = {2016},
  abstract = {The notion of consistency is used across different computer science disciplines from distributed systems to database systems to computer architecture. It turns out that consistency can mean quite different things across these disciplines, depending on who uses it and in what context it appears. We identify two broad types of consistency, state consistency and operation consistency, which differ fundamentally in meaning and scope. We explain how these types map to the many examples of consistency in each discipline.},
  file = {D\:\\GDrive\\zotero\\Aguilera\\aguilera_2016_the_many_faces_of_consistency.pdf}
}

@article{ahmedNoiseMattersUsing2018,
  title = {Noise {{Matters}}: {{Using Sensor}} and {{Process Noise Fingerprint}} to {{Detect Stealthy Cyber Attacks}} and {{Authenticate}} Sensors in {{CPS}}},
  author = {Ahmed, Chuadhry Mujeeb and Zhou, Jianying and Mathur, Aditya P},
  year = {2018},
  volume = {18},
  pages = {566--581},
  abstract = {A novel scheme is proposed to authenticate sensors and detect data integrity attacks in a Cyber Physical System (CPS). The proposed technique uses the hardware characteristics of a sensor and physics of a process to create unique patterns (herein termed as fingerprints) for each sensor. The sensor fingerprint is a function of sensor and process noise embedded in sensor measurements. Uniqueness in the noise appears due to manufacturing imperfections of a sensor and due to unique features of a physical process. To create a sensor's fingerprint a system-model based approach is used. A noise-based fingerprint is created during the normal operation of the system. It is shown that under data injection attacks on sensors, noise pattern deviations from the fingerprinted pattern enable the proposed scheme to detect attacks. Experiments are performed on a dataset from a real-world water treatment (SWaT) facility. A class of stealthy attacks is designed against the proposed scheme and extensive security analysis is carried out. Results show that a range of sensors can be uniquely identified with an accuracy as high as 98\%. Extensive sensor identification experiments are carried out on a set of sensors in SWaT testbed. The proposed scheme is tested on a variety of attack scenarios from the reference literature which are detected with high accuracy. CCS CONCEPTS \textbullet{} Security and privacy \textrightarrow{} Intrusion/anomaly detection; \textbullet{} Computer systems organization \textrightarrow{} Sensors and actuators; Embedded systems; Dependable and fault-tolerant systems and networks ;},
  file = {D\:\\GDrive\\zotero\\Ahmed\\ahmed_2018_noise_matters.pdf},
  isbn = {9781450365697},
  journal = {Dl.Acm.Org},
  keywords = {Attacks on Sen-sors,Authentication,CPS/ICS Security,Cyber Physical Systems,Device Fingerprinting,Physical Attacks,Security,Sensor Fingerprinting,Sensors and Actuators}
}

@article{ahmedNoisePrintAttackDetection2018,
  title = {{{NoisePrint}}: {{Attack}} Detection Using Sensor and Process Noise Fingerprint in Cyber Physical Systems},
  author = {Ahmed, Chuadhry Mujeeb and Qadeer, Rizwan and Ochoa, Mart{\'i}n and Murguia, Carlos and Zhou, Jianying and Mathur, Aditya P. and Ruths, Justin},
  year = {2018},
  pages = {483--497},
  doi = {10.1145/3196494.3196532},
  abstract = {An attack detection scheme is proposed to detect data integrity attacks on sensors in Cyber-Physical Systems (CPSs). A combined fingerprint for sensor and process noise is created during the normal operation of the system. Under sensor spoofing attack, noise pattern deviates from the fingerprinted pattern enabling the proposed scheme to detect attacks. To extract the noise (difference between expected and observed value) a representative model of the system is derived. A Kalman filter is used for the purpose of state estimation. By subtracting the state estimates from the real system states, a residual vector is obtained. It is shown that in steady state the residual vector is a function of process and sensor noise. A set of time domain and frequency domain features is extracted from the residual vector. Feature set is provided to a machine learning algorithm to identify the sensor and process. Experiments are performed on two testbeds, a real-world water treatment (SWaT) facility and a water distribution (WADI) testbed. A class of zero-alarm attacks, designed for statistical detectors on SWaT are detected by the proposed scheme. It is shown that a multitude of sensors can be uniquely identified with accuracy higher than 90\% based on the noise fingerprint.},
  file = {D\:\\GDrive\\zotero\\Ahmed\\ahmed_2018_noiseprint.pdf},
  isbn = {9781450355766},
  journal = {ASIACCS 2018 - Proceedings of the 2018 ACM Asia Conference on Computer and Communications Security},
  keywords = {Actuators,CPS/ICS Security,Cyber Physical Systems,Device Fingerprinting,Physical Attacks,Security,Sensors}
}

@techreport{ahoEfficientStringMatching1975,
  title = {Efficient {{String Matching}}: {{An Aid}} to {{Bibliographic Search}}},
  author = {Aho, Alfred V and Corasick, Margaret J},
  year = {1975},
  abstract = {This paper describes a simple, efficient algorithm to locate all occurrences of any of a finite number of keywords in a string of text. The algorithm consists of constructing a finite state pattern matching machine from the keywords and then using the pattern matching machine to process the text string in a single pass. Construction of the pattern matching machine takes time proportional to the sum of the lengths of the keywords. The number of state transitions made by the pattern matching machine in processing the text string is independent of the number of keywords. The algorithm has been used to improve the speed of a library bibliographic search program by a factor of 5 to 10.},
  file = {D\:\\GDrive\\zotero\\Aho\\aho_1975_efficient_string_matching.pdf},
  keywords = {371,522,525,and Phrases: keywords and phrases,bibliographic search,computational complexity CR Categories: 374,finite state machines,information re-trieval,string pattern matching,text-editing}
}

@book{ahrensHowTakeSmart2017,
  title = {How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking: For Students, Academics and Nonfiction Book Writers},
  shorttitle = {How to Take Smart Notes},
  author = {Ahrens, S{\"o}nke},
  year = {2017},
  publisher = {{CreateSpace}},
  address = {{North Charleston, SC}},
  file = {D\:\\GDrive\\zotero\\Ahrens\\ahrens_2017_how_to_take_smart_notes.pdf},
  isbn = {978-1-5428-6650-7},
  language = {eng}
}

@techreport{akidauDataflowModelPractical2150,
  title = {The {{Dataflow Model}}: {{A Practical Approach}} to {{Balancing Correctness}}, {{Latency}}, and {{Cost}} in {{Massive}}-{{Scale}}, {{Unbounded}}, {{Out}}-of-{{Order Data Processing}}},
  author = {Akidau, Tyler and Bradshaw, Robert and Chambers, Craig and Chernyak, Slava and Fern{\'a}ndez, Rafael J and {Fern{\'a}ndez-Moctezuma}, Fern{\textasciiacute} and Lax, Reuven and Mcveety, Sam and Mills, Daniel and Perry, Frances and Schmidt, Eric and Whittle Google, Sam},
  year = {2150},
  abstract = {Unbounded, unordered, global-scale datasets are increasingly common in day-today business (e.g. Web logs, mobile usage statistics, and sensor networks). At the same time, consumers of these datasets have evolved sophisticated requirements , such as event-time ordering and windowing by features of the data themselves, in addition to an insatiable hunger for faster answers. Meanwhile, practicality dictates that one can never fully optimize along all dimensions of cor-rectness, latency, and cost for these types of input. As a result , data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions, often resulting in disparate implementations and systems. We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive, old data may be retracted, and the only way to make this problem tractable is via principled abstractions that allow the practitioner the choice of appropriate tradeoffs along the axes of interest: correctness, latency, and cost. In this paper, we present one such approach, the Dataflow Model 1 , along with a detailed examination of the semantics it enables, an overview of the core principles that guided its design, and a validation of the model itself via the real-world experiences that led to its development.},
  file = {D\:\\GDrive\\zotero\\Akidau\\akidau_2150_the_dataflow_model.pdf}
}

@article{akinsWholeNewWorld2015,
  title = {A {{Whole New World}}: {{Income Tax Considerations}} of the {{Bitcoin Economy}}},
  author = {Akins, Benjamin W. and Chapman, Jennifer L. and Gordon, Jason M.},
  year = {2015},
  volume = {12},
  pages = {24--56},
  issn = {1932-1821},
  doi = {10.5195/taxreview.2014.32},
  abstract = {A Whole New World: ~Income Tax Considerations of the Bitcoin Economy},
  file = {D\:\\GDrive\\zotero\\Akins\\akins_2015_a_whole_new_world.pdf},
  journal = {Pittsburgh Tax Review},
  number = {1}
}

@article{al-faresScalableCommodityData2008,
  title = {A Scalable, Commodity Data Center Network Architecture},
  author = {{Al-Fares}, Mohammad and Loukissas, Alexander and Vahdat, Amin},
  year = {2008},
  volume = {38},
  pages = {63--74},
  issn = {01464833},
  doi = {10.1145/1402946.1402967},
  abstract = {Today's data centers may contain tens of thousands of computers with significant aggregate bandwidth requirements. The network architecture typically consists of a tree of routing and switching elements with progressively more specialized and expensive equipment moving up the network hierarchy. Unfortunately, even when deploying the highest-end IP switches/routers, resulting topologies may only support 50\% of the aggregate bandwidth available at the edge of the network, while still incurring tremendous cost. Non-uniform bandwidth among data center nodes complicates application design and limits overall system performance. In this paper, we show how to leverage largely commodity Ethernet switches to support the full aggregate bandwidth of clusters consisting of tens of thousands of elements. Similar to how clusters of commodity computers have largely replaced more specialized SMPs and MPPs, we argue that appropriately architected and interconnected commodity switches may deliver more performance at less cost than available from today's higher-end solutions. Our approach requires no modifications to the end host network interface, operating system, or applications; critically, it is fully backward compatible with Ethernet, IP, and TCP. Copyright 2008 ACM.},
  file = {D\:\\GDrive\\zotero\\Al-Fares\\al-fares_2008_a_scalable,_commodity_data_center_network_architecture.pdf;D\:\\GDrive\\zotero\\Al-Fares\\al-fares_a_scalable,_commodity_data_center_network_architecture.pdf},
  isbn = {9781605581750},
  journal = {Computer Communication Review},
  keywords = {Data center topology,Equal-cost routing},
  number = {4}
}

@book{al-shaerReturnOrientedProgrammingReturns2010,
  title = {Return-{{Oriented Programming}} without {{Returns}}},
  author = {{Al-Shaer}, Ehab S. and Keromytis, Angelos. and Shmatikov, Vitaly. and {Association for Computing Machinery. Special Interest Group on Security}, Audit and {Association for Computing Machinery.}},
  year = {2010},
  publisher = {{Association for Computing Machinery}},
  file = {D\:\\GDrive\\zotero\\Al-Shaer\\al-shaer_2010_return-oriented_programming_without_returns.pdf;D\:\\GDrive\\zotero\\Checkoway\\checkoway_2010_return-oriented_programming_without_returns.pdf},
  isbn = {978-1-4503-0244-9},
  keywords = {Algorithms,Security and Protection General Terms Security}
}

@article{al-shakarchiDart2011,
  title = {Dart},
  author = {{Al-Shakarchi}, Eddie and Taylor, Ian},
  year = {2011},
  pages = {217--234},
  doi = {10.4018/978-1-59904-663-1.ch010},
  abstract = {The validity of the fluctuation theorems for total entropy production of a colloidal particle embedded in a non-Markovian heat bath driven by a time-dependent force in a harmonic potential is probed here. The dynamics of the system is modeled by the generalized Langevin equation with colored noise. The distribution function of the total entropy production is calculated and the detailed fluctuation theorem contains a renormalized temperature term which arises due to the non-Markovian characteristics of the thermal bath.},
  file = {D\:\\GDrive\\zotero\\Al-Shakarchi\\al-shakarchi_2011_dart.pdf},
  isbn = {1595930566},
  journal = {Intelligent Music Information Systems},
  keywords = {automated test,generation,interfaces,program verification,random testing,software testing}
}

@article{ala-mutkaStudyDifficultiesNovice2005,
  title = {A Study of the Difficulties of Novice Programmers. {{Digital Competence View}} Project {{A Study}} of the {{Difficulties}} of {{Novice Programmers}}},
  author = {{Ala-Mutka}, Kirsti and J{\"a}rvinen, Hannu-Matti and Lahtinen, Essi},
  year = {2005},
  doi = {10.1145/1067445.1067453},
  abstract = {Programming is related to several fields of technology, and many university students are studying the basics of it. Unfortunately, they often face difficulties already on the basic courses. This work studies the difficulties in learning programming in order to support developing learning materials for basic programming courses. The difficulties have to be recognized to be able to aid learning and teaching in an effective way. An international survey of opinions was organized for more than 500 students and teachers. This paper analyses the results of the survey. The survey provides information of the difficulties experienced and perceived when learning and teaching programming. The survey results also provide basis for recommendations for developing learning materials and approaches.},
  file = {D\:\\GDrive\\zotero\\Ala-Mutka\\ala-mutka_2005_a_study_of_the_difficulties_of_novice_programmers.pdf},
  keywords = {difficulties,K32 [Computers and education]: Computer and Information Science Education General Terms Human Factors,Languages Keywords Programming,learning,novices,teaching}
}

@article{alabaInternetThingsSecurity2017,
  title = {Internet of {{Things}} Security: {{A}} Survey},
  author = {Alaba, Fadele Ayotunde and Othman, Mazliza and Abaker, Ibrahim and Hashem, Targio and Alotaibi, Faiz},
  year = {2017},
  doi = {10.1016/j.jnca.2017.04.002},
  abstract = {The Internet of things (IoT) has recently become an important research topic because it integrates various sensors and objects to communicate directly with one another without human intervention. The requirements for the large-scale deployment of the IoT are rapidly increasing with a major security concern. This study focuses on the state-of-the-art IoT security threats and vulnerabilities by conducting an extensive survey of existing works in the area of IoT security. The taxonomy of the current security threats in the contexts of application, architecture, and communication is presented. This study also compares possible security threats in the IoT. We discuss the IoT security scenario and provide an analysis of the possible attacks. Open research issues and security implementation challenges in IoT security are described as well. This study aims to serve as a useful manual of existing security threats and vulnerabilities of the IoT heterogeneous environment and proposes possible solutions for improving the IoT security architecture.},
  file = {D\:\\GDrive\\zotero\\Alaba\\alaba_2017_internet_of_things_security.pdf},
  keywords = {IoT,Privacy,Security}
}

@article{albertTestCaseGeneration2014,
  title = {Test Case Generation by Symbolic Execution: {{Basic}} Concepts, a {{CLP}}-Based Instance, and Actor-Based Concurrency},
  author = {Albert, Elvira and Arenas, Puri and {G{\'o}mez-Zamalloa}, Miguel and Rojas, Jose Miguel},
  year = {2014},
  volume = {8483 LNCS},
  pages = {263--309},
  issn = {16113349},
  doi = {10.1007/978-3-319-07317-0_7},
  abstract = {The focus of this tutorial is white-box test case generation (TCG) based on symbolic execution. Symbolic execution consists in executing a program with the contents of its input arguments being symbolic variables rather than concrete values. A symbolic execution tree characterizes the set of execution paths explored during the symbolic execution of a program. Test cases can be then obtained from the successful branches of the tree. The tutorial is split into three parts: (1) The first part overviews the basic techniques used in TCG to ensure termination, handling heap-manipulating programs, achieving compositionality in the process and guiding TCG towards interesting test cases. (2) In the second part, we focus on a particular implementation of the TCG framework in constraint logic programming (CLP). In essense, the imperative object-oriented program under test is automatically transformed into an equivalent executable CLP-translated program. The main advantage of CLP-based TCG is that the standard mechanism of CLP performs symbolic execution for free. The PET system is an open-source software that implements this approach. (3) Finally, in the last part, we study the extension of TCG to actor-based concurrent programs. \textcopyright{} 2014 Springer International Publishing.},
  isbn = {9783319073163},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@techreport{alexanderssonRubyCompRubytoLLVMCompiler,
  title = {{{RubyComp A Ruby}}-to-{{LLVM Compiler Prototype RubyComp}}-{{A Ruby}}-to-{{LLVM Compiler Prototype}}},
  author = {Alexandersson, Anders},
  abstract = {Dynamic programming languages are not generally pre-compiled, but are interpreted at run-time. This approach has some serious drawbacks, e.g. complex deployment, human readable source code not preserving the intellectual properties of the developers and no ability to do optimizations at compile-time or run-time. In this paper we study the possibility to pre-compile the Ruby language, a dynamic object-oriented language, into Low Level Virtual Machine (LLVM) code for execution by the LLVM run-time, a compiler framework for lifelong optimization of an application. The result of the project is a Ruby compiler prototype, describing the infrastructure and overall design principles to map the highly dynamic properties of the Ruby language into low-level static constructs of the LLVM language. The LLVM framework supports different hardware platforms , and by using LLVM as the target of compilation the benefits of that portability are gained.},
  file = {D\:\\GDrive\\zotero\\Alexandersson\\alexandersson_rubycomp_a_ruby-to-llvm_compiler_prototype_rubycomp-a_ruby-to-llvm_compiler.pdf}
}

@article{alexanderssonRubyCompRubytoLLVMCompiler2004,
  title = {{{RubyComp A Ruby}}-to-{{LLVM Compiler Prototype}}},
  author = {Alexandersson, Anders},
  year = {2004},
  abstract = {Dynamic programming languages are not generally precompiled, but are interpreted at run-time. This approach has some serious drawbacks, e.g. complex deployment, human readable source code not preserving the intellectual properties of the developers and no ability to do optimizations at compile-time or run-time. In this paper we study the possibility to precompile the Ruby language, a dynamic object-oriented language, into Low Level Virtual Machine (LLVM) code for execution by the LLVM run-time, a compiler framework for lifelong optimization of an application. The result of the project is a Ruby compiler prototype, describing the infrastructure and overall design principles to map the highly dynamic properties of the Ruby language into low-level static constructs of the LLVM language. The LLVM framework supports different hardware platforms, and by using LLVM as the target of compilation the benefits of that portability are gained.},
  file = {D\:\\GDrive\\zotero\\Alexandersson\\alexandersson_2004_rubycomp_a_ruby-to-llvm_compiler_prototype.pdf},
  journal = {Methodology}
}

@techreport{alfonsoOverviewMathematicalTheory,
  title = {An {{Overview}} of the {{Mathematical Theory}} of {{Communication Particularly}} for {{Philosophers Interested}} in {{Information}}},
  author = {Alfonso, Simon D '}
}

@article{algorithmsAccountableAlgorithms2012,
  title = {Accountable {{Algorithms}}},
  author = {Algorithms, Accountable},
  year = {2012},
  pages = {1--7},
  file = {D\:\\GDrive\\zotero\\Algorithms\\algorithms_2012_accountable_algorithms.pdf},
  isbn = {9781339157030},
  number = {September}
}

@article{aliajGAROTAGeneralizedActive2021,
  title = {{{GAROTA}}: {{Generalized Active Root}}-{{Of}}-{{Trust Architecture}}},
  shorttitle = {{{GAROTA}}},
  author = {Aliaj, Esmerald and Nunes, Ivan De Oliveira and Tsudik, Gene},
  year = {2021},
  month = mar,
  abstract = {In this paper, we set out to systematically design a minimal active RoT for tiny low-end MCU-s. We begin with the following questions: (1) What functions and hardware support are required to guarantee actions in the presence of malware?, (2) How to implement this efficiently?, and (3) What security benefits stem from such an active RoT architecture? We then design, implement, formally verify, and evaluate GAROTA: Generalized Active Root-Of-Trust Architecture. We believe that GAROTA is the first clean-slate design of an active RoT for low-end MCU-s. We show how GAROTA guarantees that even a fully software-compromised low-end MCU performs a desired action. We demonstrate its practicality by implementing GAROTA in the context of three types of applications where actions are triggered by: sensing hardware, network events and timers. We also formally specify and verify GAROTA functionality and properties.},
  archiveprefix = {arXiv},
  eprint = {2102.07014},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Aliaj et al\\aliaj_et_al_2021_garota.pdf;C\:\\Users\\Admin\\Zotero\\storage\\VV7MXR3C\\2102.html},
  journal = {arXiv:2102.07014 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@article{alizadehCONGADistributedCongestionAware2014,
  title = {{{CONGA}}: {{Distributed Congestion}}-{{Aware Load Balancing}} for {{Datacenters}}},
  author = {Alizadeh, Mohammad and Edsall, Tom and Dharmapurikar, Sarang and Vaidyanathan, Ramanan and Chu, Kevin and Fingerhut, Andy and The Lam, Vinh and Matus, Francis and Pan, Rong and Yadav, Navindra and Varghese, George},
  year = {2014},
  doi = {10.1145/2619239.2626316},
  abstract = {We present the design, implementation, and evaluation of CONGA, a network-based distributed congestion-aware load balancing mechanism for datacenters. CONGA exploits recent trends including the use of regular Clos topologies and overlays for network vir-tualization. It splits TCP flows into flowlets, estimates real-time congestion on fabric paths, and allocates flowlets to paths based on feedback from remote switches. This enables CONGA to efficiently balance load and seamlessly handle asymmetry, without requiring any TCP modifications. CONGA has been implemented in custom ASICs as part of a new datacenter fabric. In testbed experiments , CONGA has 5\texttimes{} better flow completion times than ECMP even with a single link failure and achieves 2-8\texttimes{} better through-put than MPTCP in Incast scenarios. Further, the Price of Anarchy for CONGA is provably small in Leaf-Spine topologies; hence CONGA is nearly as effective as a centralized scheduler while being able to react to congestion in microseconds. Our main thesis is that datacenter fabric load balancing is best done in the network, and requires global schemes such as CONGA to handle asymmetry.},
  file = {D\:\\GDrive\\zotero\\Alizadeh\\alizadeh_2014_conga.pdf},
  isbn = {9781450328364},
  keywords = {C21 [Computer-Communication Networks]: Network Architecture and Design Keywords: Datacenter fabric,Distributed,Load balancing}
}

@article{alizadehDataCenterTCP2010,
  title = {Data {{Center TCP}} ({{DCTCP}})},
  author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A and Padhye, Jitendra and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
  year = {2010},
  abstract = {Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic. To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
  file = {D\:\\GDrive\\zotero\\Alizadeh\\alizadeh_2010_data_center_tcp_(dctcp).pdf},
  keywords = {C22 [Computer-Communication Networks]: Network Protocols General Terms: Measurement,ECN,Performance Keywords: Data center network,TCP}
}

@techreport{alizadehDCTCPEfficientPacket,
  title = {{{DCTCP}}: {{Efficient Packet Transport}} for the {{Commoditized Data Center}}},
  author = {Alizadeh, Mohammad and Greenberg, Albert and Maltz, David A and Padhye, Jitu and Patel, Parveen and Prabhakar, Balaji and Sengupta, Sudipta and Sridharan, Murari},
  abstract = {Cloud data centers host diverse applications, mixing in the same network a plethora of workflows that require small predictable latency with others requiring large sustained through-put. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal network impairments, such as queue buildup, buffer pressure, and incast, that lead to high application latencies. Using these insights, propose a variant of TCP, DCTCP, for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) and a simple multi-bit feedback mechanism at the host. We evaluate DCTCP at 1 and 10Gbps speeds, through benchmark experiments and analysis. In the data center, operating with commodity, shallow buffered switches, DCTCP delivers the same or better throughput than TCP, while using 90\% less buffer space. Unlike TCP, it also provides hight burst tolerance and low la-tency for short flows. While TCP's limitations cause our developers to restrict the traffic they send today, using DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further , a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.},
  file = {D\:\\GDrive\\zotero\\Alizadeh\\alizadeh_dctcp.pdf}
}

@article{alkimPostquantumKeyExchange2016,
  title = {Post-Quantum Key Exchange \textendash{} a New Hope},
  author = {Alkim, Erdem and Ducas, L{\'e}o and P{\"o}ppelmann, Thomas and Schwabe, Peter},
  year = {2016},
  pages = {18},
  abstract = {At IEEE Security \& Privacy 2015, Bos, Costello, Naehrig, and Stebila proposed an instantiation of Peikert's ring-learning-with-errors\textendash based (Ring-LWE) keyexchange protocol (PQCrypto 2014), together with an implementation integrated into OpenSSL, with the affirmed goal of providing post-quantum security for TLS. In this work we revisit their instantiation and stand-alone implementation. Specifically, we propose new parameters and a better suited error distribution, analyze the scheme's hardness against attacks by quantum computers in a conservative way, introduce a new and more efficient error-reconciliation mechanism, and propose a defense against backdoors and all-for-the-price-of-one attacks. By these measures and for the same lattice dimension, we more than double the security parameter, halve the communication overhead, and speed up computation by more than a factor of 8 in a portable C implementation and by more than a factor of 27 in an optimized implementation targeting current Intel CPUs. These speedups are achieved with comprehensive protection against timing attacks.},
  file = {D\:\\GDrive\\zotero\\Alkim et al\\alkim_et_al_2016_post-quantum_key_exchange_–_a_new_hope.pdf},
  language = {en}
}

@book{alleyCraftScientificPresentations2013,
  title = {The {{Craft}} of {{Scientific Presentations}}},
  author = {Alley, Michael},
  year = {2013},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-8279-7},
  file = {D\:\\GDrive\\zotero\\Alley\\alley_2013_the_craft_of_scientific_presentations2.pdf},
  isbn = {978-1-4419-8278-0 978-1-4419-8279-7},
  language = {en}
}

@book{alleyCraftScientificWriting2018,
  title = {The {{Craft}} of {{Scientific Writing}}},
  author = {Alley, Michael},
  year = {2018},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4419-8288-9},
  file = {D\:\\GDrive\\zotero\\Alley\\alley_2018_the_craft_of_scientific_writing.pdf},
  isbn = {978-1-4419-8287-2 978-1-4419-8288-9},
  language = {en}
}

@article{allspawFaultInjectionProduction,
  title = {Fault {{Injection}} in {{Production}}},
  author = {Allspaw, John},
  pages = {6},
  file = {D\:\\GDrive\\zotero\\Allspaw\\allspaw_fault_injection_in_production.pdf},
  journal = {QUALITY ASSURANCE},
  language = {en}
}

@techreport{almeidaIntervalTreeClocks,
  title = {Interval {{Tree Clocks}}: {{A Logical Clock}} for {{Dynamic Systems}}},
  author = {Almeida, Paulo S{\'e}rgio and Baquero, Carlos and Fonte, Victor},
  file = {D\:\\GDrive\\zotero\\Almeida\\almeida_interval_tree_clocks.pdf}
}

@techreport{altinbukenCommodifyingReplicatedState,
  title = {Commodifying {{Replicated State Machines}} with {{OpenReplica}}},
  author = {Alt{\i}nb{\"u}ken, Deniz and Sirer, G{\"u}n},
  abstract = {This paper describes OpenReplica, an open service that provides replication and synchronization support for large-scale distributed systems. OpenReplica is designed to commodify Paxos replicated state machines by providing infrastructure for their construction, deployment and maintenance. OpenReplica is based on a novel Paxos replicated state machine implementation that employs an object-oriented approach in which the system actively creates and maintains live replicas for user-provided objects. Clients access these replicated objects transparently as if they are local objects. OpenReplica supports complex distributed synchronization constructs through a multi-return mechanism that enables the replicated objects to control the execution flow of their clients, in essence providing blocking and non-blocking method in-vocations that can be used to implement richer synchronization constructs. Further, it supports elasticity requirements of cloud deployments by enabling any number of servers to be replaced dynamically. A rack-aware placement manager places replicas on nodes that are unlikely to fail together. Experiments with the system show that the latencies associated with replication are comparable to ZooKeeper, and that the system scales well.},
  file = {D\:\\GDrive\\zotero\\Altınbüken\\altınbüken_commodifying_replicated_state_machines_with_openreplica.pdf}
}

@article{amanHAttHybridRemote2020,
  title = {{{HAtt}}: {{Hybrid Remote Attestation}} for the {{Internet}} of {{Things With High Availability}}},
  shorttitle = {{{HAtt}}},
  author = {Aman, Muhammad Naveed and Basheer, Mohamed Haroon and Dash, Siddhant and Wong, Jun Wen and Xu, Jia and Lim, Hoon Wei and Sikdar, Biplab},
  year = {2020},
  month = aug,
  volume = {7},
  pages = {7220--7233},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2020.2983655},
  abstract = {The critical and sensitive nature of data that the Internet-of-Things (IoT) devices produce makes them an attractive target for cyber attacks. Among the various types of attacks, malware is becoming a major concern for the IoT device. This article proposes a remote attestation protocol, hybrid remote attestation, which ensures the high availability of IoT devices during the software attestation process. The proposed attestation technique uses a randomized approach to attest different parts of an IoT device's memory. We use physical unclonable functions (PUFs) to protect the secrets of an IoT device from physical attacks. The security analysis shows that the proposed attestation technique can effectively detect roving malware. Implementation of the proposed protocol on Raspberry Pi and AVR/ARM-based ATMEL microcontrollers and comparison with existing techniques shows that the proposed protocol results in significantly higher availability and lower energy consumption.},
  file = {D\:\\GDrive\\zotero\\Aman et al\\aman_et_al_2020_hatt.pdf},
  journal = {IEEE Internet of Things Journal},
  keywords = {Hardware,Internet of Things,Internet of Things (IoT),malware,Malware,network security,Privacy,Protocols,remote attestation,Security},
  number = {8}
}

@article{amdSoftwareTechniquesManaging2018,
  title = {Software {{Techniques}} for {{Managing Speculation}} on {{AMD Processors}}},
  author = {{AMD}},
  year = {2018},
  pages = {1--8},
  file = {D\:\\GDrive\\zotero\\AMD\\amd_2018_software_techniques_for_managing_speculation_on_amd_processors.pdf}
}

@techreport{amirCognitiveComputingProgramming,
  title = {Cognitive {{Computing Programming Paradigm}}: {{A Corelet Language}} for {{Composing Networks}} of {{Neurosynaptic Cores}}},
  author = {Amir, Arnon and Datta, Pallab and Risk, William P and Cassidy, Andrew S and Kusnitz, Jeffrey A and Esser, Steve K and Andreopoulos, Alexander and Wong, Theodore M and Flickner, Myron and {Alvarez-Icaza}, Rodrigo and Mcquinn, Emmett and Shaw, Ben and Pass, Norm and Modha, Dharmendra S},
  abstract = {Marching along the DARPA SyNAPSE roadmap, IBM unveils a trilogy of innovations towards the TrueNorth cognitive computing system inspired by the brain's function and efficiency. The sequential programming paradigm of the von Neumann architecture is wholly unsuited for TrueNorth. Therefore, as our main contribution, we develop a new programming paradigm that permits construction of complex cognitive algorithms and applications while being efficient for TrueNorth and effective for programmer productivity. The programming paradigm consists of (a) an abstraction for a TrueNorth program, named Corelet, for representing a network of neurosynaptic cores that encapsulates all details except external inputs and outputs; (b) an object-oriented Corelet Language for creating, composing, and decomposing corelets; (c) a Corelet Library that acts as an ever-growing repository of reusable corelets from which programmers compose new corelets; and (d) an end-to-end Corelet Laboratory that is a programming environment which integrates with the TrueNorth architectural simulator, Compass, to support all aspects of the programming cycle from design, through development, debugging, and up to deployment. The new paradigm seamlessly scales from a handful of synapses and neurons to networks of neurosynaptic cores of progressively increasing size and complexity. The utility of the new programming paradigm is underscored by the fact that we have designed and implemented more than 100 algorithms as corelets for TrueNorth in a very short time span.},
  file = {D\:\\GDrive\\zotero\\Amir\\amir_cognitive_computing_programming_paradigm.pdf}
}

@inproceedings{ammarSIMPLERemoteAttestation2020,
  title = {{{SIMPLE}}: {{A Remote Attestation Approach}} for {{Resource}}-Constrained {{IoT}} Devices},
  shorttitle = {{{SIMPLE}}},
  booktitle = {2020 {{ACM}}/{{IEEE}} 11th {{International Conference}} on {{Cyber}}-{{Physical Systems}} ({{ICCPS}})},
  author = {Ammar, Mahmoud and Crispo, Bruno and Tsudik, Gene},
  year = {2020},
  month = apr,
  pages = {247--258},
  issn = {2642-9500},
  doi = {10.1109/ICCPS48487.2020.00036},
  abstract = {Remote Attestation (RA) is a security service that detects malware presence on remote IoT devices by verifying their software integrity by a trusted party (verifier). There are three main types of RA: software (SW)-, hardware (HW)-, and hybrid (SW/HW)-based. Hybrid techniques obtain secure RA with minimal hardware requirements imposed on the architectures of existing microcontrollers units (MCUs). In recent years, considerable attention has been devoted to hybrid techniques since prior software-based ones lack concrete security guarantees in a remote setting, while hardware-based approaches are too costly for low-end MCUs. However, one key problem is that many already deployed IoT devices neither satisfy minimal hardware requirements nor support hardware modifications, needed for hybrid RA. This paper bridges the gap between software-based and hybrid RA by proposing a novel RA scheme based on software virtualization. In particular, it proposes a new scheme, called SIMPLE, which meets the minimal hardware requirements needed for secure RA via reliable software. SIMPLE depends on a formally-verified software-based memory isolation technique, called Security MicroVisor (S{$\mu$} V). Its reliability is achieved by extending the formally-verified safety and correctness properties to cover the entire software architecture of SIMPLE. Furthermore, SIMPLE is used to construct SIMPLE+, an efficient swarm attestation scheme for static and dynamic heterogeneous IoT networks. We implement and evaluate SIMPLE and SIMPLE+ on Atmel AVR architecture, a common MCU platform.},
  file = {D\:\\GDrive\\zotero\\Ammar et al\\ammar_et_al_2020_simple.pdf},
  keywords = {Computer architecture,Hardware,Malware,Read only memory,Reliability,Security}
}

@inproceedings{ammarSlimIoTScalableLightweight2018,
  title = {{{SlimIoT}}: {{Scalable Lightweight Attestation Protocol}} for the {{Internet}} of {{Things}}},
  shorttitle = {{{SlimIoT}}},
  booktitle = {2018 {{IEEE Conference}} on {{Dependable}} and {{Secure Computing}} ({{DSC}})},
  author = {Ammar, Mahmoud and Washha, Mahdi and Ramabhadran, Gowri Sankar and Crispo, Bruno},
  year = {2018},
  month = dec,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Kaohsiung, Taiwan}},
  doi = {10.1109/DESEC.2018.8625142},
  abstract = {The Internet of Things (IoT) is increasingly intertwined with critical industrial processes, yet contemporary IoT devices offer limited security features, creating a large new attack surface. Remote attestation is a well-known technique to detect cyber threats by remotely verifying the internal state of a networked embedded device through a trusted entity. Multi-device attestation has received little attention although current single-device approaches show limited scalability in IoT applications. Though recent work has yielded some proposals for scalable attestation, several aspects remain unexplored, and thus more research is required.},
  file = {D\:\\GDrive\\zotero\\Ammar et al\\ammar_et_al_2018_slimiot.pdf},
  isbn = {978-1-5386-5790-4},
  language = {en}
}

@article{ammarWISELightweightIntelligent2018,
  title = {{{WISE}}: {{Lightweight Intelligent Swarm Attestation Scheme}} for {{IoT}} ({{The Verifier}}'s {{Perspective}})},
  shorttitle = {{{WISE}}},
  author = {Ammar, Mahmoud and Washha, Mahdi and Crispo, Bruno},
  year = {2018},
  month = nov,
  abstract = {The growing pervasiveness of Internet of Things (IoT) expands the attack surface by connecting more and more attractive attack targets, i.e. embedded devices, to the Internet. One key component in securing these devices is software integrity checking, which typically attained with Remote Attestation (RA). RA is realized as an interactive protocol, whereby a trusted party, verifier, verifies the software integrity of a potentially compromised remote device, prover. In the vast majority of IoT applications, smart devices operate in swarms, thus triggering the need for efficient swarm attestation schemes. In this paper, we present WISE, the first intelligent swarm attestation protocol that aims to minimize the communication overhead while preserving an adequate level of security. WISE depends on a resource-efficient smart broadcast authentication scheme where devices are organized in fine-grained multi-clusters, and whenever needed, the most likely compromised devices are attested. The candidate devices are selected intelligently taking into account the attestation history and the diverse characteristics (and constraints) of each device in the swarm. We show that WISE is very suitable for resource-constrained embedded devices, highly efficient and scalable in heterogenous IoT networks, and offers an adjustable level of security.},
  archiveprefix = {arXiv},
  eprint = {1811.07366},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Ammar et al\\ammar_et_al_2018_wise.pdf;C\:\\Users\\Admin\\Zotero\\storage\\SXPS7A7Y\\1811.html},
  journal = {arXiv:1811.07366 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@article{amorimMethodologyMicroPoliciesMethodology2017,
  title = {A {{Methodology For Micro}}-{{Policies A Methodology For Micro}}-{{Policies}}},
  author = {Amorim, Arthur Azevedo De},
  year = {2017},
  file = {D\:\\GDrive\\zotero\\Amorim\\amorim_2017_a_methodology_for_micro-policies_a_methodology_for_micro-policies.pdf}
}

@techreport{amsdenSurveyFunctionalReactive,
  title = {A {{Survey}} of {{Functional Reactive Programming Concepts}}, {{Implementations}}, {{Optimizations}}, and {{Applications}}},
  author = {Amsden, Edward},
  abstract = {Functional Reactive Programming (FRP) provides a conceptual framework for implementing reactive systems. It is a relatively recent model of programming, but has already been explored, implemented , and optimized in several useful ways. We survey the literature on FRP, its implementation, optimization, and uses, and present ideas for further research, along with some examples.},
  file = {D\:\\GDrive\\zotero\\Amsden\\amsden_a_survey_of_functional_reactive_programming_concepts,_implementations,.pdf},
  keywords = {[] General Terms Programming Languages,Denotational,Functional,Functional Program-ming,Functional Reactive Programming Keywords Programming,Reactive,Time}
}

@article{anandPacketCachesRouters,
  title = {Packet {{Caches}} on {{Routers}}: {{The Implications ofUniversal Redundant Traffic Elimination}}},
  author = {Anand, Ashok},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Anand\\anand_packet_caches_on_routers.pdf}
}

@techreport{anatiInnovativeTechnologyCPU2013,
  title = {Innovative {{Technology}} for {{CPU Based Attestation}} and {{Sealing}}},
  author = {Anati, Ittai and Gueron, Shay and Johnson, Simon P and Scarlata, Vincent R},
  year = {2013},
  abstract = {Intel is developing the Intel\textregistered{} Software Guard Extensions (Intel\textregistered{} SGX) technology, an extension to Intel\textregistered{} Architecture for generating protected software containers. The container is referred to as an enclave. Inside the enclave, software's code, data, and stack are protected by hardware enforced access control policies that prevent attacks against the enclave's content. In an era where software and services are deployed over the Internet, it is critical to be able to securely provision enclaves remotely, over the wire or air, to know with confidence that the secrets are protected and to be able to save secrets in non-volatile memory for future use. This paper describes the technology components that allow provisioning of secrets to an enclave. These components include a method to generate a hardware based attestation of the software running inside an enclave and a means for enclave software to seal secrets and export them outside of the enclave (for example store them in non-volatile memory) such that only the same enclave software would be able un-seal them back to their original form.},
  file = {D\:\\GDrive\\zotero\\Anati et al\\Anati et al. - Innovative Technology for CPU Based Attestation an.pdf;D\:\\GDrive\\zotero\\Anati\\anati_innovative_technology_for_cpu_based_attestation_and_sealing.pdf;D\:\\GDrive\\zotero\\Anati\\anati_innovative_technology_for_cpu_based_attestation_and_sealing2.pdf},
  keywords = {Attestation,General Terms Measurement,Local Attestation,Measurement,Remote Attestation,Sealing,Security Keywords Enclave}
}

@book{anconamassimoanconaantoniocuniRPythonStepReconciling,
  title = {{{RPython}}: A {{Step Towards Reconciling Dynamically}} and {{Statically Typed OO Languages}} *},
  author = {Ancona Massimo Ancona Antonio Cuni, Davide and Matsakis ETH Zurich, Nicholas D},
  abstract = {Although the C-based interpreter of Python is reasonably fast, implementations on the CLI or the JVM platforms offers some advantages in terms of robustness and interop-erability. Unfortunately, because the CLI and JVM are primarily designed to execute statically typed, object-oriented languages, most dynamic language implementations cannot use the native bytecodes for common operations like method calls and exception handling; as a result, they are not able to take full advantage of the power offered by the CLI and JVM. We describe a different approach that attempts to preserve the flexibility of Python, while still allowing for efficient execution. This is achieved by limiting the use of the more dynamic features of Python to an initial, bootstrapping phase. This phase is used to construct a final RPython (Re-stricted Python) program that is actually executed. RPython is a proper subset of Python, is statically typed, and does not allow dynamic modification of class or method definitions; however, it can still take advantage of Python features such as mixins and first-class methods and classes. This paper presents an overview of RPython, including its design and its translation to both CLI and JVM bytecode. We show how the bootstrapping phase can be used to implement advanced features, like extensible classes and gen-erative programming. We also discuss what work remains before RPython is truly ready for general use, and compare the performance of RPython with that of other approaches.},
  file = {D\:\\GDrive\\zotero\\Ancona Massimo Ancona Antonio Cuni\\ancona_massimo_ancona_antonio_cuni_rpython.pdf;D\:\\GDrive\\zotero\\Ancona Massimo Ancona Antonio Cuni\\ancona_massimo_ancona_antonio_cuni_rpython2.pdf},
  isbn = {978-1-59593-868-8},
  keywords = {D32 [Programming Languages],Performance,Processors-compilers}
}

@techreport{andersenResilientOverlayNetworks2001,
  title = {Resilient {{Overlay Networks}}},
  author = {Andersen, David and Balakrishnan, Hari and Kaashoek, Frans and Morris, Robert},
  year = {2001},
  abstract = {A Resilient Overlay Network (RON) is an architecture that allows distributed Internet applications to detect and recover from path outages and periods of degraded performance within several seconds , improving over today's wide-area routing protocols that take at least several minutes to recover. A RON is an application-layer overlay on top of the existing Internet routing substrate. The RON nodes monitor the functioning and quality of the Internet paths among themselves, and use this information to decide whether to route packets directly over the Internet or by way of other RON nodes, optimizing application-specific routing metrics. Results from two sets of measurements of a working RON deployed at sites scattered across the Internet demonstrate the benefits of our architecture. For instance, over a 64-hour sampling period in March 2001 across a twelve-node RON, there were 32 significant outages, each lasting over thirty minutes, over the 132 measured paths. RON's routing mechanism was able to detect, recover, and route around all of them, in less than twenty seconds on average, showing that its methods for fault detection and recovery work well at discovering alternate paths in the Internet. Furthermore, RON was able to improve the loss rate, latency, or throughput perceived by data transfers; for example, about 5\% of the transfers doubled their TCP throughput and 5\% of our transfers saw their loss probability reduced by 0.05. We found that forwarding packets via at most one intermediate RON node is sufficient to overcome faults and improve performance in most cases. These improvements, particularly in the area of fault detection and recovery, demonstrate the benefits of moving some of the control over routing into the hands of end-systems.},
  file = {D\:\\GDrive\\zotero\\Andersen\\andersen_2001_resilient_overlay_networks.pdf}
}

@book{andersonSecurityEngineering2020,
  title = {Security {{Engineering}}: {$<$}br{$>$}},
  shorttitle = {Security {{Engineering}}},
  author = {Anderson, Ross},
  year = {2020},
  publisher = {{John Wiley and Sons}},
  address = {{Indianapolis}},
  isbn = {978-1-119-64278-7}
}

@book{andersonSecurityEngineeringGuide2020,
  title = {Security Engineering: A Guide to Building Dependable Distributed Systems},
  shorttitle = {Security Engineering},
  author = {Anderson, Ross},
  year = {2020},
  edition = {Third edition},
  publisher = {{Wiley}},
  address = {{Indianapolis, Indiana}},
  file = {D\:\\GDrive\\zotero\\Anderson\\anderson_2020_security_engineering.pdf},
  isbn = {978-1-119-64278-7},
  language = {eng}
}

@techreport{andOrganizingProgramsClasses1991,
  title = {Organizing {{Programs Without Classes}} *},
  author = {And, Lisp and Computation, Symbolic},
  year = {1991},
  volume = {4},
  pages = {223--242},
  abstract = {All organizational functions carried out by classes can be accomplished in a simple and natural way by object inheritance in classless languages, with no need for special mechanisms. A single model-dividing types into prototypes and traits-supports sharing of behavior and extending or replacing representations. A natural extension, dynamic object inheritance, can model behavioral modes. Object inheritance can also be used to provide structured name spaces for well-known objects. Classless languages can even express "class-based" encapsulation. These stylized uses of object inheritance become instantly recognizable idioms, and extend the repertory of organizing principles to cover a wider range of programs.},
  file = {D\:\\GDrive\\zotero\\And\\and_1991_organizing_programs_without_classes.pdf},
  journal = {An International Journal}
}

@book{andrebarrosoDatacenterComputerIntroduction2009,
  title = {The {{Datacenter}} as a {{Computer}}: {{An Introduction}} to the {{Design}} of {{Warehouse}}-{{Scale Machines}}},
  author = {Andr{\'e} Barroso, Luiz and H{\"o}lzle, Urs},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\André Barroso\\andré_barroso_the_datacenter_as_a_computer.pdf}
}

@article{androulakiartembargervitabortnikovHyperledgerFabricDistributed,
  title = {Hyperledger {{Fabric}}: {{A Distributed Operating System}} for {{Permissioned Blockchains}}},
  author = {Androulaki Artem Barger Vita Bortnikov, Elli and Cachin Konstantinos Christidis Angelo De Caro David Enyeart, Christian and Ferris Gennady Laventman Yacov Manevich, Christopher and Muralidharan, Srinivasan and Murthy, Chet and Nguyen, Binh and Sethi Gari Singh Keith Smith Alessandro Sorniotti, Manish and Stathakopoulou Marko Vukoli{\'c} Sharon Weed Cocco Jason Yellick, Chrysoula and Androulaki, Elli and Barger, Artem and Bortnikov, Vita and Cachin, Christian and Christidis, Konstanti-nos and De Caro, Angelo and Enyeart, David and Ferris, Christopher and Laventman, Gen-nady and Manevich, Yacov and Sethi, Manish and Singh, Gari and Smith, Keith and Sorniotti, Alessandro and Stathakopoulou, Chrysoula and Vukoli{\'c}, Marko and Weed Cocco, Sharon},
  volume = {18},
  doi = {10.1145/3190508.3190538},
  abstract = {Fabric is a modular and extensible open-source system for deploying and operating permissioned blockchains and one of the Hyperledger projects hosted by the Linux Foundation (www.hyperledger.org). Fabric is the first truly extensible blockchain system for running distributed applications. It supports modular consensus protocols, which allows the system to be tailored to particular use cases and trust models. Fabric is also the first blockchain system that runs distributed applications written in standard, general-purpose programming languages, without systemic dependency on a native cryptocurrency. This stands in sharp contrast to existing block-chain platforms that require "smart-contracts" to be written in domain-specific languages or rely on a cryptocurrency. Fabric realizes the permissioned model using a portable notion of membership, which may be integrated with industry-standard identity management. To support such flexibility, Fabric introduces an entirely novel blockchain design and revamps the way blockchains cope with non-determinism, resource exhaustion, and performance attacks. This paper describes Fabric, its architecture, the rationale behind various design decisions, its most prominent implementation aspects, as well as its distributed application programming model. We further evaluate Fabric by implementing and benchmarking a Bitcoin-inspired digital currency. We show that Fabric achieves end-to-end throughput of more than 3500 transactions per second in certain popular deployment configurations, with sub-second latency, scaling well to over 100 peers.},
  file = {D\:\\GDrive\\zotero\\Androulaki Artem Barger Vita Bortnikov\\androulaki_artem_barger_vita_bortnikov_hyperledger_fabric.pdf;D\:\\GDrive\\zotero\\Androulaki\\androulaki_hyperledger_fabric.pdf},
  isbn = {9781450355841},
  journal = {EuroSys}
}

@article{andryscoSubnormalFloatingPoint2015,
  title = {On Subnormal Floating Point and Abnormal Timing},
  author = {Andrysco, Marc and Kohlbrenner, David and Mowery, Keaton and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
  year = {2015},
  volume = {2015-July},
  pages = {623--639},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2015.44},
  abstract = {We identify a timing channel in the floating point instructions of modern x86 processors: the running time of floating point addition and multiplication instructions can vary by two orders of magnitude depending on their operands. We develop a benchmark measuring the timing variability of floating point operations and report on its results. We use floating point data timing variability to demonstrate practical attacks on the security of the Fire fox browser (versions 23 through 27) and the Fuzz differentially private database. Finally, we initiate the study of mitigations to floating point data timing channels with lib fixed time fixed point, a new fixed-point, constant-time math library. Modern floating point standards and implementations are sophisticated, complex, and subtle, a fact that has not been sufficiently recognized by the security community. More work is needed to assess the implications of the use of floating point instructions in security-relevant software.},
  file = {D\:\\GDrive\\zotero\\Andrysco\\andrysco_2015_on_subnormal_floating_point_and_abnormal_timing.pdf},
  isbn = {9781467369497},
  journal = {Proceedings - IEEE Symposium on Security and Privacy}
}

@inproceedings{andryscoSubnormalFloatingPoint2015a,
  title = {On {{Subnormal Floating Point}} and {{Abnormal Timing}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Andrysco, Marc and Kohlbrenner, David and Mowery, Keaton and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
  year = {2015},
  month = may,
  pages = {623--639},
  publisher = {{IEEE}},
  address = {{San Jose, CA}},
  doi = {10.1109/SP.2015.44},
  abstract = {We identify a timing channel in the floating point instructions of modern x86 processors: the running time of floating point addition and multiplication instructions can vary by two orders of magnitude depending on their operands. We develop a benchmark measuring the timing variability of floating point operations and report on its results. We use floating point data timing variability to demonstrate practical attacks on the security of the Firefox browser (versions 23 through 27) and the Fuzz differentially private database. Finally, we initiate the study of mitigations to floating point data timing channels with libfixedtimefixedpoint, a new fixed-point, constant-time math library.},
  file = {D\:\\GDrive\\zotero\\Andrysco et al\\andrysco_et_al_2015_on_subnormal_floating_point_and_abnormal_timing.pdf},
  isbn = {978-1-4673-6949-7},
  language = {en}
}

@article{Anewapproachtolinearfilteringandpredictionproblems,
  title = {A-New-Approach-to-Linear-Filtering-and-Prediction-Problems},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HW7VFSVF\\a-new-approach-to-linear-filtering-and-prediction-problems.pdf}
}

@phdthesis{anthonyLowLevelVirtual2009,
  title = {Low {{Level Virtual Machine}} for {{Glasgow Haskell Compiler}}},
  author = {Anthony, David and Supervisor, Terei and Chakravarty, Manuel M T},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Anthony\\anthony_2009_low_level_virtual_machine_for_glasgow_haskell_compiler.pdf},
  keywords = {Compilers,GHC,Haskell,LLVM}
}

@article{antonakakisUnderstandingMiraiBotnet2017,
  title = {Understanding the {{Mirai Botnet}}},
  author = {Antonakakis, Manos and April, Tim and Bailey, Michael and Bernhard, Matthew and Bursztein, Elie and Cochran, Jaime and Durumeric, Zakir and Halderman, J Alex and Invernizzi, Luca and Kallitsis, Michalis and Kumar, Deepak and Lever, Chaz and Ma, Zane and Mason, Joshua and Menscher, Damian and Seaman, Chad and Sullivan, Nick and Thomas, Kurt and Zhou, Yi},
  year = {2017},
  pages = {19},
  abstract = {The Mirai botnet, composed primarily of embedded and IoT devices, took the Internet by storm in late 2016 when it overwhelmed several high-profile targets with massive distributed denial-of-service (DDoS) attacks. In this paper, we provide a seven-month retrospective analysis of Mirai's growth to a peak of 600k infections and a history of its DDoS victims. By combining a variety of measurement perspectives, we analyze how the botnet emerged, what classes of devices were affected, and how Mirai variants evolved and competed for vulnerable hosts. Our measurements serve as a lens into the fragile ecosystem of IoT devices. We argue that Mirai may represent a sea change in the evolutionary development of botnets \textemdash{} the simplicity through which devices were infected and its precipitous growth, demonstrate that novice malicious techniques can compromise enough low-end devices to threaten even some of the best-defended targets. To address this risk, we recommend technical and nontechnical interventions, as well as propose future research directions.},
  file = {D\:\\GDrive\\zotero\\Antonakakis et al\\antonakakis_et_al_2017_understanding_the_mirai_botnet.pdf},
  language = {en}
}

@phdthesis{antonioliDesignImplementationEvaluation2018,
  title = {Design, {{Implementation}}, and {{Evaluation}} of {{Secure Cyber}}-{{Physical}} and {{Wireless Systems}}},
  author = {Antonioli, Daniele and Siby, Sandra and Tippenhauer, Nils Ole},
  year = {2018},
  address = {{Cham}},
  doi = {10.1007/978-3-030-02641-7_19},
  collaborator = {Capkun, Srdjan and Chow, Sherman S. M.},
  file = {D\:\\GDrive\\zotero\\Antonioli et al\\antonioli_et_al_2018_design,_implementation,_and_evaluation_of_secure_cyber-physical_and_wireless.pdf},
  language = {en},
  school = {Springer International Publishing}
}

@article{aoudiTruthWillOut2018,
  title = {Truth Will out: {{Departure}}-Based Process-Level Detection of Stealthy Attacks on Control Systems},
  author = {Aoudi, Wissam and Iturbe, Mikel and Almgren, Magnus},
  year = {2018},
  pages = {817--831},
  issn = {15437221},
  doi = {10.1145/3243734.3243781},
  abstract = {Recent incidents have shown that Industrial Control Systems (ICS) are becoming increasingly susceptible to sophisticated and targeted attacks initiated by adversaries with high motivation, domain knowledge, and resources. Although traditional security mechanisms can be implemented at the IT-infrastructure level of such cyber-physical systems, the community has acknowledged that it is imperative to also monitor the process-level activity, as attacks on ICS may very well influence the physical process. In this paper, we present pasad, a novel stealthy-attack detection mechanism that monitors time series of sensor measurements in real time for structural changes in the process behavior. We demonstrate the effectiveness of our approach through simulations and experiments on data from real systems. Experimental results show that pasad is capable of detecting not only significant deviations in the process behavior, but also subtle attack-indicating changes, significantly raising the bar for strategic adversaries who may attempt to maintain their malicious manipulation within the noise level.},
  file = {D\:\\GDrive\\zotero\\Aoudi\\aoudi_2018_truth_will_out.pdf},
  isbn = {9781450356930},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Cyber-Physical Systems,Departure Detection,Industrial Control Systems,Intrusion Detection,Isometry Trick,Partial Isometry,Singular Spectrum Analysis,Stealthy Attacks}
}

@book{appelEngineeringCompiler2nd2012,
  title = {Engineering a {{Compiler}}, 2nd {{Edition}}},
  author = {Appel, Andrew W. and Ginsburg, Maia},
  year = {2012},
  doi = {10.1017/cbo9781139174930.010},
  file = {D\:\\GDrive\\zotero\\Appel_Ginsburg\\appel_ginsburg_2012_engineering_a_compiler,_2nd_edition.pdf},
  isbn = {978-0-12-088478-0}
}

@techreport{appenzellerSizingRouterBuffers2004,
  title = {Sizing {{Router Buffers General Terms}}},
  author = {Appenzeller, Guido and Keslassy, Isaac and Mckeown, Nick},
  year = {2004},
  abstract = {All Internet routers contain buffers to hold packets during times of congestion. Today, the size of the buffers is determined by the dynamics of TCP's congestion control algorithm. In particular, the goal is to make sure that when a link is congested, it is busy 100\% of the time; which is equivalent to making sure its buffer never goes empty. A widely used rule-of-thumb states that each link needs a buffer of size B = RT T \texttimes{} C, where RT T is the average round-trip time of a flow passing across the link, and C is the data rate of the link. For example, a 10Gb/s router linecard needs approximately 250ms \texttimes{} 10Gb/s = 2.5Gbits of buffers; and the amount of buffering grows linearly with the line-rate. Such large buffers are challenging for router manufacturers, who must use large, slow, off-chip DRAMs. And queueing delays can be long, have high variance, and may destabilize the congestion control algorithms. In this paper we argue that the rule-of-thumb (B = RT T \texttimes{} C) is now outdated and incorrect for backbone routers. This is because of the large number of flows (TCP connections) multiplexed together on a single backbone link. Using theory, simulation and experiments on a network of real routers, we show that a link with n flows requires no more than B = (RT T \texttimes{} C)/ {$\surd$} n, for long-lived or short-lived TCP flows. The consequences on router design are enormous: A 2.5Gb/s link carrying 10,000 flows could reduce its buffers by 99\% with negligible difference in throughput; and a 10Gb/s link carrying 50,000 flows requires only 10Mbits of buffering, which can easily be implemented using fast, on-chip SRAM.},
  file = {D\:\\GDrive\\zotero\\Appenzeller\\appenzeller_2004_sizing_router_buffers_general_terms.pdf},
  keywords = {bandwidth delay product,buffer size,Design,Performance Keywords Internet router,TCP}
}

@article{appleinc.IntroductionApplePlatform2019,
  title = {Introduction to {{Apple}} Platform Security},
  author = {{Apple Inc.}},
  year = {2019},
  abstract = {Apple designs security into the core of its platforms. Building on the experience of creating the world's most advanced mobile operating system, Apple has created security architectures that address the unique requirements of mobile, watch, desktop, and home. Every Apple device combines hardware, software, and services designed to work together for maximum security and a transparent user experience in service of the ultimate goal of keeping personal information safe. Custom security hardware powers critical security features. Software protections work to keep the operating system and third-party apps safe. Services provide a mechanism for secure and timely software updates, power a safer app ecosystem, secure communications and payments, and provide a safer experience on the Internet. Apple devices protect not only the device and its data, but the entire ecosystem, including everything users do locally, on networks, and with key Internet services. Just as we design our products to be simple, intuitive, and capable, we design them to be secure. Key security features, such as hardware-based device encryption, can't be disabled by mistake. Other features, such as Touch ID and Face ID, enhance the user experience by making it simpler and more intuitive to secure the device. And because many of these features are enabled by default, users or IT departments don't need to perform extensive configurations. This documentation provides details about how security technology and features are implemented within Apple platforms. It also helps organizations combine Apple platform security technology and features with their own policies and procedures to meet their specific security needs. The content is organized into the following topic areas: Hardware Security and Biometrics: The hardware that forms the foundation for security on Apple devices, including the Secure Enclave, a dedicated AES crypto engine, Touch ID, and Face ID. System Security: The integrated hardware and software functions that provide for the safe boot, update, and ongoing operation of Apple operating systems. Encryption and Data Protection: The architecture and design that protects user data if the device is lost or stolen, or if an unauthorized person attempts to use or modify it. App Security: The software and services that provide a safe app ecosystem and enable apps to run securely and without compromising platform integrity. Services Security: Apple's services for identification, password management, payments, communications, and finding lost devices. Network Security: Industry-standard networking protocols that provide secure authentication and encryption of data in transmission. Apple Platform Security 2},
  file = {D\:\\MEGA\\zotero\\Apple Inc.\\apple_inc._2019_introduction_to_apple_platform_security.pdf}
}

@techreport{ApplicationAwareSecurityReliability2007,
  title = {Toward {{Application}}-{{Aware Security}} and {{Reliability}}},
  year = {2007},
  file = {D\:\\GDrive\\zotero\\undefined\\2007_toward_application-aware_security_and_reliability.pdf}
}

@techreport{ardagnaAccessControlSmarter2008,
  title = {Access {{Control}} for {{Smarter Healthcare Using Policy Spaces}} \$},
  author = {Ardagna, Claudio A and De Capitani Di Vimercati, Sabrina and Foresti, Sara and Grandison, Tyrone W and Jajodia, Sushil and Samarati, Pierangela},
  year = {2008},
  volume = {1},
  abstract = {A fundamental requirement for the healthcare industry is that the delivery of care comes first and nothing should interfere with it. As a consequence, the access control mechanisms used in healthcare to regulate and restrict the disclosure of data are often bypassed in case of emergencies. This phenomenon, called "break the glass", is a common pattern in healthcare organizations and, though quite useful and mandatory in emergency situations, from a security perspective, it represents a serious system weakness. Malicious users, in fact, can abuse the system by exploiting the break the glass principle to gain unauthorized privileges and accesses. In this paper, we propose an access control solution aimed at better regulating break the glass exceptions that occur in healthcare systems. Our solution is based on the definition of different policy spaces, a language, and a composition algebra to regulate access to patient data and to balance the rigorous nature of traditional access control systems with the "delivery of care comes first" principle.},
  file = {D\:\\GDrive\\zotero\\Ardagna\\ardagna_2008_access_control_for_smarter_healthcare_using_policy_spaces_$.pdf},
  keywords = {Access control,break the glass,exceptions,policy spaces}
}

@article{Aresponsetocheritonandskeenscriticismofcausalandtotallyorderedcommunication,
  title = {A-Response-to-Cheriton-and-Skeens-Criticism-of-Causal-and-Totally-Ordered-Communication},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5E4JAREX\\a-response-to-cheriton-and-skeens-criticism-of-causal-and-totally-ordered-communication.pdf}
}

@article{arifUnderstandingUsageBehavior2014,
  title = {Towards {{Understanding}} the {{Usage Behavior}} of {{Google Cloud Users}}: {{The Mice}} and {{Elephants Phenomenon}}},
  author = {Arif, Omar and Rahman, Abdul - and Aida, Kento},
  year = {2014},
  doi = {10.1109/CloudCom.2014.75},
  abstract = {In the era of cloud computing, users encounter the challenging task of effectively composing and running their applications on the cloud. In an attempt to understand user behavior in constructing applications and interacting with typical cloud infrastructures, we analyzed a large utilization dataset of Google cluster. In the present paper, we consider user behavior in composing applications from the perspective of topology, maximum requested computational resources, and workload type. We model user dynamic behavior around the user's session view. Mass-Count disparity metrics are used to investigate the characteristics of underlying statistical models and to characterize users into distinct groups according to their composition and behavioral classes and patterns. The present study reveals interesting insight into the heterogeneous structure of the Google cloud workload.},
  file = {D\:\\GDrive\\zotero\\Arif\\arif_2014_towards_understanding_the_usage_behavior_of_google_cloud_users.pdf},
  isbn = {9781479940936}
}

@article{ariyapperumaSecurityVulnerabilitiesDNS2007,
  title = {Security Vulnerabilities in {{DNS}} and {{DNSSEC}}},
  author = {Ariyapperuma, Suranjith and Mitchell, Chris J.},
  year = {2007},
  pages = {335--342},
  doi = {10.1109/ARES.2007.139},
  abstract = {We present an analysis of security vulnerabilities in the Domain Name System (DNS) and the DNS Security Extensions (DNSSEC). DNS data that is provided by name servers lacks support for data origin authentication and data integrity. This makes DNS vulnerable to man in the middle (MITM) attacks, as well as a range of other attacks. To make DNS more robust, DNSSEC was proposed by the Internet Engineering Task Force (IETF). DNSSECprovides data origin authentication and integrity by using digital signatures. Although DNSSEC provides security for DNS data, it suffers from serious security and operational flaws. We discuss the DNS and DNSSEC architectures, and consider the associated security vulnerabilities. \textcopyright{} 2007 IEEE.},
  isbn = {0769527752},
  journal = {Proceedings - Second International Conference on Availability, Reliability and Security, ARES 2007},
  keywords = {Cryptography,DNS,DNSSEC,Security,ss}
}

@techreport{armbrustCloudsBerkeleyView2009,
  title = {Above the {{Clouds}}: {{A Berkeley View}} of {{Cloud Computing}}},
  author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D and Katz, Randy H and Konwinski, Andrew and Lee, Gunho and Patterson, David A and Rabkin, Ariel},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Armbrust\\armbrust_2009_above_the_clouds.pdf}
}

@inproceedings{armbrustSparkSQLRelational2015,
  title = {Spark {{SQL}}: {{Relational}} Data Processing in Spark},
  booktitle = {Proceedings of the {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Armbrust, Michael and Xin, Reynold S. and Lian, Cheng and Huai, Yin and Liu, Davies and Bradley, Joseph K. and Meng, Xiangrui and Kaftan, Tomer and Frankliny, Michael J. and Ghodsi, Ali and Zaharia, Matei},
  year = {2015},
  month = may,
  volume = {2015-May},
  pages = {1383--1394},
  publisher = {{Association for Computing Machinery}},
  issn = {07308078},
  doi = {10.1145/2723372.2742797},
  abstract = {Spark SQL is a new module in Apache Spark that integrates relational processing with Spark's functional programming API. Built on our experience with Shark, Spark SQL lets Spark programmers leverage the benefits of relational processing (e.g. declarative queries and optimized storage), and lets SQL users call complex analytics libraries in Spark (e.g. machine learning). Compared to previous systems, Spark SQL makes two main additions. First, it offers much tighter integration between relational and procedural processing, through a declarative DataFrame API that integrates with procedural Spark code. Second, it includes a highly extensible optimizer, Catalyst, built using features of the Scala programming language, that makes it easy to add composable rules, control code generation, and define extension points. Using Catalyst, we have built a variety of features (e.g. schema inference for JSON, machine learning types, and query federation to external databases) tailored for the complex needs of modern data analysis. We see Spark SQL as an evolution of both SQL-on-Spark and of Spark itself, offering richer APIs and optimizations while keeping the benefits of the Spark programming model.},
  file = {D\:\\GDrive\\zotero\\Armbrust\\armbrust_2015_spark_sql.pdf},
  isbn = {978-1-4503-2758-9},
  keywords = {Data warehouse,Databases,Hadoop,Machine learning,Spark}
}

@inproceedings{armbrustStructuredStreamingDeclarative2018,
  title = {Structured {{Streaming}}: {{A Declarative API}} for {{Real}}-{{Time Applications}} in {{Apache Spark}}},
  shorttitle = {Structured {{Streaming}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Armbrust, Michael and Das, Tathagata and Torres, Joseph and Yavuz, Burak and Zhu, Shixiong and Xin, Reynold and Ghodsi, Ali and Stoica, Ion and Zaharia, Matei},
  year = {2018},
  month = may,
  pages = {601--613},
  publisher = {{ACM}},
  address = {{Houston TX USA}},
  doi = {10.1145/3183713.3190664},
  abstract = {With the ubiquity of real-time data, organizations need streaming systems that are scalable, easy to use, and easy to integrate into business applications. Structured Streaming is a new high-level streaming API in Apache Spark based on our experience with Spark Streaming. Structured Streaming differs from other recent streaming APIs, such as Google Dataflow, in two main ways. First, it is a purely declarative API based on automatically incrementalizing a static relational query (expressed using SQL or DataFrames), in contrast to APIs that ask the user to build a DAG of physical operators. Second, Structured Streaming aims to support end-to-end real-time applications that integrate streaming with batch and interactive analysis. We found that this integration was often a key challenge in practice. Structured Streaming achieves high performance via Spark SQL's code generation engine and can outperform Apache Flink by up to 2\texttimes{} and Apache Kafka Streams by 90\texttimes. It also offers rich operational features such as rollbacks, code updates, and mixed streaming/batch execution. We describe the system's design and use cases from several hundred production deployments on Databricks, the largest of which process over 1 PB of data per month.},
  file = {D\:\\GDrive\\zotero\\Armbrust et al\\armbrust_et_al_2018_structured_streaming.pdf},
  isbn = {978-1-4503-4703-7},
  language = {en}
}

@techreport{armstrongMakingReliableDistributed2003,
  title = {Making Reliable Distributed Systems in the Presence of Sodware Errors {{Final}} Version (with Corrections)},
  author = {Armstrong, Joe},
  year = {2003},
  file = {D\:\\GDrive\\zotero\\Armstrong\\armstrong_2003_making_reliable_distributed_systems_in_the_presence_of_sodware_errors_final.pdf}
}

@article{arpTorbenPracticalSideChannel2015,
  title = {Torben: {{A Practical Side}}-{{Channel Attack}} for {{Deanonymizing Tor Communication}}},
  author = {Arp, Daniel and Yamaguchi, Fabian and Rieck, Konrad},
  year = {2015},
  doi = {10.1145/2714576.2714627},
  abstract = {The Tor network has established itself as de-facto standard for anonymous communication on the Internet, providing an increased level of privacy to over a million users worldwide. As a result, interest in the security of Tor is steadily growing, attracting researchers from academia as well as industry and even nation-state actors. While various attacks based on traffic analysis have been proposed, low accuracy and high false-positive rates in real-world settings still prohibit their application on a large scale. In this paper, we present Torben, a novel deanonymization attack against Tor. Our approach is considerably more reliable than existing traffic analysis attacks, simultaneously far less intrusive than browser exploits. The attack is based on an unfortunate interplay of technologies: (a) web pages can be easily manipulated to load content from untrusted origins and (b) despite encryption, low-latency anonymization networks cannot effectively hide the size of request-response pairs. We demonstrate that an attacker can abuse this interplay to design a side channel in the communication of Tor, allowing short web page markers to be transmitted to expose the web page a user visits over Tor. In an empirical evaluation with 60,000 web pages, our attack enables detecting these markers with an accuracy of over 91\% and no false positives.},
  file = {D\:\\GDrive\\zotero\\Arp et al\\arp_et_al_2015_torben.pdf},
  isbn = {9781450332453},
  keywords = {C20 [Computer-Communication Networks]: General-Security and Protection,I54 [Pattern Recognition]: Applications Keywords Anonymity,Side Channels,Traffic Analysis}
}

@inproceedings{arztFLOWDROIDPreciseContext2014,
  title = {{{FLOWDROID}}: {{Precise}} Context, Flow, Field, Object-Sensitive and Lifecycle-Aware Taint Analysis for {{Android}} Apps},
  booktitle = {{{ACM SIGPLAN Notices}}},
  author = {Arzt, Steven and Rasthofer, Siegfried and Fritz, Christian and Bodden, Eric and Bartel, Alexandre and Klein, Jacques and Le Traon, Yves and Octeau, Damien and McDaniel, Patrick},
  year = {2014},
  month = jun,
  volume = {49},
  pages = {259--269},
  publisher = {{Association for Computing Machinery}},
  issn = {15232867},
  doi = {10.1145/2594291.2594299},
  abstract = {\#FlowDroid. They do static taint analysis. Based on SOOT, IFDS framework. They handle multiple entrypoints. Callbacks support: they consider "well-known" (:-)) objcets passed as argument, to link the registration functions to all the callbacks. They map "components" to callbacks, and they extend the callgraph. They introduce the DroidBench benchmark.},
  file = {D\:\\GDrive\\zotero\\Arzt\\arzt_2014_flowdroid.pdf}
}

@book{associationforcomputingmachinery.FlexibleTimeManagement2004,
  title = {Flexible {{Time Management}} in {{Data Stream Systems}}},
  author = {{Association for Computing Machinery.} and {ACM Special Interest Group for Algorithms and Computation Theory.} and {Association for Computing Machinery. Special Interest Group on Management of Data.} and {SIGART.}},
  year = {2004},
  publisher = {{ACM Press}},
  abstract = {"ACM order number: 475040"--Title page verso.},
  file = {D\:\\MEGA\\zotero\\Association for Computing Machinery.\\association_for_computing_machinery._2004_flexible_time_management_in_data_stream_systems.pdf},
  isbn = {1-58113-858-X}
}

@article{associationforcomputingmachinery.specialinterestgroupondatacommunications.DefenseWirelessCarrier2009,
  title = {In {{Defense}} of {{Wireless Carrier Sense}}},
  author = {{Association for Computing Machinery. Special Interest Group on Data Communications.}},
  year = {2009},
  abstract = {"Computer communication review, volume. 39, number 4, October 2009." "ACM order number 80800904"--Page ii.},
  file = {D\:\\MEGA\\zotero\\Association for Computing Machinery. Special Interest Group on Data Communications.\\association_for_computing_machinery._special_interest_group_on_data_communications._2009_in_defense_of_wireless_carrier_sense.pdf}
}

@article{associationforcomputingmachinery.specialinterestgroupondatacommunications.PortLandScalableFaultTolerant2009,
  title = {{{PortLand}}: {{A Scalable Fault}}-{{Tolerant Layer}} 2 {{Data Center Network Fabric}}},
  author = {{Association for Computing Machinery. Special Interest Group on Data Communications.}},
  year = {2009},
  abstract = {"Computer communication review, volume. 39, number 4, October 2009." "ACM order number 80800904"--Page ii.},
  file = {D\:\\MEGA\\zotero\\Association for Computing Machinery. Special Interest Group on Data Communications.\\association_for_computing_machinery._special_interest_group_on_data_communications._2009_portland.pdf}
}

@article{associationforcomputingmachinery.specialinterestgroupondatacommunications.VL2ScalableFlexible2009,
  title = {{{VL2}}: {{A Scalable}} and {{Flexible Data Center Network}}},
  author = {{Association for Computing Machinery. Special Interest Group on Data Communications.}},
  year = {2009},
  abstract = {"Computer communication review, volume. 39, number 4, October 2009." "ACM order number 80800904"--Page ii.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery. Special Interest Group on Data Communications.\\association_for_computing_machinery._special_interest_group_on_data_communications._2009_vl2.pdf}
}

@article{associationforcomputingmachinery.specialinterestgroupondatacommunications.WhiteSpaceNetworking2009,
  title = {White {{Space Networking}} with {{Wi}}-{{Fi}} like {{Connectivity}}},
  author = {{Association for Computing Machinery. Special Interest Group on Data Communications.}},
  year = {2009},
  abstract = {"Computer communication review, volume. 39, number 4, October 2009." "ACM order number 80800904"--Page ii.},
  file = {D\:\\MEGA\\zotero\\Association for Computing Machinery. Special Interest Group on Data Communications.\\association_for_computing_machinery._special_interest_group_on_data_communications._2009_white_space_networking_with_wi-fi_like_connectivity.pdf}
}

@article{associationforcomputingmachinery.specialinterestgroupondatacommunicationsAmbientBackscatterWireless,
  title = {Ambient {{Backscatter}}: {{Wireless Communication Out}} of {{Thin Air}}},
  author = {{Association for Computing Machinery. Special Interest Group on Data Communications}},
  abstract = {Includes index.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery. Special Interest Group on Data Communications\\association_for_computing_machinery._special_interest_group_on_data_communications_ambient_backscatter.pdf}
}

@article{associationforcomputingmachinery.specialinterestgroupondatacommunicationsLessPainMost,
  title = {Less {{Pain}}, {{Most}} of the {{Gain}}: {{Incrementally Deployable ICN}}},
  author = {{Association for Computing Machinery. Special Interest Group on Data Communications}},
  abstract = {Includes index.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery. Special Interest Group on Data Communications\\association_for_computing_machinery._special_interest_group_on_data_communications_less_pain,_most_of_the_gain.pdf}
}

@article{associationforcomputingmachineryspecialinterestgroupondatacommunicationAccountableInternetProtocol,
  title = {Accountable {{Internet Protocol}} ({{AIP}})},
  author = {{Association for Computing Machinery Special Interest Group on Data Communication}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Data Communication\\association_for_computing_machinery_special_interest_group_on_data_communication_accountable_internet_protocol_(aip).pdf}
}

@book{associationforcomputingmachineryspecialinterestgroupondatacommunicationEthaneTakingControl,
  title = {Ethane: {{Taking Control}} of the {{Enterprise}}},
  author = {{Association for Computing Machinery Special Interest Group on Data Communication}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Data Communication\\association_for_computing_machinery_special_interest_group_on_data_communication_ethane.pdf},
  isbn = {978-1-59593-713-1}
}

@article{associationforcomputingmachineryspecialinterestgroupondatacommunicationHowSpeedySPDY,
  title = {How Speedy Is {{SPDY}}?},
  author = {{Association for Computing Machinery Special Interest Group on Data Communication} and {Association for Computing Machinery Special Interest Group on Operating Systems}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Data Communication\\association_for_computing_machinery_special_interest_group_on_data_communication_how_speedy_is_spdy.pdf}
}

@article{associationforcomputingmachineryspecialinterestgroupondatacommunicationVolunteerOrganizedPublicVPN,
  title = {Volunteer-{{Organized Public VPN Relay System}} with {{Blocking Resistance}} for {{Bypassing Government Censorship Firewalls}}},
  author = {{Association for Computing Machinery Special Interest Group on Data Communication} and {Association for Computing Machinery Special Interest Group on Operating Systems}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Data Communication\\association_for_computing_machinery_special_interest_group_on_data_communication_volunteer-organized_public_vpn_relay_system_with_blocking_resistance_for.pdf}
}

@book{associationforcomputingmachineryspecialinterestgrouponoperatingsystemsRouteBricksExploitingParallelismTo,
  title = {{{RouteBricks}}: {{Exploiting ParallelismTo Scale Software Routers}}},
  author = {{Association for Computing Machinery Special Interest Group on Operating Systems}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Operating Systems\\association_for_computing_machinery_special_interest_group_on_operating_systems_routebricks.pdf},
  isbn = {978-1-60558-752-3}
}

@article{associationforcomputingmachineryspecialinterestgrouponoperatingsystemsSimpleTestingCan,
  title = {Simple {{Testing Can Prevent Most Critical Failures}}},
  author = {{Association for Computing Machinery Special Interest Group on Operating Systems}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Operating Systems\\association_for_computing_machinery_special_interest_group_on_operating_systems_simple_testing_can_prevent_most_critical_failures.pdf}
}

@book{associationforcomputingmachineryspecialinterestgrouponprogramminglanguagesFindingRaceConditions,
  title = {Finding {{Race Conditions}} in {{Erlang}} with {{QuickCheck}} and {{Pulse}}},
  author = {{Association for Computing Machinery Special Interest Group on Programming Languages}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Association for Computing Machinery Special Interest Group on Programming Languages\\association_for_computing_machinery_special_interest_group_on_programming_languages_finding_race_conditions_in_erlang_with_quickcheck_and_pulse.pdf},
  isbn = {978-1-60558-332-7}
}

@techreport{atenieseNewApproachDNS2001,
  title = {A {{New Approach}} to {{DNS Security}} ({{DNSSEC}})},
  author = {Ateniese, Giuseppe and Mangard, Stefan},
  year = {2001},
  abstract = {The Domain Name System (DNS) is a distributed database that allows convenient storing and retrieving of resource records. DNS has been extended to provide security services (DNSSEC) mainly through public-key cryptography. We propose a new approach to DNSSEC that may result in a significantly more efficient protocol. We introduce a new strategy to build chains of trust from root servers to authoritative servers. The techniques we employ are based on symmetric-key cryptography.},
  file = {D\:\\GDrive\\zotero\\Ateniese\\ateniese_2001_a_new_approach_to_dns_security_(dnssec).pdf},
  keywords = {Authentication Protocols,Digital Signatures,Domain Name System Security (DNSSEC),Symmetric Encryption}
}

@article{AttackGraphSheyner,
  title = {{{AttackGraph}}\_{{Sheyner}}},
  file = {D\:\\GDrive\\zotero\\undefined\\attackgraph_sheyner.pdf}
}

@techreport{atzeiSurveyAttacksEthereum,
  title = {A Survey of Attacks on {{Ethereum}} Smart Contracts},
  author = {Atzei, Nicola and Bartoletti, Massimo and Cimoli, Tiziana},
  abstract = {Smart contracts are computer programs that can be correctly executed by a network of mutually distrusting nodes, without the need of an external trusted authority. Since smart contracts handle and transfer assets of considerable value, besides their correct execution it is also crucial that their implementation is secure against attacks which aim at stealing or tampering the assets. We study this problem in Ethereum, the most well-known and used framework for smart contracts so far. We analyse the security vulnerabilities of Ethereum smart contracts, providing a taxonomy of common programming pitfalls which may lead to vulnerabilities. We show a series of attacks which exploit these vulnera-bilities, allowing an adversary to steal money or cause other damage.},
  file = {D\:\\GDrive\\zotero\\Atzei\\atzei_a_survey_of_attacks_on_ethereum_smart_contracts.pdf}
}

@techreport{aumassonBLAKE2SimplerSmaller2013,
  title = {{{BLAKE2}}: Simpler, Smaller, Fast as {{MD5}}},
  shorttitle = {{{BLAKE2}}},
  author = {Aumasson, Jean-Philippe and Neves, Samuel and {Wilcox-O'Hearn}, Zooko and Winnerlein, Christian},
  year = {2013},
  abstract = {We present the hash function BLAKE2, an improved version of the SHA-3 finalist BLAKE optimized for speed in software. Target applications include cloud storage, intrusion detection, or version control systems. BLAKE2 comes in two main flavors: BLAKE2b is optimized for 64-bit platforms, and BLAKE2s for smaller architectures. On 64-bit platforms, BLAKE2 is often faster than MD5, yet provides security similar to that of SHA-3: up to 256-bit collision resistance, immunity to length extension, indifferentiability from a random oracle, etc. We specify parallel versions BLAKE2bp and BLAKE2sp that are up to 4 and 8 times faster, by taking advantage of SIMD and/or multiple cores. BLAKE2 reduces the RAM requirements of BLAKE down to 168 bytes, making it smaller than any of the five SHA-3 finalists, and 32\% smaller than BLAKE. Finally, BLAKE2 provides a comprehensive support for tree-hashing as well as keyed hashing (be it in sequential or tree mode).},
  file = {D\:\\GDrive\\zotero\\Aumasson et al\\aumasson_et_al_2013_blake2.pdf;C\:\\Users\\Admin\\Zotero\\storage\\W2L735JH\\322.html},
  keywords = {hash functions,secret-key cryptography,tree hashing},
  number = {322}
}

@article{Auniversalmodularactorformalismforartificialintelligence,
  title = {A-Universal-Modular-Actor-Formalism-for-Artificial-Intelligence},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\X6IABHL2\\a-universal-modular-actor-formalism-for-artificial-intelligence.pdf}
}

@article{AutomatedDerivationApplicationaware2009,
  title = {Automated {{Derivation Of Application}}-Aware {{Error And Attack Detectors}}},
  year = {2009},
  pages = {12--42},
  file = {D\:\\GDrive\\zotero\\undefined\\2009_automated_derivation_of_application-aware_error_and_attack_detectors.pdf}
}

@article{avgerinosAutomaticExploitGeneration2014,
  title = {Automatic Exploit Generation},
  author = {Avgerinos, Thanassis and Cha, Sang Kil and Rebert, Alexandre and Schwartz, Edward J. and Woo, Maverick and Brumley, David},
  year = {2014},
  month = feb,
  volume = {57},
  pages = {74--84},
  issn = {00010782},
  doi = {10.1145/2560217.2560219},
  abstract = {Att ackers commonly exploit buggy programs to break into computers. Security-critical bugs pave the way for attackers to install trojans, propagate worms, and use victim computers to send spam and launch denial-of-service attacks. A direct way, therefore, to make computers more secure is to find securitycritical bugs before they are exploited by attackers. Unfortunately, bugs are plentiful. For example, the Ubuntu Linux bug-management database listed more than 103,000 open bugs as of January 2013. Specific widely used programs (such as the Firefox Web browser and the Linux 3.x kernel) list 7,597 and 1,293 open bugs in their public bug trackers, respectively.a Other projects, including those that are closed-source, likely involve similar statistics. These are just the bugs we know; there is always the persistent threat of zero-day exploits, or attacks against previously unknown bugs. Among the thousands of known bugs, which should software developers fix first? Which are exploitable? \textcopyright{} 2014 ACM.},
  file = {D\:\\GDrive\\zotero\\Avgerinos\\avgerinos_2014_automatic_exploit_generation.pdf},
  journal = {Communications of the ACM},
  number = {2}
}

@techreport{awerbuchSparsePartitions,
  title = {Sparse {{Partitions}}},
  author = {Awerbuch, Baruch and Peleg, David},
  abstract = {This abstract presents a collection of clustering and decomposition techniques enabling the construction of sparse and locality preserving representations for arbitrary networks. These new clustering techniques have already found several powerful applications in the area of distributed network algorithms. Two of these applications are described in this abstract, namely, routing with polynomial communication-space tradeoff and online tracking of mobile users.},
  file = {D\:\\GDrive\\zotero\\Awerbuch\\awerbuch_sparse_partitions.pdf}
}

@techreport{axonPrivacyawarenessBlockchainbasedPKI,
  title = {Privacy-Awareness in {{Blockchain}}-Based {{PKI}}},
  author = {Axon, Louise},
  abstract = {Conventional public key infrastructure (PKI) designs are not optimal and contain security flaws; there is much work underway in improving PKI. The properties given by the Bitcoin blockchain and its derivatives are a natural solution to some of the problems with PKI-in particular, certificate transparency and elimination of single points of failure. Recently-proposed blockchain PKI designs are built as public ledgers linking identity with public key, giving no provision of privacy. We consider the suitability of a blockchain-based PKI for contexts in which PKI is required, but in which linking of identity with public key is undesirable ; specifically, we show that blockchain can be used to construct a privacy-aware PKI while simultaneously eliminating some of the problems encountered in conventional PKI.},
  file = {D\:\\GDrive\\zotero\\Axon\\axon_privacy-awareness_in_blockchain-based_pki.pdf}
}

@book{babicbabicCalystoScalablePrecise2008,
  title = {Calysto: {{Scalable}} and {{Precise Extended Static Checking}} *},
  author = {Babi{\textasciiacute}cbabi{\textasciiacute}c, Domagoj and Hu, Alan J},
  year = {2008},
  abstract = {Automatically detecting bugs in programs has been a long-held goal in software engineering. Many techniques exist, trading-off varying levels of automation, thoroughness of coverage of program behavior, precision of analysis, and scalability to large code bases. This paper presents the CALYSTO static checker, which achieves an unprecedented combination of precision and scalability in a completely automatic extended static checker. CALYSTO is interpro-cedurally path-sensitive, fully context-sensitive, and bit-accurate in modeling data operations-comparable coverage and precision to very expensive formal analyses-yet scales comparably to the leading, less precise, static-analysis-based tool for similar properties. Using CALYSTO, we have discovered dozens of bugs, completely automatically, in hundreds of thousands of lines of production , open-source applications, with a very low rate of false error reports. This paper presents the design decisions, algorithms, and optimizations behind CALYSTO's performance.},
  file = {D\:\\GDrive\\zotero\\Babi´cbabi´c\\babi´cbabi´c_2008_calysto.pdf},
  isbn = {978-1-60558-079-1},
  keywords = {D24 [Software/Program Verification]: General Terms Verification Keywords formal verification,static analysis,static checking}
}

@techreport{babicbabicStructuralAbstractionSoftware,
  title = {Structural {{Abstraction}} of {{Software Verification Conditions}} {$\star$}},
  author = {Babi{\textasciiacute}cbabi{\textasciiacute}c, Domagoj and Hu, Alan J},
  abstract = {Precise software analysis and verification require tracking the exact path along which a statement is executed (path-sensitivity), the different contexts from which a function is called (context-sensitivity), and the bit-accurate operations performed. Previously, verification with such precision has been considered too inefficient to scale to large software. In this paper, we present a novel approach to solving such verification conditions, based on an automatic abstraction-checking-refinement framework that exploits natural abstraction boundaries present in software. Experimental results show that our approach easily scales to over 200,000 lines of real C/C++ code.},
  file = {D\:\\GDrive\\zotero\\Babi´cbabi´c\\babi´cbabi´c_structural_abstraction_of_software_verification_conditions_⋆.pdf}
}

@techreport{backesInformationFlowPeerReviewing2007,
  title = {Information {{Flow}} in the {{Peer}}-{{Reviewing Process}} ({{Extended Abstract}})},
  author = {Backes, Michael and D{\"u}rmuth, Markus and Unruh, Dominique},
  year = {2007},
  abstract = {We investigate a new type of information flow in the electronic publishing process. We show that the use of PostScript in this process introduces serious confidentiality issues. In particular, we explain how the reviewer's anonymity in the peer-reviewing process can be compromised by maliciously prepared PostScript documents. A demonstration of this attack is available. We briefly discuss how this attack can be extended to other document formats as well.},
  file = {D\:\\GDrive\\zotero\\Backes\\backes_2007_information_flow_in_the_peer-reviewing_process_(extended_abstract).pdf}
}

@techreport{baconFastStaticAnalysis1996,
  title = {Fast {{Static Analysis}} of {{C}}++ {{Virtual Function Calls}}},
  author = {Bacon, David F and Sweeney, Peter F},
  year = {1996},
  abstract = {Virtual functions make code easier for programmers to reuse but also make it harder for compilers to analyze. We investigate the ability of three static analysis algorithms to improve C++ programs by resolving virtual function calls, thereby reducing compiled code size and reducing program complexity so as to improve both human and automated program understanding and analysis. In measurements of seven programs of significant size (5000 to 20000 lines of code each) we found that on average the most precise of the three algorithms resolved 71\% of the virtual function calls and reduced compiled code size by 25\%. This algorithm is very fast: it analyzes 3300 source lines per second on an 80 MHz Pow-erPC 601. Because of its accuracy and speed, this algorithm is an excellent candidate for inclusion in production C++ compilers.},
  file = {D\:\\GDrive\\zotero\\Bacon_Sweeney\\bacon_sweeney_1996_fast_static_analysis_of_c++_virtual_function_calls.pdf}
}

@techreport{bagwellIdealHashTrees,
  title = {Ideal {{Hash Trees}}},
  author = {Bagwell, Phil},
  abstract = {Hash Trees with nearly ideal characteristics are described. These Hash Trees require no initial root hash table yet are faster and use significantly less space than chained or double hash trees. Insert, search and delete times are small and constant, independent of key set size, operations are O(1). Small worst-case times for insert, search and removal operations can be guaranteed and misses cost less than successful searches. Array Mapped Tries(AMT), first described in Fast and Space Efficient Trie Searches, Bagwell [2000], form the underlying data structure. The concept is then applied to external disk or distributed storage to obtain an algorithm that achieves single access searches, close to single access inserts and greater than 80 percent disk block load factors. Comparisons are made with Linear Hashing, Litwin, Neimat, and Schneider [1993] and B-Trees, R.Bayer and E.M.McCreight [1972]. In addition two further applications of AMTs are briefly described, namely, Class/Selector dispatch tables and IP Routing tables. Each of the algorithms has a performance and space usage that is comparable to contemporary implementations but simpler.},
  file = {D\:\\GDrive\\zotero\\Bagwell\\bagwell_ideal_hash_trees.pdf},
  keywords = {Database,H4m [Information Systems]: Miscellaneous General Terms: Hashing,Hash Tables,Routers,Routing,Row Displacement,Searching}
}

@techreport{baiAutomatedCompileTimeRunTime,
  title = {Automated {{Compile}}-{{Time}} and {{Run}}-{{Time Techniques}} to {{Increase Usable Memory}} in {{MMU}}-{{Less Embedded Systems}}},
  author = {Bai, Lan S and Yang, Lei and Dick, Robert P},
  abstract = {Random access memory (RAM) is tightly-constrained in many embedded systems. This is especially true for the least expensive , lowest-power embedded systems, such as sensor network nodes and portable consumer electronics. The most widely-used sensor network nodes have only 4-10 KB of RAM and do not contain memory management units (MMUs). It is very difficult to implement increasingly complex applications under such tight memory constraints. Nonetheless, price and power consumption constraints make it unlikely that increases in RAM in these systems will keep pace with the requirements of applications. We propose the use of automated compile-time and run-time techniques to increase the amount of usable memory in MMU-less embedded systems. The proposed techniques do not increase hardware cost, and are designed to require few or no changes to existing applications. We have developed a fast compression algorithm well suited to this application, as well as run-time library routines and compiler transformations to control and optimize the automatic migration of application data between compressed and uncompressed memory regions. These techniques were experimentally evaluated on Crossbow TelosB sensor network nodes running a number of data collection and signal processing applications. The results indicate that available memory can be increased by up to 50\% with less than 10\% performance degradation for most benchmarks.},
  file = {D\:\\GDrive\\zotero\\Bai\\bai_automated_compile-time_and_run-time_techniques_to_increase_usable_memory_in.pdf},
  keywords = {D42 [Storage Manage-ment]: Virtual Memory; E4 [Coding and Information Theory]: Data Compaction and Compression General Terms: Design,Data Compression,Experimentation,Management,Per-formance Keywords: Embedded System,Wireless Sensor Network}
}

@book{baierPrinciplesModelChecking2008,
  title = {Principles {{Of Model Checking}}},
  author = {Baier, Christel and Katoen, Joost-Pieter},
  year = {2008},
  volume = {950},
  issn = {00155713},
  doi = {10.1093/comjnl/bxp025},
  abstract = {Our growing dependence on increasingly complex computer and software systems necessitates the development of formalisms, techniques, and tools for assessing functional properties of these systems. One such technique that has emerged in the last twenty years is model checking, which systematically (and automatically) checks whether a model of a given system satisfies a desired property such as deadlock freedom, invariants, or request-response properties. This automated technique for verification and debugging has developed into a mature and widely used approach with many applications. Principles of Model Checking offers a comprehensive introduction to model checking that is not only a text suitable for classroom use but also a valuable reference for researchers and practitioners in the field. The book begins with the basic principles for modeling concurrent and communicating systems, introduces different classes of properties (including safety and liveness), presents the notion of fairness, and provides automata- based algorithms for these properties. It introduces the temporal logics LTL and CTL, compares them, and covers algorithms for verifying these logics, discussing real-time systems as well as systems subject to random phenomena. Separate chapters treat such efficiency-improving techniques as abstraction and symbolic manipulation. The book includes an extensive set of examples (most of which run through several chapters) and a complete set of basic results accompanied by detailed proofs. Each chapter concludes with a summary, bibliographic notes, and an extensive list of exercises of both practical and theoretical nature.},
  isbn = {978-0-262-02649-9},
  journal = {MIT Press},
  pmid = {11275744}
}

@techreport{baileyLetterResearchStudents,
  title = {A {{Letter}} to {{Research Students}}},
  author = {Bailey, Duane A},
  file = {D\:\\GDrive\\zotero\\Bailey\\bailey_a_letter_to_research_students.pdf},
  keywords = {done}
}

@book{baileyTrustedVirtualContainers2010,
  title = {Trusted {{Virtual Containers}} on {{Demand}} *},
  author = {Bailey, Katelin A and Smith, Sean W},
  year = {2010},
  abstract = {TPM-based trusted computing aspires to use hardware and cryptography to provide a remote relying party with assurances about the trustworthiness of a computing environment. However, standard approaches to trusted computing are hampered in the areas of scalability, expressiveness, and flexibility. This paper reports on our research project to address these limitations by using TPMs inside OpenSo-laris: our kernel creates lightweight containers on demand, and uses DTrace and other tools to extend attestation to more nuanced runtime properties. We illustrate this work with prototype application scenarios from cyber infrastructure operating the U.S. power grid.},
  file = {D\:\\GDrive\\zotero\\Bailey\\bailey_2010_trusted_virtual_containers_on_demand.pdf},
  isbn = {978-1-4503-0095-7},
  keywords = {Categories and Subject Descriptors D46 [Software],Security and Protection General Terms Security}
}

@techreport{bairdSWIRLDSHASHGRAPHCONSENSUS2016,
  title = {{{THE SWIRLDS HASHGRAPH CONSENSUS ALGORITHM}}: {{FAIR}}, {{FAST}}, {{BYZANTINE FAULT TOLERANCE}}},
  author = {Baird, Leemon},
  year = {2016},
  abstract = {A new system, the Swirlds hashgraph consensus algorithm, is proposed for replicated state machines with guaranteed Byzantine fault tolerance. It achieves fairness, in the sense that it is difficult for an attacker to manipulate which of two transactions will be chosen to be first in the consensus order. It has complete asynchrony, no leaders, no round robin, no proof-of-work, eventual consensus with probability one, and high speed in the absence of faults. It is based on a gossip protocol, in which the participants don't just gossip about transactions. They gossip about gossip. They jointly build a hashgraph reflecting all of the gossip events. This allows Byzantine agreement to be achieved through virtual voting. Alice does not send Bob a vote over the Internet. Instead, Bob calculates what vote Alice would have sent, based on his knowledge of what Alice knows. This yields fair Byzantine agreement on a total order for all transactions, with very little communication overhead beyond the transactions themselves.},
  file = {D\:\\GDrive\\zotero\\Baird\\baird_2016_the_swirlds_hashgraph_consensus_algorithm.pdf},
  keywords = {Byzantine,Byzantine agreement,Byzantine fault tolerance,fair,fairness,gossip about gossip,hashgraph,replicated state machine,Swirlds,virtual voting}
}

@article{baJMonAttIntegrityMonitoring2017,
  title = {{{jMonAtt}}: {{Integrity}} Monitoring and Attestation of {{JVM}}-Based Applications in Cloud Computing},
  author = {Ba, Haihe and Zhou, Huaizhe and Bai, Shuai and Ren, Jiangchun and Wang, Zhiying and Ci, Linlin},
  year = {2017},
  pages = {419--423},
  doi = {10.1109/ICISCE.2017.94},
  abstract = {Cloud computing has expanded rapidly as a promising technology in the recent years and has drastically altered the majority of opinions about computing mode and application deployment. While the cloud has been proved to improve efficiency and earn benefit for a great number of various services providers, yet a good few enterprises still hesitate to move to the cloud because of security threats. In this paper, we present jMonAtt architecture to provide robust trustworthy guarantees for high-level cloud applications through the enforcement of the trustworthiness evaluation and dynamic attestation with the assistance of HotSpot virtual machine. Moreover, it has less impact on the execution of an attested target when our architecture is deployed in the cloud application system.},
  file = {D\:\\GDrive\\zotero\\Ba\\ba_2017_jmonatt.pdf},
  isbn = {9781538630136},
  journal = {Proceedings - 2017 4th International Conference on Information Science and Control Engineering, ICISCE 2017},
  keywords = {Cloud computing,Integrity monitoring,Remote attestation,Trustworthiness estimate}
}

@techreport{bakerEqualRightsFunctional1993,
  title = {Equal {{Rights}} for {{Functional Objects}} 1 or, {{The More Things Change}}, {{The More They Are}} the {{Same}} 2},
  author = {Baker, Henry G},
  year = {1993},
  volume = {4},
  pages = {2--27},
  abstract = {We argue that intensional object identity in object-oriented programming languages and databases is best defined operationally by side-effect semantics. A corollary is that "functional" objects have extensional semantics. This model of object identity, which is analogous to the normal forms of relational algebra, provides cleaner semantics for the value-transmission operations and built-in primitive equality predicate of a programming language, and eliminates the confusion surrounding "call-by-value" and "call-by-reference" as well as the confusion of multiple equality predicates. Implementation issues are discussed, and this model is shown to have significant performance advantages in persistent, parallel, distributed and multilingual processing environments. This model also provides insight into the "type equivalence" problem of Algol-68, Pascal and Ada.},
  file = {D\:\\GDrive\\zotero\\Baker\\baker_1993_equal_rights_for_functional_objects_1_or,_the_more_things_change,_the_more_they.pdf},
  journal = {ACM OOPS Messenger}
}

@techreport{bakerMegastoreProvidingScalable,
  title = {Megastore: {{Providing Scalable}}, {{Highly Available Storage}} for {{Interactive Services}}},
  author = {Baker, Jason and Bond, Chris and Corbett, James C and Furman, JJ and Khorlin, Andrey and Larson, James and Li, Yawei and Lloyd, Alexander and Yushprakh, Vadim},
  abstract = {Megastore is a storage system developed to meet the requirements of today's interactive online services. Megas-tore blends the scalability of a NoSQL datastore with the convenience of a traditional RDBMS in a novel way, and provides both strong consistency guarantees and high availability. We provide fully serializable ACID semantics within fine-grained partitions of data. This partitioning allows us to synchronously replicate each write across a wide area network with reasonable latency and support seamless failover between datacenters. This paper describes Megastore's semantics and replication algorithm. It also describes our experience supporting a wide range of Google production services built with Megastore.},
  file = {D\:\\GDrive\\zotero\\Baker\\baker_megastore.pdf},
  keywords = {Bigtable,C24 [Distributed Systems]: Distributed databases; H24 [Database Management]: Systems-concurrency,Design,distrib-uted databases General Terms Algorithms,Distributed transactions,Paxos,Performance,Reliability Keywords Large databases}
}

@techreport{bakerWhyStudentsEngage2008,
  title = {Why {{Students Engage}} in "{{Gaming}} the {{System}}" {{Behavior}} in {{Interactive Learning Environments}}},
  author = {Baker, Ryan and Walonoski, Jason and Heffernan, Neil and Roll, Ido and Corbett, Albert and Koedinger, Kenneth},
  year = {2008},
  volume = {19},
  pages = {185--224},
  abstract = {In recent years there has been increasing interest in the phenomena of "gaming the system," where a learner attempts to succeed in an educational environment by exploiting properties of the system's help and feedback rather than by attempting to learn the material. Developing environments that respond constructively and effectively to gaming depends upon understanding why students choose to game. In this article , we present three studies, conducted with two different learning environments, which present evidence on which student behaviors, motivations, and emotions are associated with the choice to game the system. We also present a fourth study to determine how teachers' perspectives on gaming behavior are similar to, and different from, researchers' perspectives and the data from our studies. We discuss what motivational and attitudinal patterns are associated with gaming behavior across studies, and what the implications are for the design of interactive learning environment.},
  file = {D\:\\GDrive\\zotero\\Baker\\baker_2008_why_students_engage_in_gaming_the_system_behavior_in_interactive_learning.pdf},
  journal = {Jl. of Interactive Learning Research},
  number = {2}
}

@article{bakkerGPUbasedPasswordCracking2010,
  title = {{{GPU}}-Based Password Cracking},
  author = {Bakker, Marcus and Jagt, Roel Van Der},
  year = {2010},
  pages = {62},
  abstract = {In this research the following question is answered: what should KPMG advice their clients regarding to password length and complexity, now GPU-based password cracking has become a reality. To be able to answer this question, tests with different tools and hashes were performed on a system with four high end GPUs. The test system showed an improvement of a factor fourteen in brute force speed in comparison with modern CPUs. KPMG's advice to their clients regarding password length and complexity should be one of the following (this applies to all the hashes that were researched): \textbullet{} Nine or more characters with lower and upper case letters, digits and punctuation marks. \textbullet{} Ten or more characters with lower and upper case letters and digits. \textbullet{} Twelve or more characters with lower case letters and digits. 2},
  file = {D\:\\GDrive\\zotero\\Bakker\\bakker_2010_gpu-based_password_cracking.pdf},
  journal = {University of Amsterdam, Systems and Network Engineering Working Paper}
}

@techreport{balakrishnanComparisonMechanismsImproving,
  title = {A {{Comparison}} of {{Mechanisms}} for {{Improving TCP Performance}} over {{Wireless Links}}},
  author = {Balakrishnan, Hari and Padmanabhan, Venkata N and Seshan, Srinivasan and Katz, Randy H},
  abstract = {Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant losses due to bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms , resulting in degraded end-to-end performance in wireless and lossy systems. In this paper, we compare several schemes designed to improve the performance of TCP in such networks. We classify these schemes into three broad categories: end-to-end protocols, where loss recovery is performed by the sender; link-layer protocols, that provide local reliability; and split-connection protocols, that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison. Our results show that a reliable link-layer protocol that is TCP-aware provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements .},
  file = {D\:\\GDrive\\zotero\\Balakrishnan\\balakrishnan_a_comparison_of_mechanisms_for_improving_tcp_performance_over_wireless_links.pdf}
}

@article{balakrishnanIntelligentDesignEnables2011,
  title = {Intelligent {{Design Enables Architectural Evolution}}},
  author = {Balakrishnan, Hari. and {ACM Digital Library.} and {ACM Special Interest Group on Data Communication.}},
  year = {2011},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Balakrishnan\\balakrishnan_2011_intelligent_design_enables_architectural_evolution.pdf}
}

@article{balakrishnanXIAArchitectureEvolvable2011,
  title = {{{XIA}}: {{An Architecture}} for an {{Evolvable}} and {{Trustworthy Internet}}},
  author = {Balakrishnan, Hari. and {ACM Digital Library.} and {ACM Special Interest Group on Data Communication.}},
  year = {2011},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Balakrishnan\\balakrishnan_2011_xia.pdf}
}

@inproceedings{balasubramanianSystemProgrammingRust2017,
  title = {System {{Programming}} in {{Rust}}: {{Beyond Safety}}},
  shorttitle = {System {{Programming}} in {{Rust}}},
  booktitle = {Proceedings of the 16th {{Workshop}} on {{Hot Topics}} in {{Operating Systems}}},
  author = {Balasubramanian, Abhiram and Baranowski, Marek S. and Burtsev, Anton and Panda, Aurojit and Rakamari{\'c}, Zvonimir and Ryzhyk, Leonid},
  year = {2017},
  month = may,
  pages = {156--161},
  publisher = {{ACM}},
  address = {{Whistler BC Canada}},
  doi = {10.1145/3102980.3103006},
  abstract = {Rust is a new system programming language that offers a practical and safe alternative to C. Rust is unique in that it enforces safety without runtime overhead, most importantly, without the overhead of garbage collection. While zero-cost safety is remarkable on its own, we argue that the superpowers of Rust go beyond safety. In particular, Rust's linear type system enables capabilities that cannot be implemented efficiently in traditional languages, both safe and unsafe, and that dramatically improve security and reliability of system software. We show three examples of such capabilities: zero-copy software fault isolation, efficient static information flow analysis, and automatic checkpointing. While these capabilities have been in the spotlight of systems research for a long time, their practical use is hindered by high cost and complexity. We argue that with the adoption of Rust these mechanisms will become commoditized.},
  file = {D\:\\GDrive\\zotero\\Balasubramanian et al\\balasubramanian_et_al_2017_system_programming_in_rust.pdf},
  isbn = {978-1-4503-5068-6},
  language = {en}
}

@article{baldoniSurveySymbolicExecution2018,
  title = {A Survey of Symbolic Execution Techniques},
  author = {Baldoni, Roberto and Coppa, Emilio and D'elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
  year = {2018},
  volume = {51},
  issn = {15577341},
  doi = {10.1145/3182657},
  abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program's authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the past four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience. C 2018 ACM.},
  file = {D\:\\GDrive\\zotero\\Baldoni\\baldoni_2018_a_survey_of_symbolic_execution_techniques.pdf},
  journal = {ACM Computing Surveys},
  keywords = {Concolic execution,Software testing,Static analysis,Symbolic execution},
  number = {3}
}

@techreport{ballardieCoreBasedTrees,
  title = {Core {{Based Trees}} ({{CBT}}) {{An Architecture}} for {{Scalable Inter}}-{{Domain Multicast Routing}}},
  author = {Ballardie, Tony and Francist, Paul and Bellcore, N J and Crowcroft, Jon},
  abstract = {One of the central problems in one-to-many wide-area communications is forming the delivery tree-the collection of nodes and links that a multicast packet traverses.},
  file = {D\:\\GDrive\\zotero\\Ballardie\\ballardie_core_based_trees_(cbt)_an_architecture_for_scalable_inter-domain_multicast.pdf}
}

@techreport{ballAutomaticPredicateAbstraction2001,
  title = {Automatic {{Predicate Abstraction}} of {{C Programs}}},
  author = {Ball, Thomas and Majumdar, Rupak and Millstein, Todd and Rajamani, Sriram K},
  year = {2001},
  abstract = {Model checking has been widely successful in validating and debugging designs in the hardware and protocol domains. However, state-space explosion limits the applicability of model checking tools, so model checkers typically operate on abstractions of systems. Recently, there has been significant interest in applying model checking to software. For infinite-state systems like software, abstraction is even more critical. Techniques for abstracting software are a prerequisite to making software model checking a reality. We present the first algorithm to automatically construct a predicate abstraction of programs written in an industrial programming language such as C, and its implementation in a tool-C2bp. The C2bp tool is part of the SLAM toolkit, which uses a combination of predicate abstraction, model checking, symbolic reasoning, and iterative refinement to statically check temporal safety properties of programs. Predicate abstraction of software has many applications, including detecting program errors, synthesizing program invariants, and improving the precision of program analyses through predicate sensitivity. We discuss our experience applying the C2bp predicate abstraction tool to a variety of problems, ranging from checking that list-manipulating code preserves heap invariants to finding errors in Windows NT device drivers.},
  file = {D\:\\GDrive\\zotero\\Ball\\ball_2001_automatic_predicate_abstraction_of_c_programs.pdf}
}

@techreport{ballSLAMProjectDebugging2002,
  title = {The {{SLAM Project}}: {{Debugging System Software}} via {{Static Analysis}}*},
  author = {Ball, Thomas and Rajamani, Sriram K},
  year = {2002},
  abstract = {The goal of the SLAM project is to check whether or not a program obeys "API usage rules" that specif[y what it means to be a good client of an API. The SLAM toolkit statically analyzes a C program to determine whether or not it violates given usage rules. The toolkit has two unique aspects: it does not require the programmer to annotate the source program (invariants are inferred); it minimizes noise (false error messages) through a process known as "counterexample-driven refinement". SLAM exploits and extends results fi'om program analysis, model checking and automated deduction. \vphantom\{\}V\textasciitilde{} have successfully applied the SLAM toolkit to Windows XP device drivers, to both validate behavior and find defects in their usage of kernel APIs. Context. Today, many programmers are realizing the benefits of using languages with static type systems. By providing simple specifications about the tbrm of program data, programmers receive useful compile-time error messages or guarantees about the behavior of their (type-correct) programs. Getting additional checking beyond the confines of a particular type system generally requires programmers to use assertions and pertbrm testing. A number of projects have started to tbcus on statically checking programs against user-supplied specifications, using techniques from program analysis [18, 19], model checking [21, 17, 22], and automated deduction [16, 12]. Specification. The goal of the SLAM project is to check temporal sat{$>$}ty properties of sequential C programs [7]. Roughly stated, temporal sat\textasciitilde ty properties are those properties whose violation is witnessed by a finite execution trace (see [24] ibr a formal definition). A simple example of a sat{$>$}ty property is that a lock should be alternatingly acquired and released. \vphantom\{\}\textyen\textasciitilde{} encode temporal sat\textasciitilde ty properties in a language called Sac (Specification Language for Interface Checking) [9], which allows the definition of a sat\textasciitilde ty automaton [30, 29] that monitors the execution behavior of a program at the level of function calls and returns. The *Presented by the first author.},
  file = {D\:\\GDrive\\zotero\\Ball\\ball_2002_the_slam_project.pdf}
}

@techreport{balonBibaSecurityModel2004,
  title = {The {{Biba Security Model}}},
  author = {Balon, Nathan and Thabet, Ishraq},
  year = {2004},
  file = {D\:\\GDrive\\zotero\\Balon\\balon_2004_the_biba_security_model.pdf}
}

@techreport{bangAnyDBArchitecturelessDBMS,
  title = {{{AnyDB}}: {{An Architecture}}-Less {{DBMS}} for {{Any Workload}}},
  author = {Bang, Tiemo and May, Norman and Petrov, Ilia and Binnig, Carsten},
  abstract = {In this paper, we propose a radical new approach for scale-out distributed DBMSs. Instead of hard-baking an architectural model, such as a shared-nothing architecture, into the distributed DBMS design, we aim for a new class of so-called architecture-less DBMSs. The main idea is that an architecture-less DBMS can mimic any architecture on a per-query basis on-the-fly without any additional overhead for reconfiguration. Our initial results show that our architecture-less DBMS AnyDB can provide significant speed-ups across varying workloads compared to a traditional DBMS implementing a static architecture.},
  file = {D\:\\GDrive\\zotero\\Bang\\bang_anydb.pdf}
}

@techreport{bankoScalingVeryVerya,
  title = {Scaling to {{Very Very Large Corpora}} for {{Natural Language Disambiguation}}},
  author = {Banko, Michele and Brill, Eric},
  abstract = {The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.},
  file = {D\:\\GDrive\\zotero\\Banko\\banko_scaling_to_very_very_large_corpora_for_natural_language_disambiguation.pdf;D\:\\GDrive\\zotero\\Banko\\banko_scaling_to_very_very_large_corpora_for_natural_language_disambiguation2.pdf}
}

@article{banksRemoteAttestationLiterature2020,
  title = {Remote {{Attestation}}: {{A Literature Review}}},
  author = {Banks, Alexander Sprog{\o} and Kisiel, Marek and Korsholm, Philip},
  year = {2020},
  file = {D\:\\GDrive\\zotero\\Banks et al\\banks_et_al_2020_remote_attestation.pdf}
}

@inproceedings{banoSoKConsensusAge2019,
  title = {{{SoK}}: {{Consensus}} in the {{Age}} of {{Blockchains}}},
  shorttitle = {{{SoK}}},
  booktitle = {Proceedings of the 1st {{ACM Conference}} on {{Advances}} in {{Financial Technologies}}},
  author = {Bano, Shehar and Sonnino, Alberto and {Al-Bassam}, Mustafa and Azouvi, Sarah and McCorry, Patrick and Meiklejohn, Sarah and Danezis, George},
  year = {2019},
  month = oct,
  pages = {183--198},
  publisher = {{ACM}},
  address = {{Zurich Switzerland}},
  doi = {10.1145/3318041.3355458},
  abstract = {The core technical component of blockchains is consensus: how to reach agreement among a distributed network of nodes. A plethora of blockchain consensus protocols have been proposed\textemdash ranging from new designs, to novel modifications and extensions of consensus protocols from the classical distributed systems literature. The inherent complexity of consensus protocols and their rapid and dramatic evolution makes it hard to contextualize the design landscape. We address this challenge by conducting a systematization of knowledge of blockchain consensus protocols. After first discussing key themes in classical consensus protocols, we describe: (i) protocols based on proof-of-work; (ii) proof-of-X protocols that replace proof-of-work with more energy-efficient alternatives; and (iii) hybrid protocols that are compositions or variations of classical consensus protocols. This survey is guided by a systematization framework we develop, to highlight the various building blocks of blockchain consensus design, along with a discussion on their security and performance properties. We identify research gaps and insights for the community to consider in future research endeavours.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PA89D3T3\\Bano et al. - 2019 - SoK Consensus in the Age of Blockchains.pdf},
  isbn = {978-1-4503-6732-5},
  language = {en}
}

@book{barakatImprovingFairnessEfficiency2012,
  title = {Improving {{Fairness}}, {{Efficiency}}, and {{Stability inHTTP}}-Based {{Adaptive Video Streaming}} with {{FESTIVE}}},
  author = {Barakat, Chadi. and {Association for Computing Machinery.}},
  year = {2012},
  publisher = {{ACM}},
  file = {D\:\\GDrive\\zotero\\Barakat\\barakat_2012_improving_fairness,_efficiency,_and_stability_inhttp-based_adaptive_video.pdf},
  isbn = {978-1-4503-1775-7}
}

@techreport{baratlooTransparentRunTimeDefense2000,
  title = {Transparent {{Run}}-{{Time Defense Against Stack Smashing Attacks}}},
  author = {Baratloo, Arash},
  year = {2000},
  file = {D\:\\GDrive\\zotero\\Baratloo\\baratloo_transparent_run-time_defense_against_stack_smashing_attacks.pdf}
}

@techreport{barceloUserPrivacyPublic2007,
  title = {User {{Privacy}} in the {{Public Bitcoin Blockchain}}},
  author = {Barcelo, Jaume},
  year = {2007},
  volume = {6},
  abstract = {Bitcoin is a peer-to-peer electronic cash system that maintains a public ledger with all transactions. The public availability of this information has implications for the privacy of the users. The public ledger consists of transactions that transfer funds from a set of inputs to a set of outputs. Both inputs and outputs are linked to Bitcoin addresses. In principle, the addresses are pseudonymous. In practice, it is sometimes possible to link Bitcoin addresses to real identities with the consequent privacy leaks. The possibilities of linking addresses to owners are multiplied when addresses are reused to receive funds multiple times. The reuse of addresses also multiplies the amount of private information that is leaked when an address is linked to a real identity. In this work we describe privacy-leaking effects of address reuse and gather statistics of address reuse in the Bitcoin network. We also describe collaborative (CoinJoin) transactions that prevent the privacy attacks that have been published in the literature. Then we analyze the Blockchain to find transactions that could potentially be CoinJoin transactions.},
  file = {D\:\\GDrive\\zotero\\Barcelo\\barcelo_2007_user_privacy_in_the_public_bitcoin_blockchain.pdf},
  keywords = {address reuse,CoinJoin,cryptocurrency,Index Terms-Bitcoin,privacy},
  number = {1}
}

@techreport{barhamXenArtVirtualization2003,
  title = {Xen and the {{Art}} of {{Virtualization}}},
  author = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
  year = {2003},
  abstract = {Numerous systems have been designed which use virtualization to subdivide the ample resources of a modern computer. Some require specialized hardware, or cannot support commodity operating systems. Some target 100\% binary compatibility at the expense of performance. Others sacrifice security or functionality for speed. Few offer resource isolation or performance guarantees; most provide only best-effort provisioning, risking denial of service. This paper presents Xen, an x86 virtual machine monitor which allows multiple commodity operating systems to share conventional hardware in a safe and resource managed fashion, but without sacrificing either performance or functionality. This is achieved by providing an idealized virtual machine abstraction to which operating systems such as Linux, BSD and Windows XP, can be ported with minimal effort. Our design is targeted at hosting up to 100 virtual machine instances simultaneously on a modern server. The virtualization approach taken by Xen is extremely efficient: we allow operating systems such as Linux and Windows XP to be hosted simultaneously for a negligible performance overhead-at most a few percent compared with the unvirtualized case. We considerably outperform competing commercial and freely available solutions in a range of microbenchmarks and system-wide tests.},
  file = {D\:\\GDrive\\zotero\\Barham\\barham_2003_xen_and_the_art_of_virtualization.pdf},
  keywords = {D41 [Operating Systems]: Process Management,D42 [Opera-ting Systems]: Storage Management,D48 [Operating Systems]: Performance General Terms Design; Measurement; Performance Keywords Virtual Machine Monitors; Hypervisors; Paravirtualization * Microsoft Research Cambridge; UK † Intel Research Cambridge; UK}
}

@article{BarrosoPiranhaScalable,
  title = {Barroso et al. - {{Piranha A Scalable Architecture Based}} on {{Single}}-{{C}}},
  file = {D\:\\GDrive\\zotero\\undefined\\barroso_et_al.pdf}
}

@article{barrosStaticAnalysisImplicit2015,
  title = {Static {{Analysis}} of {{Implicit Control Flow}}: {{Resolving Java Reflection}} and {{Android Intents Paulo}}},
  author = {Barros, Paulo and Vines, Paul and Ernst, Michael D},
  year = {2015},
  file = {D\:\\GDrive\\zotero\\Barros et al\\barros_et_al_2015_static_analysis_of_implicit_control_flow.pdf}
}

@techreport{barthSecurityArchitectureChromium,
  title = {The {{Security Architecture}} of the {{Chromium Browser}}},
  author = {Barth, Adam and Jackson, Collin and Reis, Charles},
  abstract = {Most current web browsers employ a monolithic architecture that combines "the user" and "the web" into a single protection domain. An attacker who exploits an arbitrary code execution vulnerability in such a browser can steal sensitive files or install malware. In this paper, we present the security architecture of Chromium, the open-source browser upon which Google Chrome is built. Chromium has two modules in separate protection domains: a browser kernel, which interacts with the operating system, and a rendering engine, which runs with restricted privileges in a sandbox. This architecture helps mitigate high-severity attacks without sacrificing compatibility with existing web sites. We define a threat model for browser exploits and evaluate how the architecture would have mitigated past vulnerabilities.},
  file = {D\:\\GDrive\\zotero\\Barth\\barth_the_security_architecture_of_the_chromium_browser.pdf}
}

@techreport{batsonHighLevelSpecificationsLessons2002,
  title = {High-{{Level Specifications}}: {{Lessons}} from {{Industry}}},
  author = {Batson, Brannon and Lamport, Leslie},
  year = {2002},
  abstract = {We explain the rationale behind the design of the TLA + specification language , and we describe our experience using it and the TLC model checker in industrial applications-including the verification of multiprocessor memory designs at Intel. Based on this experience, we challenge some conventional wisdom about high-level specifications.},
  file = {D\:\\GDrive\\zotero\\Batson\\batson_2002_high-level_specifications.pdf}
}

@article{bauer12UnitedStates2011,
  title = {(12) {{United States Patent}}},
  author = {{Bauer}},
  year = {2011},
  volume = {2},
  number = {12}
}

@techreport{bauerProgrammingAlgebraicEffects,
  title = {Programming with {{Algebraic Effects}} and {{Handlers}}},
  author = {Bauer, Andrej and Pretnar, Matija},
  abstract = {Eff is a programming language based on the algebraic approach to computational effects, in which effects are viewed as algebraic operations and effect handlers as homomorphisms from free algebras. Eff supports first-class effects and handlers through which we may easily define new computational effects, seamlessly combine existing ones, and handle them in novel ways. We give a de-notational semantics of eff and discuss a prototype implementation based on it. Through examples we demonstrate how the standard effects are treated in eff , and how eff supports programming techniques that use various forms of delimited continuations , such as backtracking, breadth-first search, selection functionals, cooperative multi-threading, and others.},
  file = {D\:\\GDrive\\zotero\\Bauer\\bauer_programming_with_algebraic_effects_and_handlers.pdf}
}

@techreport{bauSecurityEvaluationDNSSEC,
  title = {A {{Security Evaluation}} of {{DNSSEC}} with {{NSEC3}}},
  author = {Bau, Jason and Mitchell, John C},
  abstract = {Domain Name System Security Extensions (DNSSEC) and Hashed Authenticated Denial of Existence (NSEC3) are slated for adoption by important parts of the DNS hierarchy , including the root zone, as a solution to vulnerabili-ties such as "cache-poisoning" attacks. We study the security goals and operation of DNSSEC/NSEC3 using Mur, a finite-state enumeration tool, to analyze security properties that may be relevant to various deployment scenarios. Our systematic study reveals several subtleties and potential pitfalls that can be avoided by proper configuration choices, including resource records that may remain valid after the expiration of relevant signatures and potential insertion of forged names into a DNSSEC-enabled domain via the opt-out option. We demonstrate the exploitability of DNSSEC opt-out options in an enterprise setting by constructing a browser cookie-stealing attack on a laboratory domain. Under recommended configuration settings, further Mur model checking finds no vulnerabilities within our threat model, suggesting that DNSSEC with NSEC3 provides significant security benefits.},
  file = {D\:\\GDrive\\zotero\\Bau\\bau_a_security_evaluation_of_dnssec_with_nsec3.pdf}
}

@article{beckerLassieHOL4Tactics2021,
  title = {Lassie: {{HOL4 Tactics}} by {{Example}}},
  shorttitle = {Lassie},
  author = {Becker, Heiko and Bos, Nathaniel and Gavran, Ivan and Darulova, Eva and Majumdar, Rupak},
  year = {2021},
  month = jan,
  pages = {212--223},
  doi = {10.1145/3437992.3439925},
  abstract = {Proof engineering efforts using interactive theorem proving have yielded several impressive projects in software systems and mathematics. A key obstacle to such efforts is the requirement that the domain expert is also an expert in the low-level details in constructing the proof in a theorem prover. In particular, the user needs to select a sequence of tactics that lead to a successful proof, a task that in general requires knowledge of the exact names and use of a large set of tactics. We present Lassie, a tactic framework for the HOL4 theorem prover that allows individual users to define their own tactic language by example and give frequently used tactics or tactic combinations easier-to-remember names. The core of Lassie is an extensible semantic parser, which allows the user to interactively extend the tactic language through a process of definitional generalization. Defining tactics in Lassie thus does not require any knowledge in implementing custom tactics, while proofs written in Lassie retain the correctness guarantees provided by the HOL4 system. We show through case studies how Lassie can be used in small and larger proofs by novice and more experienced interactive theorem prover users, and how we envision it to ease the learning curve in a HOL4 tutorial.},
  archiveprefix = {arXiv},
  eprint = {2101.00930},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Becker et al\\becker_et_al_2021_lassie.pdf;C\:\\Users\\Admin\\Zotero\\storage\\J52DMH8K\\2101.html},
  journal = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
  keywords = {Computer Science - Logic in Computer Science,Computer Science - Programming Languages}
}

@techreport{behoviSecurityProblemsTCPAP,
  title = {Security {{Problems}} in the {{TCPAP Protocol Suite ABSTI}}){{gACT}}},
  author = {{Behovi}},
  abstract = {The TCP/IP protocol suite, which is very widely useJ today, was developed under \textasciitilde e sponsorship of the Department of Defense. Despite that, there are a number of serious secmqty flaws inherent in the protocols, mgardless of the correctness of any implementations. We describe a variety of attacks b\textasciitilde sed on these flaws, including sequence number spoofing, routing arracks, source address spoofing, mad authentication attacks. We also present defenses against these attacks, and conclude with a discussion of broad-spectrum defe.nses such as encry?rion.},
  file = {D\:\\GDrive\\zotero\\Behovi\\behovi_security_problems_in_the_tcpap_protocol_suite_absti)gact.pdf}
}

@article{behrooziDoesStressImpact2020,
  title = {Does Stress Impact Technical Interview Performance?},
  author = {Behroozi, Mahnaz and Shirolkar, Shivani and Barik, Titus and Parnin, Chris},
  year = {2020},
  pages = {481--492},
  doi = {10.1145/3368089.3409712},
  abstract = {Software engineering candidates commonly participate in whiteboard technical interviews as part of a hiring assessment. During these sessions, candidates write code while thinking aloud as they work towards a solution, under the watchful eye of an interviewer. While technical interviews should allow for an unbiased and inclusive assessment of problem-solving ability, surprisingly, technical interviews may be instead a procedure for identifying candidates who best handle and migrate stress solely caused by being examined by an interviewer (performance anxiety). To understand if coding interviews - as administered today - can induce stress that significantly hinders performance, we conducted a randomized controlled trial with 48 Computer Science students, comparing them in private and public whiteboard settings. We found that performance is reduced by more than half, by simply being watched by an interviewer. We also observed that stress and cognitive load were significantly higher in a traditional technical interview when compared with our private interview. Consequently, interviewers may be filtering out qualified candidates by confounding assessment of problem-solving ability with unnecessary stress. We propose interview modifications to make problem-solving assessment more equitable and inclusive, such as through private focus sessions and retrospective think-aloud, allowing companies to hire from a larger and diverse pool of talent.},
  file = {D\:\\GDrive\\zotero\\Behroozi\\behroozi_2020_does_stress_impact_technical_interview_performance.pdf},
  isbn = {9781450370431},
  journal = {ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  keywords = {Cognitive load,Eye-tracking,Program comprehension,Technical interviews}
}

@incollection{belenkiyRandomizableProofsDelegatable2009,
  title = {Randomizable {{Proofs}} and {{Delegatable Anonymous Credentials}}},
  booktitle = {Advances in {{Cryptology}} - {{CRYPTO}} 2009},
  author = {Belenkiy, Mira and Camenisch, Jan and Chase, Melissa and Kohlweiss, Markulf and Lysyanskaya, Anna and Shacham, Hovav},
  editor = {Halevi, Shai},
  year = {2009},
  volume = {5677},
  pages = {108--125},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03356-8_7},
  abstract = {We construct an efficient delegatable anonymous credentials system. Users can anonymously and unlinkably obtain credentials from any authority, delegate their credentials to other users, and prove possession of a credential L levels away from a given authority. The size of the proof (and time to compute it) is O(Lk), where k is the security parameter. The only other construction of delegatable anonymous credentials (Chase and Lysyanskaya, Crypto 2006) relies on general non-interactive proofs for NP-complete languages of size k{$\Omega$}(2L). We revise the entire approach to constructing anonymous credentials and identify randomizable zero-knowledge proof of knowledge systems as the key building block. We formally define the notion of randomizable non-interactive zero-knowledge proofs, and give the first instance of controlled rerandomization of non-interactive zero-knowledge proofs by a third-party. Our construction uses Groth-Sahai proofs (Eurocrypt 2008).},
  file = {D\:\\GDrive\\zotero\\Belenkiy et al\\belenkiy_et_al_2009_randomizable_proofs_and_delegatable_anonymous_credentials.pdf},
  isbn = {978-3-642-03355-1 978-3-642-03356-8},
  language = {en}
}

@techreport{bellareAuthenticatedEncryptionRelations2007,
  title = {Authenticated {{Encryption}}: {{Relations}} among Notions and Analysis of the Generic Composition Paradigm {{Chanathip Namprempre}} \textdagger},
  author = {Bellare, Mihir},
  year = {2007},
  volume = {1976},
  pages = {531--545},
  abstract = {An authenticated encryption scheme is a symmetric encryption scheme whose goal is to provide both privacy and integrity. We consider two possible notions of authenticity for such schemes, namely integrity of plaintexts and integrity of ciphertexts, and relate them (when coupled with IND-CPA) to the standard notions of privacy (IND-CCA, NM-CPA) by presenting implications and separations between all notions considered. We then analyze the security of authenticated encryption schemes designed by "generic composition," meaning making black-box use of a given symmetric encryption scheme and a given MAC. Three composition methods are considered, namely Encrypt-and-MAC, MAC-then-encrypt, and Encrypt-then-MAC. For each of these, and for each notion of security, we indicate whether or not the resulting scheme meets the notion in question assuming the given symmetric encryption scheme is secure against chosen-plaintext attack and the given MAC is unforgeable under chosen-message attack. We provide proofs for the cases where the answer is "yes" and counterexamples for the cases where the answer is "no."},
  file = {D\:\\GDrive\\zotero\\Bellare\\bellare_authenticated_encryption.pdf},
  journal = {Lecture Notes in Computer Science}
}

@incollection{bellareHedgedPublicKeyEncryption2009,
  title = {Hedged {{Public}}-{{Key Encryption}}: {{How}} to {{Protect}} against {{Bad Randomness}}},
  shorttitle = {Hedged {{Public}}-{{Key Encryption}}},
  booktitle = {Advances in {{Cryptology}} \textendash{} {{ASIACRYPT}} 2009},
  author = {Bellare, Mihir and Brakerski, Zvika and Naor, Moni and Ristenpart, Thomas and Segev, Gil and Shacham, Hovav and Yilek, Scott},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Matsui, Mitsuru},
  year = {2009},
  volume = {5912},
  pages = {232--249},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-10366-7_14},
  abstract = {Public-key encryption schemes rely for their IND-CPA security on per-message fresh randomness. In practice, randomness may be of poor quality for a variety of reasons, leading to failure of the schemes. Expecting the systems to improve is unrealistic. What we show in this paper is that we can, instead, improve the cryptography to offset the lack of possible randomness. We provide public-key encryption schemes that achieve IND-CPA security when the randomness they use is of high quality, but, when the latter is not the case, rather than breaking completely, they achieve a weaker but still useful notion of security that we call IND-CDA. This hedged public-key encryption provides the best possible security guarantees in the face of bad randomness. We provide simple RO-based ways to make in-practice IND-CPA schemes hedge secure with minimal software changes. We also provide non-RO model schemes relying on lossy trapdoor functions (LTDFs) and techniques from deterministic encryption. They achieve adaptive security by establishing and exploiting the anonymity of LTDFs which we believe is of independent interest.},
  file = {D\:\\GDrive\\zotero\\Bellare et al\\bellare_et_al_2009_hedged_public-key_encryption.pdf},
  isbn = {978-3-642-10365-0 978-3-642-10366-7},
  language = {en}
}

@techreport{bellareMessageAuthenticationUsing1996,
  title = {Message {{Authentication}} Using {{Hash Functions}}| {{The HMAC Construction}}},
  author = {Bellare, Mihir and Canetti, Ran and Krawczyk, Hugo},
  year = {1996},
  volume = {2},
  file = {D\:\\GDrive\\zotero\\Bellare\\bellare_1996_message_authentication_using_hash_functions_the_hmac_construction.pdf},
  number = {1}
}

@article{bellareSecurityCipherBlock2000,
  title = {The Security of the Cipher Block Chaining Message Authentication Code},
  author = {Bellare, Mihir and Kilian, Joe and Rogaway, Phillip},
  year = {2000},
  file = {D\:\\GDrive\\zotero\\Bellare et al\\bellare_et_al_the_security_of_the_cipher_block_chaining_message_authentication_code.pdf}
}

@techreport{bellovinLookBackSecurity,
  title = {A {{Look Back}} at "{{Security Problems}} in the {{TCP}}/{{IP Protocol Suite}}"},
  author = {Bellovin, Steven M},
  abstract = {About fifteen years ago, I wrote a paper on security problems in the TCP/IP protocol suite, In particular, I focused on protocol-level issues, rather than implementation flaws. It is instructive to look back at that paper, to see where my focus and my predictions were accurate, where I was wrong, and where dangers have yet to happen. This is a reprint of the original paper, with added commentary.},
  file = {D\:\\GDrive\\zotero\\Bellovin\\bellovin_a_look_back_at_security_problems_in_the_tcp-ip_protocol_suite.pdf},
  keywords = {ss}
}

@phdthesis{beltramelliDeepSpyingSpyingUsing2015,
  title = {Deep-{{Spying}}: {{Spying}} Using {{Smartwatch}} and {{Deep Learning}}},
  author = {Beltramelli, Tony and Risi, Sebastian},
  year = {2015},
  file = {D\:\\GDrive\\zotero\\Beltramelli\\beltramelli_2015_deep-spying.pdf}
}

@article{ben-sassonZerocashDecentralizedAnonymous2014,
  title = {Zerocash: {{Decentralized Anonymous Payments}} from {{Bitcoin}}},
  author = {{Ben-Sasson}, Eli and Chiesa, Alessandro and Garman, Christina and Green, Matthew and Miers, Ian and Tromer, Eran and Virza, Madars},
  year = {2014},
  doi = {10.1109/SP.2014.36},
  abstract = {Bitcoin is the first digital currency to see widespread adoption. While payments are conducted between pseudonyms, Bitcoin cannot offer strong privacy guarantees: payment transactions are recorded in a public decentralized ledger, from which much information can be deduced. Zerocoin (Miers et al., IEEE S\&P 2013) tackles some of these privacy issues by unlinking transactions from the payment's origin. Yet, it still reveals payments' destinations and amounts, and is limited in functionality. In this paper, we construct a full-fledged ledger-based digital currency with strong privacy guarantees. Our results leverage recent advances in zero-knowledge Succinct Non-interactive ARguments of Knowledge (zk-SNARKs). First, we formulate and construct decentralized anonymous payment schemes (DAP schemes). A DAP scheme enables users to directly pay each other privately: the corresponding transaction hides the payment's origin, destination, and transferred amount. We provide formal definitions and proofs of the construction's security. Second, we build Zerocash, a practical instantiation of our DAP scheme construction. In Zerocash, transactions are less than 1 kB and take under 6 ms to verify-orders of magnitude more efficient than the less-anonymous Zerocoin and competitive with plain Bitcoin.},
  file = {D\:\\GDrive\\zotero\\Ben-Sasson\\ben-sasson_2014_zerocash.pdf}
}

@techreport{benderLCAProblemRevisited2000,
  title = {The {{LCA Problem Revisited}}},
  author = {Bender, Michael A},
  year = {2000},
  abstract = {We present a very simple algorithm for the Least Common Ancestor problem. We thus dispel the frequently held notion that an optimal LCA computation is unwieldy and unimplementable. Interestingly, this algorithm is a sequentialization of a previously known PRAM algorithm of Berkman, Breslauer, Galil, Schieber, and Vishkin [1].},
  file = {D\:\\GDrive\\zotero\\Bender\\bender_2000_the_lca_problem_revisited.pdf},
  keywords = {Cartesian Tree,Data Structures,Least Common Ancestor (LCA),Range Minimum Query (RMQ)}
}

@article{benderLevelAncestorProblem2004,
  title = {The {{Level Ancestor Problem}} Simplified},
  author = {Bender, Michael A. and {Farach-Colton}, Mart{\'{\i}}n},
  year = {2004},
  month = jun,
  volume = {321},
  pages = {5--12},
  issn = {03043975},
  doi = {10.1016/j.tcs.2003.05.002},
  file = {D\:\\GDrive\\zotero\\Bender\\bender_2004_the_level_ancestor_problem_simplified.pdf},
  journal = {Theoretical Computer Science},
  number = {1}
}

@techreport{benetIPFSContentAddressedVersioned2014,
  title = {{{IPFS}}-{{Content Addressed}}, {{Versioned}}, {{P2P File System}} ({{DRAFT}} 3)},
  author = {Benet, Juan},
  year = {2014},
  abstract = {The InterPlanetary File System (IPFS) is a peer-to-peer distributed file system that seeks to connect all computing devices with the same system of files. In some ways, IPFS is similar to the Web, but IPFS could be seen as a single BitTorrent swarm, exchanging objects within one Git repository. In other words, IPFS provides a high through-put content-addressed block storage model, with content-addressed hyper links. This forms a generalized Merkle DAG, a data structure upon which one can build versioned file systems, blockchains, and even a Permanent Web. IPFS combines a distributed hashtable, an incentivized block exchange , and a self-certifying namespace. IPFS has no single point of failure, and nodes do not need to trust each other.},
  file = {D\:\\GDrive\\zotero\\Benet\\benet_2014_ipfs-content_addressed,_versioned,_p2p_file_system_(draft_3).pdf},
  keywords = {()}
}

@article{bengerOohAahJust2014,
  title = {``{{Ooh Aah}}\ldots{} {{Just}} a Little Bit'': {{A}} Small Amount of Side Channel Can Go a Longway},
  author = {Benger, Naomi and {van de Pol}, Joop and Smart, Nigel P. and Yarom, Yuval},
  year = {2014},
  volume = {8731},
  pages = {75--92},
  issn = {16113349},
  doi = {10.1007/978-3-662-44709-3_5},
  abstract = {We apply the FLUSH+RELOAD side-channel attack based on cache hits/misses to extract a small amount of data from OpenSSL ECDSA signature requests. We then apply a ``standard'' lattice technique to extract the private key, but unlike previous attacks we are able to make use of the side-channel information from almost all of the observed executions. This means we obtain private key recovery by observing a relatively small number of executions, and by expending a relatively small amount of post-processing via lattice reduction. We demonstrate our analysis via experiments using the curve secp256k1 used in the Bitcoin protocol. In particular we show that with as little as 200 signatures we are able to achieve a reasonable level of success in recovering the secret key for a 256-bit curve. This is significantly better than prior methods of applying lattice reduction techniques to similar side channel information},
  file = {D\:\\GDrive\\zotero\\Benger\\benger_2014_“ooh_aah…_just_a_little_bit”.pdf},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@techreport{benjaminManagementImplementationSecurity1972,
  title = {Management/ {{On}} the {{Implementation}} of {{Security Measures}} in {{Information Systems}}},
  author = {Benjamin, R and Conway, R W and Maxwell, W L and Morgan, H L},
  year = {1972},
  abstract = {The security of an information system may be represented by a model matrix whose elements are decision rules and whose row and column indices are users and data items respectively. A set of four functions is used to access this matrix at translation and execution time. Distinguishing between data dependent and data independent decision rules enables one to perform much of the checking of security only once at translation time rather than repeatedly at execution time. The model is used to explain security features of several existing systems, and serves as a framework for a proposal for general security system implementation within today's languages and operating systems.},
  file = {D\:\\GDrive\\zotero\\Benjamin\\benjamin_1972_management-_on_the_implementation_of_security_measures_in_information_systems.pdf},
  keywords = {access control confidentiality,access management,and Phrases: security,data banks,management information systems CR Categories,operating systems,privacy}
}

@incollection{bensonKBDHAssumptionFamily2013,
  title = {The K-{{BDH Assumption Family}}: {{Bilinear Map Cryptography}} from {{Progressively Weaker Assumptions}}},
  shorttitle = {The K-{{BDH Assumption Family}}},
  booktitle = {Topics in {{Cryptology}} \textendash{} {{CT}}-{{RSA}} 2013},
  author = {Benson, Karyn and Shacham, Hovav and Waters, Brent},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Dawson, Ed},
  year = {2013},
  volume = {7779},
  pages = {310--325},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-36095-4_20},
  abstract = {Over the past decade bilinear maps have been used to build a large variety of cryptosystems. In addition to new functionality, we have concurrently seen the emergence of many strong assumptions. In this work, we explore how to build bilinear map cryptosystems under progressively weaker assumptions.},
  file = {D\:\\GDrive\\zotero\\Benson et al\\benson_et_al_2013_the_k-bdh_assumption_family.pdf},
  isbn = {978-3-642-36094-7 978-3-642-36095-4},
  language = {en}
}

@inproceedings{bensonYouKnowWhere2011,
  title = {Do You Know Where Your Cloud Files Are?},
  booktitle = {Proceedings of the 3rd {{ACM}} Workshop on {{Cloud}} Computing Security Workshop - {{CCSW}} '11},
  author = {Benson, Karyn and Dowsley, Rafael and Shacham, Hovav},
  year = {2011},
  pages = {73},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/2046660.2046677},
  abstract = {Clients of storage-as-a-service systems such as Amazon's S3 want to be sure that the files they have entrusted to the cloud are available now and will be available in the future.},
  file = {D\:\\GDrive\\zotero\\Benson et al\\benson_et_al_2011_do_you_know_where_your_cloud_files_are.pdf},
  isbn = {978-1-4503-1004-8},
  language = {en}
}

@techreport{bentovProofActivityExtending,
  title = {Proof of {{Activity}}: {{Extending Bitcoin}}'s {{Proof}} of {{Work}} via {{Proof}} of {{Stake}}},
  author = {Bentov, Iddo and Lee, Charles and Mizrahi, Alex and Rosenfeld, Meni},
  abstract = {We propose a new protocol for a cryptocurrency, that builds upon the Bitcoin protocol by combining its Proof of Work component with a Proof of Stake type of system. Our Proof of Activity (PoA) protocol offers good security against possibly practical future attacks on Bitcoin, and has a relatively low penalty in terms of network communication and storage space. We explore various attack scenarios and suggest remedies to potential vulnerabilities of the PoA protocol, as well as evaluate the performance of its core subroutine.},
  file = {D\:\\GDrive\\zotero\\Bentov\\bentov_proof_of_activity.pdf}
}

@book{berganCoreDetCompilerRuntime,
  title = {{{CoreDet}}: {{A Compiler}} and {{Runtime System}} for {{Deterministic Multithreaded Execution}}},
  author = {Bergan, Tom and Anderson, Owen and Devietti, Joseph and Ceze, Luis and Grossman, Dan},
  abstract = {The behavior of a multithreaded program does not depend only on its inputs. Scheduling, memory reordering, timing, and low-level hardware effects all introduce nondeterminism in the execution of multithreaded programs. This severely complicates many tasks, including debugging, testing, and automatic replication. In this work, we avoid these complications by eliminating their root cause: we develop a compiler and runtime system that runs arbitrary multithreaded C/C++ POSIX Threads programs deterministically. A trivial non-performant approach to providing determinism is simply deterministically serializing execution. Instead, we present a compiler and runtime infrastructure that ensures determinism but resorts to serialization rarely, for handling interthread communication and synchronization. We develop two basic approaches, both of which are largely dynamic with performance improved by some static compiler optimizations. First, an ownership-based approach detects interthread communication via an evolving table that tracks ownership of memory regions by threads. Second, a buffering approach uses versioned memory and employs a deterministic commit protocol to make changes visible to other threads. While buffer-ing has larger single-threaded overhead than ownership, it tends to scale better (serializing less often). A hybrid system sometimes performs and scales better than either approach individually. Our implementation is based on the LLVM compiler infrastructure. It needs neither programmer annotations nor special hardware. Our empirical evaluation uses the PARSEC and SPLASH2 benchmarks and shows that our approach scales comparably to nondeter-ministic execution.},
  file = {D\:\\GDrive\\zotero\\Bergan\\bergan_coredet.pdf},
  isbn = {978-1-60558-839-1},
  keywords = {D13 [Programming Lan-guages]: Concurrent Programming-Parallel Programming; D34 [Programming Languages]: Processors-Compilers,Design,Optimization,Performance,Run-time environments General Terms Reliabity}
}

@book{berndtssonThesisProjectsGuide2008a,
  title = {Thesis Projects: A Guide for Students in Computer Science and Information Systems},
  shorttitle = {Thesis Projects},
  editor = {Berndtsson, Mikael},
  year = {2008},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{London}},
  file = {D\:\\GDrive\\zotero\\Berndtsson\\berndtsson_2008_thesis_projects2.pdf},
  isbn = {978-1-84800-008-7},
  keywords = {Computer science,Project method in teaching,Study and teaching (Higher)},
  lccn = {QA76.27 .P533 2008}
}

@techreport{bernsteinCachetimingAttacksAES2005,
  title = {Cache-Timing Attacks on {{AES}}},
  author = {Bernstein, Daniel J},
  year = {2005},
  abstract = {This paper demonstrates complete AES key recovery from known-plaintext timings of a network server on another computer. This attack should be blamed on the AES design, not on the particular AES library used by the server; it is extremely difficult to write constant-time high-speed AES software for common general-purpose computers. This paper discusses several of the obstacles in detail.},
  file = {D\:\\GDrive\\zotero\\Bernstein\\bernstein_cache-timing_attacks_on_aes.pdf},
  keywords = {side-channel,ss}
}

@article{bernsteinChaChaVariantSalsa202008,
  title = {{{ChaCha}}, a Variant of {{Salsa20}}},
  author = {Bernstein, Daniel J},
  year = {2008},
  pages = {1--6},
  abstract = {ChaCha8 is a 256-bit stream cipher based on the 8-round cipher Salsa20/8. The changes from Salsa20/8 to ChaCha8 are designed to improve diffusion per round, conjecturally increasing resistance to cryptanalysis, while preserving\textemdash and often improving\textemdash time per round. ChaCha12 and ChaCha20 are analogous modifications of the 12-round and 20-round ciphers Salsa20/12 and Salsa20/20. This paper presents the ChaCha family and explains the differences between Salsa20 and ChaCha.},
  file = {D\:\\GDrive\\zotero\\Bernstein\\bernstein_chacha,_a_variant_of_salsa20.pdf},
  journal = {Workshop Record of SASC},
  keywords = {ss}
}

@techreport{bernsteinNTRUPrime,
  title = {{{NTRU Prime}}},
  author = {Bernstein, Daniel J and Chuengsatiansup, Chitchanok and Lange, Tanja and Van Vredendaal, Christine},
  abstract = {Several ideal-lattice-based cryptosystems have been broken by recent attacks that exploit special structures of the rings used in those cryptosystems. The same structures are also used in the leading proposals for post-quantum lattice-based cryptography, including the classic NTRU cryptosystem and typical Ring-LWE-based cryptosystems. This paper proposes NTRU Prime, which tweaks NTRU to use rings without these structures; proposes Streamlined NTRU Prime, which optimizes NTRU Prime from an implementation perspective; finds high-security post-quantum parameters for Streamlined NTRU Prime; and optimizes a constant-time implementation of those parameters. The performance results are surprisingly competitive with the best previous speeds for lattice-based cryptography.},
  file = {D\:\\GDrive\\zotero\\Bernstein\\bernstein_ntru_prime2.pdf},
  keywords = {ideal lattices,Karatsuba,lattice-based cryptography,NTRU,post-quantum cryptography,public-key encryption,Ring-LWE,security,software implementation,Solilo-quy,Toom,vectorization}
}

@techreport{bernsteinNTRUPrimeReducing,
  title = {{{NTRU Prime}}: Reducing Attack Surface at Low Cost},
  author = {Bernstein, Daniel J and Chuengsatiansup, Chitchanok and Lange, Tanja and Van Vredendaal, Christine},
  abstract = {Several ideal-lattice-based cryptosystems have been broken by recent attacks that exploit special structures of the rings used in those cryptosystems. The same structures are also used in the leading proposals for post-quantum lattice-based cryptography, including the classic NTRU cryptosystem and typical Ring-LWE-based cryptosystems. This paper (1) proposes NTRU Prime, which tweaks NTRU to use rings without these structures; (2) proposes Streamlined NTRU Prime, a public-key cryptosystem optimized from an implementation perspective, subject to the standard design goal of IND-CCA2 security; (3) finds high-security post-quantum parameters for Streamlined NTRU Prime; and (4) optimizes a constant-time implementation of those parameters. The resulting sizes and speeds show that reducing the attack surface has very low cost.},
  file = {D\:\\GDrive\\zotero\\Bernstein\\bernstein_ntru_prime.pdf},
  keywords = {fast sorting,ideal lattices,Karatsuba,lattice-based cryptography,NTRU,post-quantum cryptography,public-key encryption,Ring-LWE,security,software implementation,Solilo-quy,vectorization}
}

@techreport{bernsteinThoughtsSecurityTen,
  title = {Some Thoughts on Security after Ten Years of Qmail 1.0},
  author = {Bernstein, Daniel J},
  abstract = {The qmail software package is a widely used Internet-mail transfer agent that has been covered by a security guarantee since 1997. In this paper, the qmail author reviews the history and security-relevant architecture of qmail; articulates partitioning standards that qmail fails to meet; analyzes the engineering that has allowed qmail to survive this failure; and draws various conclusions regarding the future of secure programming.},
  file = {D\:\\GDrive\\zotero\\Bernstein\\bernstein_some_thoughts_on_security_after_ten_years_of_qmail_1.pdf}
}

@article{beschastnikhDebuggingDistributedSystems2016,
  title = {Debugging Distributed Systems},
  author = {Beschastnikh, Ivan and Wang, Patty and Brun, Yuriy and Ernst, Michael D.},
  year = {2016},
  volume = {59},
  pages = {32--37},
  issn = {15577317},
  doi = {10.1145/2909480},
  abstract = {DISTRIBUTED SYSTEMS POSE unique challenges for software developers. Reasoning about concurrent activities of system nodes and even understanding the system's communication topology can be difficult. A standard approach to gaining insight into system activity is to analyze system logs. Unfortunately, this can be a tedious and complex process. This article looks at several key features and debugging challenges that differentiate distributed systems from other kinds of software. The article presents several promising tools and ongoing research to help resolve these challenges. Distributed systems differ from single-machine programs in ways that are simultaneously positive in providing systems with special capabilities, and negative in presenting software-development and operational challenges.},
  file = {D\:\\GDrive\\zotero\\Beschastnikh\\beschastnikh_2016_debugging_distributed_systems.pdf},
  journal = {Communications of the ACM},
  number = {8}
}

@article{besseyFewBillionLines2010,
  title = {A Few Billion Lines of Code Later: {{Using}} Static Analysis to Find Bugs in the Real World},
  author = {Bessey, Al and Block, Ken and Chelf, Ben and Chou, Andy and Fulton, Bryan and Hallem, Seth and {Henri-Gros}, Charles and Kamsky, Asya and McPeak, Scott and Engler, Dawson},
  year = {2010},
  volume = {53},
  pages = {66--75},
  issn = {00010782},
  doi = {10.1145/1646353.1646374},
  abstract = {How Coverity built a bug-finding tool, and a business, around the unlimited supply of bugs in software systems. \textcopyright{} 2010 ACM.},
  journal = {Communications of the ACM},
  number = {2}
}

@article{beyerExplicitstateSoftwareModel2013,
  title = {Explicit-State Software Model Checking Based on {{CEGAR}} and Interpolation},
  author = {Beyer, Dirk and L{\"o}we, Stefan},
  year = {2013},
  volume = {7793 LNCS},
  pages = {146--162},
  issn = {03029743},
  doi = {10.1007/978-3-642-37057-1_11},
  abstract = {Abstraction, counterexample-guided refinement, and interpolation are techniques that are essential to the success of predicate-based program analysis. These techniques have not yet been applied together to explicit-value program analysis. We present an approach that integrates abstraction and interpolationbased refinement into an explicit-value analysis, i.e., a program analysis that tracks explicit values for a specified set of variables (the precision). The algorithm uses an abstract reachability graph as central data structure and a path-sensitive dynamic approach for precision adjustment. We evaluate our algorithm on the benchmark set of the Competition on Software Verification 2012 (SV-COMP'12) to show that our new approach is highly competitive. We also show that combining our new approach with an auxiliary predicate analysis scores significantly higher than the SV-COMP'12 winner. \textcopyright{} 2013 Springer-Verlag.},
  isbn = {9783642370564},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{Beyondbesteffort,
  title = {Beyondbesteffort},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\KJ78PDFR\\beyondbesteffort.pdf}
}

@techreport{bezansonalanedelmanstefankarpinskiviralshahmitJuliaFreshApproach2015,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson Alan Edelman Stefan Karpinski Viral Shah MIT, Jeff B and Computing, Julia},
  year = {2015},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast. Julia questions notions generally held as "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow, 2. One must prototype in one language and then rewrite in another language for speed or deployment , and 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. We introduce the Julia programming language and its design-a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can have machine performance without sacrificing human convenience. *},
  file = {D\:\\GDrive\\zotero\\Bezanson Alan Edelman Stefan Karpinski Viral Shah MIT\\bezanson_alan_edelman_stefan_karpinski_viral_shah_mit_2015_julia.pdf}
}

@article{bhattacharyaTemplateAttackBlinded2017,
  title = {Template {{Attack}} on {{Blinded Scalar Multiplication}} with {{Asynchronous}} Perf-Ioctl {{Calls}}},
  author = {Bhattacharya, Sarani and Maurice, Clementine and Bhasin, Shivam and Mukhopadhyay, Debdeep},
  year = {2017},
  abstract = {In recent years, performance counters have been used as a side channel source for the branch mispredictions which has been used to attack ciphers with user privileges. However, existing research considers blinding techniques, like scalar blinding, scalar splitting as a mechanism of thwarting such attacks. In this endeavour, we reverse engineer the undisclosed model of Intel's Broadwell and Sandybridge branch predic-tor and further utilize the largely unexplored perf ioctl calls in sampling mode to granularly monitor the branch prediction events asynchronously when a victim cipher is executing. With these artifacts in place, we target scalar blinding and splitting countermeasures to develop a key retrieval process using what is called as Deduce \& Remove. The Deduce step uses template based on the number of branch misses as expected from the 3-bit model of the BPU to infer the matched candidate values. In the Remove step, we correct any erroneous conclusions that are made, by using the properties of the blinding technique under attack. It may be emphasized that as in iterated attacks the cost of a mistaken deduction could be significant, the blinding techniques actually aids in removing wrong guesses and in a way auto-corrects the key retrieval process. Fi-nally, detailed experimental results have been provided to illustrate all the above steps for point blinding, scalar blinding, and scalar splitting to show that the secret scalar can be correctly recovered with high con-fidence. The paper concludes with recommendation on some suitable countermeasure at the algorithm level to thwart such attacks.},
  file = {D\:\\GDrive\\zotero\\Bhattacharya\\bhattacharya_2017_template_attack_on_blinded_scalar_multiplication_with_asynchronous_perf-ioctl.pdf},
  keywords = {3-bit pre-,scalar blinding,scalar multiplication,scalar splitting}
}

@article{bijlaniWhereDidMy,
  title = {Where Did My 256 {{GB}} Go? {{A Measurement Analysis}} of {{Storage Consumption}} on {{Smart Mobile Devices}}},
  author = {Bijlani, Ashish},
  volume = {5},
  pages = {28},
  file = {D\:\\GDrive\\zotero\\Bijlani\\bijlani_where_did_my_256_gb_go.pdf},
  language = {en},
  number = {2}
}

@article{bijlaniWhereDidMy2021,
  title = {Where Did My 256 {{GB}} Go? {{A Measurement Analysis}} of {{Storage Consumption}} on {{Smart Mobile Devices}}},
  shorttitle = {Where Did My 256 {{GB}} Go?},
  author = {Bijlani, Ashish and Ramachandran, Umakishore and Campbell, Roy},
  year = {2021},
  month = jun,
  volume = {5},
  pages = {1--28},
  issn = {2476-1249},
  doi = {10.1145/3460095},
  abstract = {This work presents the first-ever detailed and large-scale measurement analysis of storage consumption behavior of applications (apps) on smart mobile devices. We start by carrying out a five-year longitudinal static analysis of millions of Android apps to study the increase in their sizes over time and identify various sources of app storage consumption. Our study reveals that mobile apps have evolved as large monolithic packages that are packed with features to monetize/engage users and optimized for performance at the cost of redundant storage consumption.             We also carry out a mobile storage usage study with 140 Android participants. We built and deployed a lightweight context-aware storage tracing tool, called cosmos, on each participant's device. Leveraging the traces from our user study, we show that only a small fraction of apps/features are actively used and usage is correlated to user context. Our findings suggest a high degree of app feature bloat and unused functionality, which leads to inefficient use of storage. Furthermore, we found that apps are not constrained by storage quota limits, and developers freely abuse persistent storage by frequently caching data, creating debug logs, user analytics, and downloading advertisements as needed.             Finally, drawing upon our findings, we discuss the need for efficient mobile storage management, and propose an elastic storage design to reclaim storage space when unused. We further identify research challenges and quantify expected storage savings from such a design. We believe our findings will be valuable to the storage research community as well as mobile app developers.},
  file = {D\:\\GDrive\\zotero\\Bijlani et al\\bijlani_et_al_2021_where_did_my_256_gb_go.pdf},
  journal = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  language = {en},
  number = {2}
}

@techreport{biolchiniSystematicReviewSoftware2005,
  title = {Systematic {{Review}} in {{Software Engineering}}},
  author = {Biolchini, Jorge and Gomes Mian, Paula and Candida Cruz Natali, Ana and Horta Travassos, Guilherme},
  year = {2005},
  file = {D\:\\GDrive\\zotero\\Biolchini\\biolchini_2005_systematic_review_in_software_engineering.pdf}
}

@inproceedings{biradarBuildMinimalDocker2018,
  title = {Build {{Minimal Docker Container Using Golang}}},
  booktitle = {2018 {{Second International Conference}} on {{Intelligent Computing}} and {{Control Systems}} ({{ICICCS}})},
  author = {Biradar, Sangam M and Shekhar, R and Reddy, A. Pranayanath},
  year = {2018},
  month = jun,
  pages = {1--4},
  doi = {10.1109/ICCONS.2018.8663172},
  abstract = {Docker is a platform which can contain `N' number of container. Each container internallyis an Encapsulation of Linux kernel feature i.e. namespaces and cgroup. Isolating the process from other process in the same server can be achieved using Namespace isolation. It is done so that it can't see certain portion of overall system. Control groups as its name describes it controls the process, memory and CPU. It also keeps track of used memory. Multiple containers are present on a Docker hub each is having its own virtual environment. It is like a multi tenancy where the Containers are shared through same host kernel but each of it has their own virtualized network adapter and filesystem. It allows to develop, deployment and management applications in efficient manner. Docker containers are developed using Go Language. Go language is used to provision to support system programming to the container. Containers are light weighted and portable, as said it is an encapsulation of an environment in which the application is to be to run. Containers are created from images. Itcontains all dependencies and binaries need for an application to run. Containerization is the new way to build, ship and deploy applications. To create a container with minimal storage is a challenging task. Here in this paper we have tried to show how to create a Docker container with minimal storage.},
  file = {D\:\\GDrive\\zotero\\Biradar et al\\biradar_et_al_2018_build_minimal_docker_container_using_golang.pdf},
  keywords = {cgroups,Container,Containers,Control systems,Docker,Go language,Kernel,Linux,nam espaces,Process control,Servers}
}

@techreport{birgissonMacaroonsCookiesContextual2014,
  title = {Macaroons: {{Cookies}} with {{Contextual Caveats}} for {{Decentralized Authorization}} in the {{Cloud}}},
  author = {Birgisson, Arnar and Gibbs Politz, Joe and Erlingsson, Ulfar and Taly, Ankur and Vrable, Michael and Lentczner, Mark},
  year = {2014},
  abstract = {Controlled sharing is fundamental to distributed systems; yet, on the Web, and in the Cloud, sharing is still based on rudimentary mechanisms. More flexible, decentralized cryptographic authorization credentials have not been adopted, largely because their mechanisms have not been incrementally deployable, simple enough, or efficient enough to implement across the relevant systems and devices. This paper introduces macaroons: flexible authorization credentials for Cloud services that support decentralized delegation between principals. Macaroons are based on a construction that uses nested, chained MACs (e.g., HMACs [43]) in a manner that is highly efficient, easy to deploy, and widely applicable. Although macaroons are bearer credentials, like Web cookies, macaroons embed caveats that attenuate and contextually confine when, where, by who, and for what purpose a target service should authorize requests. This paper describes macaroons and motivates their design, compares them to other credential systems, such as cookies and SPKI/SDSI [14], evaluates and measures a prototype implementation, and discusses practical security and application considerations. In particular, it is considered how macaroons can enable more fine-grained authorization in the Cloud, e.g., by strengthening mechanisms like OAuth2 [17], and a formalization of macaroons is given in authorization logic.},
  file = {D\:\\GDrive\\zotero\\Birgisson\\birgisson_2014_macaroons.pdf},
  isbn = {1891562355}
}

@techreport{birmanCLOUDCOMPUTINGRESEARCH,
  title = {{{TOWARDS A CLOUD COMPUTING RESEARCH AGENDA}}},
  author = {Birman, Ken and Chockler, Gregory and Van Renesse, Robbert},
  abstract = {The 2008 LADIS workshop on Large Scale Distributed Systems brought together leaders from the commercial cloud computing community with researchers working on a variety of topics in distributed computing. The dialog yielded some surprises: some hot research topics seem to be of limited near-term importance to the cloud builders, while some of their practical challenges seem to pose new questions to us as systems researchers. This brief note summarizes our impressions. Workshop Background LADIS is an annual workshop focusing on the state of the art in distributed systems. The workshops are by invitation, with the organizing committee setting the agenda. In 2008, the committee included ourselves, Eliezer Dekel, Paul Dantzig, Danny Dolev, and Mike Spreitzer. The workshop website, at http://www.cs.cornell.edu/projects/ladis2008/, includes the detailed agenda, white papers, and slide sets [23]; proceedings are available electronically from the ACM Portal web site [22]. LADIS 2008 Topic The 2008 LADIS topic was Cloud Computing, and more specifically: \textbullet{} Management infrastructure tools (examples would include Chubby [4], Zookeeper [28], Paxos In 2008, LADIS had three keynote speakers, one of whom shared his speaking slot with a colleague: \textbullet{} Jerry Cuomo, IBM Fellow, VP, and CTO for IBM's Websphere product line. Websphere is IBMs flagship product in the web services space, and consists of a scalable platform for deploying and managing demanding web services applications. Cuomo has been a key player in the effort since its inception. \textbullet{} James Hamilton, at that time a leader within Microsoft's new Cloud Computing Initiative. Hamilton came to the area from a career spent designing and deploying scalable database systems and clustered data management platforms, first at Oracle and then at Microsoft. (Subsequent to LADIS, he joined Amazon.com.) \textbullet{} Franco Travostino and Randy Shoup, who lead eBay's architecture and scalability effort. Both had long histories in the parallel database arena before joining eBay and both participated in eBay's scale-out from early in that company's launch. We won't try and summarize the three talks (slide sets for all of them are online at the LADIS web site, and additional materials such as blogs and videotaped talks at [18], [29]). Rather, we want to},
  file = {D\:\\GDrive\\zotero\\Birman\\birman_towards_a_cloud_computing_research_agenda.pdf},
  keywords = {[24]; [20]; Boxwood [25]; Group Membership Services; Distributed Registries; Byzantine State Machine Replication [6]; etc);,Aggregation; Monitoring (Astrolabe [33]; SDIMS [36]; Tivoli; Reputation),Network-Level and other resource-managed technologies (Virtualization and Consolidation; Resource Allocation; Load Balancing; Resource Placement; Routing; Scheduling; etc);,Scalable data sharing and event notification (examples include Pub-Sub platforms; Multicast [35]; Gossip [34]; Group Communication [8]; DSM solutions like Sinfonia [1]; etc);}
}

@techreport{birmanHistoryVirtualSynchrony,
  title = {A {{History}} of the {{Virtual Synchrony Replication Model}}},
  author = {Birman, Ken},
  abstract = {Introduction A "Cloud Computing" revolution is underway, supported by massive data centers that often contain thousands (if not hundreds of thousands) of servers. In such systems, scalability is the mantra and this, in turn, compels application developers to replicate various forms of information. By replicating the data needed to handle client requests, many services can be spread over a cluster to exploit parallelism. Servers also use replication to implement high availability and fault-tolerance mechanisms, ensure low latency, implement caching, and provide distributed management and control. On the other hand, replication is hard to implement, hence developers typically turn to standard replication solutions, packaged as sharable libraries. Virtual synchrony, the technology on which this article will focus, was created by the author and his colleagues in the early 1980's to support these sorts of applications, and was the first widely adopted solution in the area. Viewed purely as a model, virtual synchrony defines rules for replicating data or a service that will behave in a manner indistinguishable from the behavior of some non-replicated reference system running on a single non-faulty node. The model is defined in the standard asynchronous network model for crash failures. This turns out to be ideal for the uses listed above. The Isis Toolkit, which implemented virtual synchrony and was released to the public in 1987, quickly became popular. In part this was because the virtual synchrony model made it easy for developers to use replication in their applications, and in part it reflected the surprisingly good performance of the Isis protocols. For example, Isis could do replicated virtually synchronous updates at almost the same speed as one could send raw, unreliable, UDP multicast messages: a level of performance many would have assumed to be out of reach for systems providing strong guarantees. At its peak Isis was used in all sorts of critical settings (we'll talk about a few later). The virtual synchrony model was ultimately adopted by at least a dozen other systems and standardized as part of the CORBA fault-tolerance architecture. Before delving into the history of the area and the implementation details and tradeoffs that arise, it may be useful to summarize the key features of the approach. Figures 1 and 2 illustrate the model using time-space diagrams. Let's focus initially on Figure 1, which shows a nearly synchronous execution; we'll talk about Figure 2 in a moment. First, notation. Time advances from left to right, and we see timelines for processes p, q, r, s and t: active applications hosted in a network (some might run on the same machine, but probably each is on a machine by itself). Notice the shaded oval: the virtual synchrony model is focused on the creation, management and use of process groups. In the figures, process p creates a process group, which is subsequently joined by process q, and then by r, s and t. Eventually p and q are suspected of having crashed, and at time 60 the group adjusts itself to drop them. Multicasts are denoted by arrows from process to process: for example, at time 32, process q sends a multicast to the group, which is delivered to p, r, s and t: the current members during that period of the execution. Figure 1: Synchronous run. Figure 2: Virtually synchronous run.},
  file = {D\:\\GDrive\\zotero\\Birman\\birman_a_history_of_the_virtual_synchrony_replication_model.pdf}
}

@article{birmanHowHiddenHand,
  title = {How the {{Hidden Hand Shapes}} the {{Market}} for {{Software}}},
  author = {Birman, Ken},
  file = {D\:\\GDrive\\zotero\\Birman\\birman_how_the_hidden_hand_shapes_the_market_for_software.pdf}
}

@techreport{biryukovAnalysisProbingParallel2021,
  title = {Analysis and {{Probing}} of {{Parallel Channels}} in the {{Lightning Network}}},
  author = {Biryukov, Alex and Naumenko, Gleb and Tikhomirov, Sergei},
  year = {2021},
  abstract = {The Lightning Network (LN) is a prominent scalability solution for Bitcoin that allows for low-latency off-chain payments through a network of payment channels. LN users lock bitcoins into collaboratively owned addresses and redistribute the ownership of these funds without confirming each transfer on-chain. The LN introduces new privacy challenges. In this paper, we focus on channel balance probing. We propose a new model of the LN that accounts for parallel and unidirectional channels, which has not been done in prior work. We describe a probing algorithm that accurately updates the attacker's balance estimates without the need to directly connect to victims. We introduce an uncertainty-based metric to measure the attacker's information gain. We implement the first probing-focused LN simulator and suggest several countermeasures against general probing (implemented considering parallel channels). We evaluate these techniques using the simulator, as well as experiments on the real network. According to our simulations, an attacker can infer up to \$80\textbackslash\%\$\textasciitilde information regarding channel balances spending \$\textbackslash approx 20\$\textasciitilde seconds per channel. The suggested countermeasures limit the attacker's gain at \$30\textbackslash\%\$, while also increasing the attack time by 2-4x. In addition, we describe sophisticated attack techniques that combine fee-probing and channel jamming to get precise access to individual channel balances inside a hop, and test them against the real network. Finally, we discuss payment flows and their concealment.},
  file = {D\:\\GDrive\\zotero\\Biryukov et al\\biryukov_et_al_2021_analysis_and_probing_of_parallel_channels_in_the_lightning_network.pdf;C\:\\Users\\Admin\\Zotero\\storage\\R6A2QEV5\\384.html},
  keywords = {applications,Bitcoin,Lightning Network,payment channel network,payment channels,privacy},
  number = {384}
}

@article{biryukovBitcoinTorIsn2015,
  title = {Bitcoin over {{Tor}} Isn't a Good Idea},
  author = {Biryukov, Alex and Pustogarov, Ivan},
  year = {2015},
  doi = {10.1109/SP.2015.15},
  abstract = {Bitcoin is a decentralized P2P digital currency in which coins are generated by a distributed set of miners and transactions are broadcasted via a peer-to-peer network. While Bitcoin provides some level of anonymity (or rather pseudonymity) by encouraging the users to have any number of random-looking Bitcoin addresses, recent research shows that this level of anonymity is rather low. This encourages users to connect to the Bitcoin network through anonymizers like Tor and motivates development of default Tor functionality for popular mobile SPV clients. In this paper we show that combining Tor and Bitcoin creates a new attack vector. A low-resource attacker can gain full control of information flows between all users who chose to use Bitcoin over Tor. In particular the attacker can link together user's transactions regardless of pseudonyms used, control which Bitcoin blocks and transactions are relayed to user and can delay or discard user's transactions and blocks. Moreover, we show how an attacker can fingerprint users and then recognize them and learn their IP addresses when they decide to connect to the Bitcoin network directly.},
  file = {D\:\\GDrive\\zotero\\Biryukov\\biryukov_2015_bitcoin_over_tor_isn't_a_good_idea.pdf}
}

@techreport{biryukovContentPopularityAnalysis2014,
  title = {Content and Popularity Analysis of {{Tor}} Hidden Services},
  author = {Biryukov, Alex and Pustogarov, Ivan and Thill, Fabrice and Weinmann, Ralf-Philipp},
  year = {2014},
  abstract = {Tor hidden services allow running Internet services while protecting the location of the servers. Their main purpose is to enable freedom of speech even in situations in which powerful adversaries try to suppress it. However, providing location privacy and client anonymity also makes Tor hidden services an attractive platform for every kind of imaginable shady service. The ease with which Tor hidden services can be set up has spurred a huge growth of anonymously provided Internet services of both types. In this paper we analyse the landscape of Tor hidden services. We have studied 39824 hidden service descriptors collected on 4th of Feb 2013: we scanned them for open ports; in the case of 3050 HTTP services, we analysed and classified their content. We also estimated the popularity of hidden services by looking at the request rate for hidden service descriptors by clients. We found that while the content of Tor hidden services is rather varied, the most popular hidden services are related to botnets. We also propose a method for opportunistic deanonymisation of Tor Hidden Service clients. In addtiton, we identify past attempts to track "Silkroad" by consensus history analysis.},
  file = {D\:\\GDrive\\zotero\\Biryukov et al\\biryukov_et_al_2014_content_and_popularity_analysis_of_tor_hidden_services.pdf},
  keywords = {classification,hidden services,port scanning,Tor}
}

@techreport{biryukovDeanonymisationClientsBitcoin,
  title = {Deanonymisation of Clients in {{Bitcoin P2P}} Network},
  author = {Biryukov, Alex and Khovratovich, Dmitry and Pustogarov, Ivan},
  abstract = {Bitcoin is a digital currency which relies on a distributed set of miners to mint coins and on a peer-to-peer network to broadcast transactions. The identities of Bitcoin users are hidden behind pseudonyms (public keys) which are recommended to be changed frequently in order to increase transaction unlinkability. We present an efficient method to deanonymize Bitcoin users, which allows to link user pseudonyms to the IP addresses where the transactions are generated. Our techniques work for the most common and the most challenging scenario when users are behind NATs or firewalls of their ISPs. They allow to link transactions of a user behind a NAT and to distinguish connections and transactions of different users behind the same NAT. We also show that a natural countermeasure of using Tor or other anonymity services can be cutoff by abusing anti-DoS countermeasures of the Bitcoin network. Our attacks require only a few machines and have been experimentally verified. We propose several countermeasures to mitigate these new attacks 1 .},
  file = {D\:\\GDrive\\zotero\\Biryukov\\biryukov_deanonymisation_of_clients_in_bitcoin_p2p_network.pdf}
}

@incollection{biryukovDifferentialPowerAnalysis2011,
  title = {Differential {{Power Analysis}}},
  booktitle = {Encyclopedia of {{Cryptography}} and {{Security}}},
  author = {Biryukov, Alex and De Canni{\`e}re, Christophe and Winkler, William E. and Aggarwal, Charu C. and Kuhn, Markus and Bouganim, Luc and Guo, Yanli and Preneel, Bart and Bleumer, Gerrit and Helleseth, Tor and Canetti, Ran and Varia, Mayank and Peters, Christiane and Kaliski, Burt and Desmedt, Yvo and Kesidis, George and De Soete, Marijke and Bleumer, Gerrit and Schoenmakers, Berry and Biryukov, Alex and Adams, Carlisle and Biryukov, Alex and Biham, Eli and Caddy, Tom and Dwork, Cynthia and Biryukov, Alex and Just, Mike and Finiasz, Matthieu and Sendrier, Nicolas and Sako, Kazue and Gaborit, Philippe and Sendrier, Nicolas and Boneh, Dan and Cachin, Christian and Gordon, Dan and Li, Ninghui and di Vimercati, Sabrina De Capitani and Videau, Marion and Yanushkevich, Svetlana N. and Popel, Denis V. and Kang, Brent Byung Hoon and Ioannidis, John and Christodorescu, Mihai and Ganapathy, Vinod and Kang, Brent Byung Hoon and Srivastava, Anurag and Smith, Sean W.},
  year = {2011},
  volume = {1666},
  pages = {336--338},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  issn = {16113349},
  doi = {10.1007/978-1-4419-5906-5_196},
  abstract = {Cryptosystem designers frequently assume that secrets will be manipulated in closed, reliable computing environments. Unfortunately, actual computers and microchips leak information about the operations they process. This paper examines specific methods for analyzing power consumption measurements to find secret keys from tamper resistant devices. We also discuss approaches for building cryptosystems that can operate securely in existing hardware that leaks information.},
  isbn = {3-540-66347-9},
  keywords = {Cryptanalysis,DES,Differential power analysis,DPA,SPA}
}

@techreport{bishopConspiracyInformationFlow1996,
  title = {Conspiracy and {{Information Flow}} in the {{Take}}-{{Grant Protection Model}}},
  author = {Bishop, Matt},
  year = {1996},
  volume = {4},
  pages = {331--359},
  abstract = {The Take Grant Protection Model is a theoretic model of access control that captures the notion of information flow throughout the modelled system. This paper analyzes the problem of sharing information in the context of paths along which information can flow, and presents the number of actors necessary and sufficient to share information, in this model. The results are applied to information flow in a network to reduce the size of the set of actors who could have participated in the theft.},
  file = {D\:\\GDrive\\zotero\\Bishop\\bishop_1996_conspiracy_and_information_flow_in_the_take-grant_protection_model.pdf},
  journal = {Appeared in Journal of Computer Security},
  number = {4}
}

@techreport{biswasExOROpportunisticMultiHop2005,
  title = {{{ExOR}}: {{Opportunistic Multi}}-{{Hop Routing}} for {{Wireless Networks}}},
  author = {Biswas, Sanjit and Morris, Robert},
  year = {2005},
  file = {D\:\\GDrive\\zotero\\Biswas\\biswas_2005_exor.pdf}
}

@techreport{BitcoinAcademicPedigree,
  title = {Bitcoin's {{Academic}}  {{Pedigree}}},
  file = {D\:\\GDrive\\zotero\\undefined\\bitcoin’s_academic_pedigree.pdf}
}

@techreport{blackDisarminglyForthrightMSCS2009,
  title = {Disarmingly {{Forthright MSCS Advice}} *},
  author = {Black, Nick},
  year = {2009},
  abstract = {Abstract As a second-year CSMS student at GT (CSBS '05, from this same beloved Institute) and a presumptuous cad, I thought fit to provide some advice "from the trenches" as it were, sans hindsight or indeed even a 4.0.},
  file = {D\:\\GDrive\\zotero\\Black\\black_2009_disarmingly_forthright_mscs_advice.pdf},
  keywords = {research,to-read}
}

@techreport{blanchetteLittleManualAPI2008,
  title = {The {{Little Manual}} of {{API Design}}},
  author = {Blanchette, Jasmin},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Blanchette\\blanchette_2008_the_little_manual_of_api_design.pdf}
}

@article{blasLLVMObfuscatorLLVM2014,
  title = {{{LO}}! {{LLVM Obfuscator An LLVM}} Obfuscator for Binary Patch Generation},
  author = {Blas, Francisco and Riera, Izquierdo},
  year = {2014},
  file = {D\:\\GDrive\\zotero\\Blas\\blas_2014_lo.pdf},
  number = {January}
}

@book{bletschJumpOrientedProgrammingNew2011,
  title = {Jump-{{Oriented Programming}}: {{A New Class}} of {{Code}}-{{Reuse Attack}}},
  author = {Bletsch, Tyler and Jiang, Xuxian and Freeh, Vince W and Liang, Zhenkai},
  year = {2011},
  abstract = {Return-oriented programming is an effective code-reuse attack in which short code sequences ending in a ret instruction are found within existing binaries and executed in arbitrary order by taking control of the stack. This allows for Turing-complete behavior in the target program without the need for injecting attack code, thus significantly negating current code injection defense efforts (e.g., W{$\oplus$}X). On the other hand, its inherent characteristics, such as the reliance on the stack and the consecutive execution of return-oriented gadgets, have prompted a variety of defenses to detect or prevent it from happening. In this paper, we introduce a new class of code-reuse attack , called jump-oriented programming. This new attack eliminates the reliance on the stack and ret instructions (including ret-like instructions such as pop+jmp) seen in return-oriented programming without sacrificing expressive power. This attack still builds and chains functional gadgets, each performing certain primitive operations, except these gadgets end in an indirect branch rather than ret. Without the convenience of using ret to unify them, the attack relies on a dispatcher gadget to dispatch and execute the functional gadgets. We have successfully identified the availability of these jump-oriented gadgets in the GNU libc library. Our experience with an example shellcode attack demonstrates the practicality and effectiveness of this technique.},
  file = {D\:\\GDrive\\zotero\\Bletsch\\bletsch_2011_jump-oriented_programming.pdf},
  isbn = {978-1-4503-0564-8}
}

@article{BlockchainBitcoin2016,
  title = {Blockchain {{Beyond Bitcoin}}},
  year = {2016},
  doi = {10.1145/2994581},
  abstract = {Blockchain technology has the potential to revolutionize applications and redefine the digital economy. the second Internet, personal computers , and local area networks. The third platform delivers computing anywhere, immediately, and allows organizations to deploy and consume computing resources in shared communities. Says Versace, "The core capabilities of the third platform of technology are beyond any we have seen before. Innovation accelerators like blockchain mean we can achieve technology value outcomes that we couldn't achieve before." This is promising, but there are caveats. Sandeep Kumar, managing director of capital markets and a block-chain specialist at digital business consulting and technology services firm Synechron, names data privacy, scal-ability, and interoperability as three key challenges to blockchain technology that are pervasive across applica-B},
  file = {D\:\\GDrive\\zotero\\undefined\\2016_blockchain_beyond_bitcoin.pdf}
}

@techreport{bocchinojrvikramsadveVectorLLVAVirtual,
  title = {Vector {{LLVA}}: {{A Virtual Vector Instruction Set}} for {{Media Processing}} *},
  author = {Bocchino Jr Vikram S Adve, Robert L},
  abstract = {We present Vector LLVA, a virtual instruction set architecture (V-ISA) that exposes extensive static information about vector paral-lelism while avoiding the use of hardware-specific parameters. We provide both arbitrary-length vectors (for targets that allow vectors of arbitrary length, or where the target length is not known) and fixed-length vectors (for targets that have a fixed vector length, such as subword SIMD extensions), together with a rich set of operations on both vector types. We have implemented translators that compile (1) Vector LLVA written with arbitrary-length vectors to the Motorola RSVP architecture and (2) Vector LLVA written with fixed-length vectors to both AltiVec and Intel SSE2. Our translator-generated code achieves speedups competitive with handwritten native code versions of several benchmarks on all three architec-tures. These experiments show that our VISA design captures vector parallelism for two quite different classes of architectures and provides virtual object code portability within the class of subword SIMD architectures.},
  file = {D\:\\GDrive\\zotero\\Bocchino Jr Vikram S Adve\\bocchino_jr_vikram_s_adve_vector_llva.pdf},
  keywords = {D34 [Software]: Program-ming Languages-Processors General Terms Performance,Languages Keywords Multimedia,SIMD,Vector,Virtual Instruction Sets}
}

@techreport{bogdanovESSENTIAAudioAnalysis,
  title = {{{ESSENTIA}}: An {{Audio Analysis Library}} for {{Music Information Retrieval CompMusic View}} Project {{BirdVox}}: {{Machine Listening}} for {{Bird Migration Monitoring View}} Project {{ESSENTIA}}: {{AN AUDIO ANALYSIS LIBRARY FOR MUSIC INFORMATION RETRIEVAL}}},
  author = {Bogdanov, Dmitry and Herrera, Perfecto and Mayor, Oscar and Wack, Nicolas and G{\'o}mez, Emilia and Gulati, Sankalp and Roma, Gerard and Salamon, Justin and Zapata, Jos{\'e} and Serra, Xavier},
  abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music de-scriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
  file = {D\:\\GDrive\\zotero\\Bogdanov\\bogdanov_essentia.pdf}
}

@article{bohmeBitcoinEconomicsTechnology,
  title = {Bitcoin: {{Economics}}, {{Technology}}, and {{Governance}}},
  author = {B{\"o}hme, Rainer and Christin, Nicolas and Edelman, Benjamin and Moore, Tyler},
  doi = {10.1257/jep.29.2.213},
  file = {D\:\\GDrive\\zotero\\Böhme\\böhme_bitcoin.pdf}
}

@article{bohmeEfficiencyAutomatedTesting2014,
  title = {On the Efficiency of Automated Testing},
  author = {B{\"o}hme, Marcel and Paul, Soumya},
  year = {2014},
  volume = {16-21-Nove},
  pages = {632--642},
  doi = {10.1145/2635868.2635923},
  abstract = {The aim of automated program testing is to gain confidence about a program's correctness by sampling its input space. The sampling process can be either systematic or random. For every systematic testing technique the sampling is informed by the analysis of some program artefacts, like the specification, the source code (e.g., to achieve coverage), or even faulty versions of the program (e.g., mutation testing). This analysis incurs some cost. In contrast, random testing is unsystematic and does not sustain any analysis cost. In this paper, we investigate the theoretical efficiency of systematic versus random testing. First, we mathematically model the most effective systematic testing technique S0 in which every sampled test input strictly increases the "degree of confidence" and is subject to the analysis cost c. Note that the efficiency of S0 depends on c. Specifically, if we increase c, we also increase the time it takes S0 to establish the same degree of confidence. So, there exists a maximum analysis cost beyond which R is generally more efficient than S0. Given that we require the confidence that the program works correctly for x\% of its input, we prove an upper bound on c of S0, beyond which R is more efficient on the average. We also show that this bound depends asymptotically only on x. For instance, let R take 10ms time to sample one test input; to establish that the program works correctly for 90\% of its input, S0 must take less than 41ms to sample one test input. Otherwise, R is expected to establish the 90\%-degree of confidence earlier. We prove similar bounds on the cost if the software tester is interested in revealing as many errors as possible in a given time span.},
  file = {D\:\\GDrive\\zotero\\Böhme\\böhme_2014_on_the_efficiency_of_automated_testing.pdf},
  isbn = {9781450330565},
  journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
  keywords = {Efficient testing,Error-based partitioning,Partition testing,Random testing,Testing theory}
}

@article{bojinovKamouflageLossResistantPassword2010,
  title = {Kamouflage : {{Loss}}-{{Resistant Password Management}}},
  author = {Bojinov, Hristo and Bursztein, Elie and Boyen, Xavier and Boneh, Dan},
  year = {2010},
  pages = {286--302},
  abstract = {We introduce Kamouflage: a new architecture for building theft-resistant password managers. An attacker who steals a laptop or cell phone with a Kamouflage-based password manager is forced to carry out a considerable amount of online work before obtaining any user credentials. We implemented our proposal as a replacement for the built-in Firefox password manager, and provide performance measurements and the results from experiments with large real-world password sets to evaluate the feasibility and effectiveness of our approach. Kamouflage is well suited to become a standard architecture for password managers on mobile devices.},
  file = {D\:\\GDrive\\zotero\\Bojinov\\bojinov_2010_kamouflage.pdf}
}

@incollection{bonehAggregateVerifiablyEncrypted2003,
  title = {Aggregate and {{Verifiably Encrypted Signatures}} from {{Bilinear Maps}}},
  booktitle = {Advances in {{Cryptology}} \textemdash{} {{EUROCRYPT}} 2003},
  author = {Boneh, Dan and Gentry, Craig and Lynn, Ben and Shacham, Hovav},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Biham, Eli},
  year = {2003},
  volume = {2656},
  pages = {416--432},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-39200-9_26},
  abstract = {An aggregate signature scheme is a digital signature that supports aggregation: Given n signatures on n distinct messages from n distinct users, it is possible to aggregate all these signatures into a single short signature. This single signature (and the n original messages) will convince the verifier that the n users did indeed sign the n original messages (i.e., user i signed message Mi for i = 1, . . . , n). In this paper we introduce the concept of an aggregate signature, present security models for such signatures, and give several applications for aggregate signatures. We construct an efficient aggregate signature from a recent short signature scheme based on bilinear maps due to Boneh, Lynn, and Shacham. Aggregate signatures are useful for reducing the size of certificate chains (by aggregating all signatures in the chain) and for reducing message size in secure routing protocols such as SBGP. We also show that aggregate signatures give rise to verifiably encrypted signatures. Such signatures enable the verifier to test that a given ciphertext C is the encryption of a signature on a given message M . Verifiably encrypted signatures are used in contract-signing protocols. Finally, we show that similar ideas can be used to extend the short signature scheme to give simple ring signatures.},
  file = {D\:\\GDrive\\zotero\\Boneh et al\\boneh_et_al_2003_aggregate_and_verifiably_encrypted_signatures_from_bilinear_maps.pdf},
  isbn = {978-3-540-14039-9 978-3-540-39200-2},
  language = {en}
}

@inproceedings{bonehGroupSignaturesVerifierlocal2004,
  title = {Group Signatures with Verifier-Local Revocation},
  booktitle = {Proceedings of the 11th {{ACM}} Conference on {{Computer}} and Communications Security  - {{CCS}} '04},
  author = {Boneh, Dan and Shacham, Hovav},
  year = {2004},
  pages = {168},
  publisher = {{ACM Press}},
  address = {{Washington DC, USA}},
  doi = {10.1145/1030083.1030106},
  abstract = {Group signatures have recently become important for enabling privacy-preserving attestation in projects such as Microsoft's ngscb effort (formerly Palladium). Revocation is critical to the security of such systems. We construct a short group signature scheme that supports VerifierLocal Revocation (VLR). In this model, revocation messages are only sent to signature verifiers (as opposed to both signers and verifiers). Consequently there is no need to contact individual signers when some user is revoked. This model is appealing for systems providing attestation capabilities. Our signatures are as short as standard RSA signatures with comparable security. Security of our group signature (in the random oracle model) is based on the Strong DiffieHellman assumption and the Decision Linear assumption in bilinear groups. We give a precise model for VLR group signatures and discuss its implications.},
  file = {D\:\\GDrive\\zotero\\Boneh_Shacham\\boneh_shacham_2004_group_signatures_with_verifier-local_revocation.pdf},
  isbn = {978-1-58113-961-7},
  language = {en}
}

@incollection{bonehShortGroupSignatures2004,
  title = {Short {{Group Signatures}}},
  booktitle = {Advances in {{Cryptology}} \textendash{} {{CRYPTO}} 2004},
  author = {Boneh, Dan and Boyen, Xavier and Shacham, Hovav},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Franklin, Matt},
  year = {2004},
  volume = {3152},
  pages = {41--55},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28628-8_3},
  abstract = {We construct a short group signature scheme. Signatures in our scheme are approximately the size of a standard RSA signature with the same security. Security of our group signature is based on the Strong Diffie-Hellman assumption and a new assumption in bilinear groups called the Decision Linear assumption. We prove security of our system, in the random oracle model, using a variant of the security definition for group signatures recently given by Bellare, Micciancio, and Warinschi.},
  file = {D\:\\GDrive\\zotero\\Boneh et al\\boneh_et_al_2004_short_group_signatures.pdf},
  isbn = {978-3-540-22668-0 978-3-540-28628-8},
  language = {en}
}

@article{bonehShortSignaturesWeil2004,
  title = {Short {{Signatures}} from the {{Weil Pairing}}},
  author = {Boneh, Dan and Lynn, Ben and Shacham, Hovav},
  year = {2004},
  pages = {24},
  abstract = {We introduce a short signature scheme based on the Computational Diffie-Hellman assumption on certain elliptic and hyper-elliptic curves. For standard security parameters, the signature length is about half that of a DSA signature with a similar level of security. Our short signature scheme is designed for systems where signatures are typed in by a human or are sent over a low-bandwidth channel. We survey a number of properties of our signature scheme such as signature aggregation and batch verification.},
  file = {D\:\\GDrive\\zotero\\Boneh et al\\boneh_et_al_2004_short_signatures_from_the_weil_pairing.pdf},
  language = {en}
}

@techreport{bonehTwentyYearsAttacks,
  title = {Twenty {{Years}} of {{Attacks}} on the {{RSA Cryptosystem}}},
  author = {Boneh, Dan},
  file = {D\:\\GDrive\\zotero\\Boneh\\boneh_twenty_years_of_attacks_on_the_rsa_cryptosystem.pdf}
}

@techreport{bonnaire-sergeantPracticalOptionalType,
  title = {A {{Practical Optional Type System}} for {{Clojure}}},
  author = {{Bonnaire-Sergeant}, Ambrose},
  abstract = {Dynamic programming languages often abandon the advantages of static type checking in favour of their characteristic convenience and flexibility. Static type checking eliminates many common user errors at compile-time that are otherwise unnoticed, or are caught later in languages without static type checking. A recent trend is to aim to combine the advantages of both kinds of languages by adding optional static type systems to languages without static type checking, while preserving the idioms and style of the language. This dissertation describes my work on designing an optional static type system for the Clojure programming language, a dynamically typed dialect of Lisp, based on the lessons learnt from several projects, primarily Typed Racket. This work includes designing and building a type checker for Clojure running on the Java Virtual Machine. Several experiments are conducted using this prototype, particularly involving existing Clojure code that is sufficiently complicated that type checking increases confidence that the code is correct. For example, nearly all of algo.monads, a Clojure Contrib library for monadic programming, is able to be type checked. Most monad, monad transformer, and monadic function definitions can be type checked, usually by adding type annotations in natural places like function definitions. There is significant future work to fully type check all Clojure features and idioms. For example, multimethod definitions and functions with particular constraints on the number of variable arguments they accept (particularly functions taking only an even number of variable arguments) are troublesome. Also, there are desirable features from the Typed Racket project that are missing, such as automatic runtime contract generation and a sophisticated blame system, both which are designed to improve error messages when mixing typed and untyped code in similar systems. Overall, the work described in this dissertation leads to the conclusion that it appears to be both practical and useful to design and implement an optional static type system for the Clojure programming language.},
  file = {D\:\\GDrive\\zotero\\Bonnaire-Sergeant\\bonnaire-sergeant_a_practical_optional_type_system_for_clojure.pdf},
  keywords = {Clojure ii,optional type systems,Programming languages}
}

@phdthesis{bonneauGuessingHumanchosenSecrets2012,
  title = {Guessing Human-Chosen Secrets},
  author = {Bonneau, Joseph},
  year = {2012},
  number = {April}
}

@article{bonneauNumber817Quest2012,
  title = {Number 817 {{The}} Quest to Replace Passwords: A Framework for Comparative Evaluation of {{Web}} Authentication Schemes},
  author = {Bonneau, Joseph and Herley, Cormac and Van Oorschot, Paul C and Stajano, Frank},
  year = {2012},
  issn = {1476-2986},
  file = {D\:\\GDrive\\zotero\\Bonneau\\bonneau_2012_number_817_the_quest_to_replace_passwords.pdf},
  keywords = {ss}
}

@article{bonneauPasswordThicketTechnical2010,
  title = {The Password Thicket: Technical and Market Failures in Human Authentication on the Web},
  author = {Bonneau, Joseph},
  year = {2010},
  volume = {8},
  pages = {230--237},
  doi = {10.1.1.165.3804},
  abstract = {We report the results of the first large-scale empirical analysis of password implementations deployed on the Internet. Our study included 150 websites which offer free user accounts for a variety of purposes, including the most popular destinations on the web and a random sample of e-commerce, news, and communication websites. Although all sites evaluated relied on user-chosen textual passwords for authentication, we found many subtle but important technical variations in im- plementation with important security implications. Many poor practices were commonplace, such as a lack of encryption to protect transmitted passwords, storage of cleartext passwords in server databases, and little protection of passwords from brute force attacks. While a spectrum of imple- mentation quality exists with a general co-occurrence of more-secure and less-secure implementa- tion choices, we find a surprising number of inconsistent choices within individual sites, suggesting that the lack of a standards is harming security. We observe numerous ways in which the technical failures of lower-security sites can compromise higher-security sites due to the well-established ten- dency of users to re-use passwords. Our data confirms that the worst security practices are indeed found at sites with few security incentives, such as newspaper websites, while sites storing more sensitive information such as payment details or user communication implement more password se- curity. From an economic viewpoint, password insecurity is a negative externality that the market has been unable to correct, undermining the viability of password-based authentication. We also speculate that some sites deploying passwords do so primarily for psychological reasons, both as a justification for collecting marketing data and as a way to build trusted relationships with customers. This theory suggests that efforts to replace passwords with more-secure protocols or federated iden- tity systems may fail because they dont recreate the entrenched ritual of password authentication.},
  journal = {Information Security},
  keywords = {human authentication,passwords,security economic}
}

@article{bonneauScienceGuessingAnalyzing2012,
  title = {The Science of Guessing: Analyzing an Anonymized Corpus of 70 Million Passwords},
  author = {Bonneau, Joseph},
  year = {2012},
  doi = {10.1109/SP.2012.49},
  abstract = {We report on the largest corpus of user-chosen passwords ever studied, consisting of anonymized password histograms representing almost 70 million Yahoo! users, mitigating privacy concerns while enabling analysis of dozens of subpopulations based on demographic factors and site usage characteristics. This large data set motivates a thorough statistical treatment of estimating guessing difficulty by sampling from a secret distribution. In place of previously used metrics such as Shannon entropy and guessing entropy, which cannot be estimated with any realistically sized sample, we develop partial guessing metrics including a new variant of guesswork parameterized by an attacker's desired success rate. Our new metric is comparatively easy to approximate and directly relevant for security engineering. By comparing password distributions with a uniform distribution which would provide equivalent security against different forms of guessing attack, we estimate that passwords provide fewer than 10 bits of security against an online, trawling attack, and only about 20 bits of security against an optimal offline dictionary attack. We find surprisingly little variation in guessing difficulty; every identifiable group of users generated a comparably weak password distribution. Security motivations such as the registration of a payment card have no greater impact than demographic factors such as age and nationality. Even pro-active efforts to nudge users towards better password choices with graphical feedback make little difference. More surprisingly , even seemingly distant language communities choose the same weak passwords and an attacker never gains more than a factor of 2 efficiency gain by switching from the globally optimal dictionary to a population-specific lists.},
  file = {D\:\\GDrive\\zotero\\Bonneau\\bonneau_2012_the_science_of_guessing.pdf}
}

@inproceedings{bonneauSoKResearchPerspectives2015,
  title = {{{SoK}}: {{Research}} Perspectives and Challenges for Bitcoin and Cryptocurrencies},
  booktitle = {Proceedings - {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Bonneau, Joseph and Miller, Andrew and Clark, Jeremy and Narayanan, Arvind and Kroll, Joshua A. and Felten, Edward W.},
  year = {2015},
  volume = {2015-July},
  pages = {104--121},
  doi = {10.1109/SP.2015.14},
  abstract = {Bit coin has emerged as the most successful cryptographic currency in history. Within two years of its quiet launch in 2009, Bit coin grew to comprise billions of dollars of economic value despite only cursory analysis of the system's design. Since then a growing literature has identified hidden-but-important properties of the system, discovered attacks, proposed promising alternatives, and singled out difficult future challenges. Meanwhile a large and vibrant open-source community has proposed and deployed numerous modifications and extensions. We provide the first systematic exposition Bit coin and the many related crypto currencies or 'altcoins.' Drawing from a scattered body of knowledge, we identify three key components of Bit coin's design that can be decoupled. This enables a more insightful analysis of Bit coin's properties and future stability. We map the design space for numerous proposed modifications, providing comparative analyses for alternative consensus mechanisms, currency allocation mechanisms, computational puzzles, and key management tools. We survey anonymity issues in Bit coin and provide an evaluation framework for analyzing a variety of privacy-enhancing proposals. Finally we provide new insights on what we term disinter mediation protocols, which absolve the need for trusted intermediaries in an interesting set of applications. We identify three general disinter mediation strategies and provide a detailed comparison.},
  file = {D\:\\GDrive\\zotero\\Bonneau et al\\bonneau_et_al_2015_sok.pdf},
  isbn = {9781467369497},
  keywords = {ss}
}

@techreport{bonomiLNCS4168Improved2006,
  title = {{{LNCS}} 4168 - {{An Improved Construction}} for {{Counting Bloom Filters}}},
  author = {Bonomi, Flavio and Mitzenmacher, Michael and Panigrahy, Rina and Singh, Sushil and Varghese, George},
  year = {2006},
  abstract = {A counting Bloom filter (CBF) generalizes a Bloom filter data structure so as to allow membership queries on a set that can be changing dynamically via insertions and deletions. As with a Bloom filter, a CBF obtains space savings by allowing false positives. We provide a simple hashing-based alternative based on d-left hashing called a d-left CBF (dlCBF). The dlCBF offers the same functionality as a CBF, but uses less space, generally saving a factor of two or more. We describe the construction of dlCBFs, provide an analysis, and demonstrate their effectiveness experimentally.},
  file = {D\:\\GDrive\\zotero\\Bonomi\\bonomi_2006_lncs_4168_-_an_improved_construction_for_counting_bloom_filters.pdf}
}

@article{boonstoppelRWsetAttackingPath2008,
  title = {{{RWset}}: {{Attacking}} Path Explosion in Constraint-Based Test Generation},
  author = {Boonstoppel, Peter and Cadar, Cristian and Engler, Dawson},
  year = {2008},
  volume = {4963 LNCS},
  pages = {351--366},
  issn = {03029743},
  doi = {10.1007/978-3-540-78800-3_27},
  abstract = {Recent work has used variations of symbolic execution to automatically generate high-coverage test inputs [3, 4, 7, 8, 14]. Such tools have demonstrated their ability to find very subtle errors. However, one challenge they all face is how to effectively handle the exponential number of paths in checked code. This paper presents a new technique for reducing the number of traversed code paths by discarding those that must have side-effects identical to some previously explored path. Our results on a mix of open source applications and device drivers show that this (sound) optimization reduces the numbers of paths traversed by several orders of magnitude, often achieving program coverage far out of reach for a standard constraint-based execution system. \textcopyright{} 2008 Springer-Verlag Berlin Heidelberg.},
  file = {D\:\\GDrive\\zotero\\Boonstoppel\\boonstoppel_2008_rwset.pdf},
  isbn = {3540787992},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{bosHardwareAssistedFineGrainedCodeReuse2015,
  title = {Hardware-{{Assisted Fine}}-{{Grained Code}}-{{Reuse Attack Detection}}},
  author = {Bos, Herbert and Monrose, Fabian and Blanc, Gregory},
  year = {2015},
  volume = {9404},
  pages = {427--447},
  issn = {16113349},
  doi = {10.1007/978-3-319-26362-5},
  abstract = {In ransomware attacks, the actual target is the human, as opposed to the classic attacks that abuse the infected devices (e.g., botnet renting, information stealing). Mobile devices are by no means immune to ransomware attacks. However, there is little research work on thismat- ter and only traditional protections are available. Even state-of-the-art mobile malware detection approaches are ineffective against ransomware apps because of the subtle attack scheme. As a consequence, the ample attack surface formed by the billion mobile devices is left unprotected. First, in this work we summarize the results of our analysis of the exist- ing mobile ransomware families, describing their common characteristics. Second, we present HelDroid, a fast, efficient and fully automated app- roach that recognizes known and unknown scareware and ransomware samples from goodware. Our approach is based on detecting the ``build- ing blocks'' that are typically needed to implement a mobile ransomware application. Specifically, HelDroid detects, in a generic way, if an app is attempting to lock or encrypt the device without the user's consent, and if ransom requests are displayed on the screen. Our technique works without requiring that a sample of a certain family is available before- hand. We implemented HelDroid and tested it on real-world Android ran- somware samples. On a large dataset comprising hundreds of thousands of APKs including goodware,malware, scareware, and ransomware, Hel- Droid exhibited nearly zero false positives and the capability of recog- nizing unknown ransomware samples. 1},
  file = {D\:\\GDrive\\zotero\\Bos\\bos_2015_hardware-assisted_fine-grained_code-reuse_attack_detection.pdf},
  isbn = {978-3-319-26361-8},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {code-reuse attack,control flow integrity,indirect branch}
}

@techreport{bosshartP4ProgrammingProtocolIndependent,
  title = {P4: {{Programming Protocol}}-{{Independent Packet Processors}}},
  author = {Bosshart, Pat and Daly, Dan and Gibb, Glen and Izzard, Martin and Mckeown, Nick and Rexford, Jennifer and Schlesinger, Cole and Talayco, Dan and Vahdat, Amin and Varghese, George and Walker, David},
  abstract = {P4 is a high-level language for programming protocol-independent packet processors. P4 works in conjunction with SDN control protocols like OpenFlow. In its current form, OpenFlow explicitly specifies protocol headers on which it operates. This set has grown from 12 to 41 fields in a few years, increasing the complexity of the specification while still not providing the flexibility to add new headers. In this paper we propose P4 as a strawman proposal for how Open-Flow should evolve in the future. We have three goals: (1) Reconfigurability in the field: Programmers should be able to change the way switches process packets once they are deployed. (2) Protocol independence: Switches should not be tied to any specific network protocols. (3) Target independence: Programmers should be able to describe packet-processing functionality independently of the specifics of the underlying hardware. As an example, we describe how to use P4 to configure a switch to add a new hierarchical label.},
  file = {D\:\\GDrive\\zotero\\Bosshart\\bosshart_p4.pdf}
}

@article{bourgainUnifiedTheorySparse2013,
  title = {Toward a Unified Theory of Sparse Dimensionality Reduction in {{Euclidean}} Space},
  author = {Bourgain, Jean and Dirksen, Sjoerd and Nelson, Jelani},
  year = {2013},
  month = nov,
  abstract = {Let \$\textbackslash Phi\textbackslash in\textbackslash mathbb\{R\}\^\{m\textbackslash times n\}\$ be a sparse Johnson-Lindenstrauss transform [KN14] with \$s\$ non-zeroes per column. For a subset \$T\$ of the unit sphere, \$\textbackslash varepsilon\textbackslash in(0,1/2)\$ given, we study settings for \$m,s\$ required to ensure \$\$ \textbackslash mathop\{\textbackslash mathbb\{E\}\}\_\textbackslash Phi \textbackslash sup\_\{x\textbackslash in T\} \textbackslash left|\textbackslash |\textbackslash Phi x\textbackslash |\_2\^2 - 1 \textbackslash right| {$<$} \textbackslash varepsilon , \$\$ i.e. so that \$\textbackslash Phi\$ preserves the norm of every \$x\textbackslash in T\$ simultaneously and multiplicatively up to \$1+\textbackslash varepsilon\$. We introduce a new complexity parameter, which depends on the geometry of \$T\$, and show that it suffices to choose \$s\$ and \$m\$ such that this parameter is small. Our result is a sparse analog of Gordon's theorem, which was concerned with a dense \$\textbackslash Phi\$ having i.i.d. Gaussian entries. We qualitatively unify several results related to the Johnson-Lindenstrauss lemma, subspace embeddings, and Fourier-based restricted isometries. Our work also implies new results in using the sparse Johnson-Lindenstrauss transform in numerical linear algebra, classical and model-based compressed sensing, manifold learning, and constrained least squares problems such as the Lasso.},
  file = {D\:\\GDrive\\zotero\\Bourgain\\bourgain_2013_toward_a_unified_theory_of_sparse_dimensionality_reduction_in_euclidean_space.pdf}
}

@inproceedings{boyenForwardsecureSignaturesUntrusted2006,
  title = {Forward-Secure Signatures with Untrusted Update},
  booktitle = {Proceedings of the 13th {{ACM}} Conference on {{Computer}} and Communications Security  - {{CCS}} '06},
  author = {Boyen, Xavier and Shacham, Hovav and Shen, Emily and Waters, Brent},
  year = {2006},
  pages = {191--200},
  publisher = {{ACM Press}},
  address = {{Alexandria, Virginia, USA}},
  doi = {10.1145/1180405.1180430},
  abstract = {In most forward-secure signature constructions, a program that updates a user's private signing key must have full access to the private key. Unfortunately, these schemes are incompatible with several security architectures including Gnu Privacy Guard (GPG) and S/MIME, where the private key is encrypted under a user password as a ``second factor'' of security, in case the private key storage is corrupted, but the password is not.},
  file = {D\:\\GDrive\\zotero\\Boyen et al\\boyen_et_al_2006_forward-secure_signatures_with_untrusted_update.pdf},
  isbn = {978-1-59593-518-2},
  language = {en}
}

@techreport{bradenNetworkWorkingGroup1994,
  title = {Network {{Working Group}}},
  author = {Braden, R and Clark, Dave and Shenker, Scott and Zhang, Lixia and Estrin, Deborah and Jamin, Sugih and Wroclawski, John and Herzog, Shai and Braden, Bob},
  year = {1994},
  abstract = {Integrated Services in the Internet Architecture: an Overview Status of this Memo This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind. Distribution of this memo is unlimited. Abstract This memo discusses a proposed extension to the Internet architecture and protocols to provide integrated services, i.e., to support real-time as well as the current non-real-time service of IP. This extension is necessary to meet the growing need for real-time service for a variety of new applications, including teleconferencing, remote seminars, telescience, and distributed simulation. This memo represents the direct product of recent work},
  file = {D\:\\GDrive\\zotero\\Braden\\braden_1994_network_working_group.pdf}
}

@techreport{bradenProtocolStackProtocol,
  title = {From {{Protocol Stack}} to {{Protocol Heap}}-{{Role}}-{{Based Architecture}}},
  author = {Braden, Robert and Faber, Ted and Handley, Mark},
  abstract = {Questioning whether layering is still an adequate foundation for networking architectures, this paper investigates non-layered approaches to the design and implementation of network protocols. The goals are greater flexibility and control with fewer feature interation problems. The paper further proposes a specific non-layered paradigm called role-based architecture.},
  file = {D\:\\GDrive\\zotero\\Braden\\braden_from_protocol_stack_to_protocol_heap-role-based_architecture.pdf}
}

@article{brainardNewTwoServerApproach2003,
  title = {A {{New Two}}-{{Server Approach}} for {{Authentication}} with {{Short Secrets}} ( {{To}} Appear in {{USENIX Security}} ' 03 )},
  author = {Brainard, John and Juels, Ari and Kaliski, Burt and Szydlo, Michael},
  year = {2003},
  file = {D\:\\GDrive\\zotero\\Brainard\\brainard_2003_a_new_two-server_approach_for_authentication_with_short_secrets_(_to_appear_in.pdf}
}

@article{brasserEnclaveComputingParadigm2020,
  title = {Enclave {{Computing Paradigm}}: {{Hardware}}-Assisted {{Security Architectures}} \&amp; {{Applications}}},
  shorttitle = {Enclave {{Computing Paradigm}}},
  author = {Brasser, Franz Ferdinand Peter},
  year = {2020},
  publisher = {{UNSPECIFIED}},
  doi = {10.25534/TUPRINTS-00011912},
  abstract = {Hardware-assisted security solutions, and the isolation guarantees they provide, constitute the basis for the protection of modern software systems. Hardware-enforced isolation of individual components reduces complexity of the overall software as well as the size and complexity of the individual components. The basic idea is that a reduction in complexity minimizes the probability of vulnerabilities in the software, thus strengthening the system's security. In classical system architectures, an application's security depends on the security of all privileged system entities, for example the Operating System. The Trusted Execution Environment (TEE) concept overcomes the dependence of security critical components on the systems overall security. TEEs provide isolated compartments within a single system, allowing isolated operation of a system's individual components and applications. The enclave computing paradigm enhances the TEE concept by enabling self-contained isolation of system components and applications, fulfilling the needs of modern software. It enables novel use cases by providing many parallel mutually isolated TEE-instances without the need to rely on complex privileged entities. The TEE solutions developed by industry and deployed in today's systems follow distinct design approaches and come with various limitations. ARM TrustZone, which is widely available in mobile devices, is fundamentally limited to a single isolation domain. Intel's TEE solution Software Guard Extensions (SGX) provides multiple mutually isolated execution environments, called enclaves. However, SGX enclaves face severe threats, in particular side-channel leakage, that can void its security guarantees. Preventing side-channel leakage from enclaves in a universal and efficient way is a non-trivial problem. Nevertheless, these deployed TEE solutions enable various novel applications. However, different TEE architectures come with diverse properties and features that require special consideration in the design of TEE applications. Security architectures for embedded systems face additional challenges that have not been solved, neither by industry nor by academic research. These security architectures need to be compliant with and need to preserve all functional requirements of an embedded system. Since network-connected embedded devices are increasingly used in safety critical systems, such as industrial control systems or automotive scenarios, security architectures that combine safety and security aspects are vitally needed. Remote Attestation (RA) is a security service that relies on the isolation guarantees of TEEs. It is of particularly high relevance for connected embedded systems. It allows trust establishment between these devices enabling their reliable collaboration in large connected systems. However, many aspects of RA, such as its scalability in large networks or its applicability in autonomous connected systems, are unexplored. In this dissertation, we present novel isolation architectures that bring the enclave computing paradigm to mobile and embedded platforms. We present the first security architecture for small embedded systems that provides isolated execution enclaves and real-time guarantees. Moreover, we present a novel multi-TEE security architecture for TrustZone-systems bringing the enclave computing paradigm to mobile systems, overcoming TrustZone's fundamental limitation. Furthermore, we deal with Intel SGX's vulnerability to side-channel attacks. We demonstrate the severity of side-channel leakage due to observable memory access patterns of SGX enclaves. To counter side-channel attacks, we present solutions that hide memory access patterns of enclaves for both accesses to enclave-external memory as well as access patterns within enclaves' private memory. We present two TEE-applications that follow different design approaches, leveraging the specific capabilities of Intel SGX and ARM TrustZone, respectively. We introduce a cloud-based machine learning solution that enables privacy-preserving speech recognition utilizing isolated execution enclaves. We also demonstrate the limitations of the enclave computing paradigm and show a (remote) policy enforcement solution for mobile devices, which requires an isolated execution environment with elevated privileges. Additionally, we investigate novel RA schemes, which tackle many important aspects of RA that are highly relevant in emerging connected systems. We develop solutions to prevent the misuse of remote attestation for Denial-of-Service (DoS) attacks and present the first efficient multi-prover attestation scheme. Furthermore, we introduce the concept of data integrity attestation, which allows the efficient and reliable collaboration of autonomous connected devices.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  file = {D\:\\GDrive\\zotero\\Brasser\\brasser_2020_enclave_computing_paradigm.pdf},
  language = {en}
}

@article{brasserSoftwareGrandExposure2017,
  title = {Software Grand Exposure: {{SGX}} Cache Attacks Are Practical},
  author = {Brasser, Ferdinand and M{\"u}ller, Urs and Dmitrienko, Alexandra and Kostiainen, Kari and Capkun, Srdjan and Sadeghi, Ahmad Reza},
  year = {2017},
  abstract = {Intel SGX isolates the memory of security-critical applications from the untrusted OS. However, it has been speculated that SGX may be vulnerable to side-channel attacks through shared caches. We developed new cache attack techniques customized for SGX. Our attack differs from other SGX cache attacks in that it is easy to deploy and avoids known detection approaches. We demonstrate the effectiveness of our attack on two case studies: RSA decryption and genomic processing. While cache timing attacks against RSA and other cryptographic operations can be prevented by using appropriately hardened crypto libraries, the same cannot be easily done for other computations, such as genomic processing. Our second case study therefore shows that attacks on non-cryptographic but privacy sensitive operations are a serious threat. We analyze countermeasures and show that none of the known defenses eliminates the attack.},
  file = {D\:\\GDrive\\zotero\\Brasser\\brasser_2017_software_grand_exposure.pdf},
  journal = {11th USENIX Workshop on Offensive Technologies, WOOT 2017, co-located with USENIX Security 2017}
}

@article{brasserTyTANTinyTrust2015,
  title = {{{TyTAN}}: {{Tiny}} Trust Anchor for Tiny Devices},
  author = {Brasser, Ferdi NAND and El Mahjoub, Brahim and Sadeghi, Ahmad Reza and Wachsmann, Christian and Koeberl, Patrick},
  year = {2015},
  volume = {2015-July},
  publisher = {{IEEE}},
  issn = {0738100X},
  doi = {10.1145/2744769.2744922},
  abstract = {Embedded systems are at the core of many security-sensitive and safety-critical applications, including automotive, industrial control systems, and critical infrastructures. Existing protection mechanisms against (software-based) malware are inflexible, too complex, expensive, or do not meet real-time requirements. We present TyTAN, which, to the best of our knowledge, is the first security architecture for embedded systems that provides (1) hardware-assisted strong isolation of dynamically configurable tasks and (2) real-time guarantees. We implemented TyTAN on the Intel\textregistered{} Siskiyou Peak embedded platform and demonstrate its efficiency and effectiveness through extensive evaluation.},
  file = {D\:\\GDrive\\zotero\\Brasser\\brasser_2015_tytan.pdf},
  isbn = {9781450335201},
  journal = {Proceedings - Design Automation Conference}
}

@techreport{breivoldAnalyzingSoftwareEvolvability,
  title = {Analyzing {{Software Evolvability}}},
  author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Eriksson, Peter J},
  abstract = {Software evolution is characterized by inevitable changes of software and increasing software complexities, which in turn may lead to huge costs unless rigorously taking into account change accommodations. This is in particular true for long-lived systems in which changes go beyond maintainability. For such systems, there is a need to address evolvability explicitly during the entire lifecycle. Nevertheless, there is a lack of a model that can be used for analyzing, evaluating and comparing software systems in terms of evolvability. In this paper, we describe the initial establishment of an evolvability model as a framework for analysis of software evolvability. We motivate and exemplify the model through an industrial case study of a software-intensive automation system.},
  file = {D\:\\GDrive\\zotero\\Breivold\\breivold_analyzing_software_evolvability.pdf}
}

@article{BrewerConjectureFeasibility,
  title = {Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services},
  file = {D\:\\GDrive\\zotero\\undefined\\brewer's_conjecture_and_the_feasibility_of_consistent,_available,.pdf}
}

@techreport{brewerElectronicVersionRecreated1974,
  title = {Electronic Version Recreated {{The UNIX Time}}-{{Sharing System}}},
  author = {Brewer, Eric A and Ritchie, Dennis M and Thompson, Ken},
  year = {1974},
  volume = {17},
  institution = {{ACM}},
  abstract = {UNIX is a general-purpose, multiuser , interactive operating system for the Digital Equipment Corporation PDP-11/40 and 11/45 computers. It offers a number of features seldom found even in larger operating systems , including: (1) a hierarchical file system incorporating demountable volumes; (2) compatible file, device, and inter-process I/O; (3) the ability to initiate asynchro-nous processes; (4) system command language select-able on a per-user basis; and (5) over 100 subsystems including a dozen languages. This paper discusses the nature and implementation of the file system and of the user command interface.},
  file = {D\:\\GDrive\\zotero\\Brewer\\brewer_1974_electronic_version_recreated_the_unix_time-sharing_system.pdf},
  keywords = {and Phrases: time-sharing,command language,file system,operating system,PDP-11}
}

@techreport{briscoeFlowRateFairness,
  title = {Flow {{Rate Fairness}}: {{Dismantling}} a {{Religion}}},
  author = {Briscoe, Bob},
  abstract = {Resource allocation and accountability keep reappearing on every list of requirements for the Internet architecture. The reason we never resolve these issues is a broken idea of what the problem is. The applied research and standards communities are using completely unrealistic and impractical fairness criteria. The resulting mechanisms don't even allocate the right thing and they don't allocate it between the right entities. We explain as bluntly as we can that thinking about fairness mechanisms like TCP in terms of sharing out flow rates has no intellectual heritage from any concept of fairness in philosophy or social science, or indeed real life. Comparing flow rates should never again be used for claims of fairness in production networks. Instead, we should judge fairness mechanisms on how they share out the 'cost' of each user's actions on others.},
  file = {D\:\\GDrive\\zotero\\Briscoe\\briscoe_flow_rate_fairness.pdf},
  keywords = {account-ability,congestion control,fairness,identity,K41 [Computers and Society]: Public Policy Issues; C21 [Computer-communication networks]: Network Architecture and Design General Terms Economics,Security Keywords Resource allocation}
}

@techreport{broderNetworkApplicationsBloom,
  title = {Network {{Applications}} of {{Bloom Filters}}: {{A Survey}}},
  author = {Broder, Andrei and Mitzenmacher, Michael},
  volume = {1},
  pages = {485--509},
  abstract = {A Bloom filter is a simple space-efficient randomized data structure for representing a set in order to support membership queries. Bloom filters allow false positives but the space savings often outweigh this drawback when the probability of an error is controlled. Bloom filters have been used in database applications since the 1970s, but only in recent years have they become popular in the networking literature. The aim of this paper is to survey the ways in which Bloom filters have been used and modified in a variety of network problems, with the aim of providing a unified mathematical and practical framework for understanding them and stimulating their use in future applications.},
  file = {D\:\\GDrive\\zotero\\Broder\\broder_network_applications_of_bloom_filters.pdf},
  journal = {Internet Mathematics},
  number = {4}
}

@techreport{brooksNoSilverBulletEssence,
  title = {No {{Silver Bullet}}-{{Essence}} and {{Accident}} in {{Software Engineering}}},
  author = {Brooks, Frederick P},
  abstract = {There is no single development, in either technology or management technique, which by itself promises even one order-of-magnitude improvement within a decade in productivity, in reliability, in simplicity. Abstract 1 All software construction involves essential tasks, the fashioning of the complex conceptual structures that compose the abstract software entity, and accidental tasks, the representation of these abstract entities in programming languages and the mapping of these onto machine languages within space and speed constraints.},
  file = {D\:\\GDrive\\zotero\\Brooks\\brooks_no_silver_bullet-essence_and_accident_in_software_engineering.pdf}
}

@article{brownHowBuildStatic2016,
  title = {How to Build Static Checking Systems Using Orders of Magnitude Less Code},
  author = {Brown, Fraser and N{\"o}tzli, Andres and Engler, Dawson},
  year = {2016},
  volume = {51},
  pages = {143--157},
  issn = {15232867},
  doi = {10.1145/2872362.2872364},
  abstract = {Modern static bug finding tools are complex. They typically consist of hundreds of thousands of lines of code, and most of them are wedded to one language (or even one compiler). This complexity makes the systems hard to understand, hard to debug, and hard to retarget to new languages, thereby dramatically limiting their scope. This paper reduces checking system complexity by addressing a fundamental assumption, the assumption that checkers must depend on a full-blown language specification and compiler front end. Instead, our program checkers are based on drastically incomplete language grammars ("micro-grammars") that describe only portions of a language relevant to a checker. As a result, our implementation is tiny-roughly 2500 lines of code, about two orders of magnitude smaller than a typical system. We hope that this dramatic increase in simplicity will allow people to use more checkers on more systems in more languages. We implement our approach in \textmu chex, a language-agnostic framework for writing static bug checkers. We use it to build micro-grammar based checkers for six languages (C, the C preprocessor, C++, Java, JavaScript, and Dart) and find over 700 errors in real-world projects.},
  file = {D\:\\GDrive\\zotero\\Brown\\brown_2016_how_to_build_static_checking_systems_using_orders_of_magnitude_less_code.pdf},
  isbn = {9781450340915},
  journal = {ACM SIGPLAN Notices},
  keywords = {Bug finding,Micro-grammars,Parsing,Static analysis},
  number = {4}
}

@inproceedings{brownVerifiedRangeAnalysis2020,
  title = {Towards a Verified Range Analysis for {{JavaScript JITs}}},
  booktitle = {Proceedings of the 41st {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Brown, Fraser and Renner, John and N{\"o}tzli, Andres and Lerner, Sorin and Shacham, Hovav and Stefan, Deian},
  year = {2020},
  month = jun,
  pages = {135--150},
  publisher = {{ACM}},
  address = {{London UK}},
  doi = {10.1145/3385412.3385968},
  abstract = {We present VeRA, a system for verifying the range analysis pass in browser just-in-time (JIT) compilers. Browser developers write range analysis routines in a subset of C++, and verification developers write infrastructure to verify custom analysis properties. Then, VeRA automatically verifies the range analysis routines, which browser developers can integrate directly into the JIT. We use VeRA to translate and verify Firefox range analysis routines, and it detects a new, confirmed bug that has existed in the browser for six years.},
  file = {D\:\\GDrive\\zotero\\Brown et al\\brown_et_al_2020_towards_a_verified_range_analysis_for_javascript_jits.pdf},
  isbn = {978-1-4503-7613-6},
  language = {en}
}

@techreport{bruceMiniBlockchainScheme2014,
  title = {The {{Mini}}-{{Blockchain Scheme}}},
  author = {Bruce, J D},
  year = {2014},
  abstract = {Almost all P2P crypto-currencies prevent double spending and similar such attacks with a bulky "blockchain" scheme, and the ones which do not typically use some sort of pseudo-decentralized solution to manage the transactions. Here we propose a purely P2P crypto-currency scheme where old transactions can be forgotten by the network. Since nodes only require the newest portion of the blockchain in order to sync with the network, we call this portion of the chain the "mini-blockchain". We argue that the loss of security this trimming process incurs can be solved with a small "proof chain" and the loss of coin ownership data is solved with a database which holds the balance of all non-empty addresses, dubbed the "account tree". The proof chain secures the mini-blockchain and the mini-blockchain secures the account tree. This paper will describe the way in which these three mechanisms can work together to form a system which provides a high level of integrity and security, yet is much slimmer than all other purely P2P currencies. It also offers other potential benefits such as faster transactions and lower fees, quicker network synchronization, support for high levels of traffic, more block space for custom messages, and potentially even increased anonymity.},
  file = {D\:\\GDrive\\zotero\\Bruce\\bruce_2014_the_mini-blockchain_scheme.pdf}
}

@techreport{brumleyRemoteTimingAttacks2003,
  title = {Remote Timing Attacks Are Practical},
  author = {Brumley, David and Boneh, Dan},
  year = {2003},
  pages = {1--13},
  abstract = {Timing attacks are usually used to attack weak computing devices such as smartcards. We show that timing attacks apply to general software systems. Specifically, we devise a timing attack against OpenSSL. Our experiments show that we can extract private keys from an OpenSSL-based web server running on a machine in the local network. Our results demonstrate that timing attacks against network servers are practical and therefore security systems should defend against them.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\8DS732SM\\-729000187.pdf;D\:\\GDrive\\zotero\\Brumley\\brumley_2003_remote_timing_attacks_are_practical.pdf;D\:\\GDrive\\zotero\\Brumley\\brumley_remote_timing_attacks_are_practical.pdf},
  keywords = {side-channel,ss}
}

@techreport{brutlagSpeedMattersGoogle2009,
  title = {Speed {{Matters}} for {{Google Web Search}}},
  author = {Brutlag, Jake},
  year = {2009},
  abstract = {Experiments demonstrate that increasing web search latency 100 to 400 ms reduces the daily number of searches per user by 0.2\% to 0.6\%. Furthermore , users do fewer searches the longer they are exposed. For longer delays, the loss of searches persists for a time even after latency returns to previous levels.},
  file = {D\:\\GDrive\\zotero\\Brutlag\\brutlag_2009_speed_matters_for_google_web_search.pdf}
}

@article{bSecureServerBasedPseudorandom2017,
  title = {A {{Secure Server}}-{{Based Pseudorandom Number}}},
  author = {B, Hooman Alavizadeh and Alavizadeh, Hootan and Dube, Kudakwashe},
  year = {2017},
  volume = {1},
  pages = {860--876},
  doi = {10.1007/978-3-319-72359-4},
  file = {D\:\\GDrive\\zotero\\B\\b_2017_a_secure_server-based_pseudorandom_number.pdf},
  isbn = {9783319723594},
  keywords = {geographical latitude and longitude,key management,mobile security,pseudorandom number generator}
}

@article{buchananWhenGoodInstructions2008,
  title = {When {{Good Instructions Go Bad}}: {{Generalizing Return}}-{{Oriented Programming}} to {{RISC}}},
  author = {Buchanan, Erik and Roemer, Ryan and Shacham, Hovav and Savage, Stefan},
  year = {2008},
  abstract = {This paper reconsiders the threat posed by Shacham's "return-oriented programming"-a technique by which W{$\oplus$}X-style hardware protections are evaded via carefully crafted stack frames that divert control flow into the middle of existing variable-length x86 instructions creating short new instructions streams that then return. We believe this attack is both more general and a greater threat than the author appreciated. In fact, the vulnerability is not limited to the x86 architecture or any particular operating system, is readily ex-ploitable, and bypasses an entire category of malware protections. In this paper we demonstrate general return-oriented programming on the SPARC, a fixed instruction length RISC architecture with structured control flow. We construct a Turing-complete library of code gadgets using snippets of the Solaris libc, a general purpose programming language, and a compiler for constructing return-oriented exploits. Finally, we argue that the threat posed by return-oriented programming, across all architectures and systems , has negative implications for an entire class of security mechanisms: those that seek to prevent malicious computation by preventing the execution of malicious code.},
  file = {D\:\\GDrive\\zotero\\Buchanan\\buchanan_2008_when_good_instructions_go_bad.pdf},
  keywords = {Algorithms Keywords Return-oriented programming,D46 [Operating Systems]: Security and Protection General Terms Security,return-into-libc,RISC,SPARC}
}

@inproceedings{buchananWhenGoodInstructions2008a,
  title = {When Good Instructions Go Bad: Generalizing Return-Oriented Programming to {{RISC}}},
  shorttitle = {When Good Instructions Go Bad},
  booktitle = {Proceedings of the 15th {{ACM}} Conference on {{Computer}} and Communications Security - {{CCS}} '08},
  author = {Buchanan, Erik and Roemer, Ryan and Shacham, Hovav and Savage, Stefan},
  year = {2008},
  pages = {27},
  publisher = {{ACM Press}},
  address = {{Alexandria, Virginia, USA}},
  doi = {10.1145/1455770.1455776},
  abstract = {This paper reconsiders the threat posed by Shacham's ``return-oriented programming'' \textemdash{} a technique by which W{$\oplus$}X-style hardware protections are evaded via carefully crafted stack frames that divert control flow into the middle of existing variable-length x86 instructions \textemdash{} creating short new instructions streams that then return. We believe this attack is both more general and a greater threat than the author appreciated. In fact, the vulnerability is not limited to the x86 architecture or any particular operating system, is readily exploitable, and bypasses an entire category of malware protections.},
  file = {D\:\\GDrive\\zotero\\Buchanan et al\\buchanan_et_al_2008_when_good_instructions_go_bad.pdf},
  isbn = {978-1-59593-810-7},
  language = {en}
}

@article{buczakSurveyDataMining2016,
  title = {A {{Survey}} of {{Data Mining}} and {{Machine Learning Methods}} for {{Cyber Security Intrusion Detection}}},
  author = {Buczak, Anna L and Guven, Erhan},
  year = {2016},
  volume = {18},
  pages = {1153},
  doi = {10.1109/COMST.2015.2494502},
  abstract = {This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tuto-rial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.},
  file = {D\:\\GDrive\\zotero\\Buczak\\buczak_2016_a_survey_of_data_mining_and_machine_learning_methods_for_cyber_security.pdf},
  journal = {IEEE COMMUNICATIONS SURVEYS \& TUTORIALS},
  number = {2}
}

@article{Bufferbloatsolved,
  title = {Bufferbloatsolved},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\25PYQXY2\\bufferbloatsolved.pdf}
}

@article{bulckFORESHADOWExtractingKeys2018,
  title = {{{FORESHADOW}}: {{Extracting}} the {{Keys}} to the {{Intel SGX Kingdom}} with {{Transient Out}}-of-{{Order Execution}}},
  author = {Bulck, Jo Van and Minkin, Marina and Weisse, Ofir and Genkin, Daniel and Kasikci, Baris and Piessens, Frank and Silberstein, Mark and Wenisch, Thomas F and Yarom, Yuval and Strackx, Raoul},
  year = {2018},
  pages = {19},
  abstract = {Trusted execution environments, and particularly the Software Guard eXtensions (SGX) included in recent Intel x86 processors, gained significant traction in recent years. A long track of research papers, and increasingly also realworld industry applications, take advantage of the strong hardware-enforced confidentiality and integrity guarantees provided by Intel SGX. Ultimately, enclaved execution holds the compelling potential of securely offloading sensitive computations to untrusted remote platforms. We present Foreshadow, a practical software-only microarchitectural attack that decisively dismantles the security objectives of current SGX implementations. Crucially, unlike previous SGX attacks, we do not make any assumptions on the victim enclave's code and do not necessarily require kernel-level access. At its core, Foreshadow abuses a speculative execution bug in modern Intel processors, on top of which we develop a novel exploitation methodology to reliably leak plaintext enclave secrets from the CPU cache. We demonstrate our attacks by extracting full cryptographic keys from Intel's vetted architectural enclaves, and validate their correctness by launching rogue production enclaves and forging arbitrary local and remote attestation responses. The extracted remote attestation keys affect millions of devices.},
  file = {D\:\\GDrive\\zotero\\Bulck et al\\bulck_et_al_2018_foreshadow.pdf},
  language = {en}
}

@phdthesis{bunnieADAMDecentralizedParallel2002,
  title = {{{ADAM}}: {{A Decentralized Parallel Computer Architecture Featuring Fast Thread}} and {{Data Migration}} and a {{Uniform Hardware Abstraction}}},
  author = {Bunnie, Andrew " and Huang, "},
  year = {2002},
  file = {D\:\\GDrive\\zotero\\Bunnie\\bunnie_2002_adam.pdf}
}

@book{burnimHeuristicsScalableDynamic,
  title = {Heuristics for {{Scalable Dynamic Test Generation}}},
  author = {Burnim, Jacob and Sen, Koushik},
  abstract = {Recently there has been great success in using symbolic execution to automatically generate test inputs for small software systems. A primary challenge in scaling such approaches to larger programs is the combinatorial explosion of the path space. It is likely that sophisticated strategies for searching this path space are needed to generate inputs that effectively test large programs (by, e.g., achieving significant branch coverage). We present several such heuristic search strategies, including a novel strategy guided by the control flow graph of the program under test. We have implemented these strategies in CREST, our open source concolic testing tool for C, and evaluated them on two widely-used software tools, grep 2.2 (15K lines of code) and Vim 5.7 (150K lines). On these benchmarks, the presented heuristics achieve significantly greater branch coverage on the same testing budget than concolic testing with a traditional depth-first search strategy.},
  file = {D\:\\GDrive\\zotero\\Burnim\\burnim_heuristics_for_scalable_dynamic_test_generation.pdf},
  isbn = {978-1-4244-2188-6}
}

@techreport{burnsBorgOmegaKubernetes,
  title = {Borg, {{Omega}}, and {{Kubernetes}}},
  author = {Burns, Brendan and Grant, Brian and Oppenheimer, David and Brewer, Eric and Wilkes, John},
  abstract = {system evolution T hough widespread interest in software containers is a relatively recent phenomenon, at Google we have been managing Linux containers at scale for more than ten years and built three different container-management systems in that time. Each system was heavily influenced by its predecessors, even though they were developed for different reasons. This article describes the lessons we've learned from developing and operating them. The first unified container-management system developed at Google was the system we internally call Borg. 7 It was built to manage both long-running services and batch jobs, which had previously been handled by two separate systems: Babysitter and the Global Work Queue. The latter's architecture strongly influenced Borg, but was focused on batch jobs; both predated Linux control groups. Borg shares machines between these two types of applications as a way of increasing resource utilization and thereby reducing costs. Such sharing was possible because container support in the Linux kernel was becoming available (indeed, Google contributed much of the container code to the Linux kernel), which enabled better isolation between latency-sensitive user-facing services and CPU-hungry batch processes.},
  file = {D\:\\GDrive\\zotero\\Burns\\burns_borg,_omega,_and_kubernetes.pdf},
  keywords = {distributed systems}
}

@techreport{burnsDesignPatternsContainerbased,
  title = {Design Patterns for Container-Based Distributed Systems},
  author = {Burns, Brendan and Google, David Oppenheimer},
  file = {D\:\\GDrive\\zotero\\Burns\\burns_design_patterns_for_container-based_distributed_systems.pdf}
}

@techreport{burrowsChubbyLockService,
  title = {The {{Chubby}} Lock Service for Loosely-Coupled Distributed Systems},
  author = {Burrows, Mike},
  abstract = {We describe our experiences with the Chubby lock service , which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed file system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences.},
  file = {D\:\\GDrive\\zotero\\Burrows\\burrows_the_chubby_lock_service_for_loosely-coupled_distributed_systems.pdf},
  keywords = {distributed systems}
}

@article{butlerSurveyBGPSecurity2010,
  title = {A Survey of {{BGP}} Security Issues and Solutions},
  author = {Butler, Kevin and Farley, Toni R. and McDaniel, Patrick and Rexford, Jennifer},
  year = {2010},
  volume = {98},
  pages = {100--122},
  issn = {00189219},
  doi = {10.1109/JPROC.2009.2034031},
  abstract = {As the Internet's de facto interdomain routing protocol, the Border Gateway Protocol (BGP) is the glue that holds the disparate parts of the Internet together. A major limitation of BGP is its failure to adequately address security. Recent high-profile outages and security analyses clearly indicate that the Internet routing infrastructure is highly vulnerable. Moreover, the design of BGP and the ubiquity of its deployment have frustrated past efforts at securing interdomain routing. This paper considers the current vulnerabilities of the interdomain routing system and surveys both research and standardization efforts relating to BGP security. We explore the limitations and advantages of proposed security extensions to BGP, and explain why no solution has yet struck an adequate balance between comprehensive security and deployment cost. \textcopyright{} 2006 IEEE.},
  file = {D\:\\GDrive\\zotero\\Butler\\butler_2010_a_survey_of_bgp_security_issues_and_solutions.pdf},
  journal = {Proceedings of the IEEE},
  keywords = {Authentication,Authorization,BGP,Border gateway protocol,Integrity,Interdomain routing,Network security,Networks,Routing},
  number = {1}
}

@article{byunConstructingKahlerSymplectic2001,
  title = {Constructing the {{K\"ahler}} and the Symplectic Structures from Certain Spinors on 4-Manifolds},
  author = {Byun, Y. and Lee, Y. and Park, J. and Ryu, J. S.},
  year = {2001},
  volume = {129},
  pages = {1161--1168},
  issn = {0002-9939},
  doi = {10.1090/S0002-9939-00-05587-8},
  abstract = {In cloud computing environments, multiple tenants are often co-located on the same multi-processor system. Thus, preventing information leakage between tenants is crucial. While the hypervisor enforces software isola-tion, shared hardware, such as the CPU cache or mem-ory bus, can leak sensitive information. For security rea-sons, shared memory between tenants is typically dis-abled. Furthermore, tenants often do not share a physical CPU. In this setting, cache attacks do not work and only a slow cross-CPU covert channel over the memory bus is known. In contrast, we demonstrate a high-speed covert channel as well as the first side-channel attack working across processors and without any shared memory. To build these attacks, we use the undocumented DRAM address mappings. We present two methods to reverse engineer the map-ping of memory addresses to DRAM channels, ranks, and banks. One uses physical probing of the memory bus, the other runs entirely in software and is fully au-tomated. Using this mapping, we introduce DRAMA at-tacks, a novel class of attacks that exploit the DRAM row buffer that is shared, even in multi-processor systems. Thus, our attacks work in the most restrictive environ-ments. First, we build a covert channel with a capacity of up to 2 Mbps, which is three to four orders of mag-nitude faster than memory-bus-based channels. Second, we build a side-channel template attack that can automat-ically locate and monitor memory accesses. Third, we show how using the DRAM mappings improves existing attacks and in particular enables practical Rowhammer attacks on DDR4.},
  file = {D\:\\GDrive\\zotero\\Byun\\byun_2001_constructing_the_kähler_and_the_symplectic_structures_from_certain_spinors_on.pdf},
  isbn = {9781931971324},
  journal = {Proceedings of the American Mathematical Society},
  keywords = {Kahler manifold,Parallel positive spinor,Spinc structure,Symplectic manifold},
  number = {4}
}

@article{CabralLeedom1993,
  title = {Cabral and {{Leedom}} - 1993 - {{Imaging}} Vector Fields Using Line Integral Convolut},
  file = {D\:\\GDrive\\zotero\\undefined\\cabral_and_leedom_-_1993_-_imaging_vector_fields_using_line_integral_convolut.pdf}
}

@techreport{cachinArchitectureHyperledgerBlockchain2016,
  title = {Architecture of the {{Hyperledger Blockchain Fabric}} *},
  author = {Cachin, Christian},
  year = {2016},
  abstract = {Overview. A blockchain is best understood in the model of state-machine replication [8], where a service maintains some state and clients invoke operations that transform the state and generate outputs. A blockchain emulates a "trusted" computing service through a distributed protocol, run by nodes connected over the Internet. The service represents or creates an asset, in which all nodes have some stake. The nodes share the common goal of running the service but do not necessarily trust each other for more. In a "permissionless" blockchain such as the one underlying the Bitcoin cryptocurrency, anyone can operate a node and participate through spending CPU cycles and demonstrating a "proof-of-work." On the other hand, blockchains in the "permissioned" model control who participates in validation and in the protocol; these nodes typically have established identities and form a consortium. A report of Swanson compares the two models [9].},
  file = {D\:\\GDrive\\zotero\\Cachin\\cachin_2016_architecture_of_the_hyperledger_blockchain_fabric.pdf}
}

@techreport{cachinBlockchainConsensusProtocols2017,
  title = {Blockchain {{Consensus Protocols}} in the {{Wild}}},
  author = {Cachin, Christian and Vukoli{\textasciiacute}cvukoli{\textasciiacute}c, Marko},
  year = {2017},
  abstract = {A blockchain is a distributed ledger for recording transactions, maintained by many nodes without central authority through a distributed cryptographic protocol. All nodes validate the information to be appended to the blockchain, and a consensus protocol ensures that the nodes agree on a unique order in which entries are appended. Consensus protocols for tolerating Byzantine faults have received renewed attention because they also address blockchain systems. This work discusses the process of assessing and gaining confidence in the resilience of a consensus protocols exposed to faults and adversarial nodes. We advocate to follow the established practice in cryptography and computer security, relying on public reviews, detailed models, and formal proofs; the designers of several practical systems appear to be unaware of this. Moreover, we review the consensus protocols in some prominent permissioned blockchain platforms with respect to their fault models and resilience against attacks. The protocol comparison},
  file = {D\:\\GDrive\\zotero\\Cachin\\cachin_2017_blockchain_consensus_protocols_in_the_wild.pdf}
}

@book{cachinIntroductionReliableSecure2011,
  title = {Introduction to {{Reliable}} and {{Secure Distributed Programming}}},
  author = {Cachin, Christian and Guerraoui, Rachid and Rodrigues, Lu{\'i}s},
  year = {2011},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15260-3},
  file = {D\:\\GDrive\\zotero\\Cachin et al\\cachin_et_al_2011_introduction_to_reliable_and_secure_distributed_programming.pdf},
  isbn = {978-3-642-15259-7 978-3-642-15260-3},
  language = {en}
}

@article{cadarDemandDrivenCompositionalSymbolic2013,
  title = {Demand-{{Driven Compositional Symbolic Execution}}},
  author = {Cadar, Cristian and Sen, Koushik},
  year = {2013},
  pages = {2--2},
  doi = {10.1007/978-3-642-35632-2_2},
  file = {D\:\\GDrive\\zotero\\Cadar\\cadar_2013_demand-driven_compositional_symbolic_execution.pdf}
}

@techreport{cadarEXEAutomaticallyGenerating2006,
  title = {{{EXE}}: {{Automatically Generating Inputs}} of {{Death}}},
  author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M and Dill, David L and Engler, Dawson R},
  year = {2006},
  abstract = {This paper presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be "any-thing." As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression , EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXE's constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deter-ministic code). EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the udhcpd DHCP server, the pcre regular expression library, and three Linux file systems.},
  file = {D\:\\GDrive\\zotero\\Cadar\\cadar_2006_exe.pdf}
}

@techreport{cadarKLEEUnassistedAutomatic,
  title = {{{KLEE}}: {{Unassisted}} and {{Automatic Generation}} of {{High}}-{{Coverage Tests}} for {{Complex Systems Programs}}},
  author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems , and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage-on average over 90\% per tool (median: over 94\%)-and significantly beat the coverage of the developers' own handwritten test suites. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to cross-check purportedly identical BUSY-BOX and COREUTILS utilities, finding functional cor-rectness errors and a myriad of inconsistencies.},
  file = {D\:\\GDrive\\zotero\\Cadar\\cadar_klee.pdf;D\:\\GDrive\\zotero\\Cadar\\cadar_klee2.pdf}
}

@techreport{caesarBGPRoutingPolicies,
  title = {{{BGP}} Routing Policies in {{ISP}} Networks},
  author = {Caesar, Matthew and Rexford, Jennifer},
  abstract = {The Internet has quickly evolved into a vast global network owned and operated by thousands of different administrative entities. During this time, it became apparent that vanilla shortest-path routing would be insufficient to handle the myriad operational, economic, and political factors involved in routing. ISPs began to modify routing configurations to support routing policies, i.e. goals held by the router's owner that controlled which routes were chosen and which routes were propagated to neighbors. BGP, originally a simple path-vector protocol, was incrementally modified over time with a number of mechanisms to support policies, adding substantially to the complexity. Much of the mystery in BGP comes not only from the protocol complexity but also from a lack of understanding of the underlying policies and the problems ISPs face which they address. In this paper we shed light on goals operators have and their resulting routing policies, why BGP evolved the way it did, and how common policies are implemented using BGP. We also discuss recent and current work in the field that aims to address problems that arise in applying and supporting routing policies.},
  file = {D\:\\GDrive\\zotero\\Caesar\\caesar_bgp_routing_policies_in_isp_networks.pdf}
}

@phdthesis{cahillSerializableIsolationSnapshot2009,
  title = {Serializable {{Isolation}} for {{Snapshot Databases}}},
  author = {Cahill, Michael James},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Cahill\\cahill_2009_serializable_isolation_for_snapshot_databases.pdf}
}

@incollection{calderonRethinkingVerifiablyEncrypted2014,
  title = {Rethinking {{Verifiably Encrypted Signatures}}: {{A Gap}} in {{Functionality}} and {{Potential Solutions}}},
  shorttitle = {Rethinking {{Verifiably Encrypted Signatures}}},
  booktitle = {Topics in {{Cryptology}} \textendash{} {{CT}}-{{RSA}} 2014},
  author = {Calderon, Theresa and Meiklejohn, Sarah and Shacham, Hovav and Waters, Brent},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Benaloh, Josh},
  year = {2014},
  volume = {8366},
  pages = {349--366},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-04852-9_18},
  abstract = {Verifiably encrypted signatures were introduced by Boneh, Gentry, Lynn, and Shacham in 2003, as a non-interactive analogue to interactive protocols for verifiable encryption of signatures. As their name suggests, verifiably encrypted signatures were intended to capture a notion of encryption, and constructions in the literature use public-key encryption as a building block.},
  file = {D\:\\GDrive\\zotero\\Calderon et al\\calderon_et_al_2014_rethinking_verifiably_encrypted_signatures.pdf},
  isbn = {978-3-319-04851-2 978-3-319-04852-9},
  language = {en}
}

@techreport{calmanIncreasingScopeResolution,
  title = {Increasing the Scope and Resolution of {{Interprocedural Static Single Assignment}}},
  author = {Calman, Silvian and Zhu, Jianwen},
  abstract = {While intraprocedural Static Single Assignment (SSA) is ubiquitous in modern compilers, the use of interprocedural SSA, although seemingly a natural extension, is limited. We find that part of the impediment is due to the narrow scope of variables handled by previously reported approaches, leading to limited benefits in optimization. In this study, we increase the scope of Interprocedural SSA (ISSA) to record elements and singleton heap variables. We show that ISSA scales reasonably well (to all MediaBench and most of the SPEC2K), while resolving on average 1.72 times more loads to their definition. We propose and evaluate an interprocedural copy propagation and an interprocedural liveness analysis and demonstrate their effectiveness on reducing input and output instructions by 44.5\% and 23.3\%, respectively. ISSA is then leveraged for constant propagation and dead code removal, where 11.8\% additional expressions are folded.},
  file = {D\:\\GDrive\\zotero\\Calman\\calman_increasing_the_scope_and_resolution_of_interprocedural_static_single_assignment.pdf},
  keywords = {constant propagation,dataflow,interprocedural,SSA}
}

@article{camenischPracticalUniversallyComposable2012,
  title = {Practical yet Universally Composable Two-Server Password-Authenticated Secret Sharing},
  author = {Camenisch, Jan and Lysyanskaya, Anna and Neven, Gregory},
  year = {2012},
  pages = {525--536},
  issn = {15437221},
  doi = {10.1145/2382196.2382252},
  abstract = {Password-authenticated secret sharing (PASS) schemes, first introduced by Bagherzandi et al. at CCS 2011, allow users to distribute data among several servers so that the data can be recovered using a single human-memorizable password, but no single server (or even no collusion of servers up to a certain size) can mount an off-line dictionary attack on the password or learn anything about the data. We propose a new, universally composable (UC) security definition for the two-server case (2PASS) in the public-key setting that addresses a number of relevant limitations of the previous, non-UC definition. For example, our definition makes no prior assumptions on the distribution of passwords, preserves security when honest users mistype their passwords, and guarantees secure composition with other protocols in spite of the unavoidable non-negligible success rate of online dictionary attacks. We further present a concrete 2PASS protocol and prove that it meets our definition. Given the strong security guarantees, our protocol is surprisingly efficient: in its most efficient instantiation under the DDH assumption in the random-oracle model, it requires fewer than twenty elliptic-curve exponentiations on the user's device. We achieve our results by careful protocol design and by exclusively focusing on the two-server public-key setting. Copyright \textcopyright{} 2012 ACM.},
  file = {D\:\\GDrive\\zotero\\Camenisch\\camenisch_2012_practical_yet_universally_composable_two-server_password-authenticated_secret.pdf},
  isbn = {9781450316507},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Password-authenticated secret sharing,Universal composability}
}

@techreport{canellaSystematicEvaluationTransient2019,
  title = {A {{Systematic Evaluation}} of {{Transient Execution Attacks}} and {{Defenses}}},
  author = {Canella, Claudio and Van Bulck, Jo and Schwarz, Michael and Lipp, Moritz and Von Berg, Benjamin and Ortner, Philipp and Piessens, Frank and Evtyushkin, Dmitry and Gruss, Daniel},
  year = {2019},
  abstract = {Research on transient execution attacks including Spectre and Meltdown showed that exception or branch mispredic-tion events might leave secret-dependent traces in the CPU's microarchitectural state. This observation led to a proliferation of new Spectre and Meltdown attack variants and even more ad-hoc defenses (e.g., microcode and software patches). Both the industry and academia are now focusing on finding effective defenses for known issues. However, we only have limited insight on residual attack surface and the completeness of the proposed defenses. In this paper, we present a systematization of transient execution attacks. Our systematization uncovers 6 (new) transient execution attacks that have been overlooked and not been investigated so far: 2 new exploitable Meltdown effects: Meltdown-PK (Protection Key Bypass) on Intel, and Meltdown-BND (Bounds Check Bypass) on Intel and AMD; and 4 new Spectre mistraining strategies. We evaluate the attacks in our classification tree through proof-of-concept implementations on 3 major CPU vendors (Intel, AMD, ARM). Our systematization yields a more complete picture of the attack surface and allows for a more systematic evaluation of defenses. Through this systematic evaluation, we discover that most defenses, including deployed ones, cannot fully mitigate all attack variants.},
  file = {D\:\\GDrive\\zotero\\Canella\\canella_a_systematic_evaluation_of_transient_execution_attacks_and_defenses.pdf}
}

@techreport{caoNoteStorageRequirement,
  title = {A {{Note On}} the {{Storage Requirement}} for {{AKS Primality Testing Algorithm}}},
  author = {Cao, Zhengjun},
  abstract = {We remark that AKS primality testing algorithm needs about 1,000,000,000 G (gigabyte) storage space for a number of 1024 bits. Such storage requirement is hard to meet in practice. To the best of our knowledge, it is impossible for current operating systems to write and read data in so huge storage space. Thus, the running time for AKS algorithm should not be simply estimated as usual in terms of the amount of arithmetic operations.},
  file = {D\:\\GDrive\\zotero\\Cao\\cao_a_note_on_the_storage_requirement_for_aks_primality_testing_algorithm.pdf}
}

@techreport{carboneApacheFlinkStream2015,
  title = {Apache {{Flink}}\texttrademark : {{Stream}} and {{Batch Processing}} in a {{Single Engine}}},
  author = {Carbone, Paris and Katsifodimos, Asterios and Kth, {\textdagger} and Sweden, Sics and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  year = {2015},
  abstract = {Apache Flink 1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present Flink's architecture and expand on how a (seemingly diverse) set of use cases can be unified under a single execution model.},
  file = {D\:\\GDrive\\zotero\\Carbone\\carbone_2015_apache_flink™.pdf},
  keywords = {distributed systems}
}

@techreport{carboniFEEDBACKBASEDREPUTATION,
  title = {{{FEEDBACK BASED REPUTATION ON TOP OF THE BITCOIN BLOCKCHAIN}}},
  author = {Carboni, Davide},
  abstract = {The ability to assess the reputation of a member in a web community is a need addressed in many different ways according to the many different stages in which the nature of communities has evolved over time. In the case of reputation of goods/services suppliers, the solutions available to prevent the feedback abuse are generally reliable but centralized under the control of few big Internet companies. In this paper we show how a decentralized and distributed feedback management system can be built on top of the Bitcoin blockchain.},
  file = {D\:\\GDrive\\zotero\\Carboni\\carboni_feedback_based_reputation_on_top_of_the_bitcoin_blockchain.pdf},
  keywords = {Bitcoin,crypto-currency,incentive-based feedback,online reputation,peer-to-peer,Trust}
}

@techreport{cardelliUnderstandingTypesData1985,
  title = {On {{Understanding Types}}, {{Data Abstraction}}, and {{Polymorphism}}},
  author = {Cardelli, Luca and Wegner, Peter},
  year = {1985},
  volume = {4},
  pages = {471--522},
  abstract = {Our objective is to understand the notion of type in programming languages, present a model of typed, polymorphic programming languages that reflects recent research in type theory, and examine the relevance of recent research to the design of practical programming languages. Object-oriented languages provide both a framework and a motivation for exploring the interaction among the concepts of type, data abstraction, and polymorphism, since they extend the notion of type to data abstraction and since type inheritance is an important form of polymorphism. We develop a {$\lambda$}-calculus-based model for type systems that allows us to explore these interactions in a simple setting, unencumbered by complexities of production programming languages. The evolution of languages from untyped universes to monomorphic and then polymorphic type systems is reviewed. Mechanisms for polymorphism such as overloading, coercion, subtyping, and parameterization are examined. A unifying framework for polymorphic type systems is developed in terms of the typed {$\lambda$}-calculus augmented to include binding of types by quantification as well as binding of values by abstraction. The typed {$\lambda$}-calculus is augmented by universal quantification to model generic functions with type parameters, existential quantification and packaging (information hiding) to model abstract data types, and bounded quantification to model subtypes and type inheritance. In this way we obtain a simple and precise characterization of a powerful type system that includes abstract data types, parametric polymorphism, and multiple inheritance in a single consistent framework. The mechanisms for type checking for the augmented {$\lambda$}-calculus are discussed. The augmented typed {$\lambda$}-calculus is used as a programming language for a variety of illustrative examples. We christen this language Fun because fun instead of {$\lambda$} is the functional abstraction keyword and because it is pleasant to deal with. Fun is mathematically simple and can serve as a basis for the design and implementation of real programming languages with type facilities that are more powerful and expressive than those of existing programming languages. In particular, it provides a basis for the design of strongly typed object-oriented languages. 2},
  file = {D\:\\GDrive\\zotero\\Cardelli\\cardelli_1985_on_understanding_types,_data_abstraction,_and_polymorphism.pdf},
  journal = {Computing Surveys}
}

@article{cardenasAttacksProcessControl2011,
  title = {Attacks against Process Control Systems: {{Risk}} Assessment, Detection, and Response},
  author = {C{\'a}rdenas, Alvaro A. and Amin, Saurabh and Lin, Zong Syun and Huang, Yu Lun and Huang, Chi Yen and Sastry, Shankar},
  year = {2011},
  pages = {355--366},
  doi = {10.1145/1966913.1966959},
  abstract = {In the last years there has been an increasing interest in the security of process control and SCADA systems. Furthermore, recent computer attacks such as the Stuxnet worm, have shown there are parties with the motivation and resources to effectively attack control systems. While previous work has proposed new security mechanisms for control systems, few of them have explored new and fundamentally different research problems for securing control systems when compared to securing traditional information technology (IT) systems. In particular, the sophistication of new malware attacking control systems-malware including zero-days attacks, rootkits created for control systems, and software signed by trusted certificate authorities-has shown that it is very difficult to prevent and detect these attacks based solely on IT system information. In this paper we show how, by incorporating knowledge of the physical system under control, we are able to detect computer attacks that change the behavior of the targeted control system. By using knowledge of the physical system we are able to focus on the final objective of the attack, and not on the particular mechanisms of how vulnerabilities are exploited, and how the attack is hidden. We analyze the security and safety of our mechanisms by exploring the effects of stealthy attacks, and by ensuring that automatic attack-response mechanisms will not drive the system to an unsafe state. A secondary goal of this paper is to initiate the discussion between control and security practitioners-two areas that have had little interaction in the past. We believe that control engineers can leverage security engineering to design-based on a combination of their best practices-control algorithms that go beyond safety and fault tolerance, and include considerations to survive targeted attacks. Copyright 2011 ACM.},
  file = {D\:\\GDrive\\zotero\\Cárdenas\\cárdenas_2011_attacks_against_process_control_systems.pdf},
  isbn = {9781450305648},
  journal = {Proceedings of the 6th International Symposium on Information, Computer and Communications Security, ASIACCS 2011},
  keywords = {Control systems,Critical infrastructure protection,Cyber-physical systems,Ids,Scada,Security}
}

@article{carliniControlFlowBendingEffectiveness2015,
  title = {Control-{{Flow Bending}}: {{On}} the {{Effectiveness}} of {{Control}}-{{Flow Integrity}}},
  author = {Carlini, Nicolas and Barresi, Antonio and Payer, Mathias and Wagner, David},
  year = {2015},
  pages = {17},
  abstract = {Control-Flow Integrity (CFI) is a defense which prevents control-flow hijacking attacks. While recent research has shown that coarse-grained CFI does not stop attacks, fine-grained CFI is believed to be secure.},
  file = {D\:\\GDrive\\zotero\\Carlini et al\\carlini_et_al_control-flow_bending.pdf},
  language = {en}
}

@techreport{carliniROPStillDangerous2014,
  title = {{{ROP}} Is {{Still Dangerous}}: {{Breaking Modern Defenses}}},
  author = {Carlini, Nicholas and Wagner, David},
  year = {2014},
  abstract = {Return Oriented Programming (ROP) has become the exploitation technique of choice for modern memory-safety vulnerability attacks. Recently, there have been multiple attempts at defenses to prevent ROP attacks. In this paper, we introduce three new attack methods that break many existing ROP defenses. Then we show how to break kBouncer and ROPecker, two recent low-overhead defenses that can be applied to legacy software on existing hardware. We examine several recent ROP attacks seen in the wild and demonstrate that our techniques successfully cloak them so they are not detected by these defenses. Our attacks apply to many CFI-based defenses which we argue are weaker than previously thought. Future defenses will need to take our attacks into account.},
  file = {D\:\\GDrive\\zotero\\Carlini\\carlini_rop_is_still_dangerous.pdf}
}

@techreport{carlssonMessageAnalysisConcurrent,
  title = {Message {{Analysis}} for {{Concurrent Languages}}},
  author = {Carlsson, Richard and Sagonas, Konstantinos and Wilhelmsson, Jesper},
  abstract = {We describe an analysis-driven storage allocation scheme for concurrent languages that use message passing with copying semantics. The basic principle is that in such a language, data which is not part of any message does not need to be allocated in a shared data area. This allows for deallocation of thread-specific data without requiring global synchronization and often without even triggering garbage collection. On the other hand, data that is part of a message should preferably be allocated on a shared area, which allows for fast (O(1)) interprocess communication that does not require actual copying. In the context of a dynamically typed, higher-order, concurrent functional language, we present a static message analysis which guides the allocation. As shown by our performance evaluation, conducted using an industrial-strength language implementation , the analysis is effective enough to discover most data which is to be used as a message, and to allow the allocation scheme to combine the best performance characteristics of both a process-centric and a shared-heap memory architecture.},
  file = {D\:\\GDrive\\zotero\\Carlsson\\carlsson_message_analysis_for_concurrent_languages.pdf}
}

@inproceedings{carlucciHTTPUDPExperimental2015,
  title = {{{HTTP}} over {{UDP}}: {{An}} Experimental Investigation of {{QUIC}}},
  booktitle = {Proceedings of the {{ACM Symposium}} on {{Applied Computing}}},
  author = {Carlucci, Gaetano and De Cicco, Luca and Mascolo, Saverio},
  year = {2015},
  month = apr,
  volume = {13-17-April-2015},
  pages = {609--614},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/2695664.2695706},
  abstract = {Copyright 2015 ACM. This paper investigates"Quick UDP Internet Connections" (QUIC), which was proposed by Google in 2012 as a reliable protocol on top of UDP in order to reduce Web Page retrieval time. We first check, through experiments, if QUIC can be safely deployed in the Internet and then we evaluate the Web page load time in comparison with SPDY and HTTP. We have found that QUIC reduces the overall page retrieval time with respect to HTTP in case of a channel without induced random losses and outperforms SPDY in the case of a lossy channel. The FEC module, when enabled, worsens the performance of QUIC.},
  file = {D\:\\GDrive\\zotero\\Carlucci\\carlucci_2015_http_over_udp.pdf},
  isbn = {978-1-4503-3196-8},
  keywords = {Congestion control,HTTP,QUIC,SPDY,TCP,UDP}
}

@misc{CaseCSMaster,
  title = {The Case against {{CS}} Master's Degrees},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IQYWDCDK\\masters.html},
  howpublished = {https://ozwrites.com/masters/}
}

@techreport{casimirAttractionTwoPerfectly1948,
  title = {On the Attraction between Two Perfectly Conducting Plates},
  author = {{Casimir} and {HBG}},
  year = {1948},
  abstract = {Mathematics.-On the attraction between two perfectly conducting plates. By H. B. G. CASIMIR. (Communicated at the meeting of M ay 29. 1948.) In a recent paper by POLDER and CASIMIR 1) it is shown that the interaction between a perfectly conducting plate and an atom or molecule with a static polarizibility a is in the limit of large distances R given by 3 a JE=-hc-8n R4 and that the interaction between two particles with statie polarizibilities al and a2 is given in that limit by dE=-23 hC\textasciitilde RI\textasciitilde 2. 'in These formulae are obtained by taking the usual VAN DER W AALS-LONDON forces as a starting point and correcting for retardation effects. In a communication to the "Colloque sur la th\'eorie de la liaison chimi-que" (Paris. 12-17 April. 1948) the present au thor was able to show that these expressions mayalso be derived through studying by means of classical electrodynamics the change of electromagnetic zero point energy. In this note we shall apply the same method to the interaction between two perfectly conducting plates. Let us consider a cu bic cavity of volume L3 bounded by perfectly conducting walls and let a perfectly conducting square plate with side L be placed in this cavity parallel to the xy face and let us compare the situation in whieh this plate is at a small distance a from the xy face and the situation in which it is at a very large distance. say L/ 2. In both cases the expressions t \textasciitilde{} hw where the summation extends over all possible resonance frequencies of the cavities are divergent and devoid of physical meaning but the difference between these sums in the two situations.},
  file = {D\:\\GDrive\\zotero\\Casimir\\casimir_1948_on_the_attraction_between_two_perfectly_conducting_plates.pdf},
  keywords = {Mathematics}
}

@article{casteelExploitingCommonIntent2012,
  title = {Exploiting Common {{Intent}} Vulnerabilities in {{Android}} Applications},
  author = {Casteel, Kelly and Derby, Owen and Wilson, Dennis},
  year = {2012},
  pages = {1--6},
  abstract = {Problem The Android framework allows apps and components within apps to communicate with one another by passing messages, called Intents, which effectively specify both a procedure to call and the arguments to use. Applications must declare in a static manifest file which Intents each component services, as well as both application and component level permissions. While the security vulnerabilities in outgoing Intents have been well studied [1] and developer tools exist to limit potentially insecure Intents [5], little has been done to address malicious incoming Intents. Exploits of this nature have been discovered in firmware of various Android phones [4], but exploits in third-party applications are not well studied. Application developers must make sure their manifest file has been properly configured to only accept desired Intents, which can limit usability. We believe that developers will trust Intent input by default, allowing malicious input to potentially crash or abuse the application. Our work is twofold: we developed a static analysis tool to inspect third party appli-cations for malicious Intent vulnerabilities, and we built a working exploit which takes advantage of such a vulnerability. Static analysis To start, we implemented a basic static analysis tool to aid us in identifying potential vulnerabilities in Android applications. We are not the first to look at such vulnerabilities [1], nor to build such a tool [4]. However, in the absence of any freely-available such tools, we took it as a learning opportunity to build our own, borrowing ideas freely from the previous literature. Here, we describe the basic design of our tool and any interesting decisions we made, but the curious reader should reference [1] and [4] for more details. We built our tool using Androguard [2], a python FOS library built, in part, to support the creation of static analysis tools for the Android platform. It supports disassembly and decompilation of apks, Android application packages, into Java-approximate source code for human-readability, as well as intermediate basic blocks for static analysis and control-flow graph creation. It provides basic search functionality over the decompiled basic blocks. Despite this long list of features, a lot of work went into understanding how to use the library and coercing it into doing what we wanted.},
  file = {D\:\\GDrive\\zotero\\Casteel\\casteel_2012_exploiting_common_intent_vulnerabilities_in_android_applications.pdf}
}

@article{castellanosFindingDependenciesCyberPhysical2018,
  title = {Finding {{Depen}}-Dencies between {{Cyber}}-{{Physical Domains}} for {{Security Testing}} of {{Industrial Control Systems}}},
  author = {Castellanos, John H and Ochoa, Mart{\'i}n and Zhou, Jianying},
  year = {2018},
  doi = {10.1145/3274694.3274745},
  abstract = {In modern societies, critical services such as transportation, power supply, water treatment and distribution are strongly dependent on Industrial Control Systems (ICS). As technology moves along, new features improve services provided by such ICS. On the other hand, this progress also introduces new risks of cyber attacks due to the multiple direct and indirect dependencies between cyber and physical components of such systems. Performing rigorous security tests and risk analysis in these critical systems is thus a challenging task, because of the non-trivial interactions between digital and physical assets and the domain-speci\"i\textquestiondown\textquestiondown c knowledge necessary to analyse a particular system. In this work, we propose a methodology to model and analyse a System Under Test (SUT) as a data \"i\textquestiondown\textquestiondown ow graph that highlights interactions among internal entities throughout the SUT. This model is automatically extracted from production code available in Programmable Logic Controllers (PLCs). We also propose a reachability algorithm and an attack diagram that will emphasize the dependencies between cyber and physical domains, thus enabling a human analyst to gauge various attack vectors that arise from subtle dependencies in data and information propagation. We test our methodology in a functional water treatment testbed and demonstrate how an analyst could make use of our designed attack diagrams to reason on possible threats to various targets of the SUT.},
  file = {D\:\\GDrive\\zotero\\Castellanos\\castellanos_2018_finding_depen-dencies_between_cyber-physical_domains_for_security_testing_of.pdf;D\:\\GDrive\\zotero\\Castellanos\\castellanos_2018_finding_dependencies_between_cyber-physical_domains_for_security_testing_of.pdf},
  isbn = {9781450365697},
  keywords = {CCS CONCEPTS • Security and privacy → Distributed systems security,Cyber-Physical Systems,Formal security models,ICS Security,Infor-mation ï¿¿ow control,Information ow,KEYWORDS Cyber-Physical Systems; ICS Security; Information ï¿¿ow}
}

@techreport{castroPracticalByzantineFault1999,
  title = {Practical {{Byzantine Fault Tolerance}}},
  author = {Castro, Miguel and Liskov, Barbara},
  year = {1999},
  abstract = {This paper describes a new replication algorithm that is able to tolerate Byzantine faults. We believe that Byzantine-fault-tolerant algorithms will be increasingly important in the future because malicious attacks and software errors are increasingly common and can cause faulty nodes to exhibit arbitrary behavior. Whereas previous algorithms assumed a synchronous system or were too slow to be used in practice, the algorithm described in this paper is practical: it works in asynchronous environments like the Internet and incorporates several important optimizations that improve the response time of previous algorithms by more than an order of magnitude. We implemented a Byzantine-fault-tolerant NFS service using our algorithm and measured its performance. The results show that our service is only 3\% slower than a standard unreplicated NFS.},
  file = {D\:\\GDrive\\zotero\\Castro\\castro_1999_practical_byzantine_fault_tolerance.pdf}
}

@techreport{cavallaroLimitsInformationFlow,
  title = {On the {{Limits}} of {{Information Flow Techniques}} for {{Malware Analysis}} and {{Containment}}},
  author = {Cavallaro, Lorenzo and Saxena, Prateek and Sekar, R},
  abstract = {Taint-tracking is emerging as a general technique in software security to complement virtualization and static analysis. It has been applied for accurate detection of a wide range of attacks on benign software, as well as in malware defense. Although it is quite robust for tackling the former problem, application of taint analysis to untrusted (and potentially malicious) software is riddled with several difficulties that lead to gaping holes in defense. These holes arise not only due to the limitations of information flow analysis techniques, but also the nature of today's software architectures and distribution models. This paper highlights these problems using an array of simple but powerful evasion techniques that can easily defeat taint-tracking defenses. Given today's binary-based software distribution and deployment models, our results suggest that information flow techniques will be of limited use against future malware that has been designed with the intent of evading these defenses.},
  file = {D\:\\GDrive\\zotero\\Cavallaro\\cavallaro_on_the_limits_of_information_flow_techniques_for_malware_analysis_and.pdf}
}

@techreport{cerfProtocolPacketNetwork1974,
  title = {A {{Protocol}} for {{Packet Network Intercommunication}}},
  author = {Cerf, Vinton G and Kahn, Robert E},
  year = {1974},
  abstract = {A protocol that supports the sharing of resources that exist in different packet switching networks is presented. The protocol provides for variation in individual network packet sizes, transmission failures, sequencing, flow control, end-to-end error checking, and the creation and destruction of logical process-to-process connections. Some implementation issues are considered, and problems such as internetwork routing, accounting, and timeouts are exposed.},
  file = {D\:\\GDrive\\zotero\\Cerf\\cerf_1974_a_protocol_for_packet_network_intercommunication.pdf},
  journal = {IEEE Trans on Comms},
  number = {5}
}

@techreport{cervonePOINTWISEPredictingPoints,
  title = {{{POINTWISE}}: {{Predicting Points}} and {{Valuing Decisions}} in {{Real Time}} with {{NBA Optical Tracking Data A}} New Microeconomics for the {{NBA}}},
  author = {Cervone, Dan and D'amour, Alexander and Bornn, Luke and Goldsberry, Kirk},
  abstract = {Basketball is a game of decisions; at any moment, a player can change the character of a possession by choosing to pass, dribble, or shoot. The current state of basketball analytics, however, provides no way to quantitatively evaluate the vast majority of decisions that players make, as most metrics are driven by events that occur at or near the end of a possession, such as points, turnovers, and assists. We propose a framework for using player-tracking data to assign a point value to each moment of a possession by computing how many points the offense is expected to score by the end of the possession, a quantity we call expected possession value (EPV). EPV allows analysts to evaluate every decision made during a basketball game-whether it is to pass, dribble, or shoot-opening the door for a multitude of new metrics and analyses of basketball that quantify value in terms of points. In this paper, we propose a modeling framework for estimating EPV, present results of EPV computations performed using player-tracking data from the 2012-13 season, and provide several examples of EPV-derived metrics that answer real basketball questions. Basketball players, coaches, and fans often compare a possession to a high-speed chess match, where teams employ tactics that do not necessarily generate points immediately, but can yield higher-value opportunities several "moves" down the line. Watching the game in this way can reveal that the decisive moment in a given possession may not have been the open shot at the end, but the pass that led to the open shot, or even the preceding drive that collapsed the defense. These ideas lie at the heart of offensive strategies and the decisions that players make over the course of a possession. Unfortunately, contemporary basketball analytics fail to account for this core idea. Despite many recent innovations, most advanced metrics (PER [1] and +/-variations [2], for example) remain based on simple tallies relating to the terminal states of possessions like points, rebounds, and turnovers. While these have shed light on the game, they are akin to analyzing a chess match based only on the move that resulted in checkmate, leaving unexplored the possibility that the key move occurred several turns before. This leaves a major gap to be filled, as an understanding of how players contribute to the whole possession-not just the events that end it-can be critical in evaluating players, assessing the quality of their decision-making, and predicting the success of particular in-game tactics. The major obstacle to closing this gap is the current inability to evaluate the individual tactical decisions that form the substructure of every possession of every basketball game. For example, there is no current method to estimate the value of a dribble penetration or to compare the option of taking a contested shot to the option of passing to an open teammate. In this paper, we propose and implement a framework that removes this obstacle. Using player-tracking data, we develop a coherent, quantitative representation of a whole possession that summarizes each moment of the possession in terms of the number of points the offense is expected to score-a quantity we call expected possession value, or EPV (see Figure 1 for an illustration of EPV). We accomplish this by specifying and fitting a probabilistic model that encodes how ball handlers make decisions based on the spatial configuration of the players on the court.},
  file = {D\:\\GDrive\\zotero\\Cervone\\cervone_pointwise.pdf}
}

@article{chaConcolicTestingAdaptively2019,
  title = {Concolic Testing with Adaptively Changing Search Heuristics},
  author = {Cha, Sooyoung and Oh, Hakjoo},
  year = {2019},
  pages = {235--245},
  doi = {10.1145/3338906.3338964},
  abstract = {We present Chameleon, a new approach for adaptively changing search heuristics during concolic testing. Search heuristics play a central role in concolic testing as they mitigate the path-explosion problem by focusing on particular program paths that are likely to increase code coverage as quickly as possible. A variety of techniques for search heuristics have been proposed over the past decade. However, existing approaches are limited in that they use the same search heuristics throughout the entire testing process, which is inherently insufficient to exercise various execution paths. Chameleon overcomes this limitation by adapting search heuristics on the fly via an algorithm that learns new search heuristics based on the knowledge accumulated during concolic testing. Experimental results show that the transition from the traditional non-adaptive approaches to ours greatly improves the practicality of concolic testing in terms of both code coverage and bug-finding.},
  file = {D\:\\GDrive\\zotero\\Cha\\cha_2019_concolic_testing_with_adaptively_changing_search_heuristics.pdf},
  isbn = {9781450355728},
  journal = {ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  keywords = {Concolic Testing,Dynamic Symbolic Execution,Online Learning}
}

@techreport{chambersFlumeJavaEasyEfficient2010,
  title = {{{FlumeJava}}: {{Easy}}, {{Efficient Data}}-{{Parallel Pipelines}}},
  author = {Chambers, Craig and Raniwala, Ashish and Perry, Frances and Adams, Stephen and Henry, Robert R and Bradshaw, Robert and Weizenbaum, Nathan},
  year = {2010},
  abstract = {MapReduce and similar systems significantly ease the task of writing data-parallel code. However, many real-world computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient data-parallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, high-level, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently , FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of high-level abstractions for parallel data and computation , deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.},
  file = {D\:\\GDrive\\zotero\\Chambers\\chambers_flumejava.pdf},
  isbn = {9781450300193}
}

@techreport{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly {{Detection}} : {{A Survey}}},
  author = {Chandola, Varun},
  year = {2009},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  file = {D\:\\GDrive\\zotero\\Chandola\\chandola_2009_anomaly_detection.pdf},
  journal = {ACM Computing Surveys},
  keywords = {Algorithms Additional Key Words and Phrases,Anomaly Detection,Categories and Subject Descriptors,Database Applications-Data Mining General Terms,H28 [Database Management],Outlier Detection}
}

@techreport{chandraPaxosMadeLiveAn2007,
  title = {Paxos {{Made Live}}-{{An Engineering Perspective}}},
  author = {Chandra, Tushar and Griesemer, Robert and Redstone, Joshua},
  year = {2007},
  abstract = {We describe our experience in building a fault-tolerant database using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system.},
  file = {D\:\\GDrive\\zotero\\Chandra\\chandra_2007_paxos_made_live-an_engineering_perspective.pdf}
}

@techreport{changBigtableDistributedStorage,
  title = {Bigtable: {{A Distributed Storage System}} for {{Structured Data}}},
  author = {Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
  abstract = {Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.},
  file = {D\:\\GDrive\\zotero\\Chang\\chang_bigtable.pdf},
  keywords = {distributed systems}
}

@techreport{chaseAnalysisXRPLedger2018,
  title = {Analysis of the {{XRP Ledger Consensus Protocol}}},
  author = {Chase, Brad and Macbrough, Ethan},
  year = {2018},
  abstract = {The XRP Ledger Consensus Protocol is a previously developed consensus protocol powering the XRP Ledger. It is a low-latency Byzantine agreement protocol, capable of reaching consensus without full agreement on which nodes are members of the network. We present a detailed explanation of the algorithm and derive conditions for its safety and liveness.},
  file = {D\:\\GDrive\\zotero\\Chase\\chase_2018_analysis_of_the_xrp_ledger_consensus_protocol.pdf}
}

@techreport{chaumDiningCryptographersProblem1988,
  title = {The {{Dining Cryptographers Problem}}: {{Unconditional Sender}} and {{Recipient Untraceability}}},
  author = {Chaum, David},
  year = {1988},
  volume = {1},
  pages = {65--75},
  abstract = {Keeping confidential who sends which messages, in a world where any physical transmission can be traced to its origin, seems impossible. The solution presented here is unconditionally or cryptographically secure, depending on whether it is based on one-time-use keys or on public keys, respectively. It can be adapted to address efficiently a wide variety of practical considerations.},
  file = {D\:\\GDrive\\zotero\\Chaum\\chaum_1988_the_dining_cryptographers_problem.pdf},
  journal = {International Association for Cryptologic Research},
  keywords = {Pseudonymity,Unconditional Security,Untraceability}
}

@article{checkowayAreTextOnlyData2010,
  title = {Are {{Text}}-{{Only Data Formats Safe}}? {{Or}}, {{Use This LATEX Class File}} to {{Pwn Your Computer}}},
  author = {Checkoway, Stephen and Shacham, Hovav and Rescorla, Eric},
  year = {2010},
  pages = {8},
  abstract = {We show that malicious TEX, BIBTEX, and METAPOST files can lead to arbitrary code execution, viral infection, denial of service, and data exfiltration, through the file I/O capabilities exposed by TEX's Turing-complete macro language. This calls into doubt the conventional wisdom view that text-only data formats that do not access the network are likely safe. We build a TEX virus that spreads between documents on the MiKTEX distribution on Windows XP; we demonstrate data exfiltration attacks on web-based LATEX previewer services.},
  file = {D\:\\GDrive\\zotero\\Checkoway et al\\checkoway_et_al_2010_are_text-only_data_formats_safe.pdf},
  language = {en}
}

@article{checkowayCanDREsProvide2009,
  title = {Can {{DREs Provide Long}}-{{Lasting Security}}? {{The Case}} of {{Return}}-{{Oriented Programming}} and the {{AVC Advantage}}},
  author = {Checkoway, Stephen and Halderman, J Alex and Feldman, Ariel J and Felten, Edward W and Kantor, Brian and Shacham, Hovav},
  year = {2009},
  pages = {16},
  abstract = {A secure voting machine design must withstand new attacks devised throughout its multi-decade service lifetime. In this paper, we give a case study of the longterm security of a voting machine, the Sequoia AVC Advantage, whose design dates back to the early 80s. The AVC Advantage was designed with promising security features: its software is stored entirely in read-only memory and the hardware refuses to execute instructions fetched from RAM. Nevertheless, we demonstrate that an attacker can induce the AVC Advantage to misbehave in arbitrary ways \textemdash{} including changing the outcome of an election \textemdash{} by means of a memory cartridge containing a specially-formatted payload. Our attack makes essential use of a recently-invented exploitation technique called return-oriented programming, adapted here to the Z80 processor. In return-oriented programming, short snippets of benign code already present in the system are combined to yield malicious behavior. Our results demonstrate the relevance of recent ideas from systems security to voting machine research, and vice versa. We had no access either to source code or documentation beyond that available on Sequoia's web site. We have created a complete vote-stealing demonstration exploit and verified that it works correctly on the actual hardware.},
  file = {D\:\\GDrive\\zotero\\Checkoway et al\\checkoway_et_al_2009_can_dres_provide_long-lasting_security.pdf},
  language = {en}
}

@article{checkowayComprehensiveExperimentalAnalyses2011,
  title = {Comprehensive {{Experimental Analyses}} of {{Automotive Attack Surfaces}}},
  author = {Checkoway, Stephen and McCoy, Damon and Kantor, Brian and Anderson, Danny and Shacham, Hovav and Savage, Stefan and Koscher, Karl and Czeskis, Alexei and Roesner, Franziska and Kohno, Tadayoshi},
  year = {2011},
  pages = {16},
  abstract = {Modern automobiles are pervasively computerized, and hence potentially vulnerable to attack. However, while previous research has shown that the internal networks within some modern cars are insecure, the associated threat model \textemdash{} requiring prior physical access \textemdash{} has justifiably been viewed as unrealistic. Thus, it remains an open question if automobiles can also be susceptible to remote compromise. Our work seeks to put this question to rest by systematically analyzing the external attack surface of a modern automobile. We discover that remote exploitation is feasible via a broad range of attack vectors (including mechanics tools, CD players, Bluetooth and cellular radio), and further, that wireless communications channels allow long distance vehicle control, location tracking, in-cabin audio exfiltration and theft. Finally, we discuss the structural characteristics of the automotive ecosystem that give rise to such problems and highlight the practical challenges in mitigating them.},
  file = {D\:\\GDrive\\zotero\\Checkoway et al\\checkoway_et_al_2011_comprehensive_experimental_analyses_of_automotive_attack_surfaces.pdf},
  language = {en}
}

@article{checkowayIagoAttacksWhy2013,
  title = {Iago {{Attacks}}: {{Why}} the {{System Call API}} Is a {{Bad Untrusted RPC Interface}}},
  author = {Checkoway, Stephen and Shacham, Hovav},
  year = {2013},
  pages = {11},
  abstract = {In recent years, researchers have proposed systems for running trusted code on an untrusted operating system. Protection mechanisms deployed by such systems keep a malicious kernel from directly manipulating a trusted application's state. Under such systems, the application and kernel are, conceptually, peers, and the system call API defines an RPC interface between them. We introduce Iago attacks, attacks that a malicious kernel can mount in this model. We show how a carefully chosen sequence of integer return values to Linux system calls can lead a supposedly protected process to act against its interests, and even to undertake arbitrary computation at the malicious kernel's behest.},
  file = {D\:\\GDrive\\zotero\\Checkoway_Shacham\\checkoway_shacham_2013_iago_attacks.pdf},
  language = {en}
}

@article{checkowayPracticalExploitabilityDual2014,
  title = {On the {{Practical Exploitability}} of {{Dual EC}} in {{TLS Implementations}}},
  author = {Checkoway, Stephen and Fredrikson, Matthew and Niederhagen, Ruben and Everspaugh, Adam and Green, Matthew and Lange, Tanja and Ristenpart, Thomas and Bernstein, Daniel J and Maskiewicz, Jake and Shacham, Hovav},
  year = {2014},
  pages = {18},
  abstract = {This paper analyzes the actual cost of attacking TLS implementations that use NIST's Dual EC pseudorandom number generator, assuming that the attacker generated the constants used in Dual EC. It has been known for several years that an attacker generating these constants and seeing a long enough stretch of Dual EC output bits can predict all future outputs; but TLS does not naturally provide a long enough stretch of output bits, and the cost of an attack turns out to depend heavily on choices made in implementing the RNG and on choices made in implementing other parts of TLS.},
  file = {D\:\\GDrive\\zotero\\Checkoway et al\\checkoway_et_al_2014_on_the_practical_exploitability_of_dual_ec_in_tls_implementations.pdf},
  language = {en}
}

@inproceedings{checkowayReturnorientedProgrammingReturns2010,
  title = {Return-Oriented Programming without Returns},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Computer}} and Communications Security - {{CCS}} '10},
  author = {Checkoway, Stephen and Davi, Lucas and Dmitrienko, Alexandra and Sadeghi, Ahmad-Reza and Shacham, Hovav and Winandy, Marcel},
  year = {2010},
  pages = {559},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1866307.1866370},
  abstract = {We show that on both the x86 and ARM architectures it is possible to mount return-oriented programming attacks without using return instructions. Our attacks instead make use of certain instruction sequences that behave like a return, which occur with sufficient frequency in large libraries on (x86) Linux and (ARM) Android to allow creation of Turing-complete gadget sets.},
  file = {D\:\\GDrive\\zotero\\Checkoway et al\\checkoway_et_al_2010_return-oriented_programming_without_returns.pdf},
  isbn = {978-1-4503-0245-6},
  language = {en}
}

@inproceedings{checkowaySystematicAnalysisJuniper2016,
  title = {A {{Systematic Analysis}} of the {{Juniper Dual EC Incident}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Checkoway, Stephen and Maskiewicz, Jacob and Garman, Christina and Fried, Joshua and Cohney, Shaanan and Green, Matthew and Heninger, Nadia and Weinmann, Ralf-Philipp and Rescorla, Eric and Shacham, Hovav},
  year = {2016},
  month = oct,
  pages = {468--479},
  publisher = {{ACM}},
  address = {{Vienna Austria}},
  doi = {10.1145/2976749.2978395},
  abstract = {In December 2015, Juniper Networks announced multiple security vulnerabilities stemming from unauthorized code in ScreenOS, the operating system for their NetScreen VPN routers. The more sophisticated of these vulnerabilities was a passive VPN decryption capability, enabled by a change to one of the elliptic curve points used by the Dual EC pseudorandom number generator.},
  file = {D\:\\GDrive\\zotero\\Checkoway et al\\checkoway_et_al_2016_a_systematic_analysis_of_the_juniper_dual_ec_incident.pdf},
  isbn = {978-1-4503-4139-4},
  language = {en}
}

@phdthesis{chekoleEnforcingFullStackMemory,
  title = {Enforcing {{Full}}-{{Stack Memory Safety}} in {{Critical Infrastructures}}},
  author = {Chekole, Eyasu Getahun},
  file = {D\:\\GDrive\\zotero\\Chekole\\chekole_enforcing_full-stack_memory_safety_in_critical_infrastructures.pdf},
  language = {en}
}

@article{chenAutomatedDynamicAnalysis2016,
  title = {Towards {{Automated Dynamic Analysis}} for {{Linux}}-Based {{Embedded Firmware}}},
  author = {Chen, Daming D and Egele, Manuel and Woo, Maverick and Brumley, David},
  year = {2016},
  doi = {10.14722/ndss.2016.23415},
  abstract = {Commercial-off-the-shelf (COTS) network-enabled embedded devices are usually controlled by vendor firmware to perform integral functions in our daily lives. For example, wireless home routers are often the first and only line of defense that separates a home user's personal computing and information devices from the Internet. Such a vital and privileged position in the user's network requires that these devices operate securely. Unfortunately, recent research and anecdotal evidence suggest that such security assumptions are not at all upheld by the devices deployed around the world. A first step to assess the security of such embedded device firmware is the accurate identification of vulnerabilities. However, the market offers a large variety of these embedded devices, which severely impacts the scalability of existing approaches in this area. In this paper, we present FIRMADYNE, the first automated dynamic analysis system that specifically targets Linux-based firmware on network-connected COTS devices in a scalable manner. We identify a series of challenges inherent to the dynamic analysis of COTS firmware, and discuss how our design decisions address them. At its core, FIRMADYNE relies on software-based full system emulation with an instrumented kernel to achieve the scalability necessary to analyze thousands of firmware binaries automatically. We evaluate FIRMADYNE on a real-world dataset of 23,035 firmware images across 42 device vendors gathered by our system. Using a sample of 74 exploits on the 9,486 firmware images that our system can successfully extract, we discover that 887 firmware images spanning at least 89 distinct products are vulnerable to one or more of the sampled exploit(s). This includes 14 previously-unknown vulnerabilities that were discovered with the aid of our framework, which affect 69 firmware images spanning at least 12 distinct products. Furthermore, our results show that 11 of our tested attacks affect firmware images from more than one vendor, suggesting that code-sharing and common upstream manufacturers (OEMs) are quite prevalent.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_2016_towards_automated_dynamic_analysis_for_linux-based_embedded_firmware.pdf},
  isbn = {189156241X}
}

@article{chenDetectingPrivilegedSideChannel,
  title = {Detecting {{Privileged Side}}-{{Channel Attacks}} in {{Shielded Execution}} with {{D\'ej\`a Vu}}},
  author = {Chen, Sanchuan and Zhang, Xiaokuan and Reiter, Michael K and Zhang, Yinqian},
  doi = {10.1145/3052973.3053007},
  abstract = {Intel Software Guard Extension (SGX) protects the confidentiality and integrity of an unprivileged program running inside a secure enclave from a privileged attacker who has full control of the entire operating system (OS). Program execution inside this enclave is therefore referred to as shielded. Unfortunately, shielded execution does not protect programs from side-channel attacks by a privileged attacker. For instance , it has been shown that by changing page table entries of memory pages used by shielded execution, a malicious OS kernel could observe memory page accesses from the execution and hence infer a wide range of sensitive information about it. In fact, this page-fault side channel is only an instance of a category of side-channel attacks, here called privileged side-channel attacks, in which privileged attackers frequently preempt the shielded execution to obtain fine-grained side-channel observations. In this paper, we present D\'ej\`a Vu, a software framework that enables a shielded execution to detect such privileged side-channel attacks. Specifically , we build into shielded execution the ability to check program execution time at the granularity of paths in its control-flow graph. To provide a trustworthy source of time measurement, D\'ej\`a Vu implements a novel software reference clock that is protected by Intel Transactional Synchronization Extensions (TSX), a hardware implementation of transactional memory. Evaluations show that D\'ej\`a Vu effectively detects side-channel attacks against shielded execution and against the reference clock itself.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_detecting_privileged_side-channel_attacks_in_shielded_execution_with_déjà_vu.pdf},
  isbn = {9781450349444}
}

@article{chengExploitationTechniquesDataOriented2018,
  title = {Exploitation {{Techniques}} for {{Data}}-{{Oriented Attacks}} with {{Existing}} and {{Potential Defense Approaches}}*},
  author = {Cheng, Long and Ahmed, Salman and Liljestrand, Hans and Nyman, Thomas and Cai, Haipeng and Jaeger, Trent and Asokan, N},
  year = {2018},
  volume = {37},
  pages = {35},
  file = {D\:\\GDrive\\zotero\\Cheng et al\\cheng_et_al_exploitation_techniques_for_data-oriented_attacks_with_existing_and_potential.pdf},
  journal = {J. ACM},
  language = {en},
  number = {4}
}

@article{chengSchemesPerformancesDynamic2000,
  title = {The Schemes and Performances of Dynamic Branch Predictors},
  author = {Cheng, Cc},
  year = {2000},
  pages = {18},
  abstract = {The techniques of Instruction Level Parallelism (ILP) and pipeline have been used well to speed up the execution of instructions. The conditional branches are the critical factor to the effectiveness of a deep pipeline since the branch instructions can always break the flow of instructions through the pipeline and result in high execution cost. In order to achieve better CPU performance, many schemes of branch prediction have been utilized. These schemes sometimes can be categorized as program-based predictors vs. profile-based predictors, or static vs. dynamic schemes. This paper focuses on the study of the dynamic branch predictors since the dynamic approach of branch prediction has been developed much more than the static approach of branch prediction. However, their performances always have new and interesting discoveries based on different benchmarks and architectures. I studied as much documentation as I could within my very limited time, and basically perform comparisons between different techniques of dynamic branch prediction, and organize my findings in this paper.},
  file = {D\:\\GDrive\\zotero\\Cheng\\cheng_2000_the_schemes_and_performances_of_dynamic_branch_predictors.pdf},
  journal = {Berkeley Wireless Research Center, Tech. Rep}
}

@techreport{chenNonControlDataAttacksAre2005,
  title = {Non-{{Control}}-{{Data Attacks Are Realistic Threats}}},
  author = {Chen, Shuo and Xu, Jun and Sezer, Emre C and Gauriar, Prachi and Iyer, Ravishankar K},
  year = {2005},
  abstract = {Most memory corruption attacks and Internet worms follow a familiar pattern known as the control-data attack. Hence, many defensive techniques are designed to protect program control flow integrity. Although earlier work did suggest the existence of attacks that do not alter control flow, such attacks are generally believed to be rare against real-world software. The key contribution of this paper is to show that non-control-data attacks are realistic. We demonstrate that many real-world applications, including FTP, SSH, Telnet, and HTTP servers, are vulnerable to such attacks. In each case, the generated attack results in a security compromise equivalent to that due to the control-data attack exploiting the same security bug. Non-control-data attacks corrupt a variety of application data including user identity data, configuration data, user input data, and decision-making data. The success of these attacks and the variety of applications and target data suggest that potential attack patterns are diverse. Attackers are currently focused on control-data attacks, but it is clear that when control flow protection techniques shut them down, they have incentives to study and employ non-control-data attacks. This paper emphasizes the importance of future research efforts to address this realistic threat. 1 Introduction Cyber attacks against all Internet-connected computer systems, including those in critical infrastructure, have become relentless. Malicious attackers often break into computer systems by exploiting security vulnerabilities due to low-level memory corruption errors, e.g., buffer overflow, format string vulnerability, integer overflow, and double free. These vulnerabilities not only are exploited by individual intruders, but also make systems susceptible to Internet worms and distributed denial of service (DDoS) attacks. Recipe-like attack-construction documents [2][46] widely available on the Internet have made this type of attack widely understood. Most memory corruption attacks follow a similar pattern known as the control-data attack: they alter the target program's control data (data that are loaded to processor program counter at some point in program execution, e.g., return addresses and function pointers) in order to execute injected malicious code or out-of-context library code (in particular, return-to-library attacks). The attacks usually make system calls (e.g., starting a shell) with the privilege of the victim process. A quick survey of the CERT/US-CERT security advisories [11][47] and the Microsoft Security Bulletin [26] shows that control-data attacks are considered the most critical security threats. Because control-data attacks are currently dominant, many defensive techniques have been proposed against such attacks. It is reasonable to ask whether the current dominance of control-data attacks is due to an attacker's inability to mount non-control-data attacks 1 against real-world software. We suspect that attackers may in general be capable of mounting non-control-data attacks but simply lack the incentive to do so, because control-data attacks are generally easier to construct and require little application-specific knowledge on the attacker's side. If this is indeed true, when the deployment of control flow protection techniques makes control-data attacks impossible, attackers may have the incentive to bypass these defenses using non-control-data attacks. The emphasis of this paper is the viability of non-control-data attacks against real-world applications. The possibility of these attacks has been suggested in previous work [9][42][48][52]. However, the applicability of these attacks has not been extensively studied, so it is not clear how realistic they are against real-world applications. 1 Other terms are used to refer to attacks that do not alter control flow. For example, Pincus and Baker call them pure data exploits [29]. We call them non-control-data attacks mainly to contrast with control-data attacks.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_non-control-data_attacks_are_realistic_threats.pdf}
}

@article{chenOffpathTCPExploit2018,
  title = {Off-Path {{TCP}} Exploit: {{How}} Wireless Routers Can Jeopardize Your Secrets},
  author = {Chen, Weiteng and Qian, Zhiyun},
  year = {2018},
  pages = {1581--1598},
  abstract = {In this study, we discover a subtle yet serious timing side channel that exists in all generations of half-duplex IEEE 802.11 or Wi-Fi technology. Previous TCP injection attacks stem from software vulnerabilities which can be easily eliminated via software update, but the side channel we report is rooted in the fundamental design of IEEE 802.11 protocols. This design flaw means it is impossible to eliminate the side channel without substantial changes to the specification. By studying the TCP stacks of modern operating systems and their potential interactions with the side channel, we can construct reliable and practical off-path TCP injection attacks against the latest versions of all three major operating systems (macOS, Windows, and Linux). Our attack only requires a device connected to the Internet via a wireless router, and be reachable from an attack server (e.g., indirectly so by accessing to a malicious website). Among possible attacks scenarios, such as inferring the presence of connections and counting exchanged bytes, we demonstrate a particular threat where an off-path attacker can poison the web cache of an unsuspecting user within minutes (as fast as 30 seconds) under realistic network conditions.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_2018_off-path_tcp_exploit.pdf},
  isbn = {9781939133045},
  journal = {Proceedings of the 27th USENIX Security Symposium},
  keywords = {ss}
}

@techreport{chenSideChannelLeaksWeb,
  title = {Side-{{Channel Leaks}} in {{Web Applications}}: A {{Reality Today}}, a {{Challenge Tomorrow}}},
  author = {Chen, Shuo and Wang, Rui and Wang, Xiaofeng and Zhang, Kehuan},
  abstract = {With software-as-a-service becoming mainstream, more and more applications are delivered to the client through the Web. Unlike a desktop application, a web application is split into browser-side and server-side components. A subset of the application's internal information flows are inevitably exposed on the network. We show that despite encryption, such a side-channel information leak is a realistic and serious threat to user privacy. Specifically, we found that surprisingly detailed sensitive information is being leaked out from a number of high-profile, top-of-the-line web applications in healthcare, taxation, investment and web search: an eavesdropper can infer the illnesses/medications/surgeries of the user, her family income and investment secrets, despite HTTPS protection; a stranger on the street can glean enterprise employees' web search queries, despite WPA/WPA2 Wi-Fi encryption. More importantly, the root causes of the problem are some fundamental characteristics of web applications: stateful communication, low entropy input for better interaction, and significant traffic distinctions. As a result, the scope of the problem seems industry-wide. We further present a concrete analysis to demonstrate the challenges of mitigating such a threat, which points to the necessity of a disciplined engineering practice for side-channel mitigations in future web application developments.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_side-channel_leaks_in_web_applications.pdf},
  keywords = {ambiguity set,encrypted traffic,padding,side-channel-leak,Software-as-a-Service (SaaS),web application}
}

@article{chenSODAGenericOnline2020,
  title = {{{SODA}}: {{A Generic Online Detection Framework}} for {{Smart Contracts}}},
  author = {Chen, Ting and Cao, Rong and Li, Ting and Luo, Xiapu and Gu, Guofei and Zhang, Yufei and Liao, Zhou and Zhu, Hang and Chen, Gang and He, Zheyuan and Tang, Yuxing and Lin, Xiaodong and Zhang, Xiaosong},
  year = {2020},
  doi = {10.14722/ndss.2020.24449},
  abstract = {Smart contracts have become lucrative and profitable targets for attackers because they can hold a great amount of money. Unfortunately, existing offline approaches for discovering the vulnerabilities in smart contracts or checking the correctness of smart contracts cannot conduct online detection of attacking transactions. Besides, existing online approaches only focus on specific attacks and cannot be easily extended to detect other attacks. Moreover, developing a new online detection system for smart contracts from scratch is time-consuming and requires deep understanding of blockchain internals, thus making it difficult to quickly implement and deploy mechanisms to detect new attacks. In this paper, we propose a novel generic online detection framework named SODA for smart contracts on any blockchains that support Ethereum virtual machine (EVM). SODA distinguishes itself from existing online approaches through its capability, efficiency, and compatibility. First, SODA empowers users to easily develop apps for detecting various attacks online (i.e., when attacks happen) by separating information collection and attack detection with layered design. At the higher layer, SODA provides unified interfaces to develop detection apps against various attacks. At the lower layer, SODA instruments EVM to collect all primitive information necessary to detect various attacks and constructs 11 kinds of structural information for the ease of developing apps. Based on SODA, users can develop new apps in a few lines of code without modifying EVM. Second, SODA is efficient, because we design on-demand information retrieval to reduce the overhead of information collection and adopt dynamic linking to eliminate the overhead of inter-process communication. Such design allows users to develop detection apps using any programming languages that can generate dynamic link libraries. Third, since more and more blockchains adopt EVM as smart contract runtime, SODA can be easily migrated to such blockchains without modifying apps. Based on SODA, we develop 8 detection apps to detect the attacks exploiting major vulnerabilities in smart contracts, and integrate SODA (including all apps) into 3 popular blockchains: Ethereum, Expanse and Wanchain. The extensive experimental results demonstrate the effectiveness and efficiency of SODA and our detection apps.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_2020_soda.pdf},
  isbn = {1891562614},
  number = {February}
}

@article{chenStateArtDynamic2013a,
  title = {State of the Art: {{Dynamic}} Symbolic Execution for Automated Test Generation},
  author = {Chen, Ting and Zhang, Xiao-Song and Guo, Shi-Ze and Li, Hong-Yuan and Wu, Yue},
  year = {2013},
  volume = {29},
  pages = {1758--1773},
  doi = {10.1016/j.future.2012.02.006},
  abstract = {Dynamic symbolic execution for automated test generation consists of instrumenting and running a program while collecting path constraint on inputs from predicates encountered in branch instructions, and of deriving new inputs from a previous path constraint by an SMT (Satisfiability Modulo Theories) solver in order to steer next executions toward new program paths. It has been introduced into several applications, such as automated test generation, automated filter generation and malware analysis mainly for its two intrinsic properties: low false positives and high code-coverage. In this paper, we focus on the topics that are closely related to automated test generation. Our contributions are five-fold. First, we summarize the theoretical foundation of dynamic symbolic execution. Second, we highlight the challenges when turning ideas into reality. Besides, we describe the state-of-the-art solutions including advantages and disadvantages for those challenges. In addition, twelve typical tools are analyzed and many properties of those tools are censused. Finally, we outline the prospects of this research field in detail.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_2013_state_of_the_art.pdf;D\:\\GDrive\\zotero\\Chen\\chen_2013_state_of_the_art2.pdf},
  journal = {Future Generation Computer Systems},
  keywords = {Automated test generation,Challenges and solutions,Dynamic symbolic execution,High code-coverage,Low false positives,Prospects,Tools analysis}
}

@article{chenSurveyEthereumSystems2020,
  title = {A {{Survey}} on {{Ethereum Systems Security}}: {{Vulnerabilities}}, {{Attacks}}, and {{Defenses}}},
  author = {Chen, Huashan and Pendleton, Marcus and Njilla, Laurent and Xu, Shouhuai},
  year = {2020},
  volume = {53},
  doi = {10.1145/3391195},
  abstract = {Blockchain technology is believed by many to be a game changer in many application domains. While the first generation of blockchain technology (i.e., Blockchain 1.0) is almost exclusively used for cryptocurrency, the second generation (i.e., Blockchain 2.0), as represented by Ethereum, is an open and decentralized platform enabling a new paradigm of computing-Decentralized Applications (DApps) running on top of blockchains. The rich applications and semantics of DApps inevitably introduce many security vulnerabilities, which have no counterparts in pure cryptocurrency systems like Bitcoin. Since Ethereum is a new, yet complex, system, it is imperative to have a systematic and comprehensive understanding on its security from a holistic perspective , which was previously unavailable in the literature. To the best of our knowledge, the present survey, which can also be used as a tutorial, fills this void. We systematize three aspects of Ethereum systems security: vulnerabilities, attacks, and defenses. We draw insights into vulnerability root causes, attack consequences, and defense capabilities, which shed light on future research directions.},
  file = {D\:\\GDrive\\zotero\\Chen\\chen_2020_a_survey_on_ethereum_systems_security.pdf},
  journal = {ACM Comput. Surv},
  keywords = {CCS Concepts: • Security and privacy → Distributed systems security; Additional Key Words and Phrases: Blockchain,Ethereum,security ACM Reference format:,smart contract}
}

@techreport{chepurnoyRollerchainBlockchainSafely2016,
  title = {Rollerchain, a {{Blockchain With Safely Pruneable Full Blocks}}},
  author = {Chepurnoy, Alexander and Larangeira, Mario and Ojiganov, Alexander},
  year = {2016},
  abstract = {Bitcoin [1] is the first successful decentralized global digital cash system. Its mining process requires intense computational resources, therefore its usefulness remains a disputable topic. We aim to solve three problems with Bitcoin and other blockchain systems of today by repurposing their work. First, space to store a blockchain is growing linearly with number of transactions. Second, a honest node is forced to be irrational regarding storing full blocks by a way implementations are done. Third, a trustless bootstrapping process for a new node involves downloading and processing all the transactions ever written into a blockchain. In this paper we present a new consensus protocol for Bitcoin-like peer-to-peer systems where a right to generate a block is given to a party providing non-interactive proofs of storing a subset of the past state snapshots. Unlike the blockchain systems in use today, a network using our protocol is safe if the nodes prune full blocks not needed for mining. We extend the GKL model [2] to describe our Proof-of-Work scheme and a transactional model modifications needed for it. We provide a detailed analysis of our protocol and proofs of its security.},
  file = {D\:\\GDrive\\zotero\\Chepurnoy\\chepurnoy_2016_rollerchain,_a_blockchain_with_safely_pruneable_full_blocks.pdf},
  keywords = {()}
}

@article{cherdantsevaReferenceModelInformation2013,
  title = {A Reference Model of Information Assurance \& Security},
  author = {Cherdantseva, Yulia and Hilton, Jeremy},
  year = {2013},
  pages = {546--555},
  publisher = {{IEEE}},
  doi = {10.1109/ARES.2013.72},
  abstract = {Information Assurance \& Security (IAS) is a dynamic domain which changes continuously in response to the evolution of society, business needs and technology. This paper proposes a Reference Model of Information Assurance amp; Security (RMIAS), which endeavours to address the recent trends in the IAS evolution, namely diversification and deperimetrisation. The model incorporates four dimensions: Information System Security Life Cycle, Information Taxonomy, Security Goals and Security Countermeasures. In addition to the descriptive knowledge, the RMIAS embeds the methodological knowledge. A case study demonstrate show the RMIAS assists with the development and revision of an Information Security Policy Document. \textcopyright{} 2013 IEEE.},
  isbn = {9780769550084},
  journal = {Proceedings - 2013 International Conference on Availability, Reliability and Security, ARES 2013},
  keywords = {Conceptual Model,Information Assurance,Information Security,Information Security Policy Development,Reference Model}
}

@article{CheritonSkeenUnderstanding,
  title = {Cheriton and {{Skeen}} - {{Understanding}} the {{Limitations}} of {{Causally}} and {{Tota}}},
  file = {D\:\\GDrive\\zotero\\undefined\\cheriton_and_skeen_-_understanding_the_limitations_of_causally_and_tota.pdf}
}

@article{chernoffFX32ProfileDirected1998,
  title = {{{FX}}!32 - {{A Profile}}-{{Directed Binary Translator}}},
  author = {Chernoff, Anton and Herdeg, Mark and Hookway, Ray and Reeve, Chris and Rubin, Norman and Tye, Tony and Yadavalli, S. Bharadwaj and Yates, John},
  year = {1998},
  volume = {18},
  pages = {56--64},
  abstract = {Because Digital s Alpha architecture provides the world's fastest processors, many applications, especially those requiring high processor performance, have been ported to it. However, many other applications are available only under the x86 architecture. We designed Digital FX!32 to make the complete set of applications, both native and x86, available to Alpha. The goal for the software is to provide fast and transparent execution of x86 Win32 applications on Alpha systems. FX!32 achieves its goal by transparently running those applications at speeds comparable to high-performance x86 platforms. Digital FX!32 is a software utility that enables x86 Win32 applications to be run on Windows NT/Alpha platforms. Once FX!32 has been installed, almost all x86 applications can be run on Alpha without special commands and with excellent performance.},
  file = {D\:\\GDrive\\zotero\\Chernoff et al\\chernoff_et_al_1998_fx.pdf;C\:\\Users\\Admin\\Zotero\\storage\\WSG7YGU9\\download.html},
  journal = {IEEE Micro}
}

@book{chessSecureProgrammingStatic2013,
  title = {Secure {{Programming}} with {{Static Analysis}}},
  author = {Chess, B. and West, J},
  year = {2013},
  volume = {53},
  issn = {1098-6596},
  abstract = {applicability for this approach.},
  file = {D\:\\GDrive\\zotero\\Chess\\chess_2013_secure_programming_with_static_analysis.pdf},
  isbn = {978-85-7811-079-6},
  journal = {Journal of Chemical Information and Modeling},
  keywords = {icle},
  pmid = {25246403}
}

@article{chessStaticAnalysisSecurity2004,
  title = {Static Analysis for Security},
  author = {Chess, Brian and Mcgraw, Gary},
  year = {2004},
  volume = {2},
  pages = {76--79},
  issn = {15407993},
  doi = {10.1109/MSP.2004.111},
  file = {D\:\\GDrive\\zotero\\Chess\\chess_2004_static_analysis_for_security.pdf},
  journal = {IEEE Security and Privacy},
  number = {6}
}

@misc{chinneckHowOrganizeYour,
  title = {How to {{Organize Your Thesis}}},
  author = {Chinneck, John W.},
  file = {D\:\\GDrive\\zotero\\Chinneck\\chinneck_how_to_organize_your_thesis.pdf;C\:\\Users\\Admin\\Zotero\\storage\\CIERDLZ6\\thesis.html},
  howpublished = {http://www.sce.carleton.ca/faculty/chinneck/thesis.html},
  keywords = {important,research}
}

@article{chinooiSINGADistributedDeep,
  title = {{{SINGA}}: {{A Distributed Deep Learning Platform}}},
  author = {Chin Ooi, Beng and Tan, Kian-Lee and Wang, Sheng and Wang, Wei and Cai, Qingchao and Chen, Gang and Gao, Jinyang and Luo, Zhaojing and H Tung, Anthony K and Wang, Yuan and Xie, Zhongle and Zhang, Meihui and Zheng, Kaiping},
  doi = {10.1145/2733373.2807410},
  abstract = {Deep learning has shown outstanding performance in various machine learning tasks. However, the deep complex model structure and massive training data make it expensive to train. In this paper, we present a distributed deep learning system, called SINGA, for training big models over large datasets. An intuitive programming model based on the layer abstraction is provided, which supports a variety of popular deep learning models. SINGA architecture supports both synchronous and asynchronous training frameworks. Hybrid training frameworks can also be customized to achieve good scalability. SINGA provides different neural net partitioning schemes for training large models. SINGA is an Apache Incubator project released under Apache License 2.},
  file = {D\:\\GDrive\\zotero\\Chin Ooi\\chin_ooi_singa.pdf},
  isbn = {9781450334594},
  keywords = {Distributed training,H34 [Information Storage and Retrieval]: Systems and Soft-ware-Distributed System General Terms Design; Experimentation; Performance Keywords Deep learning,I51 [Pattern Recognition]: Models-Neural Nets}
}

@techreport{chiosiNetworkFunctionsVirtualisation,
  title = {Network {{Functions Virtualisation}}},
  author = {Chiosi, Margaret BT and Clarke, Don and Willis, Peter and Reid CenturyLink, Andy and Feger, James and Bugenhagen, Michael and Khan, Waqar and Fargano, Michael and Benitez, Javier and Michel, Uwe and Damker KDDI, Herbert and Ogaki, Kenichi and Matsuzaki NTT, Tetsuro and Fukui, Masaki and Shimano, Katsuhiro and Delisle, Dominique and Loudier, Quentin and Kolias, Christos and Guardini, Ivano and Demaria, Elena and Minerva, Roberto and Manzalini, Antonio and L{\'o}pez, Diego and Javier Ram{\'o}n Salguero, Francisco and Ruhl, Frank and Sen, Prodip},
  file = {D\:\\GDrive\\zotero\\Chiosi\\chiosi_network_functions_virtualisation.pdf},
  keywords = {nfv}
}

@article{chongCS153CompilersLecture,
  title = {{{CS153}}: {{Compilers Lecture}} 7: {{Structured Data}} in {{LLVM IR}}},
  author = {Chong, Stephen},
  pages = {31},
  file = {D\:\\GDrive\\zotero\\Chong\\chong_cs153.pdf},
  language = {en}
}

@inproceedings{chowdhuryEfficientCoflowScheduling2015,
  title = {Efficient Coflow Scheduling with Varys},
  booktitle = {Computer {{Communication Review}}},
  author = {Chowdhury, Mosharaf and Zhong, Yuan and Stoica, Ion},
  year = {2015},
  month = feb,
  volume = {44},
  pages = {443--454},
  publisher = {{Association for Computing Machinery}},
  issn = {19435819},
  doi = {10.1145/2619239.2626315},
  abstract = {Communication in data-parallel applications often involves a collection of parallel flows. Traditional techniques to optimize flow-level metrics do not perform well in optimizing such collections, because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two different objectives: decreasing communication time of data-intensive jobs and guaranteeing predictable communication time. We introduce the concurrent open shop scheduling with coupled resources problem, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and guaranteeing starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16X faster on average and up to 2X more coflows meet their deadlines using Varys in comparison to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5X.},
  file = {D\:\\GDrive\\zotero\\Chowdhury\\chowdhury_2015_efficient_coflow_scheduling_with_varys.pdf},
  keywords = {Coflow,Data-intensive applications,Datacenter networks}
}

@techreport{christinTravelingSilkRoad2012,
  title = {Traveling the {{Silk Road}}: {{A}} Measurement Analysis of a Large Anonymous Online Marketplace},
  author = {Christin, Nicolas},
  year = {2012},
  volume = {1654},
  pages = {2012--2023},
  abstract = {We perform a comprehensive measurement analysis of Silk Road, an anonymous, international on-line marketplace that operates as a Tor hidden service and uses Bitcoin as its exchange currency. We gather and analyze data over eight months between the end of 2011 and 2012, including daily crawls of the marketplace for nearly six months in 2012. We obtain a detailed picture of the type of goods being sold on Silk Road, and of the revenues made both by sellers and Silk Road operators. Through examining over 24,400 separate items sold on the site, we show that Silk Road is overwhelmingly used as a market for controlled substances and narcotics, and that most items sold are available for less than three weeks. The majority of sellers disappears within roughly three months of their arrival, but a core of 112 sellers has been present throughout our measurement interval. We evaluate the total revenue made by all sellers, from public listings, to slightly over USD 1.2 million per month; this corresponds to about USD 92,000 per month in commissions for the Silk Road operators. We further show that the marketplace has been operating steadily, with daily sales and number of sellers overall increasing over our measurement interval. We discuss economic and policy implications of our analysis and results, including ethical considerations for future research in this area.},
  file = {D\:\\GDrive\\zotero\\Christin\\christin_2012_traveling_the_silk_road.pdf},
  keywords = {anonymity,electronic commerce,Online crime}
}

@techreport{chuCaseEndSystem,
  title = {A {{Case}} for {{End System Multicast}}},
  author = {Chu, Yang-Hua and Rao, Sanjay G and Seshan, Srinivasan and Zhang, Hui},
  abstract = {The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related func-tionality. However, more than a decade after its initial proposal, IP Multicast is still plagued with concerns pertaining to scala-bility, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture that we term End System Multicast, where end systems implement all mul-ticast related functionality including membership management and packet replication. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular , End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delays than IP Mul-ticast. In this paper, we study these performance concerns in the context of the Narada protocol. In Narada, end systems self-organize into an overlay structure using a fully distributed protocol. Further, end systems attempt to optimize the efficiency of the overlay by adapting to network dynamics and by considering application level performance. We present details of Narada and evaluate it using both simulation and Internet experiments. Our results indicate that the performance penalties are low both from the application and the network perspectives. We believe the potential benefits of transferring multicast functionality from end systems to routers significantly outweigh the performance penalty incurred.},
  file = {D\:\\GDrive\\zotero\\Chu\\chu_a_case_for_end_system_multicast.pdf}
}

@techreport{cidonCopysetsReducingFrequency,
  title = {Copysets: {{Reducing}} the {{Frequency}} of {{Data Loss}} in {{Cloud Storage}}},
  author = {Cidon, Asaf and Rumble, Stephen and Stutsman, Ryan and Katti, Sachin and Ousterhout, John and Rosenblum, Mendel},
  abstract = {Random replication is widely used in data center storage systems to prevent data loss. However, random replica-tion is almost guaranteed to lose data in the common scenario of simultaneous node failures due to cluster-wide power outages. Due to the high fixed cost of each incident of data loss, many data center operators prefer to minimize the frequency of such events at the expense of losing more data in each event. We present Copyset Replication, a novel general-purpose replication technique that significantly reduces the frequency of data loss events. We implemented and evaluated Copyset Replication on two open source data center storage systems, HDFS and RAMCloud, and show it incurs a low overhead on all operations. Such systems require that each node's data be scattered across several nodes for parallel data recovery and access. Copyset Replication presents a near optimal trade-off between the number of nodes on which the data is scattered and the probability of data loss. For example, in a 5000-node RAMCloud cluster under a power outage, Copyset Replication reduces the probability of data loss from 99.99\% to 0.15\%. For Facebook's HDFS cluster, it reduces the probability from 22.8\% to 0.78\%.},
  file = {D\:\\GDrive\\zotero\\Cidon\\cidon_copysets.pdf}
}

@book{cifuentesParfaitDesigningScalableBuga,
  title = {Parfait-{{Designing}} a {{Scalable Bug Checker}}},
  author = {Cifuentes, Cristina and Scholz, Bernhard},
  abstract = {We present the design of Parfait, a static layered program analysis framework for bug checking, designed for scalability and precision by improving false positive rates and scale to millions of lines of code. The Parfait framework is inherently parallelizable and makes use of demand driven analyses. In this paper we provide an example of several layers of analyses for buffer overflow, summarize our initial implementation for C, and provide preliminary results. Results are quantified in terms of correctly-reported, false positive and false negative rates against the NIST SAMATE synthetic benchmarks for C code.},
  file = {D\:\\GDrive\\zotero\\Cifuentes\\cifuentes_parfait-designing_a_scalable_bug_checker.pdf;D\:\\GDrive\\zotero\\Cifuentes\\cifuentes_parfait-designing_a_scalable_bug_checker2.pdf},
  isbn = {978-1-59593-924-1},
  keywords = {D24 [Software Engineering]: Software/Program Veri-,D24 [Software Engineering]: Software/Program Veri-fication,D28 [Software Engineering]: Metrics,D34 [Programming Languages]: Processors General Te,D34 [Programming Languages]: Processors General Terms static analysis}
}

@techreport{cittadiniMPLSVirtualPrivate2013,
  title = {{{MPLS Virtual Private Networks}}},
  author = {Cittadini, L and Battista, G Di and Patrignani, M},
  year = {2013},
  pages = {275--304},
  abstract = {This chapter is devoted to Virtual Private Networks (VPNs) designed with Multi Protocol Label Switching (MPLS) [14, 15, 1], one of the most elusive protocols of the network stack. Saying that MPLS is "elusive" is not overemphasizing: starting from its arduous fitting within the ISO/OSI protocol stack, continuing with its entangled relationships with several other routing and forwarding protocols (IP, OSPF, MP-BGP, just to name a few), and ending with the complex technicalities involved in its configuration, MPLS defies classifications and challenges easy descriptions. On the other hand, and in a seemingly contradictory way, the configuration of VPNs with MPLS is rather simple and elegant, despite the complexity of the underlying architecture. Also, MPLS flexibility and maintenance ease make it a powerful tool, and account for its ubiquity in Internet Service Providers' networks. The chapter is organized as follows. Section 1 gives a brief introduction and motivation behind the concept of Virtual Private Network and explains why Layer 3 MPLS VPNs are by far the most popular widespread kind of VPNs deployed today. In Section 2 we introduce the reader to basic concept and terminology about Label Switching (also known as Label Swapping) and Virtual Private Networks. Section 3 gives a high-level step-by-step description of an MPLS VPN. This is based on three main ingredients: an any-to-any IP connectivity inside the network, a signalling mechanism to announce customer IP prefixes, and an encapsulation mechanism, based on MPLS, to transport packets across the network. Section 4 explores in detail the complex interplay between IP and MPLS that is at the basis of MPLS VPNs. More technical details about dynamic routing and connecting to the Internet, advanced usage of routing, and preserving IP-specific per-hop behavior are provided in Section 5. Strengths and limitations of MPLS VPNs are discussed in Section 6. The same section proposes further readings on the subject. The reader who is interested in getting only a high-level understanding on how MPLS VPNs work can read Sections 1, 2, and 3. An indepth view of MPLS VPNs can be gained by reading Sections 4 and 5.},
  file = {D\:\\GDrive\\zotero\\Cittadini\\cittadini_2013_mpls_virtual_private_networks.pdf}
}

@techreport{clarkDesignPhilosophyDARPA1988,
  title = {The {{Design Philosophy}} of the {{DARPA Internet Protocols}}},
  author = {Clark, David D},
  year = {1988},
  volume = {18},
  pages = {106--114},
  abstract = {The Internet protocol suite, TCP/IP, was first proposed fifteen years ago. It was developed by the Defense Advanced Research Projects Agency (DARPA), and has been used widely in military and commercial systems. While there have been papers and specifications that describe how the protocols work, it is sometimes difficult to deduce from these why the protocol is as it is. For example, the Internet protocol is based on a connectionless or datagram mode of service. The motivation for this has been greatly misunderstood. This paper attempts to capture some of the early reasoning which shaped the Internet protocols.},
  file = {D\:\\GDrive\\zotero\\Clark\\clark_1988_the_design_philosophy_of_the_darpa_internet_protocols.pdf},
  journal = {Computer Communication Review},
  number = {4}
}

@article{clarkeAutomaticVerificationFiniteState1986,
  title = {Automatic {{Verification}} of {{Finite}}-{{State Concurrent Systems Using Temporal Logic Specifications}}},
  author = {Clarke, E M},
  year = {1986},
  volume = {8},
  pages = {244--263},
  abstract = {This paper describes a system that attempts to generate test data for programs written in ANSI Fortran. Given a path, the system symbolically executes the path and creates a set of constraints on the program's input variables. If the set of constraints is linear, linear programming techniques are employed to obtain a solution. A solution to the set of constraints is test data that will drive execution down the given path. If it can be determined that the set of constraints is inconsistent, then the given path is shown to be nonexecutable. To increase the chance of detecting some of the more common programming errors, artificial constraints are temporarily created that simulate error conditions and then an attempt is made to solve each augmented set of constraints. A symbolic representation of the program's output variables in terms of the program's input variables is also created. The symbolic representation is in a human readable form that facilitates error detection as weUl as being a possible aid in assertion generation and automatic program documentation},
  file = {D\:\\GDrive\\zotero\\Clarke\\clarke_1986_automatic_verification_of_finite-state_concurrent_systems_using_temporal_logic.pdf},
  number = {2}
}

@techreport{clarkeDESIGNSYNTHESISSYNCHRONIZATION,
  title = {{{DESIGN AND SYNTHESIS OF SYNCHRONIZATION SKELETONS USING BRANCHING TIME TEMPORAL LOGIC}}},
  author = {Clarke, Edmund M and Emerson, E Allen}
}

@techreport{clarkeFreenetDistributedAnonymous,
  title = {Freenet: {{A Distributed Anonymous Information Storage}} and {{Retrieval System}}},
  author = {Clarke, Ian and Sandberg, Oskar and Wiley, Brandon and Hong, Theodore W},
  abstract = {We describe Freenet, an adaptive peer-to-peer network application that permits the publication, replication, and retrieval of data while protecting the anonymity of both authors and readers. Freenet operates as a network of identical nodes that collectively pool their storage space to store data files and cooperate to route requests to the most likely physical location of data. No broadcast search or centralized location index is employed. Files are referred to in a location-independent manner, and are dynamically replicated in locations near requestors and deleted from locations where there is no interest. It is infeasible to discover the true origin or destination of a file passing through the network, and difficult for a node operator to determine or be held responsible for the actual physical contents of her own node.},
  file = {D\:\\GDrive\\zotero\\Clarke\\clarke_freenet.pdf}
}

@book{clarkeHandbookModelChecking,
  title = {Handbook of {{Model Checking}}},
  author = {Clarke, Edmund M and Henzinger, Thomas A and Veith, Helmut and Bloem, Roderick}
}

@incollection{clarkeIncrementalMultisetHash2003,
  title = {Incremental {{Multiset Hash Functions}} and {{Their Application}} to {{Memory Integrity Checking}}},
  booktitle = {Advances in {{Cryptology}} - {{ASIACRYPT}} 2003},
  author = {Clarke, Dwaine and Devadas, Srinivas and {van Dijk}, Marten and Gassend, Blaise and Suh, G. Edward},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Laih, Chi-Sung},
  year = {2003},
  volume = {2894},
  pages = {188--207},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-40061-5_12},
  abstract = {We introduce a new cryptographic tool: multiset hash functions. Unlike standard hash functions which take strings as input, multiset hash functions operate on multisets (or sets). They map multisets of arbitrary finite size to strings (hashes) of fixed length. They are incremental in that, when new members are added to the multiset, the hash can be updated in time proportional to the change. The functions may be multiset-collision resistant in that it is difficult to find two multisets which produce the same hash, or just set-collision resistant in that it is difficult to find a set and a multiset which produce the same hash.},
  file = {D\:\\GDrive\\zotero\\Clarke et al\\clarke_et_al_2003_incremental_multiset_hash_functions_and_their_application_to_memory_integrity.pdf},
  isbn = {978-3-540-20592-0 978-3-540-40061-5},
  language = {en}
}

@misc{clarkeLectureAssuringSoftware,
  title = {Lecture 1 : {{Assuring Software Quality}} by {{Model Checking}}},
  author = {Clarke, Edmund},
  file = {D\:\\GDrive\\zotero\\Clarke\\clarke_lecture_1.pdf}
}

@article{clarkeLocalizationReductionCounterexampleguided2010,
  title = {The Localization Reduction and Counterexample-Guided Abstraction Refinement},
  author = {Clarke, Edmund M. and Kurshan, Robert P. and Veith, Helmut},
  year = {2010},
  volume = {6200 LNCS},
  pages = {61--71},
  issn = {03029743},
  doi = {10.1007/978-3-642-13754-9_4},
  abstract = {Automated abstraction is widely recognized as a key method for computer-aided verification of hardware and software. In this paper, we describe the evolution of counterexample-guided refinement and other iterative abstraction refinement techniques. \textcopyright{} 2010 Springer-Verlag Berlin Heidelberg.},
  file = {D\:\\GDrive\\zotero\\Clarke\\clarke_2010_the_localization_reduction_and_counterexample-guided_abstraction_refinement.pdf},
  isbn = {3642137539},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{clarkeSystemGenerateTest1976,
  title = {A {{System}} to {{Generate Test Data}} and {{Symbolically Execute Programs}}},
  author = {Clarke, Lori A.},
  year = {1976},
  volume = {SE-2},
  pages = {215--222},
  issn = {00985589},
  doi = {10.1109/TSE.1976.233817},
  abstract = {This paper describes a system that attempts to generate test data for programs written in ANSI Fortran. Given a path, the system symbolically executes the path and creates a set of constraints on the program's input variables. If the set of constraints is linear, linear programming techniques are employed to obtain a solution. A solution to the set of constraints is test data that will drive execution down the given path. If it can be determined that the set of constraints is inconsistent, then the given path is shown to be nonexecutable. To increase the chance of detecting some of the more common programming errors, artificial constraints are temporarily created that simulate error conditions and then an attempt is made to solve each augmented set of constraints. A symbolic representation of the program's output variables in terms of the program's input variables is also created. The symbolic representation is in a human readable form that facilitates error detection as well as being a possible aid in assertion generation and automatic program documentation. Copyright \textcopyright{} 1976 by The Institute of Electrical and Electronics Engineers, Inc.},
  file = {D\:\\GDrive\\zotero\\Clarke\\clarke_1976_a_system_to_generate_test_data_and_symbolically_execute_programs.pdf},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {Program validation,software reliability,symbolic execution,test data generation},
  number = {3}
}

@techreport{clarkGoMultithreadedDeliberative,
  title = {Go! For Multi-Threaded Deliberative Agents},
  author = {Clark, K L and Mccabe, F G},
  abstract = {Go! is a multi-paradigm programming language that is oriented to the needs of programming secure, production quality, agent based applications. It is multi-threaded, strongly typed and higher order (in the functional programming sense). It has relation, function and action procedure definitions. Threads execute action procedures, calling functions and querying relations as need be. Threads in different agents communicate and coordinate using asynchronous messages. Threads within the same agent can also use shared dynamic relations acting as memory stores. In this paper we introduce the essential features of Go! illustrating them by programming a simple multi-agent application comprising hybrid reactive/deliberative agents interacting in a simulated ballroom. The dancer agents negotiate to enter into joint commitments to dance a particular dance (e.g. polka) they both desire. When the dance is announced, they dance together. The agents' reactive and deliberative components are concurrently executing threads which communicate and coordinate using belief, desire and intention memory stores. We believe such a multi-threaded agent architecture represents a powerful and natural style of agent implementation, for which Go! is well suited.},
  file = {D\:\\GDrive\\zotero\\Clark\\clark_go.pdf}
}

@techreport{clarkLiveMigrationVirtual,
  title = {Live {{Migration}} of {{Virtual Machines}}},
  author = {Clark, Christopher and Fraser, Keir and Hand, Steven and Hansen, Jacob Gorm and Jul, Eric and Limpach, Christian and Pratt, Ian and Warfield, Andrew},
  abstract = {Migrating operating system instances across distinct physical hosts is a useful tool for administrators of data centers and clusters: It allows a clean separation between hardware and software, and facilitates fault management, load balancing, and low-level system maintenance. By carrying out the majority of migration while OSes continue to run, we achieve impressive performance with minimal service downtimes; we demonstrate the migration of entire OS instances on a commodity cluster, recording service downtimes as low as 60ms. We show that that our performance is sufficient to make live migration a practical tool even for servers running interactive loads. In this paper we consider the design options for migrating OSes running services with liveness constraints, fo-cusing on data center and cluster environments. We introduce and analyze the concept of writable working set, and present the design, implementation and evaluation of high-performance OS migration built on top of the Xen VMM.},
  file = {D\:\\GDrive\\zotero\\Clark\\clark_live_migration_of_virtual_machines.pdf}
}

@article{clarkSoKSSLHTTPS2013,
  title = {{{SoK}}: {{SSL}} and {{HTTPS}}: {{Revisiting}} Past Challenges and Evaluating Certificate Trust Model Enhancements},
  author = {Clark, Jeremy and Van Oorschot, Paul C.},
  year = {2013},
  pages = {511--525},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2013.41},
  abstract = {Internet users today depend daily on HTTPS for secure communication with sites they intend to visit. Over the years, many attacks on HTTPS and the certificate trust model it uses have been hypothesized, executed, and/or evolved. Meanwhile the number of browser-trusted (and thus, de facto, user-trusted) certificate authorities has proliferated, while the due diligence in baseline certificate issuance has declined. We survey and categorize prominent security issues with HTTPS and provide a systematic treatment of the history and on-going challenges, intending to provide context for future directions. We also provide a comparative evaluation of current proposals for enhancing the certificate infrastructure used in practice. \textcopyright{} 2013 IEEE.},
  file = {D\:\\GDrive\\zotero\\Clark_Van Oorschot\\clark_van_oorschot_2013_sok.pdf},
  isbn = {9780769549774},
  journal = {Proceedings - IEEE Symposium on Security and Privacy},
  keywords = {browser trust model,certificates,ss,SSL,usability}
}

@article{classenModelCheckingLots2010,
  title = {Model Checking Lots of Systems: {{Efficient}} Verification of Temporal Properties in Software Product Lines},
  author = {Classen, Andreas and Heymans, Patrick and Schobbens, Pierre Yves and Legay, Axel and Raskin, Jean Fran{\c c}ois},
  year = {2010},
  volume = {1},
  pages = {335--344},
  issn = {02705257},
  doi = {10.1145/1806799.1806850},
  abstract = {In product line engineering, systems are developed in families and differences between family members are expressed in terms of features. Formal modelling and verification is an important issue in this context as more and more critical systems are developed this way. Since the number of systems in a family can be exponential in the number of features, two major challenges are the scalable modelling and the efficient verification of system behaviour. Currently, the few attempts to address them fail to recognise the importance of features as a unit of difference, or do not offer means for automated verification. In this paper, we tackle those challenges at a fundamental level. We first extend transition systems with features in order to describe the combined behaviour of an entire system family. We then define and implement a model checking technique that allows to verify such transition systems against temporal properties. An empirical evaluation shows substantial gains over classical approaches. \textcopyright{} 2010 ACM.},
  file = {D\:\\GDrive\\zotero\\Classen et al\\Classen et al_2010_Model checking lots of systems.pdf},
  isbn = {9781605587196},
  journal = {Proceedings - International Conference on Software Engineering},
  keywords = {features,software product lines,specification}
}

@article{classenSymbolicModelChecking2011,
  title = {Symbolic Model Checking of Software Product Lines},
  author = {Classen, Andreas and Heymans, Patrick and Schobbens, Pierre Yves and Legay, Axel},
  year = {2011},
  pages = {321--330},
  issn = {02705257},
  doi = {10.1145/1985793.1985838},
  abstract = {We study the problem of model checking software product line (SPL) behaviours against temporal properties. This is more difficult than for single systems because an SPL with n features yields up to 2n individual systems to verify. As each individual verification suffers from state explosion, it is crucial to propose efficient formalisms and heuristics. We recently proposed featured transition systems (FTS), a compact representation for SPL behaviour, and defined algorithms for model checking FTS against linear temporal properties. Although they showed to outperform individual system verifications, they still face a state explosion problem as they enumerate and visit system states one by one. In this paper, we tackle this latter problem by using symbolic representations of the state space. This lead us to consider computation tree logic (CTL) which is supported by the industry-strength symbolic model checker NuSMV. We first lay the foundations for symbolic SPL model checking by defining a feature-oriented version of CTL and its dedicated algorithms. We then describe an implementation that adapts the NuSMV language and tool infrastructure. Finally, we propose theoretical and empirical evaluations of our results. The benchmarks show that for certain properties, our algorithm is over a hundred times faster than model checking each system with the standard algorithm. \textcopyright{} 2011 ACM.},
  isbn = {9781450304450},
  journal = {Proceedings - International Conference on Software Engineering},
  keywords = {features,software product lines,specification}
}

@techreport{clementMakingByzantineFault,
  title = {Making {{Byzantine Fault Tolerant Systems Tolerate Byzantine Faults}}},
  author = {Clement, Allen and Wong, Edmund and Alvisi, Lorenzo and Dahlin, Mike and Marchetti, Mirco},
  pages = {153},
  abstract = {This paper argues for a new approach to building Byzan-tine fault tolerant replication systems. We observe that although recently developed BFT state machine replica-tion protocols are quite fast, they don't tolerate Byzantine faults very well: a single faulty client or server is capable of rendering PBFT, Q/U, HQ, and Zyzzyva virtually unusable. In this paper, we (1) demonstrate that existing protocols are dangerously fragile, (2) define a set of principles for constructing BFT services that remain useful even when Byzantine faults occur, and (3) apply these principles to construct a new protocol, Aardvark. Aardvark can achieve peak performance within 40\% of that of the best existing protocol in our tests and provide a significant fraction of that performance when up to f servers and any number of clients are faulty. We observe useful throughputs between 11706 and 38667 requests per second for a broad range of injected faults.},
  file = {D\:\\GDrive\\zotero\\Clement\\clement_making_byzantine_fault_tolerant_systems_tolerate_byzantine_faults.pdf}
}

@article{ClosingGapLlvm2020,
  title = {Closing {{The Gap In The Llvm Backend Of K}}},
  year = {2020},
  file = {D\:\\GDrive\\zotero\\undefined\\2020_closing_the_gap_in_the_llvm_backend_of_k.pdf}
}

@article{cohnOptimizingAlphaExecutables1997,
  title = {Optimizing {{Alpha Executables}} on {{Windows NT}} with {{Spike}}},
  author = {Cohn, Robert S and Goodwin, David W and Lowney, P Geoffrey},
  year = {1997},
  volume = {9},
  pages = {18},
  file = {D\:\\GDrive\\zotero\\Cohn et al\\cohn_et_al_1997_optimizing_alpha_executables_on_windows_nt_with_spike.pdf},
  language = {en},
  number = {4}
}

@article{cokerPrinciplesRemoteAttestation2011,
  title = {Principles of Remote Attestation},
  author = {Coker, George and Guttman, Joshua and Loscocco, Peter and Herzog, Amy and Millen, Jonathan and O'Hanlon, Brian and Ramsdell, John and Segall, Ariel and Sheehy, Justin and Sniffen, Brian},
  year = {2011},
  volume = {10},
  pages = {63--81},
  issn = {16155262},
  doi = {10.1007/s10207-011-0124-7},
  abstract = {Remote attestation is the activity of making a claim about properties of a target by supplying evidence to an appraiser over a network. We identify five central principles to guide development of attestation systems. We argue that (i) attestation must be able to deliver temporally fresh evidence; (ii) comprehensive information about the target should be accessible; (iii) the target, or its owner, should be able to constrain disclosure of information about the target; (iv) attestation claims should have explicit semantics to allow decisions to be derived from several claims; and (v) the underlying attestation mechanism must be trustworthy. We illustrate how to acquire evidence from a running system, and how to transport it via protocols to remote appraisers. We propose an architecture for attestation guided by these principles. Virtualized platforms, which are increasingly well supported on stock hardware, provide a natural basis for our attestation architecture. \textcopyright{} 2011 Springer-Verlag.},
  file = {D\:\\GDrive\\zotero\\Coker\\coker_2011_principles_of_remote_attestation.pdf},
  journal = {International Journal of Information Security},
  keywords = {Cryptographic protocols,Hardware Security Modules,Operating system security architecture,Strand spaces,Trust and attestation},
  number = {2}
}

@article{cokerPrinciplesRemoteAttestation2011a,
  title = {Principles of Remote Attestation},
  author = {Coker, George and Guttman, Joshua and Loscocco, Peter and Herzog, Amy and Millen, Jonathan and O'Hanlon, Brian and Ramsdell, John and Segall, Ariel and Sheehy, Justin and Sniffen, Brian},
  year = {2011},
  month = jun,
  volume = {10},
  pages = {63--81},
  issn = {1615-5262, 1615-5270},
  doi = {10.1007/s10207-011-0124-7},
  file = {D\:\\GDrive\\zotero\\Coker et al\\coker_et_al_2011_principles_of_remote_attestation.pdf},
  journal = {International Journal of Information Security},
  language = {en},
  number = {2}
}

@techreport{ComprehensivePictureGreat,
  title = {Towards a {{Comprehensive Picture}} of the {{Great Firewall}}'s {{DNS Censorship Anonymous}}},
  abstract = {China's Great Firewall passively inspects network traffic and disrupts unwanted communication by injecting forged DNS replies or TCP resets. We attempted to comprehensively examine the structure of the DNS in-jector, using queries from both within and outside China. Using these probes, we were able to localize the DNS monitors' locations, extract the firewall's DNS blacklist of approximately 15,000 keywords, and estimate the cluster structure and active response rate by utilizing an information leakage in the Great Firewall's design.},
  file = {D\:\\GDrive\\zotero\\undefined\\towards_a_comprehensive_picture_of_the_great_firewall's_dns_censorship_anonymous.pdf}
}

@book{congMCSimEfficientSimulation,
  title = {{{MC}}-{{Sim}}: {{An Efficient Simulation Tool}} for {{MPSoC Designs}}},
  author = {Cong, Jason and Gururaj, Karthik and Han, Guoling and Kaplan, Adam and Naik, Mishali and Reinman, Glenn},
  abstract = {The ability to integrate diverse components such as processor cores, memories, custom hardware blocks and complex network-on-chip (NoC) communication frameworks onto a single chip has greatly increased the design space available for system-on-chip (SoC) designers. Efficient and accurate performance estimation tools are needed to assist the designer in making design decisions. In this paper, we present MC-Sim, a heterogeneous multi-core simulator framework which is capable of accurately simulating a variety of processor, memory, NoC configurations and application specific coprocessors. We also describe a methodology to automatically generate fast, cycle-true behavioral, C-based simulators for coprocessors using a high-level synthesis tool and integrate them with MC-Sim, thus augmenting it with the capacity to simulate coprocessors. Our C-based simulators provide on an average 45x improvement in simulation speed over that of RTL descriptions. We have used this framework to simulate a number of real-life applications such as the MPEG4 decoder and litho-simulation, and experimented with a number of design choices. Our simulator framework is able to accurately model the performance of these applications (only 7\% off the actual implementation) and allows us to explore the design space rapidly and achieve interesting design implementations.},
  file = {D\:\\GDrive\\zotero\\Cong\\cong_mc-sim.pdf},
  isbn = {978-1-4244-2820-5}
}

@techreport{congPlatformbasedBehaviorlevelSystemlevel2006,
  title = {Platform-Based Behavior-Level and System-Level Synthesis},
  author = {Cong, Jason and Fan, Yiping and Han, Guoling and Jiang, Wei and Zhang, Zhiru},
  year = {2006},
  abstract = {With the rapid increase of complexity in System-on-a-Chip (SoC) design, the electronic design automation (EDA) community is moving from RTL (Register Transfer Level) synthesis to behavioral-level and system-level synthesis. The needs of system-level verification and software/hardware co-design also prefer behavior-level executable specifications, such as C or SystemC. In this paper we present the platform-based synthesis system, named xPilot, being developed at UCLA. The first objective of xPilot is to provide novel behavioral synthesis capability for automatically generating efficient RTL code from a C or SystemC description for a given system platform and optimizing the logic, interconnects, performance, and power simultaneously. The second objective of xPilot is to provide a platform-based system-level synthesis capability, including both synthesis for application-specific configurable processors and heterogeneous multi-core systems. Preliminary experiments on FPGAs demonstrate the efficacy of our approach on a wide range of applications and its value in exploring various design tradeoffs. I. MOTIVATION The relentless tracking of Moore's curve by the entire semiconductor industry has showcased the exponential scaling of the transistor feature size by a factor of 0.7 reduction every three years. This leads to exponentially increasing transistor counts and results in an explosive growth in functionality and the amount of computing power available on a single chip. Today it is perfectly feasible to design a System-on-a-Chip (SoC) with one billion transistors [7], and it is generally believed that industry will continue to overcome technical hurdles to sustain this trend for another decade. However, the cost of developing these chips and providing production facilities is also growing at a very fast pace. For instance, the total development cost of a single complex, high-density SoC at today's 90-nm technology can easily be in the \$20 to \$30 million range. The ITRS 2005 edition [7] has also emphasized that the cost of design remains the greatest threat to continuation of the semiconductor roadmap. Unfortunately, the progress of design technologies lags behind that of process manufacturing technologies. The constantly improving CAD tools can help to mitigate the problem by delivering faster simulation, higher capacity formal verification , and better logic synthesis coupled with place-and-route. However, these improvements fail to close the design productivity gap, i.e., the number of available transistors grows faster than the ability to meaningfully design them. It is commonly acknowledged that the ultimate solution is to move to the next level of abstraction beyond RTL, and Electronic system-level (ESL) design automation has been widely identified as the next productivity boost for the semiconductor industry. However, despite some recent success in ESL simulation, the transition to ESL design will not be as well accepted as the transition to RTL without robust and efficient behavior-level and system-level synthesis technologies that automatically synthesize high-level functional descriptions into optimized software/hardware implementations. We believe that behavior-level and system-level synthesis and optimizations are becoming imperative steps in EDA design flows. They provide the following combined advantages: Better complexity management: Design abstraction is one of the most effective methods for controlling rising complexity and improving design productivity. For example, a recent study from NEC [8] shows that the code density (in terms of line counts) can be improved by nearly 10X when moved to the behavior level. In addition, behavior-level and system-level synthesis have the added value of allowing efficient reuse of soft functional/behavioral IPs, which are technology-independent and can be synthesized for different requirements. Shorter verification/simulation cycle: System-level synthesis and optimizations allow the designers to start with a specification in a high-level programming language (HPL) such as C or SystemC that is directly executable and simulat-able with high speed (up to 1000X faster than RT-level simulation according to [8]). More importantly, behavioral synthesis automatically compiles the input descriptions into RTL code through a series of formal constructive transformations. This avoids the slow and error-prone manual process and simplifies the design verification and debugging effort. Rapid system exploration: With the coexistence of microprocessors , DSPs, memories and custom logic on a single chip, more software elements are involved in the process of designing a modern embedded system. One of the fundamental challenges of system-level design is the hardware/software partitioning, a task that is too complex to be feasible at the RT level. HPL-based design methodologies (especially C-based designs) offer a promising solution to this problem. With the aid of behavior-level synthesis, the software programming languages can also be used to specify functionality in hardware. In this flow, designers can quickly experiment with different hardware/software boundaries by co-simulating the HPL descriptions and the automatically synthesized HDLs. Higher quality of results: VLSI designs in current semiconductor technologies are limited by interconnect in both delay and power. However, since the interconnects are deter},
  file = {D\:\\GDrive\\zotero\\Cong\\cong_platform-based_behavior-level_and_system-level_synthesis.pdf}
}

@techreport{conradADVICEMATHEMATICALWRITING,
  title = {{{ADVICE ON MATHEMATICAL WRITING}}},
  author = {Conrad, Keith},
  file = {D\:\\GDrive\\zotero\\Conrad\\conrad_advice_on_mathematical_writing.pdf}
}

@inproceedings{contiRADISRemoteAttestation2019,
  title = {{{RADIS}}: {{Remote Attestation}} of {{Distributed IoT Services}}},
  shorttitle = {{{RADIS}}},
  booktitle = {2019 {{Sixth International Conference}} on {{Software Defined Systems}} ({{SDS}})},
  author = {Conti, Mauro and Dushku, Edlira and Mancini, Luigi V.},
  year = {2019},
  month = jun,
  pages = {25--32},
  publisher = {{IEEE}},
  address = {{Rome, Italy}},
  doi = {10.1109/SDS.2019.8768670},
  abstract = {Remote attestation is a security technique through which a remote trusted party (i.e., Verifier) checks the trustworthiness of a potentially untrusted device (i.e., Prover). In the Internet of Things (IoT) systems, the existing remote attestation protocols propose various approaches to detect the modified software and physical tampering attacks. However, in an interoperable IoT system, in which IoT devices interact autonomously among themselves, an additional problem arises: a compromised IoT service can influence the genuine operation of other invoked service, without changing the software of the latter. In this paper, we propose a protocol for Remote Attestation of Distributed IoT Services (RADIS), which verifies the trustworthiness of distributed IoT services. Instead of attesting the complete memory content of the entire interoperable IoT devices, RADIS attests only the services involved in performing a certain functionality. RADIS relies on a control-flow attestation technique to detect IoT services that perform an unexpected operation due to their interactions with a malicious remote service. Our experiments show the effectiveness of our protocol in validating the integrity status of a distributed IoT service.},
  file = {D\:\\GDrive\\zotero\\Conti et al\\conti_et_al_2019_radis.pdf},
  isbn = {978-1-72810-722-6},
  language = {en}
}

@techreport{cookCharacterizationInstructionlevelError,
  title = {A {{Characterization}} of {{Instruction}}-Level {{Error Derating}} and Its {{Implications}} for {{Error Detection}}},
  author = {Cook, Jeffrey J and Zilles, Craig},
  abstract = {In this work, we characterize a significant source of software derating that we call instruction-level derating. Instruction-level derating encompasses the mechanisms by which computation on incorrect values can result in correct computation. We characterize the instruction-level derating that occurs in the SPEC CPU2000 INT benchmarks , classifying it (by source) into six categories: value comparison, sub-word operations, logical operations, over-flow/precision, lucky loads, and dynamically-dead values. We also characterize the temporal nature of this derating, demonstrating that the effects of a fault persist in architectural state long after the last time they are referenced. Finally, we demonstrate how this characterization can be used to avoid unnecessary error recoveries (when a fault will be masked by software anyway) in the context of a dual modular redundant (DMR) architecture.},
  file = {D\:\\GDrive\\zotero\\Cook\\cook_a_characterization_of_instruction-level_error_derating_and_its_implications_for.pdf},
  keywords = {Dual modular redundancy,error detection,fault injection,instruction-level derating,software derating}
}

@techreport{cooperFrTimeFunctionalReactive,
  title = {{{FrTime}}: {{Functional Reactive Programming}} in {{PLT Scheme}}},
  author = {Cooper, Gregory and Krishnamurthi, Shriram},
  abstract = {Functional Reactive Programming (FRP) supports the declarative construction of reactive systems through signals, or time-varying values. In this paper , we present a new language called FrTime, which provides FRP-style signals atop a dialect of Scheme. We introduce the language with a few examples and discuss its implementation. FrTime uses impure features, such as state and asyn-chronous communication, to model time and to control evaluation. The use of such features yields a scalable, event-driven implementation with several important advantages. Specifically, it eases integration with other systems, supports distribution of signals across a network, and permits various benign impurities. To illustrate the language's expressive power, we present a concise implementation of a networked paddle-ball game in FrTime.},
  file = {D\:\\GDrive\\zotero\\Cooper\\cooper_frtime.pdf}
}

@article{cooperPNUTSYahooHosted2008,
  title = {{{PNUTS}}: {{Yahoo}}!'s Hosted Data Serving Platform},
  author = {Cooper, Brian F. and Ramakrishnan, Raghu and Srivastava, Utkarsh and Silberstein, Adam and Bohannon, Philip and Jacobsen, Hans Arno and Puz, Nick and Weaver, Daniel and Yerneni, Ramana},
  year = {2008},
  volume = {1},
  pages = {1277--1288},
  issn = {21508097},
  doi = {10.14778/1454159.1454167},
  abstract = {We describe PNUTS, a massively parallel and geographically distributed database system for Yahoo!'s web applications. PNUTS provides data storage organized as hashed or ordered tables, low latency for large numbers of concurrent requests including updates and queries, and novel per-record consistency guarantees. It is a hosted, centrally managed, and geographically distributed service, and utilizes automated load-balancing and failover to reduce oper- ational complexity. The first version of the system is currently serving in production. We describe the motivation for PNUTS and the design and implementation of its table storage and replication layers, and then present experimental results. \textcopyright{} 2008 VLDB Endowment.},
  file = {D\:\\GDrive\\zotero\\Cooper\\cooper_2008_pnuts.pdf},
  isbn = {0000000000000},
  journal = {Proceedings of the VLDB Endowment},
  number = {2}
}

@techreport{cooperRevisitingGraphColoring,
  title = {Revisiting {{Graph Coloring Register Allocation}}: {{A Study}} of the {{Chaitin}}-{{Briggs}} and {{Callahan}}-{{Koblenz Algorithms}}},
  author = {Cooper, Keith and Dasgupta, Anshuman and Eckhardt, Jason},
  abstract = {Techniques for global register allocation via graph coloring have been extensively studied and widely implemented in compiler frameworks. This paper examines a particular variant-the Callahan Koblenz allocator-and compares it to the Chaitin-Briggs graph coloring register allocator. Both algorithms were published in the 1990's, yet the academic literature does not contain an assessment of the Callahan-Koblenz allocator. This paper evaluates and contrasts the allocation decisions made by both algorithms. In particular, we focus on two key differences between the allocators: Spill code: The Callahan-Koblenz allocator attempts to minimize the effect of spill code by using program structure to guide allocation and spill code placement. We evaluate the impact of this strategy on allocated code. Copy elimination: Effective register-to-register copy removal is important for producing good code. The allocators use different techniques to eliminate these copies. We compare the mechanisms and provide insights into the relative performance of the contrasting techniques. The Callahan-Koblenz allocator may potentially insert extra branches as part of the allocation process. We also measure the performance overhead due to these branches.},
  file = {D\:\\GDrive\\zotero\\Cooper\\cooper_revisiting_graph_coloring_register_allocation.pdf}
}

@techreport{cooperTailoringGraphcoloringRegister,
  title = {Tailoring {{Graph}}-Coloring {{Register Allocation For Runtime Compilation}}},
  author = {Cooper, Keith D and Dasgupta, Anshuman},
  abstract = {Just-in-time compilers are invoked during application execution and therefore need to ensure fast compilation times. Consequently, runtime compiler designers are averse to implementing compile-time intensive optimization algorithms. Instead, they tend to select faster but less effective transformations. In this paper, we explore this trade-off for an important optimization-global register allocation. We present a graph-coloring register allocator that has been redesigned for runtime compilation. Compared to Chaitin-Briggs [7], a standard graph-coloring technique, the re-formulated algorithm requires considerably less allocation time and produces allocations that are only marginally worse than those of Chaitin-Briggs. Our experimental results indicate that the allocator performs better than the linear-scan and Chaitin-Briggs allocators on most benchmarks in a runtime compilation environment. By increasing allocation efficiency and preserving optimization quality, the presented algorithm increases the suitability and profitability of a graph-coloring register allocation strategy for a runtime compiler.},
  file = {D\:\\GDrive\\zotero\\Cooper\\cooper_tailoring_graph-coloring_register_allocation_for_runtime_compilation.pdf}
}

@techreport{corbettSpannerGoogleGloballyDistributeda,
  title = {Spanner: {{Google}}'s {{Globally}}-{{Distributed Database}}},
  author = {Corbett, James C and Dean, Jeffrey and Epstein, Michael and Fikes, Andrew and Frost, Christopher and Furman, J J and Ghemawat, Sanjay and Gubarev, Andrey and Heiser, Christopher and Hochschild, Peter and Hsieh, Wilson and Kanthak, Sebastian and Kogan, Eugene and Li, Hongyi and Lloyd, Alexander and Melnik, Sergey and Mwaura, David and Nagle, David and Quinlan, Sean and Rao, Rajesh and Rolig, Lindsay and Saito, Yasushi and Szymaniak, Michal and Taylor, Christopher and Wang, Ruth and Woodford, Dale},
  abstract = {Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: non-blocking reads in the past, lock-free read-only transactions , and atomic schema changes, across all of Spanner.},
  file = {D\:\\GDrive\\zotero\\Corbett\\corbett_spanner.pdf;D\:\\GDrive\\zotero\\Corbett\\corbett_spanner2.pdf},
  keywords = {distributed systems}
}

@techreport{cormodeForwardDecayPractical,
  title = {Forward {{Decay}}: {{A Practical Time Decay Model}} for {{Streaming Systems}}},
  author = {Cormode, Graham and Shkapenyuk, Vladislav and Srivastava, Divesh and Xu, Bojian},
  abstract = {Temporal data analysis in data warehouses and data streaming systems often uses time decay to reduce the importance of older tuples, without eliminating their influence, on the results of the analysis. While exponential time decay is commonly used in practice, other decay functions (e.g. polynomial decay) are not, even though they have been identified as useful. We argue that this is because the usual definitions of time decay are "backwards": the decayed weight of a tuple is based on its age, measured backward from the current time. Since this age is constantly changing, such decay is too complex and unwieldy for scalable implementation. In this paper, we propose a new class of "forward" decay functions based on measuring forward from a fixed point in time. We show that this model captures the more practical models already known, such as exponential decay and landmark windows, but also includes a wide class of other types of time decay. We provide efficient algorithms to compute a variety of aggregates and draw samples under forward decay, and show that these are easy to implement scalably. Further, we provide empirical evidence that these can be executed in a production data stream management system with little or no overhead compared to the undecayed computations. Our implementation required no extensions to the query language or the DSMS, demonstrating that forward decay represents a practical model of time decay for systems that deal with time-based data.},
  file = {D\:\\GDrive\\zotero\\Cormode\\cormode_forward_decay.pdf}
}

@techreport{costanIntelSGXExplained,
  title = {Intel {{SGX Explained}}},
  author = {Costan, Victor and Devadas, Srinivas},
  abstract = {Intel's Software Guard Extensions (SGX) is a set of extensions to the Intel architecture that aims to provide integrity and confidentiality guarantees to security-sensitive computation performed on a computer where all the privileged software (kernel, hypervisor, etc) is potentially malicious. This paper analyzes Intel SGX, based on the 3 papers [14, 78, 137] that introduced it, on the Intel Software Developer's Manual [100] (which supersedes the SGX manuals [94, 98]), on an ISCA 2015 tutorial [102], and on two patents [108, 136]. We use the papers, reference manuals, and tutorial as primary data sources, and only draw on the patents to fill in missing information. This paper's contributions are a summary of the Intel-specific architectural and micro-architectural details needed to understand SGX, a detailed and structured presentation of the publicly available information on SGX, a series of intelligent guesses about some important but undocumented aspects of SGX, and an analysis of SGX's security properties.},
  file = {D\:\\GDrive\\zotero\\Costan\\costan_intel_sgx_explained.pdf},
  keywords = {ss}
}

@article{costinLargeScaleAnalysisSecurity2014,
  title = {A {{Large}}-{{Scale Analysis}} of the {{Security}} of {{Embedded Firmwares}}},
  author = {Costin, Andrei and Zaddach, Jonas and Francillon, Aur{\'e}lien and Balzarotti, Davide and Sophia, Eurecom and France, Antipolis},
  year = {2014},
  abstract = {As embedded systems are more than ever present in our society, their security is becoming an increasingly important issue. However, based on the results of many recent analyses of individual firmware images, embedded systems acquired a reputation of being insecure. Despite these facts, we still lack a global understanding of embedded systems' security as well as the tools and techniques needed to support such general claims. In this paper we present the first public, large-scale analysis of firmware images. In particular, we unpacked 32 thousand firmware images into 1.7 million individual files, which we then statically analyzed. We leverage this large-scale analysis to bring new insights on the security of embedded devices and to underline and detail several important challenges that need to be addressed in future research. We also show the main benefits of looking at many different devices at the same time and of linking our results with other large-scale datasets such as the ZMap's HTTPS survey. In summary, without performing sophisticated static analysis, we discovered a total of 38 previously unknown vulnerabilities in over 693 firmware images. Moreover, by correlating similar files inside apparently unrelated firmware images, we were able to extend some of those vulnerabilities to over 123 different products. We also confirmed that some of these vulnerabilities altogether are affecting at least 140K devices accessible over the Internet. It would not have been possible to achieve these results without an analysis at such wide scale. We believe that this project, which we plan to provide as a firmware unpacking and analysis web service 1 , will help shed some light on the security of embedded devices .},
  file = {D\:\\GDrive\\zotero\\Costin\\costin_a_large-scale_analysis_of_the_security_of_embedded_firmwares.pdf;D\:\\GDrive\\zotero\\Costin\\costin_a_large-scale_analysis_of_the_security_of_embedded_firmwares2.pdf}
}

@techreport{coulomEfficientSelectivityBackup,
  title = {Efficient {{Selectivity}} and {{Backup Operators}} in {{Monte}}-{{Carlo Tree Search}}},
  author = {Coulom, R{\'e}mi},
  abstract = {Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations, and can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation , that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity methods. This algorithm was implemented in a 9 \texttimes{} 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.},
  file = {D\:\\GDrive\\zotero\\Coulom\\coulom_efficient_selectivity_and_backup_operators_in_monte-carlo_tree_search.pdf}
}

@techreport{cousotAbstractInterpretationUnified1977,
  title = {Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints},
  author = {Cousot, Patrick and Cousot, Radhia},
  year = {1977},
  file = {D\:\\GDrive\\zotero\\Cousot_Cousot\\cousot_cousot_1977_abstract_interpretation.pdf}
}

@article{cowanStackGuardAutomaticAdaptive1998,
  title = {{{StackGuard}}: {{Automatic}} Adaptive Detection and Prevention of Buffer-Overflow Attacks},
  author = {Cowan, Crispin and Pu, Calton and Maier, Dave and Hinton, Heather and Walpole, Jonathan and Bakke, Peat and Beattie, Steve and Grier, Aaron and Wagle, Perry and Zhang, Qian},
  year = {1998},
  abstract = {This paper presents a systematic solution to the persistent problem of buffer overflow attacks. Buffer overflow attacks gained notorietyin 1988 as part of the Morris Worm incident on the Internet. While it is fairly simple to fix individual buffer overflow vulnerabilities, buffer overflow attacks continue to this day. Hundreds of attacks have been discovered, and while most of the obvious vulnerabilities have now been patched, more sophisticated buffer overflow attacks continue to emerge. We describe StackGuard: a simple compiler technique that virtually eliminates buffer overflow vulnerabilities with only modest performance penalties. Privileged programs that are recompiled with the StackGuard compiler extension no longer yield control to the attacker, but rather enter a fail-safe state. These programs require no source code changes at all, and are binary-compatible with existing operating systems and libraries. We describe the compiler technique (a simple patch to GCC), as well as a set of variations on the technique that tradeoff between penetration resistance and performance. We present experimental results of both the penetration resistance and the performance impact of this technique.},
  file = {D\:\\GDrive\\zotero\\Cowan\\cowan_1998_stackguard.pdf},
  isbn = {3060296103},
  journal = {Proceedings of the 7th USENIX Security Symposium}
}

@article{cowardSymbolicExecutionTesting1991,
  title = {Symbolic Execution and Testing},
  author = {Coward, PD},
  year = {1991},
  volume = {33},
  pages = {53--64},
  issn = {09505849},
  doi = {10.1016/0950-5849(91)90024-6},
  abstract = {Symbolic execution has several applications mainly in the validation of software. It may be used in the generation of test data, in program proving, and in program reduction. The paper is a tutorial on symbolic execution and how it may be used in software testing and in particular the testing of commercial data-processing (DP) software. An example is used to describe: the symbolic execution of feasible and infeasible paths; the creation of a test case from a feasible path condition; and the checking of assertions by conjoining them to the path condition. Some problems in applying symbolic execution are discussed, with particular attention given to the task of assessing path feasibility. For some programs, linear programming optimizers can be used to assess path feasibility. Commercial DP software is a class of software likely to be amenable to the linear optimizer approach, but this class brings a few problems of its own. Most of these problems can be overcome. Early software tools that used symbolic execution were aimed at numerical software written in languages such as Fortran. No symbolic execution tool has previously been built for commercial DP languages such as Cobol. The last section of the paper describes the main features of SYM-BOL, a symbolic execution testing tool for Cobol. \textcopyright{} 1991.},
  journal = {Information and Software Technology},
  keywords = {linear programming,software testing,symbolic execution,test case generation},
  number = {1}
}

@techreport{CrisisAftermath,
  title = {Crisis and {{Aftermath}}},
  file = {D\:\\GDrive\\zotero\\undefined\\crisis_and_aftermath.pdf}
}

@article{criswellKCoFICompleteControlflow2014,
  title = {{{KCoFI}}: {{Complete}} Control-Flow Integrity for Commodity Operating System Kernels},
  author = {Criswell, John and Dautenhahn, Nathan and Adve, Vikram},
  year = {2014},
  pages = {292--307},
  issn = {10816011},
  doi = {10.1109/SP.2014.26},
  abstract = {We present a new system, KCoFI, that is the first we know of to provide complete Control-Flow Integrity protection for commodity operating systems without using heavyweight complete memory safety. Unlike previous systems, KCoFI protects commodity operating systems from classical control-flow hijack attacks, return-to-user attacks, and code segment modification attacks. We formally verify a subset of KCoFI's design by modeling several features in small-step semantics and providing a partial proof that the semantics maintain control-flow integrity. The model and proof account for operations such as page table management, trap handlers, context switching, and signal delivery. Our evaluation shows that KCoFI prevents all the gadgets found by an open-source Return Oriented Programming (ROP) gadget-finding tool in the FreeBSD kernel from being used, it also reduces the number of indirect control-flow targets by 98.18\%. Our evaluation also shows that the performance impact of KCoFI on web server bandwidth is negligible while file transfer bandwidth using OpenSSH is reduced by an average of 13\%, and at worst 27\%, across a wide range of file sizes. Postmark, an extremely file-system intensive benchmark, shows 2x overhead. Where comparable numbers are available, the overheads of KCoFI are far lower than heavyweight memory-safety techniques.},
  file = {D\:\\GDrive\\zotero\\Criswell\\criswell_2014_kcofi.pdf},
  isbn = {9781479946860},
  journal = {Proceedings - IEEE Symposium on Security and Privacy},
  keywords = {compiler,control-flow integrity,formal verification,Free BSD,operating systems}
}

@book{criswellSecureVirtualArchitecture,
  title = {Secure {{Virtual Architecture}}: {{A Safe Execution Environment}} for {{Commodity Operating Systems}}},
  author = {Criswell, John and Lenharth, Andrew and Dhurjati, Dinakar and Adve, Vikram},
  abstract = {This paper describes an efficient and robust approach to provide a safe execution environment for an entire operating system, such as Linux, and all its applications. The approach, which we call Secure Virtual Architecture (SVA), defines a virtual, low-level, typed instruction set suitable for executing all code on a system, including kernel and application code. SVA code is translated for execution by a virtual machine transparently, offline or online. SVA aims to enforce fine-grained (object level) memory safety, control-flow integrity, type safety for a subset of objects, and sound analysis. A virtual machine implementing SVA achieves these goals by using a novel approach that exploits properties of existing memory pools in the kernel and by preserving the kernel's explicit control over memory, including custom al-locators and explicit deallocation. Furthermore, the safety properties can be encoded compactly as extensions to the SVA type system, allowing the (complex) safety checking compiler to be outside the trusted computing base. SVA also defines a set of OS interface operations that abstract all privileged hardware instructions, allowing the virtual machine to monitor all privileged operations and control the physical resources on a given hardware platform. We have ported the Linux kernel to SVA, treating it as a new architecture , and made only minimal code changes (less than 300 lines of code) to the machine-independent parts of the kernel and device drivers. SVA is able to prevent 4 out of 5 memory safety exploits previously reported for the Linux 2.4.22 kernel for which exploit code is available, and would prevent the fifth one simply by compiling an additional kernel library.},
  file = {D\:\\GDrive\\zotero\\Criswell\\criswell_secure_virtual_architecture.pdf},
  isbn = {978-1-59593-591-5},
  keywords = {D34 [Programming Languages] Processors,D46 [Operating Systems] Security and Protection,D47 [Operating Systems] Organization and Design,General Terms: Design; reliability; security Keywords: Operating systems; virtual machine; compiler; security; memory safety; type safety; typed assembly lan-guage}
}

@article{criswellVirtualGhostProtecting,
  title = {Virtual {{Ghost}}: {{Protecting Applications}} from {{Hostile Operating Systems}}},
  author = {Criswell, John and Dautenhahn, Nathan and Adve, Vikram},
  doi = {10.1145/2541940.2541986},
  abstract = {Applications that process sensitive data can be carefully designed and validated to be difficult to attack, but they are usually run on monolithic, commodity operating systems, which may be less secure. An OS compromise gives the attacker complete access to all of an application's data, regardless of how well the application is built. We propose a new system, Virtual Ghost, that protects applications from a compromised or even hostile OS. Virtual Ghost is the first system to do so by combining compiler instrumentation and run-time checks on operating system code, which it uses to create ghost memory that the operating system cannot read or write. Virtual Ghost interposes a thin hardware abstraction layer between the kernel and the hardware that provides a set of operations that the kernel must use to manipulate hardware, and provides a few trusted services for secure applications such as ghost memory management, encryption and signing services, and key management. Unlike previous solutions, Virtual Ghost does not use a higher privilege level than the kernel. Virtual Ghost performs well compared to previous approaches ; it outperforms InkTag on five out of seven of the LMBench microbenchmarks with improvements between 1.3x and 14.3x. For network downloads, Virtual Ghost experiences a 45\% reduction in bandwidth at most for small files and nearly no reduction in bandwidth for large files and web traffic. An application we modified to use ghost memory shows a maximum additional overhead of 5\% due to the Virtual Ghost protections. We also demonstrate Virtual Ghost's efficacy by showing how it defeats sophisticated rootkit attacks.},
  file = {D\:\\GDrive\\zotero\\Criswell\\criswell_virtual_ghost.pdf},
  isbn = {9781450323055},
  keywords = {control-flow integrity,D46 [Operating Sys-tems]: Security and Protection General Terms Security Keywords software security,inlined reference monitors,malicious op-erating systems,software fault isolation}
}

@techreport{criswellVirtualInstructionSet,
  title = {A {{Virtual Instruction Set Interface}} for {{Operating System Kernels}}},
  author = {Criswell, John and Monroe, Brent and Adve, Vikram},
  abstract = {In this paper, we propose and evaluate a virtual instruction set interface between an operating system (OS) kernel and a general purpose processor architecture. This interface is a set of operations added to a previously proposed virtual instruction set architecture called LLVA (Low Level Virtual Architecture) and can be implemented on existing processor hardware. The long-term benefits of such an interface include richer OS-information for hardware, greater flexibility in evolving hardware, and sophisticated analysis and optimization capabilities for kernel code, including across the application-kernel boundary transformations. Our interface design (LLVA-OS) contains several novel features for machine-independence and performance, including efficient saving and restoring of (hidden) native state, mechanisms for error recovery, and several primitive abstractions that expose semantic information to the underlying translator and hardware. We describe the design, a prototype implementation of LLVA-OS on x86, and our experience porting the Linux 2.4.22 kernel to LLVA-OS. We perform a performance evaluation of this kernel, identifying and explaining the root causes of key sources of virtualization overhead.},
  file = {D\:\\GDrive\\zotero\\Criswell\\criswell_a_virtual_instruction_set_interface_for_operating_system_kernels.pdf}
}

@techreport{crosbynachiappanpradanpattanayaksanjeevvermaBlockChainTechnologyBitcoin2016,
  title = {{{BlockChain Technology}}: {{Beyond Bitcoin}}},
  author = {Crosby Nachiappan Pradan Pattanayak Sanjeev Verma, Michael and Kalyanaraman, Vignesh},
  year = {2016},
  abstract = {A blockchain is essentially a distributed database of records, or public ledger of all transactions or digital events that have been executed and shared among participating parties. Each transaction in the public ledger is verified by consensus of a majority of the participants in the system. Once entered, information can never be erased. The blockchain contains a certain and verifiable record of every single transaction ever made. Bitcoin, the decentralized peer-to-peer digital currency, is the most popular example that uses blockchain technology. The digital currency bitcoin itself is highly controversial but the underlying blockchain technology has worked flawlessly and found wide range of applications in both financial and non-financial world. The main hypothesis is that the blockchain establishes a system of creating a distributed consensus in the digital online world. This allows participating entities to know for certain that a digital event happened by creating an irrefutable record in a public ledger. It opens the door for developing a democratic open and scalable digital economy from a centralized one. There are tremendous opportunities in this disruptive technology, and the revolution in this space has just begun. This white paper describes blockchain technology and some compelling specific applications in both financial and non-financial sector. We then look at the challenges ahead and business opportunities in this fundamental technology that is all set to revolutionize our digital world.},
  file = {D\:\\GDrive\\zotero\\Crosby Nachiappan Pradan Pattanayak Sanjeev Verma\\crosby_nachiappan_pradan_pattanayak_sanjeev_verma_2016_blockchain_technology.pdf}
}

@article{csailVerSumVerifiableComputations2014,
  title = {{{VerSum}}: {{Verifiable Computations}} over {{Large Public Logs Jelle}} van Den {{Hooff}}},
  author = {Csail, Mit and Kaashoek, M Frans and Zeldovich, Nickolai},
  year = {2014},
  doi = {10.1145/2660267.2660327},
  abstract = {VERSUM allows lightweight clients to outsource expensive computations over large and frequently changing data structures, such as the Bitcoin or Namecoin blockchains, or a Certificate Transparency log. VERSUM clients ensure that the output is correct by comparing the outputs from multiple servers. VERSUM assumes that at least one server is honest, and crucially, when servers disagree, VERSUM uses an efficient conflict resolution protocol to determine which server(s) made a mistake and thus obtain the correct output. VERSUM's contribution lies in achieving low server-side overhead for both incremental re-computation and conflict resolution, using three key ideas: (1) representing the computation as a functional program, which allows memoization of previous results; (2) recording the evaluation trace of the functional program in a carefully designed computation history to help clients determine which server made a mistake; and (3) introducing a new authenticated data structure for sequences, called SEQHASH, that makes it efficient for servers to construct summaries of computation histories in the presence of incremental re-computation. Experimental results with an implementation of VERSUM show that VERSUM can be used for a variety of computations, that it can support many clients, and that it can easily keep up with Bitcoin's rate of new blocks with transactions.},
  file = {D\:\\GDrive\\zotero\\Csail\\csail_2014_versum.pdf},
  isbn = {9781450329576},
  keywords = {Cloud Computing}
}

@techreport{CyberphysicalSystems2009,
  title = {Cyber-Physical {{Systems}}},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\undefined\\2009_cyber-physical_systems.pdf}
}

@article{cytronEfficientlyComputingStatic1991,
  title = {Efficiently Computing Static Single Assignment Form and the Control Dependence Graph},
  author = {Cytron, Ron and Ferrante, Jeanne and Rosen, Barry K. and Wegman, Mark N. and Zadeck, F. Kenneth},
  year = {1991},
  month = oct,
  volume = {13},
  pages = {451--490},
  issn = {0164-0925, 1558-4593},
  doi = {10.1145/115372.115320},
  file = {D\:\\GDrive\\zotero\\Cytron et al\\cytron_et_al_1991_efficiently_computing_static_single_assignment_form_and_the_control_dependence.pdf},
  journal = {ACM Transactions on Programming Languages and Systems},
  language = {en},
  number = {4}
}

@techreport{daizoviPracticalReturnorientedProgramming2010,
  title = {Practical Return-Oriented Programming},
  author = {Dai Zovi, Dino},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Dai Zovi\\dai_zovi_2010_practical_return-oriented_programming.pdf}
}

@article{dalfonsoOverviewMathematicalTheory,
  title = {An {{Overview}} of the {{Mathematical Theory}} of {{Communication Particularly}} for {{Philosophers Interested}} in {{Information}}},
  author = {D'Alfonso, Simon},
  pages = {9},
  file = {D\:\\GDrive\\zotero\\D’Alfonso\\d’alfonso_an_overview_of_the_mathematical_theory_of_communication_particularly_for.pdf},
  language = {en}
}

@techreport{damasPrincipalTypeschemesFunctional1982,
  title = {Principal Type-Schemes for Functional Programs *},
  author = {Damas, Luis and Milner, Robin},
  year = {1982},
  pages = {207--212},
  institution = {{ACM}},
  file = {D\:\\GDrive\\zotero\\Damas\\damas_1982_principal_type-schemes_for_functional_programs.pdf}
}

@techreport{dameronBeigepaperEthereumTechnical,
  title = {Beigepaper: {{An Ethereum Technical Specification}}},
  author = {Dameron, Micah},
  abstract = {The Ethereum Protocol is a deterministic but practically unbounded state-machine with two basic functions; the first being a globally accessible singleton state, and the second being a virtual machine that applies changes to that state. This paper explains the individual parts that make up these two factors.},
  file = {D\:\\GDrive\\zotero\\Dameron\\dameron_beigepaper.pdf}
}

@inproceedings{dangPerformanceCostShadow2015,
  title = {The {{Performance Cost}} of {{Shadow Stacks}} and {{Stack Canaries}}},
  booktitle = {Proceedings of the 10th {{ACM Symposium}} on {{Information}}, {{Computer}} and {{Communications Security}}},
  author = {Dang, Thurston H.Y. and Maniatis, Petros and Wagner, David},
  year = {2015},
  month = apr,
  pages = {555--566},
  publisher = {{ACM}},
  address = {{Singapore Republic of Singapore}},
  doi = {10.1145/2714576.2714635},
  abstract = {Control flow defenses against ROP either use strict, expensive, but strong protection against redirected RET instructions with shadow stacks, or much faster but weaker protections without. In this work we study the inherent overheads of shadow stack schemes. We find that the overhead is roughly 10\% for a traditional shadow stack. We then design a new scheme, the parallel shadow stack, and show that its performance cost is significantly less: 3.5\%. Our measurements suggest it will not be easy to improve performance on current x86 processors further, due to inherent costs associated with RET and memory load/store instructions. We conclude with a discussion of the design decisions in our shadow stack instrumentation, and possible lighter-weight alternatives.},
  file = {D\:\\GDrive\\zotero\\Dang et al\\dang_et_al_2015_the_performance_cost_of_shadow_stacks_and_stack_canaries.pdf},
  isbn = {978-1-4503-3245-3},
  language = {en}
}

@article{danielVerifiableMutabilityBlockchains2021,
  title = {Towards {{Verifiable Mutability}} for {{Blockchains}}},
  author = {Daniel, Erik and Tschorsch, Florian},
  year = {2021},
  month = jun,
  abstract = {Due to their immutable log of information, blockchains can be considered as a transparency-enhancing technology. The immutability, however, also introduces threats and challenges with respect to privacy laws and illegal content. Introducing a certain degree of mutability, which enables the possibility to store and remove information, can therefore increase the opportunities for blockchains. In this paper, we present a concept for a mutable blockchain structure. Our approach enables the removal of certain blocks, while maintaining the blockchain's verifiability property. Since our concept is agnostic to any consensus algorithms, it can be implemented with permissioned and permissionless blockchains.},
  archiveprefix = {arXiv},
  eprint = {2106.15935},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Daniel_Tschorsch\\daniel_tschorsch_2021_towards_verifiable_mutability_for_blockchains.pdf;C\:\\Users\\Admin\\Zotero\\storage\\QQZPETK6\\2106.html},
  journal = {arXiv:2106.15935 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@article{dasdanHowReliableAre2020,
  title = {How {{Reliable}} Are {{University Rankings}}?},
  author = {Dasdan, Ali and Van Lare, Eric and Zivaljevic, Bosko},
  year = {2020},
  pages = {1--47},
  abstract = {University or college rankings have almost become an industry of their own, published by US News \textbackslash\& World Report (USNWR) and similar organizations. Most of the rankings use a similar scheme: Rank universities in decreasing score order, where each score is computed using a set of attributes and their weights; the attributes can be objective or subjective while the weights are always subjective. This scheme is general enough to be applied to ranking objects other than universities. As shown in the related work, these rankings have important implications and also many issues. In this paper, we take a fresh look at this ranking scheme using the public College dataset; we both formally and experimentally show in multiple ways that this ranking scheme is not reliable and cannot be trusted as authoritative because it is too sensitive to weight changes and can easily be gamed. For example, we show how to derive reasonable weights programmatically to move multiple universities in our dataset to the top rank; moreover, this task takes a few seconds for over 600 universities on a personal laptop. Our mathematical formulation, methods, and results are applicable to ranking objects other than universities too. We conclude by making the case that all the data and methods used for rankings should be made open for validation and repeatability.},
  file = {D\:\\GDrive\\zotero\\Dasdan\\dasdan_2020_how_reliable_are_university_rankings.pdf}
}

@article{dasguptaElementaryProofTheorem2003,
  title = {An {{Elementary Proof}} of a {{Theorem}} of {{Johnson}} and {{Lindenstrauss}}},
  author = {Dasgupta, Sanjoy and Gupta, Anupam},
  year = {2003},
  month = jan,
  volume = {22},
  pages = {60--65},
  issn = {10429832},
  doi = {10.1002/rsa.10073},
  abstract = {A result of Johnson and Lindenstrauss [13] shows that a set of n points in high dimensional Euclidean space can be mapped into an O(log n/{$\epsilon$}2)-dimensional Euclidean space such that the distance between any two points changes by only a factor of (1 {$\pm$} {$\epsilon$}). In this note, we prove this theorem using elementary probabilistic techniques. \textcopyright{} 2002 Wiley Periodicals, Inc. Random Struct. Alg., 22: 60\textendash 65, 2002},
  file = {D\:\\GDrive\\zotero\\Dasgupta\\dasgupta_2003_an_elementary_proof_of_a_theorem_of_johnson_and_lindenstrauss.pdf},
  journal = {Random Structures and Algorithms},
  number = {1}
}

@article{dasguptaSparseJohnsonLindenstrauss2010,
  title = {A {{Sparse Johnson}}--{{Lindenstrauss Transform}}},
  author = {Dasgupta, Anirban and Kumar, Ravi and Sarl{\'o}s, Tam{\'a}s},
  year = {2010},
  month = apr,
  abstract = {Dimension reduction is a key algorithmic tool with many applications including nearest-neighbor search, compressed sensing and linear algebra in the streaming model. In this work we obtain a \{\textbackslash em sparse\} version of the fundamental tool in dimension reduction --- the Johnson--Lindenstrauss transform. Using hashing and local densification, we construct a sparse projection matrix with just \$\textbackslash tilde\{O\}(\textbackslash frac\{1\}\{\textbackslash epsilon\})\$ non-zero entries per column. We also show a matching lower bound on the sparsity for a large class of projection matrices. Our bounds are somewhat surprising, given the known lower bounds of \$\textbackslash Omega(\textbackslash frac\{1\}\{\textbackslash epsilon\^2\})\$ both on the number of rows of any projection matrix and on the sparsity of projection matrices generated by natural constructions. Using this, we achieve an \$\textbackslash tilde\{O\}(\textbackslash frac\{1\}\{\textbackslash epsilon\})\$ update time per non-zero element for a \$(1\textbackslash pm\textbackslash epsilon)\$-approximate projection, thereby substantially outperforming the \$\textbackslash tilde\{O\}(\textbackslash frac\{1\}\{\textbackslash epsilon\^2\})\$ update time required by prior approaches. A variant of our method offers the same guarantees for sparse vectors, yet its \$\textbackslash tilde\{O\}(d)\$ worst case running time matches the best approach of Ailon and Liberty.},
  file = {D\:\\GDrive\\zotero\\Dasgupta\\dasgupta_2010_a_sparse_johnson--lindenstrauss_transform.pdf}
}

@techreport{dasSWIMScalableWeaklyconsistent,
  title = {{{SWIM}}: {{Scalable Weakly}}-Consistent {{Infection}}-Style {{Process Group Membership Protocol}}},
  author = {Das, Abhinandan and Gupta, Indranil and Motivala, Ashish},
  abstract = {Several distributed peer-to-peer applications require weakly-consistent knowledge of process group membership information at all participating processes. SWIM is a generic software module that offers this service for large-scale process groups. The SWIM effort is motivated by the unscalability of traditional heart-beating protocols, which either impose network loads that grow quadratically with group size, or compromise response times or false positive frequency w.r.t. detecting process crashes. This paper reports on the design, implementation and performance of the SWIM subsystem on a large cluster of commodity PCs. Unlike traditional heartbeating protocols, SWIM separates the failure detection and membership update dissemination functionalities of the membership protocol. Processes are monitored through an efficient peer-to-peer periodic randomized probing protocol. Both the expected time to first detection of each process failure, and the expected message load per member, do not vary with group size. Information about membership changes, such as process joins, drop-outs and failures, is propagated via piggyback-ing on ping messages and acknowledgments. This results in a robust and fast infection style (also epidemic or gossip-style) of dissemination. The rate of false failure detections in the SWIM system is reduced by modifying the protocol to allow group members to suspect a process before declaring it as failed-this allows the system to discover and rectify false failure detections. Finally, the protocol guarantees a deterministic time bound to detect failures. Experimental results from the SWIM prototype are presented. We discuss the extensibility of the design to a WAN-wide scale. \textsterling},
  file = {D\:\\GDrive\\zotero\\Das\\das_swim.pdf}
}

@article{DatabasemetatheoryAskingthebigqueries,
  title = {Database-Metatheory--Asking-the-Big-Queries},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\RJGKIXG4\\database-metatheory--asking-the-big-queries.pdf}
}

@book{dattaLaTeX24Hours2017,
  title = {{{LaTeX}} in 24 {{Hours}}},
  author = {Datta, Dilip},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-47831-9},
  file = {D\:\\GDrive\\zotero\\Datta\\datta_2017_latex_in_24_hours.pdf},
  isbn = {978-3-319-47830-2 978-3-319-47831-9},
  language = {en}
}

@techreport{dattaScienceSecurityAdversary,
  title = {The {{Science}} of {{Security On Adversary Models}} and {{Compositional Security}}},
  author = {Datta, Anupam and Franklin, Jason and Garg, Deepak and Jia, Limin and Dilsun, And},
  abstract = {I n this article, we report a representative result in the science of security. To explain what we mean by a science, we can draw an analogy with physics. A physical theory consists of a model of the physical universe. The model should be general-that is, it should encompass a large class of physical phenomena. It should also support analyses that identify relationships among physical concepts, which researchers can then use to explain observed behavior in the physical universe and predict behavior that they haven't yet observed. For example, Albert Einstein's general theory of relativity presents a model of gravitation-a set of equations that describe how spacetime is curved by matter and energy (a relationship among physical concepts). It explains observed phenomena, such as the bending of light near the sun, and predicts the existence of black holes, regions of spacetime where the gravitational attraction is so strong that even light can't escape. The theory is general in that its predictions apply to a very large class of phenomena ranging from motion of bodies (apples, stars, planets) in free fall to the propagation of light. A science of security should include theories for the security universe that have similar characteristics. The security universe includes a large class of computer systems (Web browsers, hypervisors, virtual machine monitors, operating systems, trusted computing systems, and network protocols, to name a few) that are intended to provide subtle security properties in the presence of adversaries who actively interfere with a system's execution. A security theory should therefore include a model for systems, adversaries, and properties that supports analyses that identify relationships among classes of systems, adversaries, and properties. These relationships should in turn help explain observed phenomena (why specific attacks work against specific systems) and predict phenomena (how well a system will hold up as adversaries launch new attacks). A theory is general if it applies to large classes of systems , adversaries, and properties. In this article, we present the outline of a theory of compositional security that addresses a recognized scientific challenge. 1 Contemporary systems evolve from smaller components, but even if each component is secure in isolation, the composed system might not achieve the desired security property because an adversary could still exploit complex interactions between components to compromise security. A theory of compositional security should identify relationships among systems, adversaries, and properties such that precisely defined composition operations over systems and adversaries preserve security properties. Such a theory would thus enable scalable analysis of large, complex systems by constructing their security proofs from separately constructed proofs of properties of the simpler components from which they're built. In addition , if a component is used to build multiple systems, the proof of its security property could be reused in the proofs for all systems constructed. Although researchers have made progress in understanding secure composition in specific settings, A unified view of a wide range of adversary classes and composition principles for reasoning about security properties of systems are cornerstones of a science of security. They provide a systematic basis for security analysis by explaining and predicting attacks on systems.},
  file = {D\:\\GDrive\\zotero\\Datta\\datta_the_science_of_security_on_adversary_models_and_compositional_security.pdf},
  journal = {COPUBLISHED BY THE IEEE COMPUTER AND RELIABILITY SOCIETIES}
}

@misc{daverveldtLLVMbasedVEXCompiler2014,
  title = {{{LLVM}}-Based {$\rho$} -{{VEX}} Compiler},
  author = {Daverveldt, Maurice},
  year = {2014},
  file = {D\:\\GDrive\\zotero\\Daverveldt\\daverveldt_2014_llvm-based_ρ_-vex_compiler.pdf},
  journal = {MSc THESIS}
}

@inproceedings{davidEverythingYouAlways2013,
  title = {Everything You Always Wanted to Know about Synchronization but Were Afraid to Ask},
  booktitle = {{{SOSP}} 2013 - {{Proceedings}} of the 24th {{ACM Symposium}} on {{Operating Systems Principles}}},
  author = {David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios},
  year = {2013},
  pages = {33--48},
  issn = {1872-8952},
  doi = {10.1145/2517349.2522714},
  abstract = {This paper presents the most exhaustive study of syn- chronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types of architectures, from single-socket \textendash{} uniform and non- uniform \textendash{} to multi-socket \textendash{} directory and broadcast- based \textendash{} many-cores. We draw a set of observations that, roughly speaking, imply that scalability of synchroniza- tion is mainly a property of the hardware. 1},
  file = {D\:\\GDrive\\zotero\\David\\david_2013_everything_you_always_wanted_to_know_about_synchronization_but_were_afraid_to.pdf},
  isbn = {978-1-4503-2388-8},
  pmid = {22465416}
}

@book{daviDynamicIntegrityMeasurement2009,
  title = {Dynamic {{Integrity Measurement}} and {{Attestation}}: {{Towards Defense Against Return}}-{{Oriented Programming Attacks}}},
  author = {Davi, Lucas and Sadeghi, Ahmad-Reza and Winandy, Marcel},
  year = {2009},
  abstract = {Despite the many efforts made in recent years to mitigate runtime attacks such as stack and heap based buffer overflows , these attacks are still a common security concern in to-day's computing platforms. Attackers have even found new ways to enforce runtime attacks including use of a technique called return-oriented programming. Trusted Computing provides mechanisms to verify the integrity of all executable content in an operating system. But they only provide integrity at load-time and are not able to prevent or detect runtime attacks. To mitigate return-oriented programming attacks, we propose new runtime integrity monitoring techniques that use tracking instrumentation of program binaries based on taint analysis and dynamic tracing. We also describe how these techniques can be employed in a dynamic integrity measurement architecture (DynIMA). In this way we fill the gap between static load-time and dynamic run-time attestation and, in particular, extend trusted computing techniques to effectively defend against return-oriented programming attacks.},
  file = {D\:\\GDrive\\zotero\\Davi\\davi_2009_dynamic_integrity_measurement_and_attestation.pdf;D\:\\GDrive\\zotero\\Davi\\davi_2009_dynamic_integrity_measurement_and_attestation2.pdf},
  isbn = {978-1-60558-788-2},
  keywords = {attesta-tion systems,D46 [Operating Systems]: Security and Protection; H4 [Information Systems Applications]: Miscellaneous General Terms Measurement,integrity monitoring,Security Keywords return-oriented programming}
}

@article{daviesConceptMappingMind2011,
  title = {Concept Mapping, Mind Mapping and Argument Mapping: What Are the Differences and Do They Matter?},
  shorttitle = {Concept Mapping, Mind Mapping and Argument Mapping},
  author = {Davies, Martin},
  year = {2011},
  month = sep,
  volume = {62},
  pages = {279--301},
  issn = {0018-1560, 1573-174X},
  doi = {10.1007/s10734-010-9387-6},
  file = {D\:\\GDrive\\zotero\\Davies\\davies_2011_concept_mapping,_mind_mapping_and_argument_mapping.pdf},
  journal = {Higher Education},
  language = {en},
  number = {3}
}

@techreport{daviMoCFIFrameworkMitigate2012,
  title = {{{MoCFI}}: {{A Framework}} to {{Mitigate Control}}-{{Flow Attacks}} on {{Smartphones}}},
  author = {Davi, Lucas and Dmitrienko, Alexandra and Egele, Manuel and Fischer, Thomas and Holz, Thorsten and Hund, Ralf and N{\"u}rnberger, Stefan and Sadeghi, Ahmad-Reza},
  year = {2012},
  abstract = {Runtime and control-flow attacks (such as code injection or return-oriented programming) constitute one of the most severe threats to software programs. These attacks are prevalent and have been recently applied to smartphone applications as well, of which hundreds of thousands are downloaded by users every day. While a framework for control-flow integrity (CFI) enforcement, an approach to prohibit this kind of attacks, exists for the Intel x86 platform , there is no such a solution for smartphones. In this paper, we present a novel framework, MoCFI (Mobile CFI), that provides a general countermeasure against control-flow attacks on smartphone platforms by enforcing CFI. We show that CFI on typical smartphone platforms powered by an ARM processor is technically involved due to architectural differences between ARM and Intel x86, as well as the specifics of smartphone OSes. Our framework performs CFI on-the-fly during runtime without requiring the application's source code. For our reference implementation we chose Apple's iOS, because it has been an attractive target for control-flow attacks. Nevertheless, our framework is also applicable to other ARM-based devices such as Google's Android. Our performance evaluation demonstrates that MoCFI is efficient and does not induce notable overhead when applied to popular iOS applications.},
  file = {D\:\\GDrive\\zotero\\Davi\\davi_mocfi.pdf}
}

@article{daviStitchingGadgetsIneffectiveness2014,
  title = {Stitching the {{Gadgets}}: {{On}} the {{Ineffectiveness}} of {{Coarse}}-{{Grained Control}}-{{Flow Integrity Protection}}},
  author = {Davi, Lucas and Sadeghi, Ahmad-Reza and Lehmann, Daniel and Monrose, Fabian},
  year = {2014},
  abstract = {Return-oriented programming (ROP) offers a robust attack technique that has, not surprisingly, been extensively used to exploit bugs in modern software programs (e.g., web browsers and PDF readers). ROP attacks require no code injection, and have already been shown to be powerful enough to bypass fine-grained memory randomization (ASLR) defenses. To counter this ingenious attack strategy, several proposals for enforcement of (coarse-grained) control-flow integrity (CFI) have emerged. The key argument put forth by these works is that coarse-grained CFI policies are sufficient to prevent ROP attacks. As this reasoning has gained traction , ideas put forth in these proposals have even been incorporated into coarse-grained CFI defenses in widely adopted tools (e.g., Microsoft's EMET framework). In this paper, we provide the first comprehensive security analysis of various CFI solutions (covering kBouncer, ROPecker, CFI for COTS binaries, ROP-Guard, and Microsoft EMET 4.1). A key contribution is in demonstrating that these techniques can be effectively undermined, even under weak adversarial assumptions. More specifically, we show that with bare minimum assumptions, turing-complete and real-world ROP attacks can still be launched even when the strictest of enforcement policies is in use. To do so, we introduce several new ROP attack primitives, and demonstrate the practicality of our approach by transforming existing real-world exploits into more stealthy attacks that bypass coarse-grained CFI defenses.},
  file = {D\:\\GDrive\\zotero\\Davi\\davi_stitching_the_gadgets.pdf}
}

@article{davisVisualMicrophonePassive,
  title = {The {{Visual Microphone}} : {{Passive Recovery}} of {{Sound}} from {{Video}}},
  author = {Davis, Abe and Rubinstein, Michael and Freeman, William T},
  keywords = {side-channel,ss}
}

@article{dawelbeitInvestigatingElasticCloud2016,
  title = {Investigating {{Elastic Cloud Based RDF Processing}}},
  author = {Dawelbeit, Omer},
  year = {2016},
  doi = {10.13140/RG.2.1.4361.8807},
  file = {D\:\\GDrive\\zotero\\Dawelbeit\\dawelbeit_2016_investigating_elastic_cloud_based_rdf_processing.pdf},
  number = {April}
}

@techreport{dbernsteinContainersCloudLXC,
  title = {Containers and {{Cloud}}: {{From LXC}} to {{Docker}} to {{Kubernetes}}},
  author = {{D Bernstein}},
  file = {D\:\\GDrive\\zotero\\D Bernstein\\d_bernstein_containers_and_cloud.pdf}
}

@techreport{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: {{Simplified}} Data Processing on Large Clusters},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  volume = {51},
  pages = {107--113},
  issn = {00010782},
  doi = {10.1145/1327452.1327492},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  file = {D\:\\GDrive\\zotero\\Dean\\dean_2008_mapreduce.pdf},
  journal = {Communications of the ACM},
  keywords = {distributed systems},
  number = {1}
}

@phdthesis{deanParallelImplementationsFo,
  title = {Parallel {{Implementations}} Fo {{Neural Network Training}}: {{Two Back}}-{{Propagation Approaches}}},
  author = {Dean, Jeffrey},
  file = {D\:\\GDrive\\zotero\\Dean\\dean_parallel_implementations_fo_neural_network_training.pdf}
}

@techreport{decandiaDynamoAmazonHighly2007,
  title = {Dynamo: {{Amazon}}'s {{Highly Available Key}}-Value {{Store}}},
  author = {Decandia, Giuseppe and Hastorun, Deniz and Jampani, Madan and Kakulapati, Gunavardhan and Lakshman, Avinash and Pilchin, Alex and Sivasubramanian, Swaminathan and Vosshall, Peter and Vogels, Werner},
  year = {2007},
  abstract = {Reliability at massive scale is one of the biggest challenges we face at Amazon.com, one of the largest e-commerce operations in the world; even the slightest outage has significant financial consequences and impacts customer trust. The Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers and network components located in many datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these failures drives the reliability and scalability of the software systems. This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core services use to provide an "always-on" experience. To achieve this level of availability, Dynamo sacrifices consistency under certain failure scenarios. It makes extensive use of object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.},
  file = {D\:\\GDrive\\zotero\\Decandia\\decandia_2007_dynamo.pdf},
  keywords = {D42 [Operating Systems]: Performance,D42 [Operating Systems]: Storage Management,D45 [Operating Systems]: Reliability,distributed systems,General Terms Algorithms; Management; Measurement; Performance; Design; Reliability}
}

@article{deckerBitcoinMeetsStrong2016,
  title = {Bitcoin {{Meets Strong Consistency}}},
  author = {Decker, Christian and Seidel, Jochen and Wattenhofer, Roger},
  year = {2016},
  doi = {10.1145/2833312.2833321},
  abstract = {The Bitcoin system only provides eventual consistency. For everyday life, the time to confirm a Bitcoin transaction is prohibitively slow. In this paper we propose a new system , built on the Bitcoin blockchain, which enables strong consistency. Our system, PeerCensus, acts as a certification authority, manages peer identities in a peer-to-peer network, and ultimately enhances Bitcoin and similar systems with strong consistency. Our extensive analysis shows that PeerCensus is in a secure state with high probability. We also show how Discoin, a Bitcoin variant that decouples block creation and transaction confirmation, can be built on top of PeerCensus, enabling real-time payments. Unlike Bitcoin, once transactions in Discoin are committed, they stay committed.},
  file = {D\:\\GDrive\\zotero\\Decker\\decker_2016_bitcoin_meets_strong_consistency.pdf},
  isbn = {9781450340328},
  keywords = {Bitcoin,Byzantine Agreement,CCS Concepts •Networks → Peer-to-peer protocols; Keywords Blockchain}
}

@techreport{deckerInformationPropagationBitcoin,
  title = {Information {{Propagation}} in the {{Bitcoin Network}}},
  author = {Decker, Christian and Wattenhofer, Roger and Zurich, Eth},
  abstract = {Bitcoin is a digital currenc y that unlike traditional currencies does not rel y on a centralized authorit y. Instead Bit\- coin relies on a network of volunteers that collectivel y implement a replicated ledger and verif y transactions. In this paper we anal y ze how Bitcoin uses a multi-hop broadcast to propagate transactions and blocks through the network to update the ledger replicas. We then use the gathered information to verif y the conjecture that the propagation dela y in the network is the primar y cause for blockchain forks. Blockchain forks should be avoided as the y are s y mptomatic for inconsistencies among the replicas in the network. We then show what can be achieved b y pushing the current protocol to its limit with unilateral changes to the client's behavior.},
  file = {D\:\\GDrive\\zotero\\Decker\\decker_information_propagation_in_the_bitcoin_network.pdf}
}

@techreport{deeringMulticastRoutingInternetworks,
  title = {Multicast {{Routing}} in {{Internetworks}} and {{Extended LANs}}},
  author = {Deering, Stephen E},
  abstract = {Multicasting is used within local-area networks to make distributed applications more robust and more efficient. The growing need to distribute applications across multiple, interconnected networks, and the increasing availability of high-performance, high-capacity switching nodes and networks, lead us to consider providing LAN-style multicasting across an inter-network. In this paper, we propose extensions to two common internetwork routing algorithms\textasciitilde istance-vector routing and link-state routing-to support low-delay datagram multicasting. We also suggest modifications to the single-spanning-tree routing algorithm, commonly used by link-layer bridges, to reduce the costs of multi-casting in large extended LANs. Finally, we show how different link-layer and network-layer multicast routing algorithms can be combined hierarchically to support multicasting across large, heterogeneous iotemetwo*s.},
  file = {D\:\\GDrive\\zotero\\Deering\\deering_multicast_routing_in_internetworks_and_extended_lans.pdf}
}

@techreport{defrawySMARTSecureMinimal2012,
  title = {{{SMART}}: {{Secure}} and {{Minimal Architecture}} for ({{Establishing}} a {{Dynamic}}) {{Root}} of {{Trust}}},
  author = {Defrawy, Karim El and Francillon, Aur{\'e}lien and Perito, Daniele and Tsudik, Gene},
  year = {2012},
  abstract = {Remote attestation is the process of securely verifying internal state of a remote hardware platform. It can be achieved either statically (at boot time) or dynamically, at run-time in order to establish a dynamic root of trust. The latter allows full isolation of a code region from pre-existing software (including the operating system) and guarantees untampered execution of this code. Despite the untrusted state of the overall platform, a dynamic root of trust facilitates execution of critical code. Prior software-based techniques lack concrete security guarantees , while hardware-based approaches involve security co-processors that are too costly for low-end embedded devices. In this paper, we develop a new primitive (called SMART) based on hardware-software co-design. SMART is a simple, efficient and secure approach for establishing a dynamic root of trust in a remote embedded device. We focus on low-end micro-controller units (MCU) that lack specialized memory management or protection features. SMART requires minimal changes to existing MCUs (while providing concrete security guarantees) and assumes few restrictions on adversarial capabilities. We demonstrate both practicality and feasibility of SMART by implementing it-via hardware modifications-on two common MCU platforms: AVR and MSP430. Results show that SMART implementations require only a few changes to memory bus access logic. We also synthesize both implementations to an 180nm ASIC process to confirm its small impact on MCU size and overall cost.},
  file = {D\:\\GDrive\\zotero\\Defrawy\\defrawy_smart.pdf}
}

@article{delozierStrongMemoryConsistency2018,
  title = {Strong {{Memory Consistency}} for {{Parallel Programming}}},
  author = {DeLozier, Christian},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\DeLozier\\delozier_2018_strong_memory_consistency_for_parallel_programming.pdf},
  journal = {Dissertations available from ProQuest}
}

@techreport{demersEpidemicAlgorithmsReplicated1987,
  title = {Epidemic {{Algorithms}} for {{Replicated}}. {{Database Maintenance}}},
  author = {Demers, Alan and Gealy, Mark and Greene, Dan and Hauser, Carl and Irish, Wes and Larson, John and Manning, Sue and Shenker, Scott and Sturgis, Howard and Swinehart, Dan and Terry, Doug and Woods, Don},
  year = {1987},
  file = {D\:\\GDrive\\zotero\\Demers\\demers_1987_epidemic_algorithms_for_replicated.pdf}
}

@techreport{demouraZ3EfficientSMT2008,
  title = {Z3: {{An Efficient SMT Solver}}},
  author = {De Moura, Leonardo and Bj{\o}rner, Nikolaj},
  year = {2008},
  doi = {10.5555/1792734.1792766},
  abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and unin-terpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
  file = {D\:\\GDrive\\zotero\\De Moura_Bjørner\\de_moura_bjørner_2008_z3.pdf}
}

@article{denisovMullItMutation2018,
  title = {Mull It over: {{Mutation}} Testing Based on {{LLVM}}},
  author = {Denisov, Alex and Pankevich, Stanislav},
  year = {2018},
  pages = {25--31},
  publisher = {{IEEE}},
  doi = {10.1109/ICSTW.2018.00024},
  abstract = {This paper describes Mull, an open-source tool for mutation testing based on the LLVM framework. Mull works with LLVM IR, a low-level intermediate representation, to perform mutations, and uses LLVM JIT for just-in-time compilation. This design choice enables the following two capabilities of Mull: langu?age independence and fine-grained control over compilation and execution of a tested program and its mutations. Mull can work with code written in any programming language that supports compilation to LLVM IR, such as C, C++, Rust, or Swift. Direct manipulation of LLVM IR allows Mull to do less work to generate mutations: only modified fragments of IR code are recompiled, and this results in faster processing of mutated programs. To our knowledge, no existing mutation testing tool provides these capabilities for compiled programming languages. We describe the algorithm and implementation details of Mull, highlight current limitations of Mull, and present the results of our evaluation of Mull on real-world projects such as RODOS, OpenSSL, LLVM.},
  file = {D\:\\GDrive\\zotero\\Denisov\\denisov_2018_mull_it_over.pdf},
  isbn = {9781538663523},
  journal = {Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2018},
  keywords = {Llvm,Mutation testing}
}

@techreport{denningLatticeModelSecure,
  title = {A {{Lattice Model}} of {{Secure Information Flow}}},
  author = {Denning, Dorothy E},
  abstract = {This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.},
  file = {D\:\\GDrive\\zotero\\Denning\\denning_a_lattice_model_of_secure_information_flow.pdf},
  keywords = {and Phrases: protection,information flow,lattice,program certification CR Categories: 435,security,security class}
}

@techreport{derrinRunningManualApproach,
  title = {Running the {{Manual}}: {{An Approach}} to {{High}}-{{Assurance Microkernel Development}}},
  author = {Derrin, Philip and Elphinstone, Kevin and Klein, Gerwin and Cock, David and Chakravarty, Manuel M T},
  abstract = {We propose a development methodology for designing and proto-typing high assurance microkernels, and describe our application of it. The methodology is based on rapid prototyping and iterative refinement of the microkernel in a functional programming language. The prototype provides a precise semi-formal model, which is also combined with a machine simulator to form a reference implementation capable of executing real user-level software, to obtain accurate feedback on the suitability of the kernel API during development phases. We extract from the prototype a machine-checkable formal specification in higher-order logic, which may be used to verify properties of the design, and also results in corrections to the design without the need for full verification. We found the approach leads to productive, highly iterative development where formal modelling, semi-formal design and prototyping, and end use all contribute to a more mature final design in a shorter period of time.},
  file = {D\:\\GDrive\\zotero\\Derrin\\derrin_running_the_manual.pdf},
  keywords = {D32 [Programming Languages]: Lan-guage Classifications-Applicative (functional) languages,D45 [Operating Systems]: Reliability-Verification,I63 [Simulation and Modelling]: Applications General Terms Languages; Design; Documentation; Verification Keywords Operating systems; Haskell; rapid prototyping; exe-cutable specification; Isabelle/HOL; monads; formalisation; veri-fication}
}

@article{dessoukyLiteHAXLightweightHardwareassisted2018,
  title = {{{LiteHAX}}: {{Lightweight}} Hardware-Assisted Attestation of Program Execution},
  author = {Dessouky, Ghada and Abera, Tigist and Ibrahim, Ahmad and Sadeghi, Ahmad Reza},
  year = {2018},
  publisher = {{ACM}},
  issn = {10923152},
  doi = {10.1145/3240765.3240821},
  abstract = {Unlike traditional processors, embedded Internet of Things (IoT) devices lack resources to incorporate protection against modern sophisticated attacks resulting in critical consequences. Remote attestation (RA) is a security service to establish trust in the integrity of a remote device. While conventional RA is static and limited to detecting malicious modification to software binaries at load-time, recent research has made progress towards runtime attestation, such as attesting the control flow of an executing program. However, existing control-flow attestation schemes are inefficient and vulnerable to sophisticated data-oriented programming (DOP) attacks subvert these schemes and keep the control flow of the code intact. In this paper, we present LiteHAX, an efficient hardware-assisted remote attestation scheme for RISC-based embedded devices that enables detecting both control-flow attacks as well as DOP attacks. LiteHAX continuously tracks both the control-flow and data-flow events of a program executing on a remote device and reports them to a trusted verifying party. We implemented and evaluated LiteHAX on a RISC-V System-on-Chip (SoC) and show that it has minimal performance and area overhead.},
  file = {D\:\\GDrive\\zotero\\Dessouky\\dessouky_2018_litehax.pdf},
  isbn = {9781450359504},
  journal = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD}
}

@techreport{dessoukyLOFATLowOverheadControl2017,
  title = {{{LO}}-{{FAT}}: {{Low}}-{{Overhead Control Flow ATtestation}} in {{Hardware}}},
  author = {Dessouky, Ghada and Zeitouni, Shaza and Nyman, Thomas and Paverd, Andrew and Davi, Lucas and Koeberl, Patrick and Asokan, N and Sadeghi, Ahmad-Reza},
  year = {2017},
  abstract = {Attacks targeting software on embedded systems are becoming increasingly prevalent. Remote attestation is a mechanism that allows establishing trust in embedded devices. However, existing attestation schemes are either static and cannot detect control-flow attacks, or require instrumenta-tion of software incurring high performance overheads. To overcome these limitations, we present LO-FAT, the first practical hardware-based approach to control-flow attesta-tion. By leveraging existing processor hardware features and commonly-used IP blocks, our approach enables efficient control-flow attestation without requiring software instru-mentation. We show that our proof-of-concept implementation based on a RISC-V SoC incurs no processor stalls and requires reasonable area overhead.},
  file = {D\:\\GDrive\\zotero\\Dessouky\\dessouky_lo-fat.pdf}
}

@inproceedings{deutschEfficientImplementationSmalltalk801984,
  title = {Efficient Implementation of the Smalltalk-80 System},
  booktitle = {Proceedings of the 11th {{ACM SIGACT}}-{{SIGPLAN}} Symposium on {{Principles}} of Programming Languages  - {{POPL}} '84},
  author = {Deutsch, L. Peter and Schiffman, Allan M.},
  year = {1984},
  pages = {297--302},
  publisher = {{ACM Press}},
  address = {{Salt Lake City, Utah, United States}},
  doi = {10.1145/800017.800542},
  file = {D\:\\GDrive\\zotero\\Deutsch_Schiffman\\deutsch_schiffman_1984_efficient_implementation_of_the_smalltalk-80_system.pdf},
  isbn = {978-0-89791-125-2},
  language = {en}
}

@techreport{dhurjatiBackwardsCompatibleArrayBounds2006,
  title = {Backwards-{{Compatible Array Bounds Checking}} for {{C}} with {{Very Low Overhead}} *},
  author = {Dhurjati, Dinakar and Adve, Vikram},
  year = {2006},
  abstract = {The problem of enforcing correct usage of array and pointer references in C and C++ programs remains unsolved. The approach proposed by Jones and Kelly (extended by Ruwase and Lam) is the only one we know of that does not require significant manual changes to programs, but it has extremely high overheads of 5x-6x and 11x-12x in the two versions. In this paper, we describe a collection of techniques that dramatically reduce the overhead of this approach, by exploiting a fine-grain partitioning of memory called Automatic Pool Allocation. Together, these techniques bring the average overhead checks down to only 12\% for a set of benchmarks (but 69\% for one case). We show that the memory partitioning is key to bringing down this overhead. We also show that our technique successfully detects all buffer overrun violations in a test suite modeling reported violations in some important real-world programs.},
  file = {D\:\\GDrive\\zotero\\Dhurjati\\dhurjati_2006_backwards-compatible_array_bounds_checking_for_c_with_very_low_overhead.pdf},
  isbn = {159593085X},
  keywords = {array bounds checking,automatic pool allocation,D3 [Software]: Programming Languages; D25 [Software]: Software Engineering-Testing and Debugging General Terms Reliability,Languages Keywords compilers,programming languages,region management,Security}
}

@techreport{dhurjatiEfficientlyDetectingAll,
  title = {Efficiently {{Detecting All Dangling Pointer Uses}} in {{Production Servers}} *},
  author = {Dhurjati, Dinakar and Adve, Vikram},
  abstract = {In this paper, we propose a novel technique to detect all dangling pointer uses at run-time that is efficient enough for production use in server codes. One idea (previously used by Electric Fence, PageHeap) is to use a new virtual page for each allocation of the program and rely on page protection mechanisms to check dangling pointer accesses. This naive approach has two limitations that makes it impractical to use in production software: increased physical memory usage and increased address space usage. We propose two key improvements that alleviate both these problems. First, we use a new virtual page for each allocation of the program but map it to the same physical page as the original allocator. This allows using nearly identical physical memory as the original program while still retaining the dangling pointer detection capability. We also show how to implement this idea without requiring any changes to the underlying memory allocator. Our second idea alleviates the problem of virtual address space exhaustion by using a previously developed compiler transformation called Automatic Pool Allocation to reuse many virtual pages. The transformation partitions the memory of the program based on their lifetimes and allows us to reuse virtual pages when portions of memory become inaccessible. Experimentally we find that the run-time overhead for five unix servers is less than 4\%, for other unix utilities less than 15\%. However , in case of allocation intensive benchmarks, we find our overheads are much worse (up to 11x slowdown).},
  file = {D\:\\GDrive\\zotero\\Dhurjati\\dhurjati_efficiently_detecting_all_dangling_pointer_uses_in_production_servers.pdf}
}

@techreport{dhurjatiEnforcingAliasAnalysis,
  title = {Enforcing {{Alias Analysis}} for {{Weakly Typed Languages}}},
  author = {Dhurjati, Dinakar and Kowshik, Sumant and Adve, Vikram},
  abstract = {Static analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. Optimizing compilers can produce unpredictable results when such errors occur, but this is quite undesirable for many tools that aim to analyze security and reliability properties with guarantees of soundness. We describe a compilation strategy for standard C programs that guarantees sound semantics for an aggressive interprocedural pointer analysis (or simpler ones), a call graph, and type information for a subset of memory. These provide the foundation for sophisticated static analyses to be applied to such programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hard-to-detect memory errors (dangling pointer references and certain array bounds errors) cannot invalidate the call graph, points-to information or type information. The key insights behind our approach is that pool allocation can be used to create a run-time partitioning of memory that matches the compile-time memory partitioning in a points-to graph, and efficient checks can be used to isolate the run-time partitions. Furthermore, we show that the sound analysis information enables static checking techniques that reliably eliminate many run-time checks. We formalize our approach as a new type system with the necessary run-time checks in operational semantics and prove the correctness of our approach for a subset of C. Our approach requires no source code changes, allows memory to be managed explicitly, and does not use meta-data on pointers or individual tag bits for memory. Using several benchmarks and system codes, we show experimentally that the run-time overheads are low (less than 10\% in nearly all cases and 30\% in the worst case we have seen). We also show the effectiveness of reliable static analyses in eliminating run-time checks.},
  file = {D\:\\GDrive\\zotero\\Dhurjati\\dhurjati_enforcing_alias_analysis_for_weakly_typed_languages.pdf}
}

@article{dhurjatiMemorySafetyGarbage2005,
  title = {Memory {{Safety Without Garbage Collection}} for {{Embedded Applications}}},
  author = {Dhurjati, Dinakar and Kowshik, Sumant and Adve, Vikram and Lattner, Chris},
  year = {2005},
  volume = {4},
  pages = {73--111},
  issn = {15583465},
  doi = {10.1145/1053271.1053275},
  abstract = {Traditional approaches to enforcing memory safety of programs rely heavily on run-time checks of memory accesses and on garbage collection, both of which are unattractive for embedded applications. The goal of our work is to develop advanced compiler techniques for enforcing memory safety with minimal run-time overheads. In this paper, we describe a set of compiler techniques that, together with minor semantic restrictions on C programs and no new syntax, ensure memory safety and provide most of the error-detection capabilities of type-safe languages, without using garbage collection, and with no run-time software checks, (on systems with standard hardware support for memory management). The language permits arbitrary pointer-based data structures, explicit deallocation of dynamically allocated memory, and restricted array operations. One of the key results of this paper is a compiler technique that ensures that dereferencing dangling pointers to freed memory does not violate memory safety, without annotations, run-time checks, or garbage collection, and works for arbitrary type-safe C programs. Furthermore, we present a new interprocedural analysis for static array bounds checking under certain assumptions. For a diverse set of embedded C programs, we show that we are able to ensure memory safety of pointer and dynamic memory usage in all these programs with no run-time software checks (on systems with standard hardware memory protection), requiring only minor restructuring to conform to simple type restrictions. Static array bounds checking fails for roughly half the programs we study due to complex array references, and these are the only cases where explicit run-time software checks would be needed under our language and system assumptions. \textcopyright{} 2005, ACM. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Dhurjati\\dhurjati_2005_memory_safety_without_garbage_collection_for_embedded_applications.pdf},
  journal = {ACM Transactions on Embedded Computing Systems},
  keywords = {automatic pool allocation,compilers,Embedded systems,Languages,programming languages,region management,security,Security,static analysis},
  number = {1}
}

@article{dhurjatiMemorySafetyRuntime,
  title = {Memory {{Safety Without Runtime Checks}} or {{Garbage Collection}}},
  author = {Dhurjati, Dinakar and Kowshik, Sumant and Adve, Vikram and Lattner, Chris},
  volume = {213},
  file = {D\:\\GDrive\\zotero\\Dhurjati\\dhurjati_memory_safety_without_runtime_checks_or_garbage_collection.pdf},
  isbn = {1581136471}
}

@techreport{dhurjatisumantkowshikvikramadveSAFECodeEnforcingAlias,
  title = {{{SAFECode}}: {{Enforcing Alias Analysis}} for {{Weakly Typed Languages}} *},
  author = {Dhurjati Sumant Kowshik Vikram Adve, Dinakar},
  abstract = {Static analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. We describe a compilation strategy for standard C programs that guarantees that aggressive interprocedural pointer analysis (or less precise ones), a call graph, and type information for a subset of memory, are never invalidated by any possible memory errors. We formalize our approach as a new type system with the necessary run-time checks in operational semantics and prove the correctness of our approach for a subset of C. Our semantics provide the foundation for other sophisticated static analyses to be applied to C programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hard-to-detect memory errors (dan-gling pointer references and certain array bounds errors) cannot invalidate the call graph, points-to information or type information. The key insight behind our approach is that pool allocation can be used to create a run-time partitioning of memory that matches the compile-time memory partitioning in a points-to graph, and efficient checks can be used to isolate the run-time partitions. Furthermore , we show that the sound analysis information enables static checking techniques that eliminate many run-time checks. Our approach requires no source code changes, allows memory to be managed explicitly, and does not use meta-data on pointers or individual tag bits for memory. Using several benchmarks and system codes, we show experimentally that the run-time overheads are low (less than 10\% in nearly all cases and 30\% in the worst case we have seen). We also show the effectiveness of static analyses in eliminating run-time checks.},
  file = {D\:\\GDrive\\zotero\\Dhurjati Sumant Kowshik Vikram Adve\\dhurjati_sumant_kowshik_vikram_adve_safecode.pdf},
  keywords = {alias analysis,automatic pool allocation *,D3 [Software]: Program-ming Languages General Terms Reliability,Languages Keywords Compilers,programming languages,re-gion management,Security}
}

@article{diangeloSurveyToolsAnalyzing2019,
  title = {A Survey of Tools for Analyzing Ethereum Smart Contracts},
  author = {Di Angelo, Monika and Salzer, Gernot},
  year = {2019},
  pages = {69--78},
  doi = {10.1109/DAPPCON.2019.00018},
  abstract = {Smart contracts are at the heart of many decentralized applications, encapsulating core parts of the business logic. They handle the exchange of valuable assets like crypto-currencies or tokens in a transparent, decentralized manner. Being computer programs, they are also prone to programming errors, which have already lead to spectacular losses. Therefore, methods and tools have emerged to support the development of secure smart contracts and to aid the analysis of deployed ones. Assessing the quality of such tools turns out to be difficult. There are academic tools, tools developed by companies, and community tools in open repositories, but no comprehensive survey that may serve as a guide. Most discussions of related work in research papers are not helpful either, as they concentrate on methods rather than tools, base their review on publications about the tools rather than the tools themselves, or disregard tools outside of academia. Our survey aims at filling this gap by considering tools regardless of their provenance and by installing and testing them. It is meant as a guide for those who intend to analyze already deployed code, want to develop secure smart contracts, or plan to teach a related subject. We investigate 27 tools for analyzing Ethereum smart contracts regarding availability, maturity level, methods employed, and detection of security issues.},
  file = {D\:\\GDrive\\zotero\\Di Angelo\\di_angelo_2019_a_survey_of_tools_for_analyzing_ethereum_smart_contracts.pdf},
  isbn = {9781728112640},
  journal = {Proceedings - 2019 IEEE International Conference on Decentralized Applications and Infrastructures, DAPPCON 2019},
  keywords = {Analysis,Comparison,Ethereum,Smart contracts,Survey,Tools}
}

@techreport{diffieAuthenticationAuthenticatedKey1992,
  title = {Authentication and {{Authenticated Key Exchanges}}},
  author = {Diffie, Whitfield and Van, Paul C and Wiener, Michael J},
  year = {1992},
  volume = {2},
  pages = {107--125},
  abstract = {We discuss two-party mutual authentication protocols providing authenticated key exchange, focusing on those using asymmetric techniques. A simple, efficient protocol referred to as the station-to-station (STS) protocol is introduced, examined in detail, and considered in relation to existing protocols. The definition of a secure protocol is considered, and desirable characteristics of secure protocols are discussed.},
  file = {D\:\\GDrive\\zotero\\Diffie\\diffie_1992_authentication_and_authenticated_key_exchanges.pdf;D\:\\GDrive\\zotero\\Diffie\\diffie_1992_authentication_and_authenticated_key_exchanges2.pdf},
  journal = {Codes and Cryptography}
}

@techreport{diffieNewDirectionsCryptography,
  title = {New {{Directions}} in {{Cryptography Invited Paper}}},
  author = {Diffie, Whitfield and Hellman, Martin E},
  abstract = {Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing .},
  file = {D\:\\GDrive\\zotero\\Diffie\\diffie_new_directions_in_cryptography_invited_paper.pdf}
}

@techreport{dijkstraSelfstabilizingSystemsSpite1974,
  title = {Self-Stabilizing {{Systems}} in {{Spite}} of {{Distributed Control}}},
  author = {Dijkstra, Edsger W},
  year = {1974},
  abstract = {The synchronization task between loosely coupled cyclic sequential processes (as can be distinguished in, for instance, operating systems) can be viewed as keeping the relation "the system is in a legitimate state" invariant. As a result, each individual process step that could possibly cause violation of that relation has to be preceded by a test deciding whether the process in question is allowed to proceed or has to be delayed. The resulting design is readily-and quite systematically implemented if the different processes can be granted mutually exclusive access to a common store in which "the current system state" is recorded. A complication arises if there is no such commonly accessible store and, therefore, "the current system state" must be recorded in variables distributed over the various processes; and a further complication arises if the communication facilities are limited in the sense that each process can only exchange information with "its neighbors," i.e. a small subset of the total set of processes. The complication is that the behavior of a process can 0nly be influenced by that part of the total current system state description that is available to it, local actions taken on account of local information must accomplish a global objective. Such systems (with what is quite aptly called "distributed control") have been designed, but all such designs I was familiar with were not "self-stabilizing" in the sense that, when once (erroneously) in an illegitimate state, they could-and usually did !-remain so forever. Whether th.e property of self self-stabilization-for a more precise definition,},
  file = {D\:\\GDrive\\zotero\\Dijkstra\\dijkstra_1974_self-stabilizing_systems_in_spite_of_distributed_control.pdf},
  keywords = {and Phrases: multiprocessing,distributed control,error recovery,harmonious cooperation,mutual exclusion,networks,robustness,self-repair CR Categories: 432,self-stabilization,sharing,synchronization}
}

@techreport{dijxstraSolutionProblemConcurrent,
  title = {Solution of a {{Problem}} in {{Concurrent Programming Control}}},
  author = {Dijxstra, E W},
  abstract = {A number of mainly independent sequential-cyclic processes with restricted means of communication with each other can be made in such a way that at any moment one and only one of them is engaged in the "critical section" of its cycle.},
  file = {D\:\\GDrive\\zotero\\Dijxstra\\dijxstra_solution_of_a_problem_in_concurrent_programming_control.pdf}
}

@inproceedings{dilleyEmpiricalStudyMessaging2019,
  title = {An {{Empirical Study}} of {{Messaging Passing Concurrency}} in {{Go Projects}}},
  booktitle = {2019 {{IEEE}} 26th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Dilley, Nicolas and Lange, Julien},
  year = {2019},
  month = feb,
  pages = {377--387},
  issn = {1534-5351},
  doi = {10.1109/SANER.2019.8668036},
  abstract = {Go is a popular programming language renowned for its good support for system programming and its channel-based message passing concurrency mechanism. These strengths have made it the language of choice of many platform software such as Docker and Kubernetes. In this paper, we analyse 865 Go projects from GitHub in order to understand how message passing concurrency is used in publicly available code. Our results include the following findings: (1) message passing primitives are used frequently and intensively, (2) concurrency-related features are generally clustered in specific parts of a Go project, (3) most projects use synchronous communication channels over asynchronous ones, and (4) most Go projects use simple concurrent thread topologies, which are however currently unsupported by existing static verification frameworks.},
  file = {D\:\\GDrive\\zotero\\Dilley_Lange\\dilley_lange_2019_an_empirical_study_of_messaging_passing_concurrency_in_go_projects.pdf},
  keywords = {Communication channels,Computer languages,Concurrent computing,empirical study,Golang,message passing,Message passing,Message systems,Programming,Software,static analysis}
}

@book{dingEfficientProtectionPathSensitive2017,
  title = {Efficient {{Protection}} of {{Path}}-{{Sensitive Control Security}}},
  author = {Ding, Ren and Qian, Chenxiong and Tech, Georgia and Song, Chengyu and Riverside, U C and Harris, Bill and Kim, Taesoo and Lee, Wenke and Harris, William},
  year = {2017},
  abstract = {Control-Flow Integrity (CFI), as a means to prevent control-flow hijacking attacks, enforces that each instruction transfers control to an address in a set of valid targets. The security guarantee of CFI thus depends on the definition of valid targets, which conventionally are defined as the result of a static analysis. Unfortunately, previous research has demonstrated that such a definition, and thus any implementation that enforces it, still allows practical control-flow attacks. In this work, we present a path-sensitive variation of CFI that utilizes runtime path-sensitive point-to analysis to compute the legitimate control transfer targets. We have designed and implemented a runtime environment, PITTYPAT, that enforces path-sensitive CFI efficiently by combining commodity, low-overhead hardware monitoring and a novel runtime points-to analysis. Our formal analysis and empirical evaluation demonstrate that, compared to CFI based on static analysis, PITTYPAT ensures that applications satisfy stronger security guarantees, with acceptable overhead for security-critical contexts.},
  file = {D\:\\GDrive\\zotero\\Ding\\ding_efficient_protection_of_path-sensitive_control_security.pdf},
  isbn = {978-1-931971-40-9}
}

@article{dingledineDingledineTorSecondgeneration,
  title = {Dingledine et al. - {{Tor}} - the Second-Generation Onion Router - 2004},
  author = {Dingledine, Roger and Mathewson, Nick and Syverson, Paul}
}

@techreport{dingledineTorSecondGenerationOnion2004,
  title = {Tor: {{The Second}}-{{Generation Onion Router}}},
  author = {Dingledine, Roger},
  year = {2004},
  abstract = {We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design by adding perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for location-hidden services via rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than 30 nodes. We close with a list of open problems in anonymous communication. 1 Overview Onion Routing is a distributed overlay network designed to anonymize TCP-based applications like web browsing, secure shell, and instant messaging. Clients choose a path through the network and build a circuit, in which each node (or "onion router" or "OR") in the path knows its predecessor and successor, but no other nodes in the circuit. Traffic flows down the circuit in fixed-size cells, which are unwrapped by a symmetric key at each node (like the layers of an onion) and relayed downstream. The Onion Routing project published several design and analysis papers [27, 41, 48, 49]. While a wide area Onion Routing network was deployed briefly, the only long-running public implementation was a fragile proof-of-concept that ran on a single machine. Even this simple deployment processed connections from over sixty thousand distinct IP addresses from all over the world at a rate of about fifty thousand per day. But many critical design and deployment issues were never resolved, and the design has not been updated in years. Here we describe Tor, a protocol for asyn-chronous, loosely federated onion routers that provides the following improvements over the old Onion Routing design: Perfect forward secrecy: In the original Onion Routing design, a single hostile node could record traffic and later compromise successive nodes in the circuit and force them to decrypt it. Rather than using a single multiply encrypted data structure (an onion) to lay each circuit, Tor now uses an incremental or telescoping path-building design, where the initiator negotiates session keys with each successive hop in the circuit. Once these keys are deleted, subsequently compromised nodes cannot decrypt old traffic. As a side benefit, onion replay detection is no longer necessary, and the process of building circuits is more reliable, since the initiator knows when a hop fails and can then try extending to a new node. Separation of "protocol cleaning" from anonymity: Onion Routing originally required a separate "application proxy" for each supported application protocol-most of which were never written, so many applications were never supported. Tor uses the standard and near-ubiquitous SOCKS [32] proxy interface, allowing us to support most TCP-based programs without modification. Tor now relies on the filtering features of privacy-enhancing application-level proxies such as Privoxy [39], without trying to duplicate those features itself. No mixing, padding, or traffic shaping (yet): Onion Routing originally called for batching and reordering cells as they arrived, assumed padding between ORs, and in later designs added padding between onion proxies (users) and ORs [27, 41]. Tradeoffs between padding protection and cost were discussed, and traffic shaping algorithms were theorized [49] to provide good security without expensive padding, but no concrete padding scheme was suggested. Recent research [1] and deployment experience [4] suggest that this level of resource use is not practical or economical; and even full link padding is still vulnerable [33]. Thus, until we have a proven and convenient design for traffic shaping or low-latency mixing that improves anonymity against a realistic adversary, we leave these strategies out. Many TCP streams can share one circuit: Onion Routing originally built a separate circuit for each application-level request, but this required multiple public key operations for every request, and also presented a threat to anonymity from building so many circuits; see Section 9. Tor multi},
  file = {D\:\\GDrive\\zotero\\Dingledine\\dingledine_2004_tor.pdf}
}

@techreport{diotDeploymentIssuesMulticast,
  title = {Deployment {{Issues}} for the {{I P Multicast}} '{{Service}} and {{Arch}} Itecture {{IP Multicasf The Current Service Mode}}},
  author = {Diot, Christophe and Kassem, Hassan and Doug Balensiefen, Sprintlink},
  abstract = {IP multicast offers the scalable point-to-multipoint delivery necessary for using group communication applications on the Internet. However, the IP multicast service has seen slow commercial deployment by lSPs and carriers. The original service model was designed without a clear understanding of commercial requirements or a robust implementation strate y The very limited number of applications and the complexity of the architecturaydesign-which we believe is a conse uence of the open service model-have deterred widespread deployment as wej. W e examine the issues that have limited the commercial deploymeni of IP multicast from the viewpoint of carriers. W e analyze where the model fails and what it does not offer, and we discuss requirements for successful deployment of multicast services. ince its introduction [l], IP multicast has seen slow commercial deployment in thc Internet. Although it has been available through the experimental Mhone for a number of years, it is just beginning to see commercial support from carriers, Internet service providers (ISPs), and common operating systems. IP-based networks offer point-to-multipoint and multipoint-to-multipoint bcst-effort delivery of datagrams by means of the IP multicast service and architcc-turc.' The current service model in IP multicast was defined without a commercial service explicitly in mind, which is one possible reason for its slow deployment. Although each of these issues is the subject of current research efforts, the scrvicc model and architecture do not efficiently provide or addrcss many features required of a robust commercial implemcntation of multicast. Some of these issues include: * Group management, including authorization for group creation , receiver authorization, and sender authorization-Distributed multicast addrcss allocation Security, including protection against attacks on multicast routes and sessions, as well as support for data integrity mechanisms ' Support for network management Consequently, the currcnt IP-multicast architccturc dcployed hy carriers and ISPs to compensate for these issues is complex and has limited scalability. Trying to generalize ' By architecture, we mean the se1 of pro to col\^\^ supported by the IETF and vendors to realize the sewice model. and commercialize multicast from thc current service model and protocol architecture is difficult, and, in the worst case, advcrscly impacts the long-term success of multicast. In this article we examine, from the viewpoint of ISPs and carriers , the current IP multicast service model and thc issues that have limited the commercial dcployment of IP multicast. We discuss the motivations of ISPs and users for using multicast. We show where the architccture has become too complex, which services are not addressed by thc model, and what is required for long-term successful deployment of multicast service. The goal of this article is not to provc or show that the current model is wrong. Rather, it is to show that the open multi-cast service modcl and the complexity in providing the necessary functionality for ISPs are limiting the possibility of Internet-wide multicast. In the next section we review thc current service model and the architecture that supports it. We then analyze the motivations of ISPs and customers for using a multicast service. Ncxt, we examine the difficulties ISPs have had with the current model and architecture. Wc discuss the functionalities that are lacking from the service model, and propose alternate services models that are more aligned with commercial deployment. Finally, we offer our concluding rcmarks.},
  file = {D\:\\GDrive\\zotero\\Diot\\diot_deployment_issues_for_the_i_p_multicast_'service_and_arch_itecture_ip_multicasf.pdf},
  isbn = {08908044/00}
}

@article{disselkoenPrimeAbortTimerfree2017,
  title = {Prime+{{Abort}}: {{A}} Timer-Free High-Precision {{L3}} Cache Attack Using Intel {{TSX}}},
  author = {Disselkoen, Craig and Kohlbrenner, David and Porter, Leo and Tullsen, Dean},
  year = {2017},
  pages = {51--67},
  abstract = {Last-Level Cache (LLC) attacks typically exploit timing side channels in hardware, and thus rely heavily on timers for their operation. Many proposed defenses against such side-channel attacks capitalize on this reliance. This paper presents PRIME+ABORT, a new cache attack which bypasses these defenses by not depending on timers for its function. Instead of a timing side channel, PRIME+ABORT leverages the Intel TSX hardware widely available in both server- and consumer-grade processors. This work shows that PRIME+ABORT is not only invulnerable to important classes of defenses, it also outperforms state-of-the-art LLC PRIME+PROBE attacks in both accuracy and efficiency, having a maximum detection speed (in events per second) 3\texttimes{} higher than LLC PRIME+PROBE on Intel's Skylake architecture while producing fewer false positives.},
  file = {D\:\\GDrive\\zotero\\Disselkoen\\disselkoen_2017_prime+abort.pdf},
  isbn = {9781931971409},
  journal = {Proceedings of the 26th USENIX Security Symposium}
}

@techreport{divackyClangBSD2010,
  title = {{{ClangBSD}}},
  author = {Divacky, Roman and ClangBSD, FreeBSDorg},
  year = {2010}
}

@article{dMcmillanInterpolation,
  title = {{{mcmillanInterpolation}}},
  author = {{\DH}, {\`O} {\O} {\"E} {\`I} {\~N} {\'O} and {\'O}, {\`O} {\~N} {\O} {\'O} {\'O} {\'U} {\"O} {\'Y} {\`O} {\O} {\~N}{\^o}{\'o}{\"o} {\DH} {\^O}{\"o}{\'o}{\^o} {\"O} {\O}},
  file = {D\:\\GDrive\\zotero\\Ð\\ð_mcmillaninterpolation.pdf}
}

@inproceedings{dobbelaereIndustryPaperKafka2017,
  title = {Industry Paper: {{Kafka}} versus {{RabbitMQ}}: {{A}} Comparative Study of Two Industry Reference Publish/Subscribe Implementations},
  booktitle = {{{DEBS}} 2017 - {{Proceedings}} of the 11th {{ACM International Conference}} on {{Distributed Event}}-{{Based Systems}}},
  author = {Dobbelaere, Philippe and Esmaili, Kyumars Sheykh},
  year = {2017},
  month = jun,
  pages = {227--238},
  publisher = {{Association for Computing Machinery, Inc}},
  doi = {10.1145/3093742.3093908},
  abstract = {Publish/subscribe is a distributed interaction paradigmwell adapted to the deployment of scalable and loosely coupled systems. Apache Kafka and RabbitMQ are two popular open-source and commercially-supported pub/sub systems that have been around for almost a decade and have seen wide adoption. Given the popularity of these two systems and the fact that both are branded as pub/sub systems, two frequently asked questions in the relevant online forums are: how do they compare against each other and which one to use? In this paper, we frame the arguments in a holistic approach by establishing a common comparison framework based on the core functionalities of pub/sub systems. Using this framework, we then venture into a qualitative and quantitative (i.e. empirical) compari- son of the common features of the two systems. Additionally, we also highlight the distinct features that each of these systems has. Aster enumerating a set of use cases that are best suited for Rab- bitMQ or Kafka, we try to guide the reader through a determination table to choose the best architecture given his/her particular set of requirements.},
  file = {D\:\\GDrive\\zotero\\Dobbelaere\\dobbelaere_2017_industry_paper.pdf},
  isbn = {978-1-4503-5065-5},
  keywords = {AMQP,Apache kafka,Log files,Message brokers,Performance,Publish/Subscribe systems,RabbitMQ,Reliability}
}

@article{dongShieldingSoftwarePrivileged,
  title = {Shielding {{Software From Privileged Side}}-{{Channel Attacks}}},
  author = {Dong, Xiaowan and Shen, Zhuojia and Criswell, John and Cox, Alan L and Dwarkadas, Sandhya},
  abstract = {Commodity operating system (OS) kernels, such as Windows , Mac OS X, Linux, and FreeBSD, are susceptible to numerous security vulnerabilities. Their mono-lithic design gives successful attackers complete access to all application data and system resources. Shielding systems such as InkTag, Haven, and Virtual Ghost protect sensitive application data from compromised OS kernels. However, such systems are still vulnerable to side-channel attacks. Worse yet, compromised OS kernels can leverage their control over privileged hardware state to exacerbate existing side channels; recent work has shown that a compromised OS kernel can steal entire documents via side channels. This paper presents defenses against page table and last-level cache (LLC) side-channel attacks launched by a compromised OS kernel. Our page table defenses restrict the OS kernel's ability to read and write page table pages and defend against page allocation attacks, and our LLC defenses utilize the Intel Cache Allocation Technology along with memory isolation primitives. We prototype our solution in a system we call Apparition, building on an optimized version of Virtual Ghost. Our evaluation shows that our side-channel defenses add 1\% to 18\% (with up to 86\% for one application) overhead to the optimized Virtual Ghost (relative to the native kernel) on real-world applications.},
  file = {D\:\\GDrive\\zotero\\Dong\\dong_shielding_software_from_privileged_side-channel_attacks.pdf}
}

@techreport{douceurSybilAttack,
  title = {The {{Sybil Attack}}},
  author = {Douceur, John R},
  abstract = {"One can have, some claim, as many electronic personas as one has time and energy to create."-Judith S. Donath [12] Abstract-Large-scale peer-to-peer systems face security threats from faulty or hostile remote computing elements. To resist these threats, many such systems employ redundancy. However, if a single faulty entity can present multiple identities, it can control a substantial fraction of the system, thereby undermining this redundancy. One approach to preventing these "Sybil attacks" is to have a trusted agency certify identities. This paper shows that, without a logically centralized authority, Sybil attacks are always possible except under extreme and unrealistic assumptions of resource parity and coordination among entities.},
  file = {D\:\\GDrive\\zotero\\Douceur\\douceur_the_sybil_attack.pdf}
}

@techreport{douglasmcllroyTechnicalNotesProgramming1982,
  title = {Technical {{Notes Programming Techniques}} and {{Data Structures Approximation Algorithms}} for {{Convex Hulls}}},
  author = {Douglas Mcllroy, M and Bentley, Jon Louis and Faust, Mark G and Preparata, Franco P},
  year = {1982},
  abstract = {The problem of constructing the convex hull of a finite point set in a Euclidean space arises in many applications. In this paper we study a set of algorithms for constructing approximate convex hulls. We show that an E-approximate hull of N points in the plane can be constructed in O(n + I/e) time. The planar algorithm has been implemented and is very fast on point sets that arise in practice. The method can be generalized to compute hulls of point sets in higher dimensional spaces.},
  file = {D\:\\GDrive\\zotero\\Douglas Mcllroy\\douglas_mcllroy_1982_technical_notes_programming_techniques_and_data_structures_approximation.pdf}
}

@techreport{downeyLittleBookSemaphores2016,
  title = {The {{Little Book}} of {{Semaphores}}},
  author = {Downey, Allen B},
  year = {2016},
  file = {D\:\\GDrive\\zotero\\Downey\\downey_2016_the_little_book_of_semaphores.pdf},
  keywords = {()}
}

@book{DrSoftware,
  title = {Dr. {{Software}}},
  file = {D\:\\GDrive\\zotero\\_\\dr.pdf}
}

@article{dtuEnablingPolyhedralOptimizations2011,
  title = {Enabling Polyhedral Optimizations in {{LLVM}}},
  author = {Dtu, Gprs and Io, M A D P L C},
  year = {2011},
  abstract = {[1] Dtu G, Io MADPLC. No 主観的健康感を中心とした在宅高齢者における 健康関連指標に関する共分散構造分析Title 2011:2011.},
  file = {D\:\\GDrive\\zotero\\Dtu\\dtu_2011_enabling_polyhedral_optimizations_in_llvm.pdf},
  keywords = {ac 使用说明书,n80-m21mad-dc,矩形科技有限公司},
  number = {April}
}

@techreport{duchijduchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}} * {{Elad Hazan}}},
  author = {Duchi JDUCHI, John and Singer, Yoram},
  year = {2011},
  volume = {12},
  pages = {2121--2159},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function , which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file = {D\:\\GDrive\\zotero\\Duchi JDUCHI\\duchi_jduchi_2011_adaptive_subgradient_methods_for_online_learning_and_stochastic_optimization.pdf},
  journal = {Journal of Machine Learning Research},
  keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods}
}

@techreport{duganWritingComputerScience2006,
  title = {Writing for Computer Science: {{A}} Taxonomy of Writing Tasks and General Advice},
  author = {Dugan, Robert F and Polanski, Virginia G},
  year = {2006},
  abstract = {Computer science graduates lack written communication skills crucial to success in the workplace. Professional and academic organizations including ACM, IEEE, ABET, CSAB, and NACE have stressed the importance of teaching computer science undergraduates to write for years, yet the writing problem persists. In this paper we provide guidance to computer science instructors who want student writing skills to improve. First, we organize prior work on writing for computer science into a goal-oriented taxonomy of writing tasks. Each task includes a clear, concise, and detailed model that can be used as the framework for a student writing assignment. Second, we provide general advice for incorporating writing into any computer science course. Finally, we discuss the application of our taxonomy and advice to writing tasks in several computer science courses.},
  file = {D\:\\GDrive\\zotero\\Dugan_Polanski\\dugan_polanski_2006_writing_for_computer_science.pdf}
}

@article{dukkipatiProportionalRateReduction2011,
  title = {Proportional {{Rate Reduction}} for {{TCP}}},
  author = {Dukkipati, Nandita and Mathis, Matt and Cheng, Yuchung and Ghobadi, Monia},
  year = {2011},
  abstract = {Packet losses increase latency for Web users. Fast recovery is a key mechanism for TCP to recover from packet losses. In this paper, we explore some of the weaknesses of the standard algorithm described in RFC 3517 and the non-standard algorithms implemented in Linux. We find that these algorithms deviate from their intended behavior in the real world due to the combined effect of short flows, application stalls, burst losses, acknowledgment (ACK) loss and reordering, and stretch ACKs. Linux suffers from excessive congestion window reductions while RFC 3517 transmits large bursts under high losses, both of which harm the rest of the flow and increase Web latency. Our primary contribution is a new design to control transmission in fast recovery called proportional rate reduction (PRR). PRR recovers from losses quickly, smoothly and accurately by pacing out retransmissions across received ACKs. In addition to PRR, we evaluate the TCP early retransmit (ER) algorithm which lowers the duplicate acknowledgment threshold for short transfers, and show that delaying early retransmissions for a short interval is effective in avoiding spurious retransmissions in the presence of a small degree of reordering. PRR and ER reduce the TCP latency of connections experiencing losses by 3-10\% depending on the response size. Based on our instrumentation on Google Web and YouTube servers in U.S. and India, we also present key statistics on the nature of TCP retransmissions.},
  file = {D\:\\GDrive\\zotero\\Dukkipati\\dukkipati_2011_proportional_rate_reduction_for_tcp.pdf}
}

@techreport{dukkipatiWhyFlowCompletionTime,
  title = {Why {{Flow}}-{{Completion Time}} Is the {{Right Metric}} for {{Congestion Control}}},
  author = {Dukkipati, Nandita and Mckeown, Nick},
  abstract = {Users typically want their flows to complete as quickly as possible. This makes Flow Completion Time (FCT) an important arguably the most important-performance metric for the user. Yet research on congestion control focuses almost entirely on maximizing link throughput, utilization and fairness, which matter more to the operator than the user. In this paper we show that with typical Internet flow sizes, existing (TCP Reno) and newly proposed (XCP) congestion control algorithms make flows last much longer than necessary-often by one or two orders of magnitude. In contrast, we show how a new and practical algorithm-RCP (Rate Control Protocol)-enables flows to complete close to the minimum possible.},
  file = {D\:\\GDrive\\zotero\\Dukkipati\\dukkipati_why_flow-completion_time_is_the_right_metric_for_congestion_control.pdf}
}

@article{dupontDeAnonymizingBitcoinMapping,
  title = {Toward {{De}}-{{Anonymizing Bitcoin}} by {{Mapping Users Location}}},
  author = {Dupont, Jules and Squicciarini, Anna C},
  doi = {10.1145/2699026.2699128},
  abstract = {The Bitcoin system (https://bitcoin.org) is a pseudo-anonymous currency that can dissociate a user from any real-world identity. In that context, a successful breach of the virtual and physical divide represents a significant flaw in the Bit-coin system [1]. In this project we demonstrate how to glean information about the real-world users behind Bitcoin transactions. We analyze publicly available data about the cryp-tocurrency. In particular, we focus on determining information about a Bitcoin user's physical location by examining that user's spending habits.},
  file = {D\:\\GDrive\\zotero\\Dupont\\dupont_toward_de-anonymizing_bitcoin_by_mapping_users_location.pdf},
  isbn = {9781450331913}
}

@article{durumericNeitherSnowRain,
  title = {Neither {{Snow Nor Rain Nor MITM}}. .. {{An Empirical Analysis}} of {{Email Delivery Security}}},
  author = {Durumeric, Zakir and Adrian, David and Mirian, Ariana and Kasten, James and Bursztein, Elie and Lidzborski, Nicolas and Thomas, Kurt and Eranti, Vijay and Bailey, Michael and Halderman, J Alex},
  doi = {10.1145/2815675.2815695},
  abstract = {The SMTP protocol is responsible for carrying some of users' most intimate communication, but like other Internet protocols, authen-tication and confidentiality were added only as an afterthought. In this work, we present the first report on global adoption rates of SMTP security extensions, including: STARTTLS, SPF, DKIM, and DMARC. We present data from two perspectives: SMTP server configurations for the Alexa Top Million domains, and over a year of SMTP connections to and from Gmail. We find that the top mail providers (e.g., Gmail, Yahoo, and Outlook) all proactively encrypt and authenticate messages. However, these best practices have yet to reach widespread adoption in a long tail of over 700,000 SMTP servers, of which only 35\% successfully configure encryption, and 1.1\% specify a DMARC authentication policy. This security patchwork -paired with SMTP policies that favor failing open to allow gradual deployment-exposes users to attackers who downgrade TLS connections in favor of cleartext and who falsify MX records to reroute messages. We present evidence of such attacks in the wild, highlighting seven countries where more than 20\% of inbound Gmail messages arrive in cleartext due to network attackers.},
  file = {D\:\\GDrive\\zotero\\Durumeric\\durumeric_neither_snow_nor_rain_nor_mitm.pdf},
  isbn = {9781450338486},
  keywords = {ss}
}

@article{dushkuRemoteAttestationEnsure2020,
  title = {Remote Attestation to Ensure the Security of Future {{Internet}} of {{Things}} Services},
  author = {DUSHKU, EDLIRA},
  year = {2020},
  abstract = {The Internet of Things (IoT) evolution is gradually reshaping the physical world into smart environments that involve a large number of interconnected resource-constrained devices which collect, process, and exchange enormous amount of (more or less) sensitive information. With the increasing number of interconnected IoT devices and their capabilities to control the environment, IoT systems are becoming a prominent target of sophisticated cyberattacks. To deal with the expanding attack surface, IoT systems require adequate security mechanisms to verify the reliability of IoT devices. Remote attestation protocols have recently gained wide attention in IoT systems as valuable security mechanisms that detect the adversarial presence and guarantee the legitimate state of IoT devices. Various attestation schemes have been proposed to optimize the effectiveness and efficiency of remote attestation protocols of a single IoT device or a group of IoT devices. Nevertheless, some cyber attacks remain undetected by current attestation methods, and attestation protocols still introduce non-negligible computational overheads for resource-constrained devices. This thesis presents the following new contributions in the area of remote attestation protocols that verify the trustworthiness of IoT devices. First, this thesis shows the limitations of existing attestation protocols against runtime attacks which, by compromising a device, may maliciously influence the operation of other genuine devices that interact with the compromised one. To detect such an attack, this thesis introduces the service perspective in remote attestation and presents a synchronous remote attestation protocol for distributed IoT services. Second, this thesis designs, implements and evaluates a novel remote attestation scheme that releases the constraint of synchronous interaction between devices and enables the attestation of asynchronous distributed IoT services. The proposed scheme also attests asynchronously a group of IoT devices, without interrupting the regular operations of all the devices at the same time. Third, this thesis proposes a new approach that aims to reduce the interruption time of the regular work that remote attestation introduces in an IoT device. This approach intends to decrease the computational overhead of attestation by allowing an IoT device to securely offload the attestation process to a cloud service, which then performs attestation independently on the cloud, on behalf of the IoT device},
  file = {D\:\\GDrive\\zotero\\DUSHKU\\dushku_remote_attestation_to_ensure_the_security_of_future_internet_of_things_services.pdf;C\:\\Users\\Admin\\Zotero\\storage\\RMVNBSST\\296244518.html}
}

@techreport{duttaPairingBasedCryptographicProtocols,
  title = {Pairing-{{Based Cryptographic Protocols}} : {{A Survey}}},
  author = {Dutta, Ratna and Barua, Rana and Sarkar, Palash},
  file = {D\:\\GDrive\\zotero\\Dutta\\dutta_pairing-based_cryptographic_protocols.pdf}
}

@article{Dwork1988Consensus,
  title = {Dwork et al. - 1988 - {{Consensus}} in the Presence of Partial Synchrony},
  file = {D\:\\GDrive\\zotero\\undefined\\dwork_et_al.pdf}
}

@techreport{dworkPricingProcessingCombatting,
  title = {Pricing via {{Processing}} or {{Combatting Junk Mail}}},
  author = {Dwork, Cynthia and Naor, Moni},
  abstract = {We present a computational technique for combatting junk mail in particular and controlling access to a shared resource in general. The main idea is t o require a user t o compute a moderately hard, but not intractable, function in order to gain access to the resource, thus preventing frivolous use. To this end we suggest several pricing functions, based on, respectively, extracting square roots modulo a prime, the Fiat-Shamir signature scheme, and the Ong-Schnorr-Shamir (cracked) signature scheme.},
  file = {D\:\\GDrive\\zotero\\Dwork\\dwork_pricing_via_processing_or_combatting_junk_mail.pdf}
}

@techreport{ebnerGeneralizedInstructionSelection,
  title = {Generalized {{Instruction Selection}} Using {{SSA}}-{{Graphs}} *},
  author = {Ebner, Dietmar and Brandner, Florian and Scholz, Bernhard and Krall, Andreas and Wiedermann, Peter and Kadlec, Albrecht},
  abstract = {Instruction selection is a well-studied compiler phase that translates the compiler's intermediate representation of programs to a sequence of target-dependent machine instructions optimizing for various compiler objectives (e.g. speed and space). Most existing instruction selection techniques are limited to the scope of a single statement or a basic block and cannot cope with irregular instruction sets that are frequently found in embedded systems. We consider an optimal technique for instruction selection that uses Static Single Assignment (SSA) graphs as an intermediate representation of programs and employs the Partitioned Boolean Quadratic Problem (PBQP) for finding an optimal instruction selection. While existing approaches are limited to instruction patterns that can be expressed in a simple tree structure, we consider complex patterns producing multiple results at the same time including pre/post increment addressing modes, div-mod instructions , and SIMD extensions frequently found in embedded systems. Although both instruction selection on SSA-graphs and PBQP are known to be NP-complete, the problem can be solved efficiently-even for very large instances. Our approach has been implemented in LLVM for an embedded ARMv5 architecture. Extensive experiments show speedups of up to 57\% on typical DSP kernels and up to 10\% on SPECINT 2000 and MiBench benchmarks. All of the test programs could be compiled within less than half a minute using a heuristic PBQP solver that solves 99.83\% of all instances optimally.},
  file = {D\:\\GDrive\\zotero\\Ebner\\ebner_generalized_instruction_selection_using_ssa-graphs.pdf},
  isbn = {0123456789},
  keywords = {Code Generation,D34 [Programming Lan-guages]: Processors-Compilers General Terms Algorithms,Instruction Selection,Languages,PBQP,Performance Keywords Compiler}
}

@book{eggertMakingMiddleboxesSomeone2012,
  title = {Making {{Middleboxes Someone Else}}'s {{Problem}}:{{Network Processing}} as a {{Cloud Service}}},
  author = {Eggert, Lars. and {ACM Digital Library.} and {ACM Special Interest Group on Data Communication.}},
  year = {2012},
  publisher = {{ACM}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Eggert\\eggert_2012_making_middleboxes_someone_else’s_problem.pdf},
  isbn = {978-1-4503-1419-0}
}

@article{egnersMessingAndroidPermission2012,
  title = {Messing with {{Android}}'s Permission Model},
  author = {Egners, Andr{\'e} and Meyer, Ulrike and Marschollek, Bj{\"o}rn},
  year = {2012},
  pages = {505--514},
  doi = {10.1109/TrustCom.2012.203},
  abstract = {Permission models have become very common on smartphone operating systems to control the rights granted to installed third party applications (apps). Prior to installing an app, the user is typically presented with a dialog box showing the permissions requested by the app. The user has to decide either to accept all of the requested permissions, or choose not to proceed with the installation. Most regular users are not able to fully grasp which set of permissions granted to the application is potentially harmful. In addition to the knowledge gap between user and application programmer, the missing granularity and alterability of most permission model implementations help an attacker to circumvent the permission model. In this paper we focus on the permission model of Google's Android platform. We detail the permission model, and present a selection of attacks that can be composed to fully compromise a user's device using inconspicuously looking applications requesting non-suspicious permissions. \textcopyright{} 2012 IEEE.},
  file = {D\:\\GDrive\\zotero\\Egners\\egners_2012_messing_with_android's_permission_model.pdf},
  isbn = {9780769547459},
  journal = {Proc. of the 11th IEEE Int. Conference on Trust, Security and Privacy in Computing and Communications, TrustCom-2012 - 11th IEEE Int. Conference on Ubiquitous Computing and Communications, IUCC-2012},
  keywords = {Android,Attack,Permission,Smartphone},
  number = {December}
}

@techreport{eideVolatilesAreMiscompiled,
  title = {Volatiles {{Are Miscompiled}}, and {{What}} to {{Do}} about {{It}}},
  author = {Eide, Eric and Regehr, John},
  abstract = {C's volatile qualifier is intended to provide a reliable link between operations at the source-code level and operations at the memory-system level. We tested thirteen production-quality C compilers and, for each, found situations in which the compiler generated incorrect code for accessing volatile variables. This result is disturbing because it implies that embedded software and operating systems-both typically coded in C, both being bases for many mission-critical and safety-critical applications, and both relying on the correct translation of volatiles-may be being miscompiled. Our contribution is centered on a novel technique for finding volatile bugs and a novel technique for working around them. First, we present access summary testing: an efficient, practical, and automatic way to detect code-generation errors related to the volatile qualifier. We have found a number of compiler bugs by performing access summary testing on randomly generated C programs. Some of these bugs have been confirmed and fixed by compiler developers. Second, we present and evaluate a workaround for the compiler defects we discovered. In 96\% of the cases in which one of our randomly generated programs is miscompiled, we can cause the faulty C compiler to produce correctly behaving code by applying a straightforward source-level transformation to the test program.},
  file = {D\:\\GDrive\\zotero\\Eide\\eide_volatiles_are_miscompiled,_and_what_to_do_about_it.pdf},
  keywords = {D25 [Software Engineer-ing]: Testing and Debugging-testing tools,D32 [Programming Languages]: Language Classifications-C,D34 [Programming Languages]: Processors-compilers General Terms Languages; Reliability Keywords compiler testing; compiler defect; automated testing; random testing; random program generation; volatile}
}

@techreport{einsteinELECTRODYNAMICSMOVINGBODIES1905,
  title = {{{ON THE ELECTRODYNAMICS OF MOVING BODIES}}},
  author = {Einstein, A},
  year = {1905},
  file = {D\:\\GDrive\\zotero\\Einstein\\einstein_1905_on_the_electrodynamics_of_moving_bodies.pdf}
}

@misc{eisnerHowFindResearch1997,
  title = {How to {{Find Research Problems}}},
  author = {Eisner, Jason},
  year = {1997},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\MGPTNENG\\how-to-find-research-problems.html},
  howpublished = {https://www.cs.jhu.edu/\textasciitilde jason/advice/how-to-find-research-problems.html}
}

@misc{eisnerHowOrganizeYour2000,
  title = {How to {{Organize Your Files}}},
  author = {Eisner, Jason},
  year = {2000},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\NFPFS3Y5\\how-to-organize-your-files.html},
  howpublished = {https://www.cs.jhu.edu/\textasciitilde jason/advice/how-to-organize-your-files.html}
}

@misc{eisnerHowReadTechnical2009,
  title = {How to {{Read}} a {{Technical Paper}}},
  author = {Eisner, Jason},
  year = {2009},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\52ATJR9F\\how-to-read-a-paper.html},
  howpublished = {https://www.cs.jhu.edu/\textasciitilde jason/advice/how-to-read-a-paper.html}
}

@misc{eisnerHowWritePh2009,
  title = {How to {{Write Up}} a {{Ph}}.{{D}}. {{Dissertation}}},
  author = {Eisner, Jason},
  year = {2009},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7RYFVSBX\\how-to-write-a-thesis.html},
  howpublished = {https://www.cs.jhu.edu/\textasciitilde jason/advice/how-to-write-a-thesis.html}
}

@misc{eisnerWritePaperFirst2010,
  title = {Write the {{Paper First}}},
  author = {Eisner, Jason},
  year = {2010},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9T4VCA7D\\write-the-paper-first.html},
  howpublished = {https://www.cs.jhu.edu/\textasciitilde jason/advice/write-the-paper-first.html}
}

@techreport{eldman10Y7SocietyIndustrial,
  title = {@ {{10Y7 Society}} for {{Industrial}} and {{Applied Mathematics AN OPTIMAL PROBABILISTIC PROTOCOL FOR SYNCHRONOUS BYZANTINE AGREEMENT}}"},
  author = {E L D M A N, Pesech F},
  abstract = {Broadcasting guarantees the recipient of a message that everyone else has received the same message. This guarantee no longer exists in a setting in which all communication is person-to-person and some of the people involved are untrustworthy: though he may claim to send the same message t o everyone. an untrustworthy sender may send different messages to different people. In such a setting, Byzantine agreement offers the "best alternative" to broadcasting. Thus far, however, reaching Byzantine agreement has required either many rounds of communication (i.e., messages had to be sent back and forth a number of times that grew with the size of the network) or the help of some external trusted party. In this paper, for the standard communication model of synchronous networks in which each pair of processors is connected by a private communication line, we exhibit a protocol that, in probabilistic polynomial time and without relying on any external trusted party, reaches Byzantine agreement in an expected constant number of rounds and in the worst natural fault model. In fact, our protocol successfully tolerates that up to 113 of the processors in the network may deviate from their prescribed instructions in an arbitrary way, cooperate with each other, and perform arbitrarily long computations. Our protocol effectively demonstrates the power of randomization and zero-knowledge computation against errors. Indeed, it proves that "privacy" (a fundamental ingredient of one of our primitives), even when is not a desired goal in itself (as for the Byzantine agreement problem), can be a crucial tool for achieving correctness. Our protocol also introduces three new primitives-graded broadcast, graded verifiable secret sharing, and oblivious common coin-that are of independent interest, and may be effectively used in more practical protocols than ours. A M S s u b j e c t classifications. 68Q22, 68R05? 68bI15, 94A60. 94.499, 94B99 PII. SO097539790187084},
  keywords = {broadcasting,Byzantine agreement,fault-tolerant computation,raridomization}
}

@book{elenkovAndroidSecurityInternals2015,
  title = {Android {{Security Internals}}},
  author = {Elenkov, Nikolay},
  year = {2015},
  volume = {2015},
  issn = {13534858},
  doi = {10.1016/s1353-4858(15)30046-5},
  abstract = {There are nearly a billion Android devices in use today, and every one is a potential security breach. Love it or hate it, the security of Android-based devices is of major concern to users and developers alike. In "Android Security Internals," author Nikolay Elenkov delves into Android components and subsystems to give you a very deep and complete understanding of the security internals of Android devices. Elenkov's coverage of security topics ranges from package and user management to the details of cryptographic providers and credential storage. "Android Security Internals" is destined to be one book that that all security-minded Android developers will have to have on their bookshelves.},
  isbn = {978-1-59327-581-5},
  journal = {Network Security}
}

@article{elgamalComputingLogarithmsFinite1986,
  title = {On {{Computing Logarithms Over Finite Fields}}},
  author = {ElGamal, Taher},
  year = {1986},
  volume = {100},
  pages = {402--411},
  issn = {0029-0440},
  file = {D\:\\GDrive\\zotero\\ElGamal\\elgamal_1986_on_computing_logarithms_over_finite_fields.pdf},
  number = {7}
}

@article{elgamalNewPredicamentsSecurity2009,
  title = {The New Predicaments of Security Practitioners},
  author = {Elgamal, Taher},
  year = {2009},
  volume = {2009},
  pages = {12--14},
  publisher = {{Elsevier Ltd}},
  issn = {13613723},
  doi = {10.1016/S1361-3723(09)70140-X},
  abstract = {You can always make the statement that there have been lots of developments in encryption in the past several years, but how, in 2009, do those developments affect the day-to-day work of security practitioners? \textcopyright{} 2009 Elsevier Ltd. All rights reserved.},
  file = {D\:\\MEGA\\zotero\\Elgamal\\elgamal_2009_the_new_predicaments_of_security_practitioners.pdf},
  journal = {Computer Fraud and Security},
  number = {11}
}

@article{elgamalPublicKeyCryptosystem1985,
  title = {A {{Public Key Cryptosystem}} and a {{Signature Scheme Based}} on {{Discrete Logarithms}}},
  author = {ElGamal, Taher},
  year = {1985},
  volume = {196 LNCS},
  pages = {10--18},
  issn = {16113349},
  doi = {10.1007/3-540-39568-7_2},
  abstract = {A new signature scheme is proposed together with an implementation of the Diffie - Hellman key distribution scheme that achieves a public key cryptosystem. The security of both systems relies on the difficulty of computing discrete logarithms over finite fields.},
  file = {D\:\\GDrive\\zotero\\ElGamal\\elgamal_1985_a_public_key_cryptosystem_and_a_signature_scheme_based_on_discrete_logarithms.pdf},
  isbn = {9783540156581},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{elgamalSubexponentialTimeAlgorithmComputing1985,
  title = {A {{Subexponential}}-{{Time Algorithm}} for {{Computing Discrete Logarithms Over GF}}(p)2},
  author = {Elgamal, Taher},
  year = {1985},
  volume = {31},
  pages = {473--481},
  issn = {15579654},
  doi = {10.1109/TIT.1985.1057075},
  abstract = {An algorithm for computing discrete logarithms over GF(p2), where p is a prime, in subexponential time is described. The algorithm is similar to the Merkle\textendash Adleman algorithm for computing logarithms over GF(p), but it uses quadratic fields as the appropriate algebraic structure. It also makes use of the idea of a virtual spanning set due to Hellman and Reyneri for computing discrete logarithms over GF(pm), for m growing and p fixed. \textcopyright{} 1985 IEEE.},
  journal = {IEEE Transactions on Information Theory},
  number = {4}
}

@techreport{elldocuConSisTe,
  title = {Con Sis Te n t * {{Comple}} Te {{A}} r Ti Fac t {{A Program Optimization}} for {{Automatic Database Result Caching}}},
  author = {Ell Docu, W and Scully, Ziv and Chlipala, Adam},
  abstract = {Most popular Web applications rely on persistent databases based on languages like SQL for declarative specification of data models and the operations that read and modify them. As applications scale up in user base, they often face challenges responding quickly enough to the high volume of requests. A common aid is caching of database results in the application's memory space, taking advantage of program-specific knowledge of which caching schemes are sound and useful, embodied in handwritten modifications that make the program less maintainable. These modifications also require nontriv-ial reasoning about the read-write dependencies across operations. In this paper, we present a compiler optimization that automatically adds sound SQL caching to Web applications coded in the Ur/Web domain-specific functional language, with no modifications required to source code. We use a custom cache implementation that supports concurrent operations without compromising the transactional semantics of the database abstraction. Through experiments with microbenchmarks and production Ur/Web applications, we show that our optimization in many cases enables an easy doubling or more of an application's throughput, requiring nothing more than passing an extra command-line flag to the compiler.},
  file = {D\:\\GDrive\\zotero\\Ell Docu\\ell_docu_con_sis_te_n_t_comple_te_a_r_ti_fac_t_a_program_optimization_for_automatic.pdf}
}

@article{emanuelssonComparativeStudyIndustrial2008,
  title = {A {{Comparative Study}} of {{Industrial Static Analysis Tools}}},
  author = {Emanuelsson, P{\"a}r and Nilsson, Ulf},
  year = {2008},
  doi = {10.1016/j.entcs.2008.06.039},
  abstract = {Tools based on static analysis can be used to find defects in programs. Tools that do shallow analyses based on pattern matching have existed since the 1980's and although they can analyze large programs they have the drawback of producing a massive amount of warnings that have to be manually analyzed to see if they are real defects or not. Recent technology advances has brought forward tools that do deeper analyses that discover more defects and produce a limited amount of false warnings. These tools can still handle large industrial applications with millions lines of code. This article surveys the underlying supporting technology of three state-of-the-art static analysis tools. The survey relies on information in research articles and manuals, and includes the types of defects checked for (such as memory management, arithmetics, security vulnerabilities), soundness, value and aliasing analyses, incrementality and IDE integration. This survey is complemented by practical experiences from evaluations at the Ericsson telecom company.},
  file = {D\:\\GDrive\\zotero\\Emanuelsson\\emanuelsson_2008_a_comparative_study_of_industrial_static_analysis_tools.pdf},
  keywords = {dataflow analysis,defects,security vulnerabilities,Static analysis}
}

@techreport{emersonSometimesNotNever,
  title = {"{{Sometimes}}" and "{{Not Never}}" {{Revisited}}: {{On Branching}} versus {{Linear Time Temporal Logic}}},
  author = {Emerson, E Allen and Halpern, Joseph Y},
  abstract = {The differences between and appropriateness of branching versus linear time temporal logic for reasoning about concurrent programs are studied. These issues have been previously considered by Lamport. To facilitate a careful examination of these issues, a language, CTL*, in which a universal or existential path quantifier can prefix an arbitrary linear time assertion, is defined. The expressive power of a number of sublanguages is then compared. CTL* is also related to the logics MPL of Abrahamson and PL of Hare\& Kozen, and Parikh. The paper concludes with a comparison of the utility of branching and linear time temporal logics.}
}

@article{enckTaintDroidInformationflowTracking2019,
  title = {{{TaintDroid}}: {{An}} Information-Flow Tracking System for Realtime Privacy Monitoring on Smartphones},
  author = {Enck, William and Gilbert, Peter and Chun, Byung Gon and Cox, Landon P. and Jung, Jaeyeon and McDaniel, Patrick and Sheth, Anmol N.},
  year = {2019},
  volume = {32},
  pages = {393--407},
  abstract = {Today's smartphone operating systems frequently fail to provide users with adequate control over and visibility into how third-party applications use their private data. We address these shortcomings with TaintDroid, an efficient, system-wide dynamic taint tracking and analysis system capable of simultaneously tracking multiple sources of sensitive data. TaintDroid provides realtime analysis by leveraging Android's virtualized execution environment. TaintDroid incurs only 14\% performance overhead on a CPU-bound micro-benchmark and imposes negligible overhead on interactive third-party applications. Using TaintDroid to monitor the behavior of 30 popular third-party Android applications, we found 68 instances of potential misuse of users' private information across 20 applications. Monitoring sensitive data with TaintDroid provides informed use of third-party applications for phone users and valuable input for smartphone security service firms seeking to identify misbehaving applications.},
  file = {D\:\\GDrive\\zotero\\Enck\\enck_2019_taintdroid.pdf},
  isbn = {9781931971799},
  journal = {Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2010},
  number = {2}
}

@article{enckwilliamStudyAndroidApplication2011,
  title = {A {{Study}} of {{Android Application Security}}},
  author = {al. Enck, William, et},
  year = {2011},
  pages = {29--50},
  doi = {10.1007/978-1-4615-0935-6_3},
  file = {D\:\\GDrive\\zotero\\Enck, William\\enck,_william_2011_a_study_of_android_application_security.pdf},
  journal = {Compression and Coding Algorithms},
  number = {August}
}

@techreport{ensinkCoordinatingAdaptationsDistributed,
  title = {Coordinating {{Adaptations}} in {{Distributed Systems}}},
  author = {Ensink, Brian},
  abstract = {Distributed applications may use sophisticated run-time adaptation strategies to meet their performance or quality-of-service goals. Coordinating an adaptation that involves multiple processes can require complex communication or synchronization, in addition to communication in the base application. We propose conceptually simple high-level directives and a sophisticated runtime algorithm for coordinating adaptation automatically and transparently in distributed applications. The coordination directives specify when to adapt, in terms of the relative computational progress of each relevant process. The coordination algorithm relies on simple compiler transformations to track the progress of the processes, and performs the adaptive changes locally and asynchronously at each process. Measurements of the runtime overhead of the automatic coordination algorithm for two adaptive applications (a parallel PDE solver and a distributed video tracking code) show that the overhead is less than 1\% of execution time for both these codes, even with relatively frequent adaptations, and does not grow significantly with the number of coordinating processes.},
  file = {D\:\\GDrive\\zotero\\Ensink\\ensink_coordinating_adaptations_in_distributed_systems.pdf}
}

@techreport{EPITAXISSystemSyntactic2010,
  title = {{{EPITAXIS A System}} for {{Syntactic}} and {{Semantic Software Queries}} Using {{Deductive Retrieval}} and {{Symbolic Execution}}},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\undefined\\2010_epitaxis_a_system_for_syntactic_and_semantic_software_queries_using_deductive.pdf}
}

@article{erhardtDesignImplementationTriCore2009,
  title = {Design and {{Implementation}} of a {{TriCore Backend}} for the {{LLVM Compiler Framework}}},
  author = {Erhardt, Christoph},
  year = {2009},
  abstract = {This thesis describes the development and implementation of a new backend for the LLVM compiler framework that allows the generation of machine code for the TriCore processor architecture. The TriCore architecture is an advanced platform for embedded systems which unites the features of a microcontroller, a RISC CPU and a digital signal processor. It finds use primarily in the automotive sector and other real-time systems and is employed by the Chair of Distributed Systems and Operating Systems at the University of Erlangen- Nuremberg for various research projects. LLVM is a modern compiler infrastructure with a particular focus on modularity and extensibility. For this reason, lattnerLLVMCompilationFrameworkLLVM is finding increasingly widespread use in most diverse projects \textendash{} from code analysis tools via just-in-time code generators through to full-blown general-purpose system compilers. In the course of the thesis, initially a technical overview of LLVM and the TriCore architecture is given. Subsequently, the design and implementation of the compiler backend are discussed in detail and a comparison to the existing GCC port is made.},
  file = {D\:\\GDrive\\zotero\\Erhardt\\erhardt_2009_design_and_implementation_of_a_tricore_backend_for_the_llvm_compiler_framework.pdf},
  number = {September}
}

@article{ermanSPDYIerMobile2015,
  title = {Towards a {{SPDY}}'ier {{Mobile Web}}?},
  author = {Erman, Jeffrey and Gopalakrishnan, Vijay and Jana, Rittwik and Ramakrishnan, Kadangode K.},
  year = {2015},
  month = dec,
  volume = {23},
  pages = {2010--2023},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {10636692},
  doi = {10.1109/TNET.2015.2462737},
  abstract = {Despite its widespread adoption and popularity, the Hypertext Transfer Protocol (HTTP) suffers from fundamental performance limitations. SPDY, a recently proposed alternative to HTTP, tries to address many of the limitations of HTTP (e.g., multiple connections, setup latency). With cellular networks fast becoming the communication channel of choice, we perform a detailed measurement study to understand the benefits of using SPDY over cellular networks. Through careful measurements conducted over four months, we provide a detailed analysis of the performance of HTTP and SPDY, how they interact with the various layers, and their implications on web design. Our results show that unlike in wired and 802.11 networks, SPDY does not clearly outperform HTTP over cellular networks. We identify, as the underlying cause, a lack of harmony between how TCP and cellular networks interact. In particular, the performance of most TCP implementations is impacted by their implicit assumption that the network round-trip latency does not change after an idle period, which is typically not the case in cellular networks. This causes spurious retransmissions and degraded throughput for both HTTP and SPDY. We conclude that a viable solution has to account for these unique cross-layer dependencies to achieve improved performance over cellular networks.},
  file = {D\:\\GDrive\\zotero\\Erman\\erman_2015_towards_a_spdy'ier_mobile_web.pdf},
  journal = {IEEE/ACM Transactions on Networking},
  keywords = {Cellular networks,mobile Web performance,SPDY,wireless protocol},
  number = {6}
}

@techreport{ernstPredicateDispatchingUniied,
  title = {Predicate {{Dispatching}}: {{A Uniied Theory}} of {{Dispatch}}},
  author = {Ernst, Michael and Kaplan, Craig and Chambers, Craig},
  pages = {186--211},
  abstract = {Predicate dispatching generalizes previous method dispatch mechanisms by permitting arbitrary predicates to control method applicability and by using logical implication between predicates as the overriding relationship. The method selected to handle a message send can depend not just on the classes of the arguments, as in ordinary object-oriented dispatch, but also on the classes of subcomponents, on an argument's state, and on relationships between objects. This simple mechanism subsumes and extends object-oriented single and multiple dispatch, ML-style pattern matching, predicate classes, and classiiers, which can all be regarded as syntactic sugar for predicate dispatching. This paper introduces predicate dispatching, gives motivating examples, and presents its static and dynamic semantics. An implementation of predicate dispatching is available.},
  file = {D\:\\GDrive\\zotero\\Ernst\\ernst_predicate_dispatching.pdf}
}

@techreport{erpekDeepLearningWireless,
  title = {Deep {{Learning}} for {{Wireless Communications}}},
  author = {Erpek, Tugba and O'shea, Timothy J and Sagduyu, Yalin E and Shi, Yi and Clancy, T Charles},
  abstract = {Existing communication systems exhibit inherent limitations in translating theory to practice when handling the complexity of optimization for emerging wireless applications with high degrees of freedom. Deep learning has a strong potential to overcome this challenge via data-driven solutions and improve the performance of wireless systems in utilizing limited spectrum resources. In this chapter, we first describe how deep learning is used to design an end-to-end communication system using autoencoders. This flexible design effectively captures channel impairments and optimizes transmitter and receiver operations jointly in single-antenna, multiple-antenna, and multiuser communications. Next, we present the benefits of deep learning in spectrum situation awareness ranging from channel modeling and estimation to signal detection and classification tasks. Deep learning improves the performance when the model-based methods fail. Finally, we discuss how deep learning applies to wireless communication security. In this context, adversarial machine learning provides novel means to launch and defend against wireless attacks. These applications demonstrate the power of deep learning in providing novel means to design, optimize, adapt, and secure wireless communications. Index Terms Deep learning, wireless systems, physical layer, end-to-end communication, signal detection and classification, wireless security. I. INTRODUCTION It is of paramount importance to deliver information in wireless medium from one point to another quickly, reliably, and securely. Wireless communications is a field of rich expert knowledge that involves designing waveforms (e.g., long-term evolution (LTE) and fifth generation mobile communications systems (5G)), modeling channels (e.g., multipath fading), handling interference (e.g., jamming) and traffic (e.g., network congestion) effects, compensating for radio hardware imperfections (e.g., RF front end non-linearity), developing communication chains (i.e., transmitter and receiver), recovering distorted symbols and bits (e.g., forward error correction), and supporting wireless security (e.g., jammer detection). The design and implementation of conventional communication systems are built upon strong probabilistic analytic models and assumptions. However, existing communication theories exhibit strong limitations in utilizing limited spectrum resources and handling the complexity of optimization for emerging wireless applications (such as spectrum sharing, multimedia, Internet of Things (IoT), virtual and augmented reality), each with high degrees of freedom. Instead of following a rigid design, new generations of wireless systems empowered by cognitive radio [1] can learn from spectrum data, and optimize their spectrum utilization to enhance their performance. These smart communication systems rely on various detection, classification, and prediction tasks such as signal detection and signal type identification in spectrum sensing to increase situational awareness. To achieve the tasks set forth in this vision, machine learning (especially deep learning) provides powerful automated means for communication systems to learn from spectrum data and adapt to spectrum dynamics [2]. Wireless communications combine various waveform, channel, traffic, and interference effects, each with its own complex structures that quickly change over time, as illustrated in Fig. 1. The data underlying wireless communications come in large volumes and at high rates, e.g., gigabits per second in 5G, and},
  file = {D\:\\GDrive\\zotero\\Erpek\\erpek_deep_learning_for_wireless_communications.pdf}
}

@techreport{escrivaWarpMultiKeyTransactions,
  title = {Warp: {{Multi}}-{{Key Transactions}} for {{Key}}-{{Value Stores}}},
  author = {Escriva, Robert and Wong, Bernard and Sirer, G{\"u}n},
  abstract = {Implementing ACID transactions has been a long-standing challenge for NoSQL systems. Because these systems are based on a sharded architecture, transactions necessarily require coordination across multiple servers. Past work in this space has relied either on heavyweight protocols such as Paxos or clock synchronization for this coordination. This paper presents a novel protocol for coordinating distributed transactions with ACID semantics on top of a sharded data store. Called linear transactions, this protocol achieves scalability by distributing the coordination task to only those servers that hold relevant data for each transaction. It achieves high performance by serializing only those transactions whose concurrent execution could potentially yield a violation of ACID semantics. Finally, it naturally integrates chain-replication and can thus tolerate faults of both clients and servers. We have fully implemented linear transactions in a commercially available data store. Experiments show that the throughput of this system achieves 1-9\texttimes{} more through-put than MongoDB, Cassandra and HyperDex on the Ya-hoo! Cloud Serving Benchmark, even though none of the latter systems provide transactional guarantees.},
  file = {D\:\\GDrive\\zotero\\Escriva\\escriva_warp.pdf}
}

@techreport{ETHEREUMSECUREDECENTRALISED,
  title = {{{ETHEREUM}}: {{A SECURE DECENTRALISED GENERALISED TRANSACTION LEDGER}}},
  abstract = {The blockchain paradigm when coupled with cryptographically-secured transactions has demonstrated its utility through a number of projects, with Bitcoin being one of the most notable ones. Each such project can be seen as a simple application on a decentralised, but singleton, compute resource. We can call this paradigm a transactional singleton machine with shared-state. Ethereum implements this paradigm in a generalised manner. Furthermore it provides a plurality of such resources, each with a distinct state and operating code but able to interact through a message-passing framework with others. We discuss its design, implementation issues, the opportunities it provides and the future hurdles we envisage.}
}

@book{eurosys12proceedingscommitteePracticalTDMADatacenter2013,
  title = {Practical  {{TDMA}}  for  {{Datacenter}}  {{Ethernet}}},
  author = {{Eurosys 12 Proceedings Committee}},
  year = {2013},
  publisher = {{Association for Computing Machinery}},
  file = {D\:\\GDrive\\zotero\\Eurosys 12 Proceedings Committee\\eurosys_12_proceedings_committee_2013_practical_tdma_for_datacenter_ethernet.pdf},
  isbn = {978-1-4503-1223-3}
}

@article{evansBitcoinIslamicBanking2015,
  title = {Bitcoin in {{Islamic Banking}} and {{Finance}}},
  author = {Evans, Charles W.},
  year = {2015},
  volume = {3},
  pages = {1--11},
  issn = {23742666},
  doi = {10.15640/jibf.v3n1a1},
  abstract = {This paper analyzes the compliance of distributed, autonomous block chain management systems (BMS) like Bitcoin\textemdash also referred to as 'virtual currencies'\textemdash with the requirements of Islamic Banking and Finance. While intended as a narrow financial and economic analysis, and not as an in-depth analysis of the subtleties and nuances of Shari'a as they relate to banking and finance, it shows that a BMS can conform with the prohibition of riba (usury) and incorporate the principles of maslaha (social benefits of positive externalities) and mutualrisk-sharing (as opposed to risk-shifting). It concludes that Bitcoin or a similar system might be a more appropriate medium of exchange in Islamic Banking and Finance than riba-backed central bank fiat currency, especially among the unbanked and in small-scale cross-border trade.},
  file = {D\:\\GDrive\\zotero\\Evans\\evans_2015_bitcoin_in_islamic_banking_and_finance.pdf},
  journal = {Journal of Islamic Banking and Finance},
  keywords = {bitcoin,islamic banking,islamic finance,virtual currency},
  number = {1}
}

@inproceedings{evansControlJujutsuWeaknesses2015,
  title = {Control {{Jujutsu}}: {{On}} the {{Weaknesses}} of {{Fine}}-{{Grained Control Flow Integrity}}},
  shorttitle = {Control {{Jujutsu}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Evans, Isaac and Long, Fan and Otgonbaatar, Ulziibayar and Shrobe, Howard and Rinard, Martin and Okhravi, Hamed and {Sidiroglou-Douskos}, Stelios},
  year = {2015},
  month = oct,
  pages = {901--913},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/2810103.2813646},
  file = {D\:\\GDrive\\zotero\\Evans et al\\evans_et_al_2015_control_jujutsu.pdf},
  isbn = {978-1-4503-3832-5},
  language = {en}
}

@book{evansHowWriteBetter2014,
  title = {How to {{Write}} a {{Better Thesis}}},
  author = {Evans, David and Gruba, Paul and Zobel, Justin},
  year = {2014},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-04286-2},
  file = {D\:\\GDrive\\zotero\\Evans et al\\evans_et_al_2014_how_to_write_a_better_thesis.pdf},
  isbn = {978-3-319-04285-5 978-3-319-04286-2},
  language = {en}
}

@article{evansImprovingSecurityUsing2002,
  title = {Improving Security Using Extensible Lightweight Static Analysis},
  author = {Evans, David and Larochelle, David},
  year = {2002},
  volume = {19},
  pages = {42--51},
  publisher = {{IEEE}},
  issn = {07407459},
  doi = {10.1109/52.976940},
  file = {D\:\\GDrive\\zotero\\Evans\\evans_2002_improving_security_using_extensible_lightweight_static_analysis.pdf},
  journal = {IEEE Software},
  number = {1}
}

@article{evansUserAuthenticationScheme1974,
  title = {A {{User Authentication Scheme}} Not {{Requiring Secrecy}} in the {{Computer}}},
  author = {Evans, Arthur and Kantrowitz, William and Weiss, Edwin},
  year = {1974},
  volume = {17},
  pages = {437--442},
  issn = {15577317},
  doi = {10.1145/361082.361087},
  abstract = {In many computer operating systems a user authenticates himself by entering a secret password known solely to himself and the system. The system compares this password with one recorded in a Password Table which is available to only the authentication program. The integrity of the system depends on keeping the table secret. In this paper a password scheme is presented which does not require secrecy in the computer. All aspects of the system, including all relevant code and data bases, may be known by anyone attempting to intrude. The scheme is based on using a function H which the would-be intruder is unable to invert. This function is applied to the user's password and the result compared to a table entry, a match being interpreted as authentication of the user. The intruder may know all about H and have access to the table, but he can penetrate the system only if he can invert H to determine an input that produces a given output. This paper discusses issues surrounding selection of a suitable H. Two different plausible arguments are given that penetration would be exceedingly difficult, and it is then argued that more rigorous results are unlikely. Finally, some human engineering problems relating to the scheme are discussed. \textcopyright{} 1974, ACM. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Evans\\evans_1974_a_user_authentication_scheme_not_requiring_secrecy_in_the_computer.pdf;D\:\\GDrive\\zotero\\Evans\\evans_1974_a_user_authentication_scheme_not_requiring_secrecy_in_the_computer2.pdf},
  journal = {Communications of the ACM},
  keywords = {authentication,cryptology,one-way encryption,operating system security,passwords,security},
  number = {8}
}

@article{evtyushkinJumpASLRAttacking2016,
  title = {Jump {{Over ASLR}}: {{Attacking Branch Predictors}} to {{Bypass ASLR}}},
  author = {Evtyushkin, Dmitry and Ponomarev, Dmitry and {Abu-Ghazaleh}, Nael},
  year = {2016},
  abstract = {Address Space Layout Randomization (ASLR) is a widely-used technique that protects systems against a range of attacks. ASLR works by randomizing the offset of key program segments in virtual memory, making it difficult for an attacker to derive the addresses of specific code objects and consequently redirect the control flow to this code. In this paper, we develop an attack to derive kernel and user-level ASLR offset using a side-channel attack on the branch target buffer (BTB). Our attack exploits the observation that an adversary can create BTB collisions between the branch instructions of the attacker process and either the user-level victim process or on the kernel executing on its behalf. These collisions, in turn, can impact the timing of the attacker's code, allowing the attacker to identify the locations of known branch instructions in the address space of the victim process or the kernel. We demonstrate that our attack can reliably recover kernel ASLR in about 60 milliseconds when performed on a real Haswell processor running a recent version of Linux. Finally, we describe several possible protection mechanisms, both in software and in hardware.},
  file = {D\:\\GDrive\\zotero\\Evtyushkin\\evtyushkin_2016_jump_over_aslr.pdf;D\:\\GDrive\\zotero\\Evtyushkin\\evtyushkin_jump_over_aslr.pdf},
  keywords = {Address Space Layout Randomization,Bypass,Exploit Mitigation,Kernel Vulnerabilities,Side Channel,Timing Attacks,Timing Channel}
}

@techreport{ExploringBlockchainTechnologyBitcoin2018,
  title = {Exploring {{Blockchain}}-{{Technology}} behind {{Bitcoin}} and {{Implications}} for {{Transforming Transportation Final}} Report {{Exploring Blockchain}}-{{Technology}} behind {{Bitcoin}} and {{Implications}} for {{Transforming Transportation}}},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\undefined\\2018_exploring_blockchain-technology_behind_bitcoin_and_implications_for.pdf}
}

@book{eyalBitcoinNGScalableBlockchain,
  title = {Bitcoin-{{NG}}: {{A Scalable Blockchain Protocol}}},
  author = {Eyal, Ittay and Gencer, Adem Efe and Sirer, G{\"u}n and Van Renesse, Robbert and Efe, Adem and Emin, Gencer},
  abstract = {Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous on-line payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade off between throughput and latency, which withhold the realization of this potential. This paper presents Bitcoin-NG (Next Generation), a new blockchain protocol designed to scale. Bitcoin-NG is a Byzantine fault tolerant blockchain protocol that is robust to extreme churn and shares the same trust model as Bitcoin. In addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15\% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network.},
  file = {D\:\\GDrive\\zotero\\Eyal\\eyal_bitcoin-ng.pdf},
  isbn = {978-1-931971-29-4}
}

@article{eyalMajorityNotEnough2018,
  title = {Majority {{Is Not Enough}}: {{Bitcoin Mining Is Vulnerable}}},
  author = {Eyal, Ittay and Sirer, Emin G{\"u}n},
  year = {2018},
  volume = {61},
  doi = {10.1145/3212998},
  abstract = {The Bitcoin cryptocurrency records its transactions in a public log called the blockchain. Its security rests critically on the distributed protocol that maintains the blockchain, run by participants called miners. Conventional wisdom asserts that the mining protocol is incentive-compatible and secure against colluding minority groups, that is, it incentiv-izes miners to follow the protocol as prescribed. We show that the Bitcoin mining protocol is not incentive-compatible. We present an attack with which colluding miners' revenue is larger than their fair share. The attack can have significant consequences for Bitcoin: Rational miners will prefer to join the attackers, and the colluding group will increase in size until it becomes a majority. At this point, the Bitcoin system ceases to be a decentralized currency. Unless certain assumptions are made, selfish mining may be feasible for any coalition size of colluding miners. We propose a practical modification to the Bitcoin protocol that protects Bitcoin in the general case. It prohibits selfish mining by a coalition that command less than 1/4 of the resources. This threshold is lower than the wrongly assumed 1/2 bound, but better than the current reality where a coalition of any size can compromise the system.},
  file = {D\:\\GDrive\\zotero\\Eyal\\eyal_2018_majority_is_not_enough.pdf},
  journal = {COMMUNICATIONS OF THE ACM},
  number = {7}
}

@techreport{fakultatEndtoEndArgumentsInternet2010,
  title = {End-to-{{End Arguments}} in the {{Internet}}: {{Principles}}, {{Practices}}, and {{Theory}} Vorgelegt von {{M}} a t t h i a s {{B}} \"a r w o l f f},
  author = {Fakult{\"a}t, Der and Clark, David D},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Fakultät\\fakultät_2010_end-to-end_arguments_in_the_internet.pdf}
}

@techreport{fallDelayTolerantNetworkArchitecture2003,
  title = {A {{Delay}}-{{Tolerant Network Architecture}} for {{Challenged Internets}}},
  author = {Fall, Kevin},
  year = {2003},
  abstract = {The highly successful architecture and protocols of today's Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.},
  file = {D\:\\GDrive\\zotero\\Fall\\fall_2003_a_delay-tolerant_network_architecture_for_challenged_internets.pdf},
  keywords = {C21 [Network Architecture and Design]: Computer Networks,Design,Network Architecture General Terms: Algorithms,Security}
}

@phdthesis{fangDesignHighData2020,
  title = {Design of a {{High Data Rate}} and {{Low}}-{{Latency 60GHz Communication System}}},
  author = {Fang, Zhongyuan and Lou, Liheng and Tang, Kai and Wang, Wensong and Wang, Yisheng and Guo, Ting and Yang, Chuanshi and Zheng, Yuanjin},
  year = {2020},
  month = sep,
  file = {D\:\\GDrive\\zotero\\Fang et al\\fang_et_al_2020_design_of_a_high_data_rate_and_low-latency_60ghz_communication_system.pdf},
  language = {en}
}

@article{FastLearningAlgorithm,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Net}}},
  file = {D\:\\GDrive\\zotero\\undefined\\a_fast_learning_algorithm_for_deep_belief_net.pdf}
}

@techreport{feamsterRoadSDNIntellectual,
  title = {The {{Road}} to {{SDN An}} Intellectual History of Programmable Networks},
  author = {Feamster, Nick and Rexford, Jennifer and Zegura, Ellen},
  abstract = {Designing and managing networks has become more innovative over the past few years with the aid of SDN (software-defined networking). This technology seems to have appeared suddenly, but it is actually part of a long history of trying to make computer networks more programmable. Computer networks are complex and difficult to manage. They involve many kinds of equipment, from routers and switches to middleboxes such as firewalls, network address translators, server load balancers, and intrusion-detection systems. Routers and switches run complex, distributed control software that is typically closed and proprietary. The software implements network protocols that undergo years of standardization and interoperability testing. Network administrators typically configure individual network devices using configuration interfaces that vary between vendors-and even between different products from the same vendor. Although some network-management tools offer a central vantage point for configuring the network, these systems still operate at the level of individual protocols, mechanisms, and configuration interfaces. This mode of operation has slowed innovation, increased complexity, and inflated both the capital and the operational costs of running a network. SDN is changing the way networks are designed and managed. It has two defining characteristics. First, SDN separates the control plane (which decides how to handle the traffic) from the data plane (which forwards traffic according to decisions that the control plane makes). Second, SDN consolidates the control plane, so that a single software control program controls multiple data-plane elements. The SDN control plane exercises direct control over the state in the network's data-plane elements (i.e., routers, switches, and other middleboxes) via a well-defined API. OpenFlow 51 is a prominent example of such an API. An OpenFlow switch has one or more tables of packet-handling rules. Each rule matches a subset of traffic and performs certain actions on the traffic that matches a rule; actions include dropping, forwarding, or flooding. Depending on the rules installed by a controller application, an OpenFlow switch can behave as a router, switch, firewall, network address translator, or something in between. SDN has gained significant traction in the industry. Many commercial switches support the OpenFlow API. HP, NEC, and Pronto were among the first vendors to support OpenFlow; this list has since expanded dramatically. Many different controller platforms have emerged. 23, 30, 37, 46, 55, 63, 80 Programmers have used these platforms to create many applications, such as dynamic access control, 16,53 server load balancing, 39,81 network virtualization, 54,67 energy-efficient networking, 42 and seamless virtual-machine migration and user mobility. 24 Early commercial successes, such as Google's wide-area traffic-management system 44 and Nicira's Network Virtualization Platform, 54 have garnered significant industry attention. Many of the world's largest information-technology},
  file = {D\:\\GDrive\\zotero\\Feamster\\feamster_the_road_to_sdn_an_intellectual_history_of_programmable_networks.pdf}
}

@book{feamsterTheCaseforSeparatingRoutingfromRouters2004,
  title = {{{TheCaseforSeparatingRoutingfromRouters}}},
  author = {Feamster, Nick},
  year = {2004},
  publisher = {{ACM}},
  file = {D\:\\GDrive\\zotero\\Feamster\\feamster_2004_thecaseforseparatingroutingfromrouters.pdf;D\:\\GDrive\\zotero\\Feng\\feng_2004_the_case_for_separating_routing_from_routers.pdf},
  isbn = {1-58113-942-X}
}

@techreport{feitelsonHowDevelopersChoose2021,
  title = {How {{Developers Choose Names}}},
  author = {Feitelson, Dror G and Mizrahi, Ayelet and Noy, Nofar and Ben, Aviad and Or, Shabat and Sheffer, Eliyahu Roy},
  year = {2021},
  abstract = {The names of variables and functions serve as implicit documentation and are instrumental for program comprehension. But choosing good meaningful names is hard. We perform a sequence of experiments in which a total of 334 subjects are required to choose names in given programming scenarios. The first experiment shows that the probability that two developers would select the same name is low: in the 47 instances in our experiments the median probability was only 6.9\%. At the same time, given that a specific name is chosen, it is usually understood by the majority of developers. Analysis of the names given in the experiment suggests a model where naming is a (not necessarily cognizant or serial) three-step process: (1) selecting the concepts to include in the name, (2) choosing the words to represent each concept, and (3) constructing a name using these words. A followup experiment, using the same experimental setup, then checked whether using this model explicitly can improve the quality of names. The results were that names selected by subjects using the model were judged by two independent judges to be superior to names chosen in the original experiment by a ratio of two-to-one. Using the model appears to encourage the use of more concepts and longer names.},
  file = {D\:\\GDrive\\zotero\\Feitelson\\feitelson_2021_how_developers_choose_names.pdf},
  keywords = {code comprehension,Index Terms-variable naming}
}

@techreport{felberTransactifyingApplicationsUsing,
  title = {Transactifying {{Applications}} Using an {{Open Compiler Framework}}},
  author = {Felber, Pascal and Fetzer, Christof and M{\"u}ller, Ulrich and Riegel, Torvald and S{\"u}{\ss}kraut, Martin and Sturzrehm, Heiko},
  abstract = {Transactional memory dramatically reduces the complexity of writing concurrent code. Yet, seamless integration of transactional constructs in application code typically comes with a significant performance penalty. Recent studies have shown that compiler support allows producing highly efficient STM-based applications without putting the hassle on the programmer. So far, STM integration has been partially implemented in custom, proprietary compiler infrastruc-tures. In this paper, we propose and evaluate the use of the LLVM open compiler framework to generate efficient concurrent applications using word-based STM libraries. Since LLVM uses the GCC compiler suite as front-end, it can process code written in C or C++ (with partial support for other languages). We also present a tool that allows "transactify-ing" assembly code and can complement LLVM for legacy code and libraries. Experiments using a lightweight C word-based STM library show that LLVM integration performs as well as hand-optimized calls to the STM library and better than assembly code instrumentation of the application code.},
  file = {D\:\\GDrive\\zotero\\Felber\\felber_transactifying_applications_using_an_open_compiler_framework.pdf}
}

@techreport{felleisenConsiderationPublicationFunctional,
  title = {Under Consideration for Publication in {{J}}. {{Functional Programming The Structure}} and {{Interpretation}} of the {{Computer Science Curriculum}}},
  author = {Felleisen, Matthias and Findler, Robert Bruce and Flatt, Matthew and Krishnamurthi, Shriram},
  abstract = {Twenty years ago Abelson and Sussman's Structure and Interpretation of Computer Programs radically changed the intellectual landscape of introductory computing courses. Instead of teaching some currently fashionable programming language, it employed Scheme and functional programming to teach important ideas. Introductory courses based on the book showed up around the world and made Scheme and functional programming popular. Unfortunately, these courses quickly disappeared again due to shortcomings of the book and the whimsies of Scheme. Worse, the experiment left people with a bad impression of Scheme and functional programming in general. In this pearl, we propose an alternative role for functional programming in the first-year curriculum. Specifically, we present a framework for discussing the first-year curriculum and, based on it, the design rationale for our book and course, dubbed How to Design Programs. The approach emphasizes the systematic design of programs. Experience shows that it works extremely well as a preparation for a course on object-oriented programming. 1 History and critique The publication of Abelson and Sussman's Structure and Interpretation of Computer Programs (sicp) (Abelson et al., 1985) revolutionized the landscape of the introductory computing curriculum in the 1980s. Most importantly, the book liberated the introductory course from the tyranny of syntax. Instead of arranging a course around the syntax of a currently fashionable programming language, sicp focused the first course on the study of important ideas in computing: functional abstraction , data abstraction, streams, data-directed programming, implementation of message-passing objects, interpreters, compilers, and register machines. Over a short period, many universities in the US and around the world switched their first course to sicp and Scheme. The book became a major bestseller for MIT Press. 1 Along with sicp, the Scheme programming language (Sussman \& Steele Jr., 1 According to Bob Prior (editor at MIT Press), sicp sold 45,000 copies in its first five years [personal communication, 9 June 2003].},
  file = {D\:\\GDrive\\zotero\\Felleisen\\felleisen_under_consideration_for_publication_in_j.pdf}
}

@techreport{feltAndroidPermissionsUser2012,
  title = {Android {{Permissions}}: {{User Attention}}, {{Comprehension}}, and {{Behavior}}},
  author = {Felt, Adrienne Porter and Ha, Elizabeth and Egelman, Serge and Haney, Ariel and Chin, Erika and Wagner, David},
  year = {2012},
  abstract = {Android's permission system is intended to inform users about the risks of installing applications. When a user installs an application, he or she has the opportunity to review the application's permission requests and cancel the installation if the permissions are excessive or objectionable. We examine whether the Android permission system is effective at warning users. In particular, we evaluate whether Android users pay attention to, understand, and act on permission information during installation. We performed two usability studies: an Internet survey of 308 Android users, and a laboratory study wherein we interviewed and observed 25 Android users. Study participants displayed low attention and comprehension rates: both the Internet survey and laboratory study found that 17\% of participants paid attention to permissions during installation, and only 3\% of In-ternet survey respondents could correctly answer all three permission comprehension questions. This indicates that current Android permission warnings do not help most users make correct security decisions. However, a notable minority of users demonstrated both awareness of permission warnings and reasonable rates of comprehension. We present recommendations for improving user attention and comprehension, as well as identify open challenges.},
  file = {D\:\\GDrive\\zotero\\Felt\\felt_android_permissions.pdf},
  keywords = {ss}
}

@techreport{feltenTimingAttacksWeb2000,
  title = {Timing {{Attacks}} on {{Web Privacy}}},
  author = {Felten, Edward W and Schneider, Michael A},
  year = {2000},
  abstract = {We describe a class of attacks that can compromise the privacy of users' Web-browsing histories. The attacks allow a malicious Web site to determine whether or not the user has recently visited some other, unrelated Web page. The malicious page can determine this information by measuring the time the user's browser requires to perform certain operations. Since browsers perform various forms of caching, the time required for operations depends on the user's browsing history; this paper shows that the resulting time variations convey enough information to compromise users' privacy. This attack method also allows other types of information gathering by Web sites, such as a more invasive form of Web "cookies". The attacks we describe can be carried out without the victim's knowledge , and most "anonymous browsing" tools fail to prevent them. Other simple countermeasures also fail to prevent these attacks. We describe a way of reengineering browsers to prevent most of them.},
  file = {D\:\\GDrive\\zotero\\Felten\\felten_2000_timing_attacks_on_web_privacy.pdf}
}

@misc{feltHowWriteResearch,
  title = {How to {{Write}} a {{Research Paper}}: {{A}} Guide for Software Engineers and Practitioners},
  author = {Felt, Adrienne Porter},
  file = {D\:\\GDrive\\zotero\\Felt\\felt_how_to_write_a_research_paper.pdf}
}

@article{feltImprovingSSLWarnings2015,
  title = {Improving {{SSL Warnings}}: {{Comprehension}} and {{Adherence}}},
  author = {Felt, Adrienne Porter and Ainslie, Alex and Reeder, Robert W and Consolvo, Sunny and Thyagaraja, Somas and Bettes, Alan and Harris, Helen and Grimes, Jeff},
  year = {2015},
  doi = {10.1145/2702123.2702442},
  abstract = {Browsers warn users when the privacy of an SSL/TLS connection might be at risk. An ideal SSL warning would empower users to make informed decisions and, failing that, guide confused users to safety. Unfortunately, users struggle to understand and often disregard real SSL warnings. We report on the task of designing a new SSL warning, with the goal of improving comprehension and adherence. We designed a new SSL warning based on recommendations from warning literature and tested our proposal with micro-surveys and a field experiment. We ultimately failed at our goal of a well-understood warning. However, nearly 30\% more total users chose to remain safe after seeing our warning. We attribute this success to opinionated design, which promotes safety with visual cues. Subsequently, our proposal was released as the new Google Chrome SSL warning. We raise questions about warning comprehension advice and recommend that other warning designers use opinionated design.},
  file = {D\:\\GDrive\\zotero\\Felt et al\\felt_et_al_2015_improving_ssl_warnings.pdf},
  isbn = {9781450331456},
  keywords = {design,Google Consumer Surveys,HTTPS,microsurveys,security,SSL,TLS/SSL,warnings}
}

@article{fengSurveyPrivacyProtection2019,
  title = {A Survey on Privacy Protection in Blockchain System},
  author = {Feng, Qi and He, Debiao and Zeadally, Sherali and Khurram Khan, Muhammad and Kumar, Neeraj},
  year = {2019},
  volume = {126},
  pages = {45--58},
  doi = {10.1016/j.jnca.2018.10.020},
  abstract = {Blockchain, as a decentralized and distributed public ledger technology in peer-to-peer network, has received considerable attention recently. It applies a linked block structure to verify and store data, and applies the trusted consensus mechanism to synchronize changes in data, which makes it possible to create a tamper-proof digital platform for storing and sharing data. It is believed that blockchain can be utilized in diverse Internet interactive systems (e.g., Internet of Things, supply chain systems, identity management, and so on). However, there are some privacy challenges that may hinder the applications of blockchain. The goal of this survey is to provide some insights into the privacy issues associated with blockchain. We analyze the privacy threats in blockchain and discuss existing cryptographic defense mechanisms, i.e., anonymity and transaction privacy preservation. Furthermore, we summarize some typical implementations of privacy preservation mechanisms in blockchain and explore future research challenges that still need to be addressed in order to preserve privacy when blockchain is used.},
  file = {D\:\\GDrive\\zotero\\Feng\\feng_2019_a_survey_on_privacy_protection_in_blockchain_system.pdf},
  journal = {Journal of Network and Computer Applications},
  keywords = {Anonymity,Blockchain,Cryptocurrency,Cryptography,Privacy}
}

@techreport{ferranteProgramDependenceGraph1987,
  title = {The {{Program Dependence Graph}} and {{Its Use}} in {{Optimization}}},
  author = {Ferrante, Jeanne and Warren, Joe D},
  year = {1987},
  abstract = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependence5 for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependence5 are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
  file = {D\:\\GDrive\\zotero\\Ferrante_Warren\\ferrante_warren_the_program_dependence_graph_and_its_use_in_optimization.pdf},
  keywords = {branch deletion,code motion,D34 [Programming Languages]: Processors-compilers,debugging,dependence analysis,incremental data flow analysis,intermediate program represen-tation,internal program form,Languages,loop fusion,loop unrolling,node splitting,optimization General Terms: Algorithms,parallelism,Performance Additional Key Words and Phrases: Data flow,slicing,vectorization}
}

@article{ferstImplementationSecureCommunication2019,
  title = {Implementation of Secure Communication with Modbus and Transport Layer Security Protocols},
  author = {Ferst, Matheus K. and De Figueiredo, Hugo F.M. and Denardin, Gustavo and Lopes, Juliano},
  year = {2019},
  pages = {155--162},
  publisher = {{IEEE}},
  doi = {10.1109/INDUSCON.2018.8627306},
  abstract = {Industrial Control Systems (ICS) and Supervisory Control systems and Data Acquisition (SCADA) networks implement industrial communication protocols to enable their operations. Unfortunately, wide used protocols, such as Modbus and DNP3, lack basic security mechanisms that lead to multiple vulnerabilities. The exploitation of such flaws may greatly impact companies and the general population, especially for attacks targeting critical infrastructural assets such as power plants, water distribution, and railway transportation systems. Such problem gets worse in the context of photovoltaic Distributed Energy Resources (DER), where devices are commonly located in customers facilities, making difficult to enforce appropriate security policies. This paper addresses the security problems of the Modbus protocol, proposing a new secure version based on the Transport Layer Security protocol. Experimental results shows that the proposed solution achieves request/response times way below the 16.67 ms period of the power grid 60 Hz cycle, revealing a negligible impact in power grids applications.},
  file = {D\:\\GDrive\\zotero\\Ferst\\ferst_2019_implementation_of_secure_communication_with_modbus_and_transport_layer_security.pdf},
  isbn = {9781538679951},
  journal = {2018 13th IEEE International Conference on Industry Applications, INDUSCON 2018 - Proceedings},
  keywords = {Modbus,SCADA,Security,TLS}
}

@phdthesis{fieldingArchitecturalStylesDesign2000,
  title = {Architectural {{Styles}} and the {{Design}} of {{Network}}-Based {{Software Architectures}}},
  author = {Fielding, Roy Thomas},
  year = {2000},
  file = {D\:\\GDrive\\zotero\\Fielding\\fielding_2000_architectural_styles_and_the_design_of_network-based_software_architectures.pdf}
}

@article{fieldingArchitecturalStylesDesign2000a,
  title = {Architectural {{Styles}} and the {{Design}} of {{Network}}-Based {{Software Architectures}}},
  author = {Fielding, Roy Thomas},
  year = {2000},
  pages = {180},
  file = {D\:\\GDrive\\zotero\\Fielding\\fielding_2000_architectural_styles_and_the_design_of_network-based_software_architectures2.pdf},
  language = {en}
}

@article{filieriModelCountingComplex2015,
  title = {Model Counting for Complex Data Structures},
  author = {Filieri, Antonio and Frias, Marcelo F. and P{\u a}s{\u a}reanu, Corina S. and Visser, Willem},
  year = {2015},
  volume = {9232},
  pages = {222--241},
  issn = {16113349},
  doi = {10.1007/978-3-319-23404-5_15},
  abstract = {We extend recent approaches for calculating the probability of program behaviors, to allow model counting for complex data structures with numeric fields. We use symbolic execution with lazy initialization to compute the input structures leading to the occurrence of a target event, while keeping a symbolic representation of the constraints on the numeric data. Off-the-shelf model counting tools are used to count the solutions for numerical constraints and field bounds encoding data structure invariants are used to reduce the search space. The technique is implemented in the Symbolic PathFinder tool and evaluated on several complex data structures. Results show that the technique is much faster than an enumeration-based method that uses the Korat tool and also highlight the benefits of using the field bounds to speed up the analysis.},
  file = {D\:\\GDrive\\zotero\\Filieri\\filieri_2015_model_counting_for_complex_data_structures.pdf},
  isbn = {9783319234038},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Model counting,Probabilistic software analysis,Symbolic Execution}
}

@techreport{flattComposableCompilableMacros2002,
  title = {Composable and {{Compilable Macros You Want}} It {{When}}?},
  author = {Flatt, Matthew},
  year = {2002},
  abstract = {Many macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compile-time macro transformers is usually the same as the language for implementing run-time functions. As a side effect of this sharing, implementations tend to allow the mingling of compile-time values and run-time values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macro-definition protocols and by inserting phase-distinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme-the language of the PLT Scheme tool suite-addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.},
  file = {D\:\\GDrive\\zotero\\Flatt\\flatt_2002_composable_and_compilable_macros_you_want_it_when.pdf},
  keywords = {D212 [Software Engineering]: Interoperability General Terms Languages; Design Keywords Macros; modules; language tower,D33 [Software]: Programming Languages-language constructs and features; Scheme,D34 [Software]: Processors-parsing; pre-processors}
}

@article{floydAssigningMeaningsPrograms1967,
  title = {Assigning Meanings to Programs},
  author = {Floyd, Robert W.},
  year = {1967},
  pages = {19--32},
  doi = {10.1090/psapm/019/0235771},
  abstract = {This paper attempts to provide an adequate basis for formal definitions of the meanings of programs in appropriately defined programming languages, in such a way that a rigorous standard is established for proofs about computer programs, including proofs of correctness, equivalence, and termination. The basis of our approach is the notion of an interpretation of a program: that is, an association of a proposition with each connection in the flow of control through a program, where the proposition is asserted to hold whenever that connection is taken. To prevent an interpretation from being chosen arbitrarily, a condition is imposed on each command of the program. This condition guarantees that whenever a command is reached by way of a connection whose associated proposition is then true, it will be left (if at all) by a connection whose associated proposition will be true at that time. Then by induction on the number of commands executed, one sees that if a program is entered by a connection whose associated proposition is then true, it will be left (if at all) by a connection whose associated proposition will be true at that time. By this means, we may prove certain properties of programs, particularly properties of the form: `If the initial values of the program variables satisfy the relation R l, the final values on completion will satisfy the relation R 2'.},
  file = {D\:\\GDrive\\zotero\\Floyd\\floyd_1967_assigning_meanings_to_programs.pdf}
}

@techreport{floydEquationBasedCongestionControl,
  title = {Equation-{{Based Congestion Control}} for {{Unicast Applications}}},
  author = {Floyd, Sally and Handley, Mark and Padhye, Jitendra and Org Widmer, J {\textasciidieresis}},
  abstract = {This paper proposes a mechanism for equation-based congestion control for unicast traffic. Most best-effort traffic in the current Internet is well-served by the dominant transport protocol, TCP. However, traffic such as best-effort unicast streaming multimedia could find use for a TCP-friendly congestion control mechanism that refrains from reducing the sending rate in half in response to a single packet drop. With our mechanism, the sender explicitly adjusts its sending rate as a function of the measured rate of loss events, where a loss event consists of one or more packets dropped within a single round-trip time. We use both simulations and experiments over the Internet to explore performance. We consider equation-based congestion control a promising avenue of development for congestion control of multicast traffic, and so an additional motivation for this work is to lay a sound basis for the further development of multicast congestion control.},
  file = {D\:\\GDrive\\zotero\\Floyd\\floyd_equation-based_congestion_control_for_unicast_applications.pdf}
}

@techreport{floydReliableMulticastFramework1997,
  title = {A {{Reliable Multicast Framework}} for {{Light}}-Weight {{Sessions}} and {{Application Level Framing}}},
  author = {Floyd, Sally and Jacobson, Van and Liu, Ching-Gung and Mccanne, Steven and Zhang, Lixia},
  year = {1997},
  abstract = {This paper describes SRM (Scalable Reliable Multicast), a reliable multicast framework for lightweight sessions and application level framing. The algorithms of this framework are efficient, robust, and scale well to both very large networks and very large sessions. The SRM framework has been prototyped in wb, a distributed whiteboard application, which has been used on a global scale with sessions ranging from a few to a few hundred participants. The paper describes the principles that have guided the SRM design, including the IP multicast group delivery model, an end-to-end, receiver-based model of reliability, and the application level framing protocol model. As with unicast communications, the performance of a reliable multicast delivery algorithm depends on the underlying topol-ogy and operational environment. We investigate that dependence via analysis and simulation, and demonstrate an adaptive algorithm that uses the results of previous loss recovery events to adapt the control parameters used for future loss recovery. With the adaptive algorithm, our reliable multicast delivery algorithm provides good performance over a wide range of underlying topologies.},
  file = {D\:\\GDrive\\zotero\\Floyd\\floyd_1997_a_reliable_multicast_framework_for_light-weight_sessions_and_application_level.pdf},
  journal = {IEEE/ACM Transactions on Networking}
}

@techreport{Focus50IEEE,
  title = {Focus 50 {{IEEE SECURITY}} \& {{PRIVACY}}},
  abstract = {fections have been reported. The rogue driver DLL contained three controller code sets: two destined for a Siemens 315 controller, and the third looking for a 417 controller. The first two were almost identical and will be treated as one digital warhead in the following discussion. The 417 attack code was much more complex and four times the size of the 315 attack code. For starters, the 417 is Siemens' top-of-the-line controller product, whereas the 315 is a small, general-purpose controller. Controller Hijacking Once Stuxnet found a matching controller target, it loaded rogue code on the controller from one of its three code sets, along with any data blocks that the rogue code was working on. The rogue code was logically structured in a set of subfunctions; the attackers' goal was to get any of these functions called. Stuxnet achieved this goal by injecting code into an executive loop. Both the 315 and the 417 attack codes used the main cycle for this purpose-the 315 also injected code into the 100-ms timer. (The 100-ms timer is like an interrupt handler that the operating system calls automatically every 100 milliseconds .) We can think of a control-ler's main cycle as the main() function in a C program, with the notable difference being that the operating system automatically executes the main cycle, hence the name. The code injections got Stux-net in business-it could then do its thing and prevent legitimate controller code from doing anything useful. However, it didn't. Stuxnet was a stealth control system that resided on the controller alongside legitimate code, which continued to be executed; Stux-net only occasionally took over. Both on the 315 and on the 417, the attacks were implemented as state machines that didn't require any interaction with command-and-control servers. Instead, the attacks were triggered by complex timer and process conditions. In the initial state, attack code just stayed put and let the legitimate controller code do its thing, although it monitored it closely. When strike time came, rogue code took control without the legitimate controller code noticing. During the attack, the legitimate code was simply disabled. How this happened differed considerably for the 315 and the 417 attacks. For the 315 attack, execution of legitimate code simply halted during the strike condition, which can take up to 50 minutes. This is the infamous DEADFOOT condition that I first discovered and published on 16 Septem-ber 2010 (www.langner.com/ en/},
  file = {D\:\\GDrive\\zotero\\undefined\\focus_50_ieee_security_&_privacy.pdf}
}

@article{fogMicroarchitectureIntelAMD2012,
  title = {3. {{The}} Microarchitecture of \{\vphantom\}{{Intel}}\vphantom\{\}, \{\vphantom\}{{AMD}}\vphantom\{\} and \{\vphantom\}{{VIA}}\vphantom\{\} \{\vphantom\}{{CPU}}\vphantom\{\}s},
  author = {Fog, Agner},
  year = {2012},
  file = {D\:\\GDrive\\zotero\\Fog\\fog_2012_3.pdf},
  keywords = {AMD,assembly,Intel}
}

@inproceedings{fongPreventingSybilAttacks2011,
  title = {Preventing {{Sybil}} Attacks by {{Privilege Attenuation}}: {{A}} Design Principle for {{Social Network Systems}}},
  booktitle = {Proceedings - {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Fong, Philip W.L.},
  year = {2011},
  pages = {263--278},
  issn = {10816011},
  doi = {10.1109/SP.2011.16},
  abstract = {In Facebook-style Social Network Systems (FSNSs), which are a generalization of the access control model of Facebook, an access control policy specifies a graph-theoretic relationship between the resource owner and resource accessor that must hold in the social graph in order for access to be granted. Pseudonymous identities may collude to alter the topology of the social graph and gain access that would otherwise be forbidden. We formalize Denning's Principle of Privilege Attenuation (POPA) as a run-time property, and demonstrate that it is a necessary and sufficient condition for preventing the above form of Sybil attacks. A static policy analysis is then devised for verifying that an FSNS is POPA compliant (and thus Sybil free). The static analysis is proven to be both sound and complete. We also extend our analysis to cover a peculiar feature of FSNS, namely, what Fong et al. dubbed as Stage-I Authorization. We discuss the anomalies resulted from this extension, and point out the need to redesign Stage-I Authorization to support a rational POPA-compliance analysis. \textcopyright{} 2011 IEEE.},
  file = {D\:\\GDrive\\zotero\\Fong\\fong_2011_preventing_sybil_attacks_by_privilege_attenuation.pdf},
  isbn = {978-0-7695-4402-1},
  keywords = {Access control,Policy analysis,Principle of Privilege Attenuation,Social Network Systems,Soundness and completeness of static analysis,Sybil attacks}
}

@techreport{footePatternLanguagesProgram1999,
  title = {Pattern {{Languages}} of {{Program Design}} 4 Edited by {{Big Ball}} of {{Mud Contents Big Ball}} of {{Mud}}},
  author = {Foote, Brian and Yoder, Joseph and Harrison, Neil and {Addison-Wesley}, Hans Rohnert},
  year = {1999},
  abstract = {While much attention has been focused on high-level software architectural patterns, what is, in effect, the de-facto standard software architecture is seldom discussed. This paper examines this most frequently deployed of software architectures: the BIG BALL OF MUD. A BIG BALL OF MUD is a casually, even haphazardly, structured system. Its organization, if one can call it that, is dictated more by expediency than design. Yet, its enduring popularity cannot merely be indicative of a general disregard for architecture. These patterns explore the forces that encourage the emergence of a BIG BALL OF MUD, and the undeniable effectiveness of this approach to software architecture. What are the people who build them doing right? If more high-minded architectural approaches are to compete, we must understand what the forces that lead to a BIG BALL OF MUD are, and examine alternative ways to resolve them.},
  file = {D\:\\GDrive\\zotero\\Foote\\foote_1999_pattern_languages_of_program_design_4_edited_by_big_ball_of_mud_contents_big.pdf}
}

@techreport{fordAvailabilityGloballyDistributed,
  title = {Availability in {{Globally Distributed Storage Systems}}},
  author = {Ford, Daniel and Labelle, Fran{\c c}ois and Popovici, Florentina I and Stokely, Murray and Truong, Van-Anh and Barroso, Luiz and Grimes, Carrie and Quinlan, Sean},
  abstract = {Highly available cloud storage is often implemented with complex, multi-tiered distributed systems built on top of clusters of commodity servers and disk drives. Sophisticated management, load balancing and recovery techniques are needed to achieve high performance and availability amidst an abundance of failure sources that include software, hardware, network connectivity, and power issues. While there is a relative wealth of failure studies of individual components of storage systems, such as disk drives, relatively little has been reported so far on the overall availability behavior of large cloud-based storage services. We characterize the availability properties of cloud storage systems based on an extensive one year study of Google's main storage infrastructure and present statistical models that enable further insight into the impact of multiple design choices, such as data placement and replication strategies. With these models we compare data availability under a variety of system parameters given the real patterns of failures observed in our fleet.},
  file = {D\:\\GDrive\\zotero\\Ford\\ford_availability_in_globally_distributed_storage_systems.pdf}
}

@article{fordPackratParsingPractical2002,
  title = {Packrat {{Parsing}}: A {{Practical Linear}}-{{Time Algorithm}} with {{Backtracking}}},
  author = {Ford, Bryan},
  year = {2002},
  pages = {112},
  abstract = {Packrat parsing is a novel and practical method for implementing linear-time parsers for grammars defined in Top-Down Parsing Language (TDPL). While TDPL was originally created as a formal model for top-down parsers with backtracking capability, this thesis extends TDPL into a powerful general-purpose notation for describing language syntax, providing a compelling alternative to traditional context-free grammars (CFGs). Common syntactic idioms that cannot be represented concisely in a CFG are easily expressed in TDPL, such as longest-match disambiguation and ``syntactic predicates,'' making it possible to describe the complete lexical and grammatical syntax of a practical programming language in a single TDPL grammar.},
  file = {D\:\\GDrive\\zotero\\Ford\\ford_packrat_parsing2.pdf},
  language = {en}
}

@article{fordPackratParsingSimple,
  title = {Packrat {{Parsing}}: {{Simple}}, {{Powerful}}, {{Lazy}}, {{Linear Time}}},
  author = {Ford, Bryan},
  pages = {12},
  abstract = {Packrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of top-down parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional linear-time algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longest-match rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a linear-time packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.},
  file = {D\:\\GDrive\\zotero\\Ford\\ford_packrat_parsing.pdf},
  language = {en}
}

@article{forejtAutomatedVerificationTechniques2011,
  title = {Automated {{Verification Techniques}} for {{Probabilistic Systems To}} Cite This Version : {{HAL Id}} : Hal-00648037},
  author = {Forejt, Vojtech and Kwiatkowska, Marta and Norman, Gethin and Parker, David and Forejt, Vojtech and Kwiatkowska, Marta and Norman, Gethin and Parker, David and Verification, Automated},
  year = {2011},
  file = {D\:\\GDrive\\zotero\\Forejt\\forejt_2011_automated_verification_techniques_for_probabilistic_systems_to_cite_this.pdf}
}

@article{forgetImprovingTextPasswords2008,
  title = {Improving Text Passwords through Persuasion},
  author = {Forget, Alain and Chiasson, Sonia and Van Oorschot, P. C. and Biddle, Robert},
  year = {2008},
  pages = {1--12},
  doi = {10.1145/1408664.1408666},
  abstract = {Password restriction policies and advice on creating secure passwords have limited effects on password strength. Influencing users to create more secure passwords remains an open problem. We have developed Persuasive Text Passwords (PTP), a text password creation system which leverages Persuasive Technology principles to influence users in creating more secure passwords without sacrificing usability. After users choose a password during creation, PTP improves its security by placing randomly-chosen characters at random positions into the password. Users may shuffle to be presented with randomly-chosen and positioned characters until they find a combination they feel is memorable. In this paper, we present an 83-participant user study testing four PTP variations. Our results show that the PTP variations significantly improved the security of users' passwords. We also found that those participants who had a high number of random characters placed into their passwords would deliberately choose weaker pre-improvement passwords to compensate for the memory load. As a consequence of this compensatory behaviour, there was a limit to the gain in password security achieved by PTP.},
  file = {D\:\\GDrive\\zotero\\Forget\\forget_2008_improving_text_passwords_through_persuasion.pdf},
  isbn = {9781605582764},
  journal = {SOUPS 2008 - Proceedings of the 4th Symposium on Usable Privacy and Security},
  keywords = {Authentication,Passwords,Persuasive technology,Usable security}
}

@article{foroglouFurtherApplicationsBlockchain2015,
  title = {Further Applications of the Blockchain},
  author = {Foroglou, Georgios and Tsilidou, Anna Lali},
  year = {2015},
  pages = {0--8},
  doi = {10.13140/RG.2.1.2350.8568},
  abstract = {In this research we investigate into the blockchain technology; its current use and explore other possible implementations of this protocol. In the first part a thorough explanation of the technology and the problems it is trying to tackle is attempted. At the same time a background on Bitcoin (the first application of the technology) is provided. In the second part it is examined whether the technology could be leveraged to solve problems in different fields, while some specific recommendations for the Greek economy are also made. [Bitcoin, Blockchain, Applications ]},
  file = {D\:\\GDrive\\zotero\\Foroglou\\foroglou_2015_further_applications_of_the_blockchain.pdf},
  journal = {Conference: 12th Student Conference on Managerial Science and Technology, At Athens},
  number = {MAY}
}

@techreport{fournetJoinCalculusLanguage,
  title = {The {{Join Calculus}}: A {{Language}} for {{Distributed Mobile Programming}}},
  author = {Fournet, C{\'e}dric and Gonthier, Georges},
  abstract = {In these notes, we give an overview of the join calculus, its semantics, and its equational theory. The join calculus is a language that models distributed and mobile programming. It is characterized by an explicit notion of locality, a strict adherence to local synchronization, and a direct embedding of the ML programming language. The join calculus is used as the basis for several distributed languages and implementations, such as JoCaml and functional nets. Local synchronization means that messages always travel to a set destination , and can interact only after they reach that destination; this is required for an efficient implementation. Specifically, the join calculus uses ML's function bindings and pattern-matching on messages to program these synchronizations in a declarative manner. Formally, the language owes much to concurrency theory, which provides a strong basis for stating and proving the properties of asynchronous programs. Because of several remarkable identities, the theory of process equivalences admits simplifications when applied to the join calculus. We prove several of these identities, and argue that equivalences for the join calculus can be rationally organized into a five-tiered hierarchy, with some trade-off between expressiveness and proof techniques. We describe the mobility extensions of the core calculus, which allow the programming of agent creation and migration. We briefly present how the calculus has been extended to model distributed failures on the one hand, and cryptographic protocols on the other.},
  file = {D\:\\GDrive\\zotero\\Fournet\\fournet_the_join_calculus.pdf}
}

@article{fovinoChapterDesignImplementation2009,
  title = {Chapter 6 {{Design And Implementation}} of {{Software}}},
  author = {Fovino, Igor Nai and Carcano, Andrea and Masera, Marcelo and Betta, Alberto Trom-},
  year = {2009},
  pages = {107--121},
  file = {D\:\\GDrive\\zotero\\Fovino\\fovino_2009_chapter_6_design_and_implementation_of_software.pdf},
  keywords = {modbus,scada systems,secure protocol}
}

@techreport{foxHarvestYieldScalable,
  title = {Harvest, {{Yield}}, and {{Scalable Tolerant Systems}}},
  author = {Fox, Armando and Brewer, Eric A},
  abstract = {The cost of reconciling consistency and state management with high availability is highly magnified by the unprecedented scale and robustness requirements of today's Internet applications. We propose two strategies for improving overall availability using simple mechanisms that scale over large applications whose output behavior tolerates graceful degradation. We characterize this degradation in terms of harvest and yield, and map it directly onto engineering mechanisms that enhance availability by improving fault isolation, and in some cases also simplify programming. By collecting examples of related techniques in the literature and illustrating the surprising range of applications that can benefit from these approaches, we hope to motivate a broader research program in this area. 1. Motivation, Hypothesis, Relevance Increasingly, infrastructure services comprise not only routing, but also application-level resources such as search engines [15], adaptation proxies [8], and Web caches [20]. These applications must confront the same \textcent{} \textexclamdown{} \textcurrency{} \textsterling{} \textbrokenbar{} \textyen{} operational expectations and exponentially-growing user loads as the routing infrastructure, and consequently are absorbing comparable amounts of hardware and software. The current trend of harnessing commodity-PC clusters for scalability and availability [9] is reflected in the largest web server installations. These sites use tens to hundreds of PC's to deliver 100M or more read-mostly page views per day, primarily using simple replication or relatively small data sets to increase throughput. The scale of these applications is bringing the well-known tradeoff between consistency and availability [4] into very sharp relief. In this paper we propose two general directions for future work in building large-scale robust systems. Our approaches tolerate partial failures by emphasizing simple composition mechanisms that promote fault containment, and by translating possible partial failure modes into engineering mechanisms that provide smoothly-degrading functionality rather than lack of availability of the service as a whole. The approaches were developed in the context of cluster computing, where it is well accepted [22] that one of the major challenges is the nontrivial software engineering required to automate partial-failure handling in order to keep system management tractable.},
  file = {D\:\\GDrive\\zotero\\Fox\\fox_harvest,_yield,_and_scalable_tolerant_systems.pdf}
}

@article{Fq,
  title = {Fq},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SXWLKAJ2\\fq.pdf}
}

@article{francillonCodeInjectionAttacks2008,
  title = {Code {{Injection Attacks}} on {{Harvard}}-{{Architecture Devices}}},
  author = {Francillon, Aur{\'e}lien and Castelluccia, Claude},
  year = {2008},
  abstract = {Harvard architecture CPU design is common in the embedded world. Examples of Harvard-based architecture devices are the Mica family of wireless sensors. Mica motes have limited memory and can process only very small packets. Stack-based buffer overflow techniques that inject code into the stack and then execute it are therefore not applicable. It has been a common belief that code injection is impossible on Harvard architectures. This paper presents a remote code injection attack for Mica sensors. We show how to exploit program vulnerabilities to permanently inject any piece of code into the program memory of an Atmel AVR-based sensor. To our knowledge, this is the first result that presents a code injection technique for such devices. Previous work only succeeded in injecting data or performing transient attacks. Injecting permanent code is more powerful since the attacker can gain full control of the target sensor. We also show that this attack can be used to inject a worm that can propagate through the wireless sensor network and possibly create a sensor botnet. Our attack combines different techniques such as return oriented programming and fake stack injection. We present implementation details and suggest some countermeasures .},
  file = {D\:\\GDrive\\zotero\\Francillon\\francillon_code_injection_attacks_on_harvard-architecture_devices.pdf},
  keywords = {Buffer Overflow,Code Injection Attacks,Computer Worms,D46 [Operating Systems]: Security and Protection General Terms Experimentation,Embedded Devices,Gadgets,Return Ori-ented Programming,Security Keywords Harvard Architecture,Wireless Sensor Networks}
}

@article{francillonMinimalistApproachRemote2014,
  title = {A Minimalist Approach to {{Remote Attestation}}},
  author = {Francillon, Aur{\'e}lien and Nguyen, Quan and Rasmussen, Kasper B. and Tsudik, Gene},
  year = {2014},
  publisher = {{EDAA}},
  issn = {15301591},
  doi = {10.7873/DATE2014.257},
  abstract = {Embedded computing devices increasingly permeate many aspects of modern life: from medical to automotive, from building and factory automation to weapons, from critical infrastructures to home entertainment. Despite their specialized nature as well as limited resources and connectivity, these devices are now becoming an increasingly popular and attractive target for attacks, especially, malware infections. A number of approaches have been suggested to detect and/or mitigate such attacks. They vary greatly in terms of application generality and underlying assumptions. However, one common theme is the need for Remote Attestation, a distinct security service that allows a trusted party (verifier) to check the internal state of a remote untrusted embedded device (prover). Many prior methods assume some form of trusted hardware on the prover, which is not a good option for small and low-end embedded devices. To this end, we investigate the feasibility of Remote Attestation without trusted hardware. This paper provides a systematic treatment of Remote Attestation, starting with a precise definition of the desired service and proceeding to its systematic deconstruction into necessary and sufficient properties. Next, these are mapped into a minimal collection of hardware and software components that result in secure Remote Attestation. One distinguishing feature of this line of research is the need to prove (or, at least argue for) architectural minimality - an aspect rarely encountered in security research. This work also provides a promising platform for attaining more advanced security services and guarantees. \textcopyright{} 2014 EDAA.},
  file = {D\:\\GDrive\\zotero\\Francillon\\francillon_2014_a_minimalist_approach_to_remote_attestation.pdf},
  isbn = {9783981537024},
  journal = {Proceedings -Design, Automation and Test in Europe, DATE}
}

@inproceedings{francillonMinimalistApproachRemote2014a,
  title = {A Minimalist Approach to {{Remote Attestation}}},
  booktitle = {2014 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Francillon, Aur{\'e}lien and Nguyen, Quan and Rasmussen, Kasper B. and Tsudik, Gene},
  year = {2014},
  month = mar,
  pages = {1--6},
  issn = {1558-1101},
  doi = {10.7873/DATE.2014.257},
  abstract = {Embedded computing devices increasingly permeate many aspects of modern life: from medical to automotive, from building and factory automation to weapons, from critical infrastructures to home entertainment. Despite their specialized nature as well as limited resources and connectivity, these devices are now becoming an increasingly popular and attractive target for attacks, especially, malware infections. A number of approaches have been suggested to detect and/or mitigate such attacks. They vary greatly in terms of application generality and underlying assumptions. However, one common theme is the need for Remote Attestation, a distinct security service that allows a trusted party (verifier) to check the internal state of a remote untrusted embedded device (prover). Many prior methods assume some form of trusted hardware on the prover, which is not a good option for small and low-end embedded devices. To this end, we investigate the feasibility of Remote Attestation without trusted hardware. This paper provides a systematic treatment of Remote Attestation, starting with a precise definition of the desired service and proceeding to its systematic deconstruction into necessary and sufficient properties. Next, these are mapped into a minimal collection of hardware and software components that result in secure Remote Attestation. One distinguishing feature of this line of research is the need to prove (or, at least argue for) architectural minimality - an aspect rarely encountered in security research. This work also provides a promising platform for attaining more advanced security services and guarantees.},
  file = {D\:\\GDrive\\zotero\\Francillon et al\\francillon_et_al_2014_a_minimalist_approach_to_remote_attestation.pdf},
  keywords = {Embedded systems,Hardware,Malware,Protocols,Read only memory}
}

@article{francoVulnerabilityAnalysisAutomotive2018,
  title = {Vulnerability {{Analysis}} of an {{Automotive Infotainment System Submitted}} By},
  author = {Franco, Edwin and Josephlal, Myloth and Tippenhauer, Nils Ole},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Franco\\franco_2018_vulnerability_analysis_of_an_automotive_infotainment_system_submitted_by.pdf}
}

@techreport{frankCollaborationOpportunitiesContent2013,
  title = {Collaboration {{Opportunities}} for {{Content Delivery}} and {{Network Infrastructures}}},
  author = {Frank, Benjamin and Poese, Ingmar and Smaragdakis, Georgios and Feldmann, Anja and Maggs, Bruce M and Uhlig, Steve and Aggarwal, Vinay and Schneider, Fabian},
  year = {2013},
  pages = {305--377},
  file = {D\:\\GDrive\\zotero\\Frank\\frank_2013_collaboration_opportunities_for_content_delivery_and_network_infrastructures.pdf}
}

@article{Functionalreactiveanimation,
  title = {Functional-Reactive-Animation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LVNA65VR\\functional-reactive-animation.pdf}
}

@article{fuRegressionTestSelection2018,
  title = {Regression {{Test Selection}} for {{C}} ++ {{Based}} on {{Call Graph Analysis}}},
  author = {Fu, Ben},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Fu\\fu_2018_regression_test_selection_for_c_++_based_on_call_graph_analysis.pdf}
}

@article{gabowPathbasedDepthfirstSearch2000,
  title = {Path-Based Depth-First Search for Strong and Biconnected Components},
  author = {Gabow, Harold N.},
  year = {2000},
  month = may,
  volume = {74},
  pages = {107--114},
  issn = {00200190},
  doi = {10.1016/S0020-0190(00)00051-X},
  file = {D\:\\GDrive\\zotero\\Gabow\\gabow_2000_path-based_depth-first_search_for_strong_and_biconnected_components.pdf},
  journal = {Information Processing Letters},
  language = {en},
  number = {3-4}
}

@article{gadientSecurityCodeSmells2018,
  title = {Security Code Smells in {{Android ICC}}},
  author = {Gadient, Pascal and Ghafari, Mohammad and Frischknecht, Patrick and Nierstrasz, Oscar},
  year = {2018},
  issn = {15737616},
  doi = {10.1007/s10664-018-9673-y},
  abstract = {Android Inter-Component Communication (ICC) is complex, largely unconstrained, and hard for developers to understand. As a consequence, ICC is a common source of security vulnerabilities in Android apps. To promote secure programming practices, we have reviewed related research, and identified avoidable ICC vulnerabilities in Android-run devices and the security code smells that indicate their presence. We explain the vulnerabilities and their corresponding smells, and we discuss how they can be eliminated or mitigated during development. We present a lightweight static analysis tool on top of Android Lint that analyzes the code under development and provides just-in-time feedback within the IDE about the presence of such smells in the code. Moreover, with the help of this tool we study the prevalence of security code smells in more than 700 open-source apps, and manually inspect around 15\% of the apps to assess the extent to which identifying such smells uncovers ICC security vulnerabilities.},
  file = {D\:\\GDrive\\zotero\\Gadient\\gadient_2018_security_code_smells_in_android_icc.pdf},
  journal = {Empirical Software Engineering},
  keywords = {Android,Security code smells,Static analysis,Vulnerability}
}

@incollection{gangeAbstractDomainUninterpreted2016a,
  title = {An {{Abstract Domain}} of {{Uninterpreted Functions}}},
  booktitle = {Verification, {{Model Checking}}, and {{Abstract Interpretation}}},
  author = {Gange, Graeme and Navas, Jorge A. and Schachte, Peter and S{\o}ndergaard, Harald and Stuckey, Peter J.},
  editor = {Jobstmann, Barbara and Leino, K. Rustan M.},
  year = {2016},
  volume = {9583},
  pages = {85--103},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-49122-5_4},
  file = {D\:\\GDrive\\zotero\\Gange\\gange_an_abstract_domain_of_uninterpreted_functions.pdf},
  isbn = {978-3-662-49121-8 978-3-662-49122-5}
}

@article{gaoJuliaLanguageMachine2020,
  title = {Julia Language in Machine Learning: {{Algorithms}}, Applications, and Open Issues},
  author = {Gao, Kaifeng and Mei, Gang and Piccialli, Francesco and Cuomo, Salvatore and Tu, Jingzhi and Huo, Zenan},
  year = {2020},
  month = aug,
  volume = {37},
  pages = {100254},
  publisher = {{Elsevier Ireland Ltd}},
  issn = {15740137},
  doi = {10.1016/j.cosrev.2020.100254},
  abstract = {Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the applications of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning.},
  file = {D\:\\GDrive\\zotero\\Gao\\gao_2020_julia_language_in_machine_learning.pdf},
  journal = {Computer Science Review},
  keywords = {Artificial neural networks,Deep learning,Julia language,Machine learning,Supervised learning,Unsupervised learning}
}

@techreport{garcaa-molrnaSagas,
  title = {Sagas},
  author = {{Garcaa-Molrna}, Hector and Salem, Kenneth},
  abstract = {Long lived transactions (LLTs) hold on to database resources for relatively long periods of time, slgmficantly delaymg the termmatlon of shorter and more common transactions To alleviate these problems we propose the notion of a saga A LLT 1s a saga if it can be written as a sequence of transactions that can be interleaved with other transactions The database management system guarantees that either all the transactions m a saga are successfully completed or compensatmg transactions are run to amend a partial execution Both the concept of saga and its lmplementatlon are relatively simple, but they have the potential to improve performance slgmficantly We analyze the various lmplemen-tatron issues related to sagas, including how they can be run on an exlstmg system that does not directly support them We also discuss techniques for database and LLT design that make it feasible to break up LLTs mto sagas},
  file = {D\:\\GDrive\\zotero\\Garcaa-Molrna\\garcaa-molrna_sagas.pdf}
}

@book{garcia-luna-acevesCabernetVehicularContent2008,
  title = {Cabernet: {{Vehicular Content Delivery Using WiFi}}},
  author = {{Garcia-Luna-Aceves}, J. J. and {ACM Digital Library.} and {ACM SIGMOBILE.}},
  year = {2008},
  publisher = {{ACM}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Garcia-Luna-Aceves\\garcia-luna-aceves_2008_cabernet.pdf},
  isbn = {978-1-60558-096-8}
}

@techreport{garfinkelOstiaDelegatingArchitecture,
  title = {Ostia: {{A Delegating Architecture}} for {{Secure System Call Interposition}}},
  author = {Garfinkel, Tal and Pfaff, Ben and Rosenblum, Mendel},
  abstract = {Application sandboxes provide restricted execution environments that limit an application's access to sensitive OS resources. These systems are an increasingly popular method for limiting the impact of a compromise. While a variety of mechanisms for building these systems have been proposed, the most thoroughly implemented and studied are based on system call interposition. Current interposition-based architectures offer a wide variety of properties that make them an attractive approach for building sandbox-ing systems. Unfortunately, these architectures also possess several critical properties that make their implementation error prone and limit their functionality. We present a study of Ostia, a sandboxing system we have developed that relies on a "delegating" architecture which overcomes many of the limitations of today's sand-boxing systems. We compare this delegating architecture to the "filtering" architecture commonly used for sandboxes today. We present the salient features of each architecture and examine the design choices that significantly impact security , compatibility, flexibility, deployability, and performance in this class of system.},
  file = {D\:\\GDrive\\zotero\\Garfinkel\\garfinkel_ostia.pdf}
}

@article{garfinkelRoadLessTrusted2020,
  title = {The {{Road}} to {{Less Trusted Code}}},
  author = {Garfinkel, Tal and Stefan, Deian},
  year = {2020},
  volume = {45},
  pages = {8},
  file = {D\:\\GDrive\\zotero\\Garfinkel_Stefan\\garfinkel_stefan_2020_the_road_to_less_trusted_code.pdf},
  language = {en},
  number = {4}
}

@article{gargFeedbackdirectedUnitTest2013,
  title = {Feedback-Directed Unit Test Generation for {{C}}/{{C}}++ Using Concolic Execution},
  author = {Garg, Pranav and Ivancic, Franjo and Balakrishnan, Gogul and Maeda, Naoto and Gupta, Aarti},
  year = {2013},
  pages = {132--141},
  issn = {02705257},
  doi = {10.1109/ICSE.2013.6606559},
  abstract = {In industry, software testing and coverage-based metrics are the predominant techniques to check correctness of software. This paper addresses automatic unit test generation for programs written in C/C++. The main idea is to improve the coverage obtained by feedback-directed random test generation methods, by utilizing concolic execution on the generated test drivers. Furthermore, for programs with numeric computations, we employ non-linear solvers in a lazy manner to generate new test inputs. These techniques significantly improve the coverage provided by a feedback-directed random unit testing framework, while retaining the benefits of full automation. We have implemented these techniques in a prototype platform, and describe promising experimental results on a number of C/C++ open source benchmarks. \textcopyright{} 2013 IEEE.},
  file = {D\:\\GDrive\\zotero\\Garg\\garg_2013_feedback-directed_unit_test_generation_for_c-c++_using_concolic_execution.pdf},
  isbn = {9781467330763},
  journal = {Proceedings - International Conference on Software Engineering}
}

@techreport{garzikPublicPrivateBlockchains2015,
  title = {Public versus {{Private Blockchains Part}} 1: {{Permissioned Blockchains White Paper BitFury Group}} in Collaboration With},
  author = {Garzik, Jeff},
  year = {2015},
  abstract = {Blockchain-based solutions are one of the major areas of research for institutions, particularly in the financial and the government sectors. There is little disagreement that backbone technologies currently used in these sectors are outdated and need an overhaul to conform to the needs of the times. Distributed or decentralized ledgers in the form of blockchains are one of the most discussed potential solutions to the stated problem. We provide a description of permissioned blockchain systems that could be used in creating secure ledgers or timestamped registries. We contend that the blockchain protocol and data should be accessible to end users to provide a higher level of decentralization and transparency and argue that proof of work could be effectively used in permissioned blockchains as a means of providing and diversifying security.},
  file = {D\:\\GDrive\\zotero\\Garzik\\garzik_2015_public_versus_private_blockchains_part_1.pdf}
}

@inproceedings{geddesCoverYourACKs2013,
  title = {Cover Your {{ACKs}}: {{Pitfalls}} of Covert Channel Censorship Circumvention},
  booktitle = {Proceedings of the {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Geddes, John and Schuchard, Max and Hopper, Nicholas},
  year = {2013},
  pages = {361--372},
  issn = {15437221},
  doi = {10.1145/2508859.2516742},
  abstract = {In response to increasingly sophisticated methods of blocking ac- cess to censorship circumvention schemes such as Tor, recently proposed systems such as Skypemorph, FreeWave, and Censor- Spoofer have used voice and video conferencing protocols as ``cover channels'' to hide proxy connections. We demonstrate that even with perfect emulation of the cover channel, these systems can be vulnerable to attacks that detect or disrupt the covert communica- tions while having no effect on legitimate cover traffic. Our attacks stem from differences in the channel requirements for the cover protocols, which are peer-to-peer and loss tolerant, and the covert traffic, which is client-proxy and loss intolerant. These differences represent significant limitations and suggest that such protocols are a poor choice of cover channel for general censorship circumven- tion schemes.},
  file = {D\:\\GDrive\\zotero\\Geddes\\geddes_2013_cover_your_acks.pdf},
  isbn = {978-1-4503-2477-9},
  keywords = {anonymity,censorship,censorspoofer,freewave,skypemorph}
}

@inproceedings{geFineGrainedControlFlowIntegrity2016,
  title = {Fine-{{Grained Control}}-{{Flow Integrity}} for {{Kernel Software}}},
  booktitle = {2016 {{IEEE European Symposium}} on {{Security}} and {{Privacy}} ({{EuroS}}\&{{P}})},
  author = {Ge, Xinyang and Talele, Nirupama and Payer, Mathias and Jaeger, Trent},
  year = {2016},
  month = mar,
  pages = {179--194},
  publisher = {{IEEE}},
  address = {{Saarbrucken}},
  doi = {10.1109/EuroSP.2016.24},
  abstract = {Modern systems assume that privileged software always behaves as expected, however, such assumptions may not hold given the prevalence of kernel vulnerabilities. One idea is to employ defenses to restrict how adversaries may exploit such vulnerabilities, such as Control-Flow Integrity (CFI), which restricts execution to a Control-Flow Graph (CFG). However, proposed applications of CFI enforcement to kernel software are too coarse-grained to restrict the adversary effectively and either fail to enforce CFI comprehensively or are very expensive.},
  file = {D\:\\GDrive\\zotero\\Ge et al\\ge_et_al_2016_fine-grained_control-flow_integrity_for_kernel_software.pdf},
  isbn = {978-1-5090-1751-5 978-1-5090-1752-2},
  language = {en}
}

@article{geGRIFFINGuardingControl2017,
  title = {{{GRIFFIN}}: {{Guarding Control Flows Using Intel Processor Trace}}},
  author = {Ge, Xinyang and Cui, Weidong and Jaeger, Trent},
  year = {2017},
  doi = {10.1145/3037697.3037716},
  abstract = {Researchers are actively exploring techniques to enforce control-flow integrity (CFI), which restricts program execution to a predefined set of targets for each indirect control transfer to prevent code-reuse attacks. While hardware-assisted CFI enforcement may have the potential for advantages in performance and flexibility over software in-strumentation, current hardware-assisted defenses are either incomplete (i.e., do not enforce all control transfers) or less efficient in comparison. We find that the recent introduction of hardware features to log complete control-flow traces, such as Intel Processor Trace (PT), provides an opportunity to explore how efficient and flexible a hardware-assisted CFI enforcement system may become. While Intel PT was designed to aid in offline debugging and failure diagnosis, we explore its effectiveness for online CFI enforcement over unmodified binaries by designing a parallelized method for enforcing various types of CFI policies. We have implemented a prototype called GRIFFIN in the Linux 4.2 kernel that enables complete CFI enforcement over a variety of software, including the Firefox browser and its jitted code. Our experiments show that GRIFFIN can enforce fine-grained CFI policies with shadow stack as recommended by researchers at a performance that is comparable to software-only instru-mentation techniques. In addition, we find that alternative logging approaches yield significant performance improvements for trace processing, identifying opportunities for further hardware assistance.},
  file = {D\:\\GDrive\\zotero\\Ge\\ge_griffin.pdf},
  isbn = {9781450344654},
  keywords = {Control-Flow Integrity}
}

@article{gehlPowerFreedomDark,
  title = {Power/Freedom on the Dark Web: {{A}} Digital Ethnography of the {{Dark Web Social Network}}},
  author = {Gehl, Robert W},
  volume = {18},
  pages = {1219--1235},
  doi = {10.1177/1461444814554900},
  abstract = {This essay is an early ethnographic exploration of the Dark Web Social Network (DWSN), a social networking site only accessible to Web browsers equipped with The Onion Router. The central claim of this essay is that the DWSN is an experiment in power/freedom, an attempt to simultaneously trace, deploy, and overcome the historical conditions in which it finds itself: the generic constraints and affordances of social networking as they have been developed over the past decade by Facebook and Twitter, and the ideological constraints and affordances of public perceptions of the dark web, which hold that the dark web is useful for both taboo activities and freedom from state oppression. I trace the DWSN's experiment with power/freedom through three practices: anonymous/social networking, the banning of child pornography, and the productive aspects of techno-elitism. I then use these practices to specify particular forms of power/freedom on the DWSN.},
  file = {D\:\\GDrive\\zotero\\Gehl\\gehl_power-freedom_on_the_dark_web.pdf},
  number = {7}
}

@inproceedings{geldenhuysProbabilisticSymbolicExecution2012,
  title = {Probabilistic Symbolic Execution},
  booktitle = {Proceedings of the 2012 {{International Symposium}} on {{Software Testing}} and {{Analysis}} - {{ISSTA}} 2012},
  author = {Geldenhuys, Jaco and Dwyer, Matthew B. and Visser, Willem},
  year = {2012},
  pages = {166},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2338965.2336773},
  abstract = {The continued development of efficient automated decision procedures has spurred the resurgence of research on symbolic execution over the past decade. Researchers have applied symbolic execution to a wide range of software analysis problems including: checking programs against contract specifications, inferring bounds on worst-case execution performance, and generating path-adequate test suites for widely used library code. In this paper, we explore the adaptation of symbolic execution to perform a more quantitative type of reasoning --- the calculation of estimates of the probability of executing portions of a program. We present an extension of the widely used Symbolic PathFinder symbolic execution system that calculates path probabilities. We exploit state-of-the-art computational algebra techniques to count the number of solutions to path conditions, yielding exact results for path probabilities. To mitigate the cost of using these techniques, we present two optimizations, PC slicing and count memoization, that significantly reduce the cost of probabilistic symbolic execution. Finally, we present the results of an empirical evaluation applying our technique to challenging library container implementations and illustrate the benefits that adding probabilities to program analyses may offer.},
  file = {D\:\\GDrive\\zotero\\Geldenhuys\\geldenhuys_2012_probabilistic_symbolic_execution.pdf},
  isbn = {978-1-4503-1454-1},
  keywords = {at uc,brid and embedded software,center for hy-,chess,cyber-physical systems,embedded systems,in part by the,systems,this work was supported}
}

@article{genkinDrivebyKeyextractionCache2018,
  title = {Drive-by Key-Extraction Cache Attacks from Portable Code},
  author = {Genkin, Daniel and Pachmanov, Lev and Tromer, Eran and Yarom, Yuval},
  year = {2018},
  volume = {10892 LNCS},
  pages = {83--102},
  issn = {16113349},
  doi = {10.1007/978-3-319-93387-0_5},
  abstract = {We show how malicious web content can extract cryptographic secret keys from the user's computer. The attack uses portable scripting languages supported by modern browsers to induce contention for CPU cache resources, and thereby gleans information about the memory accesses of other programs running on the user's computer. We show how this side-channel attack can be realized in WebAssembly and PNaCl; how to attain fine-grained measurements; and how to extract ElGamal, ECDH and RSA decryption keys from various cryptographic libraries. The attack does not rely on bugs in the browser's nominal sandboxing mechanisms, or on fooling users. It applies even to locked-down platforms with strong confinement mechanisms and browser-only functionality, such as Chromebook devices. Moreover, on browser-based platforms the attacked software too may be written in portable JavaScript; and we show that in this case even implementations of supposedly-secure constant-time algorithms, such as Curve25519's, are vulnerable to our attack.},
  file = {D\:\\GDrive\\zotero\\Genkin\\genkin_2018_drive-by_key-extraction_cache_attacks_from_portable_code.pdf},
  isbn = {9783319933863},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{genkinECDSAKeyExtraction2016,
  title = {{{ECDSA}} Key Extraction from Mobile Devices via Nonintrusive Physical Side Channels},
  author = {Genkin, Daniel and Pachmanov, Lev and Pipman, Itamar and Tromer, Eran and Yarom, Yuval},
  year = {2016},
  volume = {24-28-Octo},
  pages = {1626--1638},
  issn = {15437221},
  doi = {10.1145/2976749.2978353},
  abstract = {We show that elliptic-curve cryptography implementations on mobile devices are vulnerable to electromagnetic and power side-channel attacks. We demonstrate full extraction of ECDSA secret signing keys from OpenSSL and CoreBitcoin running on iOS devices, and partial key leakage from OpenSSL running on Android and from iOS's Common-Crypto. These non-intrusive attacks use a simple magnetic probe placed in proximity to the device, or a power probe on the phone's USB cable. They use a bandwidth of merely a few hundred kHz, and can be performed cheaply using an audio card and an improvised magnetic probe.},
  file = {D\:\\GDrive\\zotero\\Genkin\\genkin_2016_ecdsa_key_extraction_from_mobile_devices_via_nonintrusive_physical_side_channels.pdf},
  isbn = {9781450341394},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security}
}

@article{genkinRSAKeyExtraction2013,
  title = {{{RSA Key Extraction}} via {{Low}}-{{Bandwidth Acoustic Cryptanalysis}} *},
  author = {Genkin, Daniel and Shamir, Adi and Tromer, Eran},
  year = {2013},
  abstract = {Many computers emit a high-pitched noise during operation, due to vibration in some of their electronic components. These acoustic emanations are more than a nuisance: they can convey information about the software running on the computer, and in particular leak sensitive information about security-related computations. In a preliminary presentation (Eurocrypt'04 rump session), we have shown that different RSA keys induce different sound patterns, but it was not clear how to extract individual key bits. The main problem was that the acoustic side channel has a very low bandwidth (under 20 kHz using common microphones, and a few hundred kHz using ultrasound microphones), many orders of magnitude below the GHz-scale clock rates of the attacked computers. In this paper we describe a new acoustic cryptanalysis key extraction attack, applicable to GnuPG's current implementation of RSA. The attack can extract full 4096-bit RSA decryption keys from laptop computers (of various models), within an hour, using the sound generated by the computer during the decryption of some chosen ciphertexts. We experimentally demonstrate that such attacks can be carried out, using either a plain mobile phone placed next to the computer, or a more sensitive microphone placed 4 meters away. Beyond acoustics, we demonstrate that a similar low-bandwidth attack can be performed by measuring the electric potential of a computer chassis. A suitably-equipped attacker need merely touch the target computer with his bare hand, or get the required leakage information from the ground wires at the remote end of VGA, USB or Ethernet cables. * The authors thank Lev Pachmanov for programming and experiment support during the course of this research.},
  file = {D\:\\GDrive\\zotero\\Genkin\\genkin_2014_rsa_key_extraction_via_low-bandwidth_acoustic_cryptanalysis.pdf}
}

@book{geoffrayLazyDeveloperApproach2008,
  title = {A {{Lazy Developer Approach}}: {{Building}} a {{JVM}} with {{Third Party Software}}},
  author = {Geoffray, Nicolas and Thomas, Ga{\"e}l and Cl{\'e}ment, Charles and Folliot, Bertil},
  year = {2008},
  abstract = {The development of a complete Java Virtual Machine (JVM) implementation is a tedious process which involves knowledge in different areas: garbage collection, just in time compilation , interpretation, file parsing, data structures, etc. The result is that developing its own virtual machine requires a considerable amount of man/year. In this paper we show that one can implement a JVM with third party software and with performance comparable to industrial and top open-source JVMs. Our proof-of-concept implementation uses existing versions of a garbage collector, a just in time compiler, and the base library, and is robust enough to execute complex Java applications such as the OSGi Felix implementation and the Tomcat servlet container.},
  file = {D\:\\GDrive\\zotero\\Geoffray\\geoffray_2008_a_lazy_developer_approach.pdf},
  isbn = {978-1-60558-223-8},
  keywords = {BoehmGC,D211 [Software Engineering]: Software Architectures General Terms Languages,Design Keywords Java,GNU Classpath,JnJVM,LLVM}
}

@techreport{geoffreymFistfulBitcoinsCharacterizing,
  title = {A {{Fistful}} of {{Bitcoins Characterizing Payments Among Men}} with {{No Names}}},
  author = {{Geoffrey M}},
  file = {D\:\\GDrive\\zotero\\Geoffrey M\\geoffrey_m_a_fistful_of_bitcoins_characterizing_payments_among_men_with_no_names.pdf}
}

@inproceedings{gervaisSecurityPerformanceProof2016,
  title = {On the {{Security}} and {{Performance}} of {{Proof}} of {{Work Blockchains}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Gervais, Arthur and Karame, Ghassan O. and W{\"u}st, Karl and Glykantzis, Vasileios and Ritzdorf, Hubert and Capkun, Srdjan},
  year = {2016},
  month = oct,
  pages = {3--16},
  publisher = {{ACM}},
  address = {{Vienna Austria}},
  doi = {10.1145/2976749.2978341},
  abstract = {Proof of Work (PoW) powered blockchains currently account for more than 90\% of the total market capitalization of existing digital cryptocurrencies. Although the security provisions of Bitcoin have been thoroughly analysed, the security guarantees of variant (forked) PoW blockchains (which were instantiated with different parameters) have not received much attention in the literature.},
  file = {D\:\\GDrive\\zotero\\Gervais et al\\gervais_et_al_2016_on_the_security_and_performance_of_proof_of_work_blockchains.pdf},
  isbn = {978-1-4503-4139-4},
  language = {en}
}

@article{geSurveyMicroarchitecturalTiming2018,
  title = {A Survey of Microarchitectural Timing Attacks and Countermeasures on Contemporary Hardware},
  author = {Ge, Qian and Yarom, Yuval and Cock, David and Heiser, Gernot},
  year = {2018},
  volume = {8},
  issn = {21908516},
  doi = {10.1007/s13389-016-0141-6},
  abstract = {Microarchitectural timing channels expose hidden hardware states though timing. We survey recent attacks that exploit microarchitectural features in shared hardware, especially as they are relevant for cloud computing. We classify types of attacks according to a taxonomy of the shared resources leveraged for such attacks. Moreover, we take a detailed look at attacks used against shared caches. We survey existing countermeasures. We finally discuss trends in attacks, challenges to combating them, and future directions, especially with respect to hardware support.},
  file = {D\:\\GDrive\\zotero\\Ge\\ge_2018_a_survey_of_microarchitectural_timing_attacks_and_countermeasures_on.pdf},
  journal = {Journal of Cryptographic Engineering},
  keywords = {Cache-based timing attacks,Countermeasures,Microarchitectural timing attacks,Trend in the attacks},
  number = {1}
}

@article{geYourProcessorLeaks2016,
  title = {Your {{Processor Leaks Information}} - and {{There}}'s {{Nothing You Can Do About It}}},
  author = {Ge, Qian and Yarom, Yuval and Li, Frank and Heiser, Gernot},
  year = {2016},
  abstract = {Timing channels are information flows, encoded in the relative timing of events, that bypass the system's protection mechanisms. Any microarchitectural state that depends on execution history and affects the rate of progress of later executions potentially establishes a timing channel, unless explicit steps are taken to close it. Such state includes CPU caches, TLBs, branch predictors and prefetchers; removing the channels requires that the OS can partition such state or flush it on a switch of security domains. We measure the capacities of channels based on these microarchitectural features on several generations of processors across the two mainstream ISAs, x86 and ARM, and investigate the effectiveness of the flushing mechanisms provided by the respective ISA.We find that in all processors we studied, at least one significant channel remains. This implies that closing all timing channels seems impossible on contemporary mainstream processors.},
  file = {D\:\\GDrive\\zotero\\Ge\\ge_2016_your_processor_leaks_information_-_and_there's_nothing_you_can_do_about_it.pdf}
}

@article{ghaeiniHAMIDSHierarchicalMonitoring2016,
  title = {{{HAMIDS}}: {{Hierarchical}} Monitoring Intrusion Detection System for Industrial Control Systems},
  author = {Ghaeini, Hamid Reza and Tippenhauer, Nils Ole},
  year = {2016},
  pages = {103--111},
  doi = {10.1145/2994487.2994492},
  abstract = {In this paper, we propose a hierarchical monitoring intrusion detection system (HAMIDS) for industrial control systems (ICS). The HAMIDS framework detects the anomalies in both level 0 and level 1 of an industrial control plant. In addition, the framework aggregates the cyber-physical process data in one point for further analysis as part of the intrusion detection process. The novelty of this framework is its ability to detect anomalies that have a distributed impact on the cyber-physical process. The performance of the proposed framework evaluated as part of SWaT security showdown (S3) in which six international teams were invited to test the framework in a real industrial control system. The proposed framework outperformed other proposed academic IDS in term of detection of ICS threats during the S3 event, which was held from July 25-29, 2016 at Singapore University of Technology and Design.},
  file = {D\:\\GDrive\\zotero\\Ghaeini\\ghaeini_2016_hamids.pdf},
  isbn = {9781450345682},
  journal = {CPS-SPC 2016 - Proceedings of the 2nd ACM Workshop on Cyber-Physical Systems Security and PrivaCy, co-located with CCS 2016},
  keywords = {EtherNet/IP,Intrusion detection,SCADA}
}

@article{ghavamniaTemporalSystemCall2020,
  title = {Temporal {{System Call Specialization}} for {{Attack Surface Reduction}}},
  author = {Ghavamnia, Seyedhamed and Palit, Tapti and Mishra, Shachee and Polychronakis, Michalis},
  year = {2020},
  pages = {19},
  abstract = {Attack surface reduction through the removal of unnecessary application features and code is a promising technique for improving security without incurring any additional overhead. Recent software debloating techniques consider an application's entire lifetime when extracting its code requirements, and reduce the attack surface accordingly.},
  file = {D\:\\GDrive\\zotero\\Ghavamnia et al\\ghavamnia_et_al_2020_temporal_system_call_specialization_for_attack_surface_reduction.pdf},
  language = {en}
}

@techreport{ghemawatGoogleFileSystem2003,
  title = {The {{Google File System}}},
  author = {Ghemawat, Sanjay and Gobioff, Howard and Leung Google, Shun-Tak},
  year = {2003},
  abstract = {We have designed and implemented the Google File System , a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment , both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.},
  file = {D\:\\GDrive\\zotero\\Ghemawat\\ghemawat_2003_the_google_file_system.pdf},
  keywords = {clustered storage,D [4]: 3-Distributed file systems General Terms Design,data storage,distributed systems,measurement Keywords Fault tolerance,performance,reliability,scalability}
}

@inproceedings{ghodsiChoosyMaxminFair2013,
  title = {Choosy: Max-Min Fair Sharing for Datacenter Jobs with Constraints},
  shorttitle = {Choosy},
  booktitle = {Proceedings of the 8th {{ACM European Conference}} on {{Computer Systems}} - {{EuroSys}} '13},
  author = {Ghodsi, Ali and Zaharia, Matei and Shenker, Scott and Stoica, Ion},
  year = {2013},
  pages = {365},
  publisher = {{ACM Press}},
  address = {{Prague, Czech Republic}},
  doi = {10.1145/2465351.2465387},
  abstract = {Max-Min Fairness is a flexible resource allocation mechanism used in most datacenter schedulers. However, an increasing number of jobs have hard placement constraints, restricting the machines they can run on due to special hardware or software requirements. It is unclear how to define, and achieve, max-min fairness in the presence of such constraints. We propose Constrained Max-Min Fairness (CMMF), an extension to max-min fairness that supports placement constraints, and show that it is the only policy satisfying an important property that incentivizes users to pool resources. Optimally computing CMMF is challenging, but we show that a remarkably simple online scheduler, called Choosy, approximates the optimal scheduler well. Through experiments, analysis, and simulations, we show that Choosy on average differs 2\% from the optimal CMMF allocation, and lets jobs achieve their fair share quickly.},
  file = {D\:\\GDrive\\zotero\\Ghodsi et al\\ghodsi_et_al_2013_choosy.pdf},
  isbn = {978-1-4503-1994-2},
  language = {en}
}

@article{ghodsiMultiresourceFairQueueing,
  title = {Multi-Resource Fair Queueing for Packet Processing},
  author = {Ghodsi, Ali and Sekar, Vyas and Zaharia, Matei and Stoica, Ion},
  pages = {12},
  abstract = {Middleboxes are ubiquitous in today's networks and perform a variety of important functions, including IDS, VPN, firewalling, and WAN optimization. These functions differ vastly in their requirements for hardware resources (e.g., CPU cycles and memory bandwidth). Thus, depending on the functions they go through, different flows can consume different amounts of a middlebox's resources. While there is much literature on weighted fair sharing of link bandwidth to isolate flows, it is unclear how to schedule multiple resources in a middlebox to achieve similar guarantees. In this paper, we analyze several natural packet scheduling algorithms for multiple resources and show that they have undesirable properties. We propose a new algorithm, Dominant Resource Fair Queuing (DRFQ), that retains the attractive properties that fair sharing provides for one resource. In doing so, we generalize the concept of virtual time in classical fair queuing to multi-resource settings. The resulting algorithm is also applicable in other contexts where several resources need to be multiplexed in the time domain.},
  file = {D\:\\GDrive\\zotero\\Ghodsi et al\\ghodsi_et_al_multi-resource_fair_queueing_for_packet_processing.pdf},
  language = {en}
}

@techreport{gilbertBrewerConjectureFeasibility,
  title = {Brewer's {{Conjecture}} and the {{Feasibility}} of {{Consistent}}, {{Available}}, {{Partition}}-{{Tolerant Web Services}}},
  author = {Gilbert, Seth and Lynch, Nancy},
  abstract = {When designing distributed web services, there are three properties that are commonly desired: consistency, availability , and partition tolerance. It is impossible to achieve all three. In this note, we prove this conjecture in the asyn-chronous network model, and then discuss solutions to this dilemma in the partially synchronous model.},
  file = {D\:\\GDrive\\zotero\\Gilbert\\gilbert_brewer's_conjecture_and_the_feasibility_of_consistent,_available,.pdf}
}

@techreport{gilbertPerspectivesCAPTheorem,
  title = {Perspectives on the {{CAP Theorem}}},
  author = {Gilbert, Seth and Lynch, Nancy A},
  abstract = {Almost twelve years ago, in 2000, Eric Brewer introduced the idea that there is a fundamental trade-off between consistency, availability, and partition tolerance. This trade-off, which has become known as the CAP Theorem, has been widely discussed ever since. In this paper, we review the CAP Theorem and situate it within the broader context of distributed computing theory. We then discuss the practical implications of the CAP Theorem, and explore some general techniques for coping with the inherent trade-offs that it implies.},
  file = {D\:\\GDrive\\zotero\\Gilbert\\gilbert_perspectives_on_the_cap_theorem.pdf}
}

@article{ginsbachConstraintProgrammingHeterogeneous2020,
  title = {From {{Constraint Programming}} to {{Heterogeneous Parallelism}}},
  author = {Ginsbach, Philip},
  year = {2020},
  file = {D\:\\GDrive\\zotero\\Ginsbach\\ginsbach_2020_from_constraint_programming_to_heterogeneous_parallelism.pdf}
}

@techreport{godefroidAutomatedWhiteboxFuzz,
  title = {Automated {{Whitebox Fuzz Testing}}},
  author = {Godefroid, Patrice and Levin Microsoft, Michael Y and Molnar, David},
  abstract = {Fuzz testing is an effective technique for finding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a program and test the resulting values. We present an alternative whitebox fuzz testing approach inspired by recent advances in symbolic execution and dynamic test generation. Our approach records an actual run of the program under test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to find defects as fast as possible. We have implemented this algorithm in SAGE (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary file-reading Windows applications. We describe key optimizations needed to make dynamic test generation scale to large input files and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-specific knowledge , SAGE detects the MS07-017 ANI vulnerability, which was missed by extensive blackbox fuzzing and static analysis tools. Furthermore, while still in an early stage of development , SAGE has already discovered 30+ new bugs in large shipped Windows applications including image processors , media players, and file decoders. Several of these bugs are potentially exploitable memory access violations.},
  file = {D\:\\GDrive\\zotero\\Godefroid\\godefroid_automated_whitebox_fuzz_testing.pdf}
}

@article{godefroidCompositionalMaymustProgram2010,
  title = {Compositional May-Must Program Analysis: {{Unleashing}} the Power of Alternation},
  author = {Godefroid, Patrice and Nori, Aditya V. and Rajamani, Sriram K. and Tetali, Sai Deep},
  year = {2010},
  pages = {43--55},
  issn = {07308566},
  doi = {10.1145/1706299.1706307},
  abstract = {Program analysis tools typically compute two types of information: (1) may information that is true of all program executions and is used to prove the absence of bugs in the program, and (2) must information that is true of some program executions and is used to prove the existence of bugs in the program. In this paper, we propose a new algorithm, dubbed SMASH, which computes both may and must information compositionally . At each procedure boundary, may and must information is represented and stored as may and must summaries, respectively. Those summaries are computed in a demand driven manner and possibly using summaries of the opposite type. We have implemented SMASH using predicate abstraction (as in SLAM) for the may part and using dynamic test generation (as in DART) for the must part. Results of experiments with 69 Microsoft Windows 7 device drivers show that SMASH can significantly outperform may-only, must-only and non-compositional may-must algorithms. Indeed, our empirical results indicate that most complex code fragments in large programs are actually often either easy to prove irrelevant to the specific property of interest using may analysis or easy to traverse using directed testing. The fine-grained coupling and alternation of may (universal) and must (existential) summaries allows SMASH to easily navigate through these code fragments while traditional may-only, must-only or non-compositional may-must algorithms are stuck in their specific analyses. Copyright \textcopyright{} 2010 ACM.},
  file = {D\:\\GDrive\\zotero\\Godefroid\\godefroid_2010_compositional_may-must_program_analysis.pdf},
  isbn = {9781605584799},
  journal = {Conference Record of the Annual ACM Symposium on Principles of Programming Languages},
  keywords = {Abstraction refinement,Directed testing,Software model checking}
}

@techreport{godefroidDARTDirectedAutomated2005,
  title = {{{DART}}: {{Directed Automated Random Testing}}},
  author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  year = {2005},
  doi = {10.1145/1065010.1065036},
  abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  file = {D\:\\GDrive\\zotero\\Godefroid\\godefroid_2005_dart.pdf}
}

@article{godefroidSAGEWhiteboxFuzzing2012,
  title = {{{SAGE}}: {{Whitebox}} Fuzzing for Security Testing},
  author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
  year = {2012},
  volume = {55},
  pages = {40--44},
  issn = {00010782},
  doi = {10.1145/2093548.2093564},
  abstract = {SAGE has had a remarkable impact at Microsoft. \textcopyright{} 2012 ACM.},
  file = {D\:\\GDrive\\zotero\\Godefroid\\godefroid_2012_sage.pdf},
  journal = {Communications of the ACM},
  number = {3}
}

@article{godefroidTestGenerationUsing2012,
  title = {Test Generation Using Symbolic Execution},
  author = {Godefroid, Patrice},
  year = {2012},
  volume = {18},
  pages = {24--33},
  issn = {18688969},
  doi = {10.4230/LIPIcs.FSTTCS.2012.24},
  abstract = {This paper presents a short introduction to automatic code-driven test generation using symbolic execution. It discusses some key technical challenges, solutions and milestones, but is not an exhaustive survey of this research area. \textcopyright{} Patrice Godefroid.},
  isbn = {9783939897477},
  journal = {Leibniz International Proceedings in Informatics, LIPIcs},
  keywords = {Symbolic execution,Test generation,Testing,Verification},
  number = {Fsttcs}
}

@techreport{goelHerbivoreScalableEfficient,
  title = {Herbivore: {{A Scalable}} and {{Efficient Protocol}} for {{Anonymous Communication}}},
  author = {Goel, Sharad and Robson, Mark and Polte, Milo and Sirer, Emin G{\"u}n},
  abstract = {Anonymity is increasingly important for networked applications amidst concerns over censorship and privacy. In this paper, we describe Herbivore, a peer-to-peer, scalable, tamper-resilient communication system that provides provable anonymity and privacy. Building on dining cryptographer networks, Herbivore scales by partitioning the network into anonymizing cliques. Adversaries able to monitor all network traffic cannot deduce the identity of a sender or receiver beyond an anonymiz-ing clique. In addition to strong anonymity, Herbivore simultaneously provides high efficiency and scalability, distinguishing it from other anonymous communication protocols. Performance measurements from a prototype implementation show that the system can achieve high bandwidths and low latencies when deployed over the Internet.},
  file = {D\:\\GDrive\\zotero\\Goel\\goel_herbivore.pdf}
}

@article{goelWaitFreeStack2015,
  title = {A {{Wait}}-{{Free Stack}}},
  author = {Goel, Seep and Aggarwal, Pooja and Sarangi, Smruti R.},
  year = {2015},
  month = oct,
  abstract = {In this paper, we describe a novel algorithm to create a con- current wait-free stack. To the best of our knowledge, this is the first wait-free algorithm for a general purpose stack. In the past, researchers have proposed restricted wait-free implementations of stacks, lock-free implementations, and efficient universal constructions that can support wait-free stacks. The crux of our wait-free implementation is a fast pop operation that does not modify the stack top; instead, it walks down the stack till it finds a node that is unmarked. It marks it but does not delete it. Subsequently, it is lazily deleted by a cleanup operation. This operation keeps the size of the stack in check by not allowing the size of the stack to increase beyond a factor of W as compared to the actual size. All our operations are wait-free and linearizable.},
  file = {D\:\\GDrive\\zotero\\Goel et al\\goel_et_al_2015_a_wait-free_stack.pdf}
}

@techreport{gogertyDeKoElectricitybackedCurrency2011,
  title = {{{DeKo An}} Electricity-Backed Currency Proposal},
  author = {Gogerty, Nick and Zitoli, Joseph},
  year = {2011},
  abstract = {Currencies play an important role in facilitating trade and economic growth. Shifts in currency values may lead to economic dislocations deleterious to trade and growth. Most currencies are issued by government central banks. These central banks hold assets in the form of government debt and gold against the currency they issue. Alternatives to traditional debt and gold assets may make sense for central banks to hold as a supplement or substitute. One alternative asset for a central bank to hold consists of electricity delivering assets. Electricity delivering assets don't need to be physical assets such as fuel or power plants, but rather may be claims in the form of standardized Power Purchase Agreements for the delivery of electricity from power producers. Electricity delivering assets can hold their economic value more effectively than gold or debt due to price stability and resistance to devaluation from over-issuance. The DeKo-backed currency concept advocates a portfolio of diversified electricity delivering assets that offers more social benefits than gold and retains monetary value better than government debt.},
  file = {D\:\\GDrive\\zotero\\Gogerty\\gogerty_2011_deko_an_electricity-backed_currency_proposal.pdf}
}

@techreport{goguenandSECURITYPOLICIESSlY311ITY,
  title = {{{SECURITYPOLICIES AND SlY311}}?{{ITY MODELS}}},
  author = {Goguenand, J A and Mesajuer, J},
  file = {D\:\\GDrive\\zotero\\Goguenand\\goguenand_securitypolicies_and_sly311.pdf}
}

@techreport{goguenUnwindingInferenceControl,
  title = {Unwinding and {{Inference Control}}},
  author = {Goguen, Joseph A},
  file = {D\:\\GDrive\\zotero\\Goguen\\goguen_unwinding_and_inference_control.pdf}
}

@article{gohDatasetSupportResearch2017,
  title = {A Dataset to Support Research in the Design of Secure Water Treatment Systems},
  author = {Goh, Jonathan and Adepu, Sridhar and Junejo, Khurum Nazir and Mathur, Aditya},
  year = {2017},
  volume = {10242 LNCS},
  pages = {88--99},
  issn = {16113349},
  doi = {10.1007/978-3-319-71368-7_8},
  abstract = {This paper presents a dataset to support research in the design of secure Cyber Physical Systems (CPS). The data collection process was implemented on a six-stage Secure Water Treatment (SWaT) testbed. SWaT represents a scaled down version of a real-world industrial water treatment plant producing 5~gallons per minute of water filtered via membrane based ultrafiltration and reverse osmosis units. This plant allowed data collection under two behavioral modes: normal and attacked. SWaT was run non-stop from its ``empty'' state to fully operational state for a total of 11-days. During this period, the first seven days the system operated normally i.e. without any attacks or faults. During the remaining days certain cyber and physical attacks were launched on SWaT while data collection continued. The dataset reported here contains the physical properties related to the plant and the water treatment process, as well as network traffic in the testbed. The data of both physical properties and network traffic contains attacks that were created and generated by our research team.},
  file = {D\:\\GDrive\\zotero\\Goh\\goh_2017_a_dataset_to_support_research_in_the_design_of_secure_water_treatment_systems.pdf},
  isbn = {9783319713670},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Cyber physical systems,Datasets,Network traffic,Physical properties}
}

@article{gohSiRiUSSecuringRemote,
  title = {{{SiRiUS}}: {{Securing Remote Untrusted Storage}}},
  author = {Goh, Eu-Jin and Shacham, Hovav and Modadugu, Nagendra and Boneh, Dan},
  pages = {25},
  abstract = {This paper presents SiRiUS, a secure file system designed to be layered over insecure network and P2P file systems such as NFS, CIFS, OceanStore, and Yahoo! Briefcase. SiRiUS assumes the network storage is untrusted and provides its own read-write cryptographic access control for file level sharing. Key management and revocation is simple with minimal out-of-band communication. File system freshness guarantees are supported by SiRiUS using hash tree constructions. SiRiUS contains a novel method of performing file random access in a cryptographic file system without the use of a block server. Extensions to SiRiUS include large scale group sharing using the NNL key revocation construction. Our implementation of SiRiUS performs well relative to the underlying file system despite using cryptographic operations.},
  file = {D\:\\GDrive\\zotero\\Goh et al\\goh_et_al_sirius.pdf},
  language = {en}
}

@inproceedings{goktasOutControlOvercoming2014,
  title = {Out of {{Control}}: {{Overcoming Control}}-{{Flow Integrity}}},
  shorttitle = {Out of {{Control}}},
  booktitle = {2014 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Goktas, Enes and Athanasopoulos, Elias and Bos, Herbert and Portokalidis, Georgios},
  year = {2014},
  month = may,
  pages = {575--589},
  publisher = {{IEEE}},
  address = {{San Jose, CA}},
  doi = {10.1109/SP.2014.43},
  abstract = {As existing defenses like ALSR, DEP, and stack cookies are not sufficient to stop determined attackers from exploiting our software, interest in Control Flow Integrity (CFI) is growing. In its ideal form, CFI prevents any flow of control that was not intended by the original program, effectively putting a stop to exploitation based on return oriented programming (and many other attacks besides). Two main problems have prevented CFI from being deployed in practice. First, many CFI implementations require source code or debug information that is typically not available for commercial software. Second, in its ideal form, the technique is very expensive. It is for this reason that current research efforts focus on making CFI fast and practical. Specifically, much of the work on practical CFI is applicable to binaries, and improves performance by enforcing a looser notion of control flow integrity. In this paper, we examine the security implications of such looser notions of CFI: are they still able to prevent code reuse attacks, and if not, how hard is it to bypass its protection? Specifically, we show that with two new types of gadgets, return oriented programming is still possible. We assess the availability of our gadget sets, and demonstrate the practicality of these results with a practical exploit against Internet Explorer that bypasses modern CFI implementations.},
  file = {D\:\\GDrive\\zotero\\Goktas et al\\goktas_et_al_2014_out_of_control.pdf},
  isbn = {978-1-4799-4686-0},
  language = {en}
}

@techreport{gongComplexityComposabilitySecure,
  title = {The {{Complexity}} and {{Composability}} of {{Secure Interoperation}}*},
  author = {Gong, Li and Qian, Xiaolei},
  abstract = {Advances in distributed systems and networking technology have made interoperation not only feasible but also increasingly popular. We define the interop-eration of secure systems and its security, and prove complexity and composability results on obtaining optimal and secure interoperation. Most problems are NP-complete even f o r systems with very sample access control structures. Nevertheless, composobility reduces complexity in that secure global interoperation can be obtained incrementally by composing secure local in-leroperation. W e illustrate, through an application, how these theoretical results can help system designers in practice.},
  file = {D\:\\GDrive\\zotero\\Gong\\gong_the_complexity_and_composability_of_secure_interoperation.pdf}
}

@techreport{goodhopeBuildingLinkedInRealtime2012,
  title = {Building {{LinkedIn}}'s {{Real}}-Time {{Activity Data Pipeline}}},
  author = {Goodhope, Ken and Koshy, Joel and Kreps, Jay and Narkhede, Neha and Park, Richard and Rao, Jun and Yang, Victor and Linkedin, Ye},
  year = {2012},
  abstract = {One trend in the implementation of modern web systems is the use of activity data in the form of log or event messages that capture user and server activity. This data is at the heart of many internet systems in the domains of advertising, relevance, search, recommendation systems, and security, as well as continuing to fulfill its traditional role in analytics and reporting. Many of these uses place real-time demands on data feeds. Activity data is extremely high volume and real-time pipelines present new design challenges. This paper discusses the design and engineering problems we encountered in moving LinkedIn's data pipeline from a batch-oriented file aggregation mechanism to a real-time publish-subscribe system called Kafka. This pipeline currently runs in production at LinkedIn and handles more than 10 billion message writes each day with a sustained peak of over 172,000 messages per second. Kafka supports dozens of subscribing systems and delivers more than 55 billion messages to these consumer processing each day. We discuss the origins of this systems, missteps on the path to real-time, and the design and engineering problems we encountered along the way.},
  file = {D\:\\GDrive\\zotero\\Goodhope et al\\goodhope_et_al_2012_building_linkedin's_real-time_activity_data_pipeline.pdf}
}

@techreport{googleWhySilentUpdates,
  title = {Why {{Silent Updates Boost Security}}},
  author = {Google, Thomas Duebendorfer and Gmbh, Switzerland and Frei, Stefan},
  abstract = {Security fixes and feature improvements don't benefit the end user of software if the update mechanism and strategy is not effective. In this paper we analyze the effectiveness of different Web browsers update mechanisms; from Google Chrome's silent update mechanism to Opera's update requiring a full re-installation. We use anonymized logs from Google's world wide distributed Web servers. An analysis of the logged HTTP user-agent strings that Web browsers report when requesting any Web page is used to measure the daily browser version shares in active use. To the best of our knowledge, this is the first global scale measurement of Web browser update effectiveness comparing four different Web browser update strategies including Google Chrome. Our measurements prove that silent updates and little dependency on the underlying operating system are most effective to get users of Web browsers to surf the Web with the latest browser version. However, there is still room for improvement as we found. Google Chrome's advantageous silent update mechanism has been open sourced in April 2009. We recommend any software vendor to seriously consider deploying silent updates as this benefits both the vendor and the user, especially for widely used attack-exposed applications like Web browsers and browser plug-ins.},
  file = {D\:\\GDrive\\zotero\\Google\\google_why_silent_updates_boost_security.pdf}
}

@techreport{goslingJavaOverview1995,
  title = {Java: An {{Overview}}},
  author = {Gosling, James},
  year = {1995},
  file = {D\:\\GDrive\\zotero\\Gosling\\gosling_1995_java.pdf}
}

@book{goslingMasteringYourPhD2011,
  title = {Mastering {{Your PhD}}},
  author = {Gosling, Patricia and Noordam, Lambertus D.},
  year = {2011},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15847-6},
  file = {D\:\\GDrive\\zotero\\Gosling_Noordam\\gosling_noordam_2011_mastering_your_phd.pdf},
  isbn = {978-3-642-15846-9 978-3-642-15847-6},
  language = {en}
}

@article{grasASLRLinePractical2017,
  title = {{{ASLR}} on the {{Line}}: {{Practical Cache Attacks}} on the {{MMU}}},
  author = {Gras, Ben and Razavi, Kaveh and Bosman, Erik and Bos, Herbert and Giuffrida, Cristiano},
  year = {2017},
  doi = {10.14722/ndss.2017.23271},
  abstract = {\textemdash Address space layout randomization (ASLR) is an important first line of defense against memory corruption attacks and a building block for many modern countermeasures. Existing attacks against ASLR rely on software vulnerabilities and/or on repeated (and detectable) memory probing. In this paper, we show that neither is a hard requirement and that ASLR is fundamentally insecure on modern cache-based architectures, making ASLR and caching conflicting requirements (ASLR{$\oplus$}Cache, or simply AnC). To support this claim, we describe a new EVICT+TIME cache attack on the virtual address translation performed by the memory management unit (MMU) of modern processors. Our AnC attack relies on the property that the MMU's page-table walks result in caching page-table pages in the shared last-level cache (LLC). As a result, an attacker can derandomize virtual addresses of a victim's code and data by locating the cache lines that store the page-table entries used for address translation. Relying only on basic memory accesses allows AnC to be implemented in JavaScript without any specific instructions or software features. We show our JavaScript implementation can break code and heap ASLR in two major browsers running on the latest Linux operating system with 28 bits of entropy in 150 seconds. We further verify that the AnC attack is applicable to every modern architecture that we tried, including Intel, ARM and AMD. Mitigating this attack without naively disabling caches is hard, since it targets the low-level operations of the MMU. We conclude that ASLR is fundamentally flawed in sandboxed environments such as JavaScript and future defenses should not rely on randomized virtual addresses as a building block.},
  file = {D\:\\GDrive\\zotero\\Gras\\gras_2017_aslr_on_the_line.pdf},
  number = {March}
}

@book{grattonAchievingSuccessEngineering2020,
  title = {Achieving {{Success}} with the {{Engineering Dissertation}}},
  author = {Gratton, Petra and Gratton, Guy},
  year = {2020},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-33192-4},
  file = {D\:\\GDrive\\zotero\\Gratton_Gratton\\gratton_gratton_2020_achieving_success_with_the_engineering_dissertation.pdf},
  isbn = {978-3-030-33191-7 978-3-030-33192-4},
  language = {en}
}

@book{gratzerMoreMathLaTeX2016,
  title = {More {{Math Into LaTeX}}},
  author = {Gr{\"a}tzer, George},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23796-1},
  file = {D\:\\GDrive\\zotero\\Grätzer\\grätzer_2016_more_math_into_latex.pdf},
  isbn = {978-3-319-23795-4 978-3-319-23796-1},
  language = {en}
}

@book{gratzerPracticalLaTeX2014,
  title = {Practical {{LaTeX}}},
  author = {Gr{\"a}tzer, George},
  year = {2014},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-06425-3},
  file = {D\:\\GDrive\\zotero\\Grätzer\\grätzer_2014_practical_latex.pdf},
  isbn = {978-3-319-06424-6 978-3-319-06425-3},
  language = {en}
}

@techreport{greenbergCleanSlate4Da,
  title = {A {{Clean Slate 4D Approach}} to {{Network Control}} and {{Management}} *},
  author = {Greenberg, Albert and Hjalmtysson, Gisli and Maltz, David A and Myers, Andy and Rexford, Jennifer and Xie, Geoffrey and Yan, Hong and Zhan, Jibin and Zhang, Hui},
  abstract = {Today's data networks are surprisingly fragile and difficult to manage. We argue that the root of these problems lies in the complexity of the control and management planes-the software and protocols coordinating network elements-and particularly the way the decision logic and the distributed-systems issues are inexorably intertwined. We advocate a complete refactoring of the function-ality and propose three key principles-network-level objectives, network-wide views, and direct control-that we believe should underlie a new architecture. Following these principles, we identify an extreme design point that we call "4D," after the architecture's four planes: decision, dissemination, discovery, and data. The 4D architecture completely separates an AS's decision logic from protocols that govern the interaction among network elements. The AS-level objectives are specified in the decision plane, and enforced through direct configuration of the state that drives how the data plane forwards packets. In the 4D architecture, the routers and switches simply forward packets at the behest of the decision plane, and collect measurement data to aid the decision plane in controlling the network. Although 4D would involve substantial changes to today's control and management planes, the format of data packets does not need to change; this eases the deployment path for the 4D architecture, while still enabling substantial innovation in network control and management. We hope that exploring an extreme design point will help focus the attention of the research and industrial communities on this crucially important and intellectually challenging area.},
  file = {D\:\\GDrive\\zotero\\Greenberg\\greenberg_a_clean_slate_4d_approach_to_network_control_and_management.pdf;D\:\\GDrive\\zotero\\Greenberg\\greenberg_a_clean_slate_4d_approach_to_network_control_and_management2.pdf},
  keywords = {C21 [Network Architecture and Design]: Packet Switching Net-works,C22 [Network Protocols]: Routing Protocols,C23 [Network Operations]: Network Management General Terms Measurement; Control; Performance; Reliability Keywords Network management; robustness; control *}
}

@book{greenbergNextGenerationData2008,
  title = {Towards a {{Next Generation Data Center Architecture}}: {{Scalability}} and {{Commoditization}}},
  author = {Greenberg, Albert and Lahiri, Parantap and Maltz, David A and Patel, Parveen and Sengupta, Sudipta},
  year = {2008},
  abstract = {Applications hosted in today's data centers suffer from internal fragmentation of resources, rigidity, and bandwidth constraints imposed by the architecture of the network connecting the data cen-ter's servers. Conventional architectures statically map web services to Ethernet VLANs, each constrained in size to a few hundred servers owing to control plane overheads. The IP routers used to span traffic across VLANs and the load balancers used to spray requests within a VLAN across servers are realized via expensive customized hardware and proprietary software. Bisection bandwidth is low, severly constraining distributed computation. Further, the conventional architecture concentrates traffic in a few pieces of hardware that must be frequently upgraded and replaced to keep pace with demand-an approach that directly contradicts the prevailing philosophy in the rest of the data center, which is to scale out (adding more cheap components) rather than scale up (adding more power and complexity to a small number of expensive components). Commodity switching hardware is now becoming available with programmable control interfaces and with very high port speeds at very low port cost, making this the right time to redesign the data center networking infrastructure. In this paper, we describe Monsoon , a new network architecture, which scales and commoditizes data center networking. Monsoon realizes a simple mesh-like architecture using programmable commodity layer-2 switches and servers. In order to scale to 100,000 servers or more, Monsoon makes modifications to the control plane (e.g., source routing) and to the data plane (e.g., hot-spot free multipath routing via Valiant Load Balancing). It disaggregates the function of load balancing into a group of regular servers, with the result that load balancing server hardware can be distributed amongst racks in the data center leading to greater agility and less fragmentation. The architecture creates a huge, flexible switching domain, supporting any server/any service and unfragmented server capacity at low cost.},
  file = {D\:\\GDrive\\zotero\\Greenberg\\greenberg_2008_towards_a_next_generation_data_center_architecture.pdf},
  isbn = {978-1-60558-181-1}
}

@article{greenbergVL2ScalableFlexible2009,
  title = {{{VL2}}: {{A Scalable}} and {{Flexible Data Center Network}}},
  author = {Greenberg, Albert and Hamilton, James R and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, David A and Patel, Parveen and Sengupta, Sudipta},
  year = {2009},
  abstract = {To be agile and cost eeective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VLL, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-semantics. VLL uses (())at addressing to allow service instances to be placed anywhere in the network, (() Valiant Load Balancing to spread traac uniformly across network paths, and (() end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VLL's design is driven by detailed measurements of traac and fault data from a large operational cloud service provider. VLL's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scal-able and reliable network architecture. As a result, VLL networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VLL design using measurement, analysis, and experiments. Our VLL prototype shuues s.. TB of data among servers in nnn seconds-sustaining a rate that is sss of the maximum possible.},
  file = {D\:\\GDrive\\zotero\\Greenberg\\greenberg_2009_vl2.pdf},
  keywords = {C [Computer-Communi-cation Network]: Network Architecture and Design General Terms: Design,commoditization,Performance,Reliability Keywords: Data center network}
}

@techreport{gremzowCompiledLowLevelVirtual,
  title = {Compiled {{Low}}-{{Level Virtual Instruction Set Simulation}} and {{Profiling}} for {{Code Partitioning}} and {{ASIP}}-{{Synthesis}} in {{Hardware}}/{{Software Co}}-{{Design}}},
  author = {Gremzow, Carsten},
  abstract = {We present ongoing work and first results in static and detailed quantitative runtime analysis of LLVM byte code for the purpose of automatic procedural level partitioning and co-synthesis of complex software systems. Runtime behaviour is captured by reverse compilation of LLVM bytecode into augmented , self-profiling ANSI-C simulator programs retaining the LLVM instruction level. The actual global data flow is captured both in quantity and value range to guide function unit layout in the synthesis of application specific processors. Currently the implemented tool LLILA (Low Level Intermediate Language Analyzer) focuses on static code analysis on the inter-procedural data flow via e.g. function parameters and global variables to uncover a program's potential paths of data exchange.},
  file = {D\:\\GDrive\\zotero\\Gremzow\\gremzow_compiled_low-level_virtual_instruction_set_simulation_and_profiling_for_code.pdf},
  isbn = {1565553160},
  keywords = {Coarse-Grained Parallelism,Hardware/Software Co-Synthesis,Instruction Set Architecture Simulation,LLVM,Profiling,Quantitative Dataflow Analysis}
}

@techreport{grosserPollyPolyhedralOptimizationLLVM,
  title = {Polly-{{Polyhedral}} Optimization in {{LLVM}}},
  author = {Grosser, Tobias and Zheng, Hongbin and Aloor, Raghesh and Simb{\"u}rger, Andreas and Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l},
  abstract = {Various powerful polyhedral techniques exist to optimize computation intensive programs effectively. Applying these techniques on any non-trivial program is still surprisingly difficult and often not as effective as expected. Most poly-hedral tools are limited to a specific programming language. Even for this language, relevant code needs to match specific syntax that rarely appears in existing code. It is therefore hard or even impossible to process existing programs automatically. In addition, most tools target C or OpenCL code, which prevents effective communication with compiler internal optimizers. As a result target architecture specific optimizations are either little effective or not approached at all. In this paper we present Polly, a project to enable polyhe-dral optimizations in LLVM. Polly automatically detects and transforms relevant program parts in a language-independent and syntactically transparent way. Therefore, it supports programs written in most common programming languages and constructs like C++ iterators, goto based loops and pointer arithmetic. Internally it provides a state-of-the-art polyhedral library with full support for Z-polyhedra, advanced data dependency analysis and support for external optimizers. Polly includes integrated SIMD and OpenMP code generation. Through LLVM, machine code for CPUs and GPU accelerators, C source code and even hardware descriptions can be targeted.},
  file = {D\:\\GDrive\\zotero\\Grosser\\grosser_polly-polyhedral_optimization_in_llvm.pdf},
  keywords = {Loop Transformation,OpenMP,Polyhedral Model,SIMD,Tiling,Vectorization}
}

@article{Grover1996Fast,
  title = {Grover - 1996 - {{A}} Fast Quantum Mechanical Algorithm for Database s},
  file = {D\:\\GDrive\\zotero\\undefined\\grover_-_1996_-_a_fast_quantum_mechanical_algorithm_for_database_s.pdf}
}

@book{grubaHowWriteYour2017a,
  title = {How {{To Write Your First Thesis}}},
  author = {Gruba, Paul and Zobel, Justin},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-61854-8},
  file = {D\:\\GDrive\\zotero\\Gruba_Zobel\\gruba_zobel_2017_how_to_write_your_first_thesis2.pdf},
  isbn = {978-3-319-61853-1 978-3-319-61854-8},
  language = {en}
}

@article{grussCacheTemplateAttacks2015,
  title = {Cache Template Attacks: {{Automating}} Attacks on Inclusive Last-Level Caches},
  author = {Gruss, Daniel and Spreitzer, Raphael and Mangard, Stefan},
  year = {2015},
  pages = {897--912},
  abstract = {Recent work on cache attacks has shown that CPU caches represent a powerful source of information leakage. However, existing attacks require manual identification of vulnerabilities, i.e., data accesses or instruction execution depending on secret information. In this paper, we present Cache Template Attacks. This generic attack technique allows us to profile and exploit cache-based information leakage of any program automatically, without prior knowledge of specific software versions or even specific system information. Cache Template Attacks can be executed online on a remote system without any prior offline computations or measurements. Cache Template Attacks consist of two phases. In the profiling phase, we determine dependencies between the processing of secret information, e.g., specific key inputs or private keys of cryptographic primitives, and specific cache accesses. In the exploitation phase, we derive the secret values based on observed cache accesses. We illustrate the power of the presented approach in several attacks, but also in a useful application for developers. Among the presented attacks is the application of Cache Template Attacks to infer keystrokes and\textemdash even more severe\textemdash the identification of specific keys on Linux and Windows user interfaces. More specifically, for lowercase only passwords, we can reduce the entropy per character from log2(26) = 4.7 to 1.4 bits on Linux systems. Furthermore, we perform an automated attack on the T-table-based AES implementation of OpenSSL that is as efficient as state-of-the-art manual cache attacks.},
  file = {D\:\\GDrive\\zotero\\Gruss\\gruss_2015_cache_template_attacks.pdf},
  isbn = {9781931971232},
  journal = {Proceedings of the 24th USENIX Security Symposium}
}

@article{grussFlushFlushFast2016,
  title = {Flush+{{Flush}}: {{A}} Fast and Stealthy Cache Attack},
  author = {Gruss, Daniel and Maurice, Cl{\'e}mentine and Wagner, Klaus and Mangard, Stefan},
  year = {2016},
  volume = {9721},
  pages = {279--299},
  issn = {16113349},
  doi = {10.1007/978-3-319-40667-1_14},
  abstract = {Research on cache attacks has shown that CPU caches leak significant information. Proposed detection mechanisms assume that all cache attacks cause more cache hits and cache misses than benign applications and use hardware performance counters for detection. In this article, we show that this assumption does not hold by developing a novel attack technique: the Flush+Flush attack. The Flush+Flush attack only relies on the execution time of the flush instruction, which depends on whether data is cached or not. Flush+Flush does not make any memory accesses, contrary to any other cache attack. Thus, it causes no cache misses at all and the number of cache hits is reduced to a minimum due to the constant cache flushes. Therefore, Flush+Flush attacks are stealthy, i.e., the spy process cannot be detected based on cache hits and misses, or state-of-the-art detection mechanisms. The Flush+Flush attack runs in a higher frequency and thus is faster than any existing cache attack. With 496 KB/s in a cross-core covert channel it is 6.7 times faster than any previously published cache covert channel.},
  file = {D\:\\GDrive\\zotero\\Gruss\\gruss_2016_flush+flush.pdf},
  isbn = {9783319406664},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{grussKASLRDeadLong2017,
  title = {{{KASLR}} Is Dead: {{Long}} Live {{KASLR}}},
  author = {Gruss, Daniel and Lipp, Moritz and Schwarz, Michael and Fellner, Richard and Maurice, Cl{\'e}mentine and Mangard, Stefan},
  year = {2017},
  volume = {10379 LNCS},
  pages = {161--176},
  issn = {16113349},
  doi = {10.1007/978-3-319-62105-0_11},
  abstract = {Modern operating system kernels employ address space layout randomization (ASLR) to prevent control-flow hijacking attacks and code-injection attacks. While kernel security relies fundamentally on preventing access to address information, recent attacks have shown that the hardware directly leaks this information. Strictly splitting kernel space and user space has recently been proposed as a theoretical concept to close these side channels. However, this is not trivially possible due to architectural restrictions of the x86 platform. In this paper we present KAISER, a system that overcomes limitations of x86 and provides practical kernel address isolation. We implemented our proof-of-concept on top of the Linux kernel, closing all hardware side channels on kernel address information. KAISER enforces a strict kernel and user space isolation such that the hardware does not hold any information about kernel addresses while running in user mode. We show that KAISER protects against double page fault attacks, prefetch side-channel attacks, and TSX-based side-channel attacks. Finally, we demonstrate that KAISER has a runtime overhead of only 0.28\%.},
  file = {D\:\\GDrive\\zotero\\Gruss\\gruss_2017_kaslr_is_dead.pdf},
  isbn = {9783319621043},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{grussPrefetchSideChannelAttacks2016,
  title = {Prefetch {{Side}}-{{Channel Attacks}}: {{Bypassing SMAP}} and Kernel {{ASLR}}},
  author = {Gruss, Daniel and Maurice, Cl{\'e}mentine and Fogh, Anders and Lipp, Moritz and Mangard, Stefan},
  year = {2016},
  volume = {24-28-Octo},
  pages = {368--379},
  issn = {15437221},
  doi = {10.1145/2976749.2978356},
  abstract = {Modern operating systems use hardware support to protect against control-flow hijacking attacks such as code-injection attacks. Typically, write access to executable pages is prevented and kernel mode execution is restricted to kernel code pages only. However, current CPUs provide no protection against code-reuse attacks like ROP. ASLR is used to prevent these attacks by making all addresses unpredictable for an attacker. Hence, the kernel security relies fundamentally on preventing access to address information. We introduce Prefetch Side-Channel Attacks, a new class of generic attacks exploiting major weaknesses in prefetch instructions. This allows unprivileged attackers to obtain address information and thus compromise the entire system by defeating SMAP, SMEP, and kernel ASLR. Prefetch can fetch inaccessible privileged memory into various caches on Intel x86. It also leaks the translation-level for virtual addresses on both Intel x86 and ARMv8-A. We build three attacks exploiting these properties. Our first attack retrieves an exact image of the full paging hierarchy of a process, defeating both user space and kernel space ASLR. Our second attack resolves virtual to physical addresses to bypass SMAP on 64-bit Linux systems, enabling ret2dir attacks. We demonstrate this from unprivileged user programs on Linux and inside Amazon EC2 virtual machines. Finally, we demonstrate how to defeat kernel ASLR on Windows 10, enabling ROP attacks on kernel and driver binary code. We propose a new form of strong kernel isolation to protect commodity systems incuring an overhead of only 0.06-5.09\%.},
  file = {D\:\\GDrive\\zotero\\Gruss\\gruss_2016_prefetch_side-channel_attacks.pdf},
  isbn = {9781450341394},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {ASLR,Kernel vulnerabilities,Timing attacks}
}

@article{grussRowhammerJsRemote2016,
  title = {Rowhammer.Js: {{A}} Remote Software-Induced Fault Attack in {{JavaScript}}},
  author = {Gruss, Daniel and Maurice, Cl{\'e}mentine and Mangard, Stefan},
  year = {2016},
  volume = {9721},
  pages = {300--321},
  issn = {16113349},
  doi = {10.1007/978-3-319-40667-1_15},
  abstract = {A fundamental assumption in software security is that a memory location can only be modified by processes that may write to this memory location. However, a recent study has shown that parasitic effects in DRAM can change the content of a memory cell without accessing it, but by accessing other memory locations in a high frequency. This so-called Rowhammer bug occurs in most of today's memory modules and has fatal consequences for the security of all affected systems, e.g., privilege escalation attacks. All studies and attacks related to Rowhammer so far rely on the availability of a cache flush instruction in order to cause accesses to DRAM modules at a sufficiently high frequency. We overcome this limitation by defeating complex cache replacement policies. We show that caches can be forced into fast cache eviction to trigger the Rowhammer bug with only regular memory accesses. This allows to trigger the Rowhammer bug in highly restricted and even scripting environments. We demonstrate a fully automated attack that requires nothing but a website with JavaScript to trigger faults on remote hardware. Thereby we can gain unrestricted access to systems of website visitors. We show that the attack works on off-the-shelf systems. Existing countermeasures fail to protect against this new Rowhammer attack.},
  file = {D\:\\GDrive\\zotero\\Gruss\\gruss_2016_rowhammer.pdf},
  isbn = {9783319406664},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{grussStrongEfficientCache2017,
  title = {Strong and Efficient Cache Side-Channel Protection Using Hardware Transactional Memory},
  author = {Gruss, Daniel and Lettner, Julian and Schuster, Felix and Ohrimenko, Olga and Haller, Istvan and Costa, Manuel},
  year = {2017},
  pages = {219--233},
  abstract = {Cache-based side-channel attacks are a serious problem in multi-tenant environments, for example, modern cloud data centers. We address this problem with Cloak, a new technique that uses hardware transactional memory to prevent adversarial observation of cache misses on sensitive code and data. We show that Cloak provides strong protection against all known cache-based side-channel attacks with low performance overhead. We demonstrate the efficacy of our approach by retrofitting vulnerable code with Cloak and experimentally confirming immunity against state-of-the-art attacks. We also show that by applying Cloak to code running inside Intel SGX enclaves we can effectively block information leakage through cache side channels from enclaves, thus addressing one of the main weaknesses of SGX.},
  file = {D\:\\GDrive\\zotero\\Gruss\\gruss_2017_strong_and_efficient_cache_side-channel_protection_using_hardware_transactional.pdf},
  isbn = {9781931971409},
  journal = {Proceedings of the 26th USENIX Security Symposium}
}

@techreport{guarnieriSPECTECTORPrincipledDetection,
  title = {{{SPECTECTOR}}: {{Principled Detection}} of {{Speculative Information Flows}}},
  author = {Guarnieri, Marco and K{\"o}pf, Boris and Morales, Jos{\'e} F and Reineke, Jan and S{\'a}nchez, Andr{\'e}s},
  abstract = {Since the advent of SPECTRE, a number of countermeasures have been proposed and deployed. Rigorously reasoning about their effectiveness, however, requires a well-defined notion of security against speculative execution attacks, which has been missing until now. In this paper (1) we put forward speculative non-interference, the first semantic notion of security against speculative execution attacks, and (2) we develop SPECTECTOR, an algorithm based on symbolic execution to automatically prove speculative non-interference, or to detect violations. We implement SPECTECTOR in a tool, which we use to detect subtle leaks and optimizations opportunities in the way major compilers place SPECTRE countermeasures. A scalability analysis indicates that checking speculative non-interference does not exhibit fundamental bottlenecks beyond those inherited by symbolic execution.},
  file = {D\:\\GDrive\\zotero\\Guarnieri\\guarnieri_spectector.pdf}
}

@techreport{gudeNOXOperatingSystem,
  title = {{{NOX}}: {{Towards}} an {{Operating System}} for {{Networks}}},
  author = {Gude, Natasha and Koponen, Teemu and Pettit, Justin and Pfaff, Ben and Casado, Mart{\'i}n and Mckeown, Nick and Shenker, Scott},
  file = {D\:\\GDrive\\zotero\\Gude\\gude_nox.pdf}
}

@article{gullaschCacheGamesBringing2011,
  title = {Cache Games - {{Bringing}} Access-Based Cache Attacks on {{AES}} to Practice},
  author = {Gullasch, David and Bangerter, Endre and Krenn, Stephan},
  year = {2011},
  pages = {490--505},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2011.22},
  abstract = {Side channel attacks on cryptographic systems exploit information gained from physical implementations rather than theoretical weaknesses of a scheme. In recent years, major achievements were made for the class of so called access-driven cache attacks. Such attacks exploit the leakage of the memory locations accessed by a victim process. In this paper we consider the AES block cipher and present an attack which is capable of recovering the full secret key in almost realtime for AES-128, requiring only a very limited number of observed encryptions. Unlike previous attacks, we do not require any information about the plaintext (such as its distribution, etc.). Moreover, for the first time, we also show how the plaintext can be recovered without having access to the ciphertext at all. It is the first working attack on AES implementations using compressed tables. There, no efficient techniques to identify the beginning of AES rounds is known, which is the fundamental assumption underlying previous attacks. We have a fully working implementation of our attack which is able to recover AES keys after observing as little as 100 encryptions. It works against the OpenSSL 0.9.8n implementation of AES on Linux systems. Our spy process does not require any special privileges beyond those of a standard Linux user. A contribution of probably independent interest is a denial of service attack on the task scheduler of current Linux systems (CFS), which allows one to observe (on average) every single memory access of a victim process. \textcopyright{} 2011 IEEE.},
  file = {D\:\\GDrive\\zotero\\Gullasch\\gullasch_2011_cache_games_-_bringing_access-based_cache_attacks_on_aes_to_practice.pdf},
  isbn = {9780769544021},
  journal = {Proceedings - IEEE Symposium on Security and Privacy},
  keywords = {Access-based cache attacks,AES,Side channel}
}

@article{gunawiWhatBugsLive,
  title = {What {{Bugs Live}} in the {{Cloud}}? {{A Study}} of 3000+ {{Issues}} in {{Cloud Systems}}},
  author = {Gunawi, Haryadi S and Hao, Mingzhe and Adityatama, Jeffry and Eliazar, Kurnia J and Leesatapornwongsa, Tanakorn and Do, Thanh and Laksono, Agung and Lukman, Jeffrey F and {Patana-Anake}, Tiratat and Martin, Vincentius and Satria, Anang D},
  doi = {10.1145/2670979.2670986},
  abstract = {We conduct a comprehensive study of development and deployment issues of six popular and important cloud systems (Hadoop MapReduce, HDFS, HBase, Cassandra, ZooKeeper and Flume). From the bug repositories, we review in total 21,399 submitted issues within a three-year period (2011-2014). Among these issues, we perform a deep analysis of 3655 "vital" issues (i.e., real issues affecting deployments) with a set of detailed classifications. We name the product of our one-year study Cloud Bug Study database (CBSDB) [9], with which we derive numerous interesting insights unique to cloud systems. To the best of our knowledge, our work is the largest bug study for cloud systems to date.},
  file = {D\:\\GDrive\\zotero\\Gunawi\\gunawi_what_bugs_live_in_the_cloud.pdf},
  isbn = {9781450332521}
}

@book{guoPhGrindPh2012,
  title = {The {{Ph}}.{{D}}. {{Grind}}: {{A Ph}}.{{D}}. {{Student Memoir}}},
  author = {Guo, Philip J.},
  year = {2012},
  file = {D\:\\GDrive\\zotero\\Guo\\guo_2012_the_ph.pdf}
}

@techreport{guptaKelipsBuildingEfficient,
  title = {Kelips * : {{Building}} an {{Efficient}} and {{Stable P2P DHT Through Increased Memory}} and {{Background Overhead}}},
  author = {Gupta, Indranil and Birman, Ken and Linga, Prakash and Demers, Al and Van Renesse, Robbert},
  abstract = {A peer-to-peer (p2p) distributed hash table (DHT) system allows hosts to join and fail silently (or leave), as well as to insert and retrieve files (ob-jects). This paper explores a new point in design space in which increased memory usage and constant background communication overheads are tolerated to reduce file lookup times and increase stability to failures and churn. Our system, called Kelips, uses peer-to-peer gossip to partially repli-cate file index information. In Kelips, (a) under normal conditions, file lookups are resolved with O(1) time and complexity (i.e., independent of system size), and (b) membership changes (e.g., even when a large number of nodes fail) are detected and disseminated to the system quickly. Per-node memory requirements are small in medium-sized systems. When there are failures, lookup success is ensured through query rerouting. Kelips achieves load balancing comparable to existing systems. Locality is supported by using topologically aware gossip mechanisms. Initial results of an ongoing experimental study are also discussed. * System name derived from kelip-kelip, Malay name for the self-synchronizing fireflies that accumulate after dusk on branches of mangrove trees in Selangor, Malaysia [11]. Our system organizes similarly into affinity groups, and nodes in a group "synchronize" loosely to store information for a common set of file indices.},
  file = {D\:\\GDrive\\zotero\\Gupta\\gupta_kelips.pdf}
}

@article{guRemoteAttestationProgram2008,
  title = {Remote {{Attestation}} on {{Program Execution}}},
  author = {Gu, Liang and Ding, Xuhua and Deng, Robert H and Xie, Bing and Mei, Hong},
  year = {2008},
  abstract = {Remote attestation provides the basis for one platform to establish trusts on another. In this paper, we consider the problem of attesting the correctness of program executions. We propose to measure the target program and all the objects it depends on, with an assumption that the Secure Kernel and the Trusted Platform Module provide a secure execution environment through process separation. The at-testation of the target program begins with a program analysis on the source code or the binary code in order to find out the relevant executables and data objects. Whenever such a data object is accessed or a relevant executable is invoked due to the execution of the target program, its state is measured for attestation. Our scheme not only testifies to a program's execution, but also supports fine-granularity attestations and information flow checking.},
  file = {D\:\\GDrive\\zotero\\Gu\\gu_2008_remote_attestation_on_program_execution.pdf},
  keywords = {and Enhancement-Restructuring,and reengi-neering General Terms Security Keywords Trusted computing,D46 [Operating System]: Security and Protection; D24 [Software Engineering]: Distribution,Maintenance,program dependency,remote attestation,reverse engineering}
}

@incollection{gurfinkelSeaHornVerificationFramework2015,
  title = {The {{SeaHorn Verification Framework}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Gurfinkel, Arie and Kahsai, Temesghen and Komuravelli, Anvesh and Navas, Jorge A.},
  editor = {Kroening, Daniel and P{\u a}s{\u a}reanu, Corina S.},
  year = {2015},
  volume = {9206},
  pages = {343--361},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-21690-4_20},
  abstract = {In this paper, we present SeaHorn, a software verification framework. The key distinguishing feature of SeaHorn is its modular design that separates the concerns of the syntax of the programming language, its operational semantics, and the verification semantics. SeaHorn encompasses several novelties: it (a) encodes verification conditions using an efficient yet precise inter-procedural technique, (b) provides flexibility in the verification semantics to allow different levels of precision, (c) leverages the state-of-the-art in software model checking and abstract interpretation for verification, and (d) uses Horn-clauses as an intermediate language to represent verification conditions which simplifies interfacing with multiple verification tools based on Horn-clauses. SeaHorn provides users with a powerful verification tool and researchers with an extensible and customizable framework for experimenting with new software verification techniques. The effectiveness and scalability of SeaHorn are demonstrated by an extensive experimental evaluation using benchmarks from SV-COMP 2015 and real avionics code.},
  file = {D\:\\GDrive\\zotero\\Gurfinkel et al\\gurfinkel_et_al_2015_the_seahorn_verification_framework.pdf},
  isbn = {978-3-319-21689-8 978-3-319-21690-4},
  language = {en}
}

@article{haasBringingWebSpeed2017,
  title = {Bringing the {{Web}} up to {{Speed}} with {{WebAssembly}}},
  author = {Haas, Andreas and Rossberg, Andreas and Schuff, Derek L and Titzer, Ben L and Holman, Michael and Gohman, Dan and Wagner, Luke and Zakai, Alon and Bastien, JF},
  year = {2017},
  pages = {16},
  abstract = {The maturation of the Web platform has given rise to sophisticated and demanding Web applications such as interactive 3D visualization, audio and video software, and games. With that, efficiency and security of code on the Web has become more important than ever. Yet JavaScript as the only builtin language of the Web is not well-equipped to meet these requirements, especially as a compilation target.},
  file = {D\:\\GDrive\\zotero\\Haas et al\\haas_et_al_2017_bringing_the_web_up_to_speed_with_webassembly.pdf},
  language = {en}
}

@techreport{haberHowTimeStampDigital1991,
  title = {How to {{Time}}-{{Stamp}} a {{Digital Document}}},
  author = {Haber, Stuart and Stornetta, W Scott},
  year = {1991},
  pages = {999111},
  abstract = {The prospect of a world in which all text, audio, picture, and video documents are in digital form on easily modiiable media raises the issue of how to certify when a document was created or last changed. The problem is to time-stamp the data, not the medium. We propose computationally practical procedures for digital time-stamping of such documents so that it is infeasible for a user either to backdate or to forward-date his document, even with the collusion of a time-stamping service. Our procedures maintain complete privacy of the documents themselves, and require no record-keeping by the time-stamping service.},
  file = {D\:\\GDrive\\zotero\\Haber\\haber_1991_how_to_time-stamp_a_digital_document.pdf},
  journal = {Journal of Cryptology},
  number = {2}
}

@techreport{haldarSemanticRemoteAttestationA2004,
  title = {Semantic {{Remote Attestation}}-{{A Virtual Machine}} Directed Approach to {{Trusted Computing}}},
  author = {Haldar, Vivek and Chandra, Deepak and Franz, Michael},
  year = {2004},
  abstract = {Remote attestation is one of the core functionalities provided by trusted computing platforms. It holds the promise of enabling a variety of novel applications. However, current techniques for remote attestation are static, inexpressive and fundamentally incompatible with today's heterogeneous distributed computing environments and commodity open systems. Using language-based virtual machines enables the remote attestation of complex, dynamic, and high-level program properties-in a platform-independent way. We call this semantic remote attestation. This enables a number of novel applications that distribute trust dynamically. We have implemented a prototype framework for semantic remote at-testation, and present two example applications built on it-a peer-to-peer network protocol, and a distributed computing application.},
  file = {D\:\\GDrive\\zotero\\Haldar\\haldar_semantic_remote_attestation-a_virtual_machine_directed_approach_to_trusted.pdf}
}

@techreport{haldarSemanticRemoteAttestationA2004a,
  title = {Semantic {{Remote Attestation}}-{{A Virtual Machine}} Directed Approach to {{Trusted Computing}}},
  author = {Haldar, Vivek and Chandra, Deepak and Franz, Michael},
  year = {2004},
  abstract = {Remote attestation is one of the core functionali-ties provided by trusted computing platforms. It holds the promise of enabling a variety of novel applications. However, current techniques for remote attestation are static, inexpressive and fundamentally incompatible with today's heterogeneous distributed computing environments and commodity open systems. Using language-based virtual machines enables the remote attestation of complex, dynamic, and high-level program properties-in a platform-independent way. We call this semantic remote attestation. This enables a number of novel applications that distribute trust dynamically. We have implemented a prototype framework for semantic remote attestation, and present two example applications built on it-a peer-to-peer network protocol, and a distributed computing application.},
  file = {D\:\\GDrive\\zotero\\Haldar et al\\haldar_et_al_2004_semantic_remote_attestation-a_virtual_machine_directed_approach_to_trusted.pdf}
}

@techreport{haldermanLestWeRemember,
  title = {Lest {{We Remember}}: {{Cold Boot Attacks}} on {{Encryption Keys}}},
  author = {Halderman, J Alex and Schoen, Seth D and Heninger, Nadia and Clarkson, William and Paul, William and Calandrino, Joseph A and Feldman, Ariel J and Appelbaum, Jacob and Felten, Edward W},
  pages = {45},
  abstract = {Contrary to popular assumption, DRAMs used in most modern computers retain their contents for several seconds after power is lost, even at room temperature and even if removed from a motherboard. Although DRAMs become less reliable when they are not refreshed, they are not immediately erased, and their contents persist sufficiently for malicious (or forensic) acquisition of usable full-system memory images. We show that this phenomenon limits the ability of an operating system to protect cryptographic key material from an attacker with physical access. We use cold reboots to mount successful attacks on popular disk encryption systems using no special devices or materials. We experimentally characterize the extent and predictability of memory remanence and report that remanence times can be increased dramatically with simple cooling techniques. We offer new algorithms for finding cryptographic keys in memory images and for correcting errors caused by bit decay. Though we discuss several strategies for partially mitigating these risks, we know of no simple remedy that would eliminate them.},
  file = {D\:\\GDrive\\zotero\\Halderman\\halderman_lest_we_remember.pdf},
  keywords = {side-channel,ss}
}

@article{haldermanYouGoElections2008,
  title = {You {{Go}} to {{Elections}} with the {{Voting System You Have}}: {{Stop}}-{{Gap Mitigations}} for {{Deployed Voting Systems}}},
  author = {Halderman, J Alex and Shacham, Hovav and Rescorla, Eric and Wagner, David},
  year = {2008},
  pages = {14},
  abstract = {In light of the systemic vulnerabilities uncovered by recent reviews of deployed e-voting systems, the surest way to secure the voting process would be to scrap the existing systems and design new ones. Unfortunately, engineering new systems will take years, and many jurisdictions are unlikely to be able to afford new equipment in the near future. In this paper we ask how jurisdictions can make the best use of the equipment they already own until they can replace it. Starting from current practice, we propose defenses that involve new but realistic procedures, modest changes to existing software, and no changes to existing hardware. Our techniques achieve greatly improved protection against outsider attacks: they provide containment of viral spread, improve the integrity of vote tabulation, and offer some detection of individual compromised devices. They do not provide security against insiders with access to election management systems, which appears to require significantly greater changes to the existing systems.},
  file = {D\:\\GDrive\\zotero\\Halderman et al\\halderman_et_al_2008_you_go_to_elections_with_the_voting_system_you_have.pdf},
  language = {en}
}

@techreport{hallerRAYIntegratingRx,
  title = {{{RAY}}: {{Integrating Rx}} and {{Async}} for {{Direct}}-{{Style Reactive Streams}}},
  author = {Haller, Philipp and Miller EPFL, Heather},
  abstract = {Languages like F\#, C\#, and recently also Scala, provide "async" extensions which aim to make asynchronous programming easier by avoiding an inversion of control that is inherent in traditional callback-based programming models (for the purpose of this paper called the "Async" model). This paper outlines a novel approach to integrate the Async model with observable streams of the Reactive Extensions model which is best-known from the .NET platform, and of which popular implementations exist for Java, Ruby, and other widespread languages. We outline the translation of "Reactive Async" programs to efficient state machines, in a way that generalizes the state machine translation of regular Async programs. Finally, we sketch a formalization of the Reactive Async model in terms of a small-step operational semantics.},
  file = {D\:\\GDrive\\zotero\\Haller\\haller_ray.pdf}
}

@techreport{halperin80211Multiple,
  title = {802.11 with {{Multiple Antennas}} for {{Dummies}}},
  author = {Halperin, Daniel and Hu, Wenjun and Sheth, Anmol and Wetherall, David},
  abstract = {The use of multiple antennas and MIMO techniques based on them is the key feature of 802.11n equipment that sets it apart from earlier 802.11a/g equipment. It is responsible for superior performance, reliability and range. In this tu-torial, we provide a brief introduction to multiple antenna techniques. We describe the two main classes of those techniques , spatial diversity and spatial multiplexing. To ground our discussion, we explain how they work in 802.11n NICs in practice.},
  file = {D\:\\GDrive\\zotero\\Halperin\\halperin_802.pdf},
  keywords = {80211n,A1 [General Literature]: Introductory and Survey General Terms Design,Experimentation Keywords MIMO,Multiple Antennas}
}

@techreport{hamiltonDesigningDeployingInternetScale,
  title = {On {{Designing}} and {{Deploying Internet}}-{{Scale Services}}},
  author = {Hamilton, James},
  abstract = {The system-to-administrator ratio is commonly used as a rough metric to understand administrative costs in high-scale services. With smaller, less automated services this ratio can be as low as 2:1, whereas on industry leading, highly automated services, we've seen ratios as high as 2,500:1. Wi t h i n Microsoft services, Autopilot [1] is often cited as the magic behind the success of the Windows Live Search team in achieving high system-to-administrator ratios. While auto-administration is important, the most important factor is actually the service itself. Is the service efficient to auto-mate? Is it what we refer to more generally as operations-friendly? Services that are operations-friendly require little human intervention, and both detect and recover from all but the most obscure failures without administrative intervention. This paper summarizes the best practices accumulated over many years in scaling some of the largest services at MSN and Windows Live.},
  file = {D\:\\GDrive\\zotero\\Hamilton\\hamilton_on_designing_and_deploying_internet-scale_services.pdf}
}

@article{hamletTestingProgramsFinite1977,
  title = {Testing {{Programs}} with {{Finite Sets}} of {{Data}}},
  author = {Hamlet, R. G.},
  year = {1977},
  volume = {20},
  pages = {232--237},
  issn = {00104620},
  doi = {10.1093/comjnl/20.3.232},
  abstract = {The techniques of compiler optimization can be applied to aid a programmer in writing a program which cannot be improved by these techniques. A finite, representative set of test data can be useful in this process. The theoretical basis for the (nonconstructive) existence of test sets which serve as maximally effective standins for an unlimited number of input possibilities is given. It is argued that although the time required by a compiler to fully exercise a program on a set of data may be large, the corresponding improvement in the reliability of the program may also be large if the set meets the given theoretical requirements.},
  file = {D\:\\GDrive\\zotero\\Hamlet\\hamlet_1977_testing_programs_with_finite_sets_of_data.pdf},
  journal = {Computer Journal},
  number = {3}
}

@book{HandbookDigitalCurrency2015,
  title = {Handbook of {{Digital Currency}}},
  year = {2015},
  doi = {10.1016/b978-0-12-802117-0.09989-6},
  file = {D\:\\GDrive\\zotero\\undefined\\2015_handbook_of_digital_currency.pdf},
  isbn = {978-0-12-802117-0}
}

@techreport{handleyXORPOpenPlatform,
  title = {{{XORP}}: {{An Open Platform}} for {{Network Research}}},
  author = {Handley, Mark and Hodson, Orion and Kohler, Eddie},
  abstract = {Network researchers face a significant problem when deploying software in routers, either for experimentation or for pilot deployment. Router platforms are generally not open systems, in either the open-source or the open-API sense. In this paper we discuss the problems this poses, and present an eXtensible Open Router Platform (XORP) that we are developing to address these issues. Key goals are extensibility, performance and robustness. We show that different parts of a router need to prioritize these differently, and examine techniques by which we can satisfy these often conflicting goals. We aim for XORP to be both a research tool and a stable deployment platform, thus easing the transition of new ideas from the lab to the real world.},
  file = {D\:\\GDrive\\zotero\\Handley\\handley_xorp.pdf}
}

@misc{hansonEfficientReadingPapers2000,
  title = {Efficient {{Reading}} of {{Papers}} in {{Science}} and {{Technology}}},
  author = {Hanson, Michael J.},
  year = {2000},
  file = {D\:\\GDrive\\zotero\\Hanson\\hanson_2000_efficient_reading_of_papers_in_science_and_technology.pdf}
}

@book{hanzalekOmegaFlexibleScalable,
  title = {Omega:  Flexible,  Scalable  Schedulers  for  Large  Compute  Clusters},
  author = {Hanz{\'a}lek, Zden{\v e}k and H{\"a}rtig, Hermann and Castro, Miguel and Kaashoek, Frans and {ACM Special Interest Group in Operating Systems} and {Association for Computing Machinery} and {ACM Digital Library.}},
  file = {D\:\\GDrive\\zotero\\Hanzálek\\hanzálek_omega.pdf},
  isbn = {978-1-4503-1994-2}
}

@article{haoChallengeBeingEngineer,
  title = {The {{Challenge}} of {{Being}} an {{Engineer}} \textendash{} {{Reflections}} from a {{Security Engineer}}},
  author = {Hao, Feng},
  pages = {5},
  file = {D\:\\GDrive\\zotero\\Hao\\hao_the_challenge_of_being_an_engineer_–_reflections_from_a_security_engineer.pdf},
  language = {en}
}

@techreport{harchol-balterApplyingPhPrograms,
  title = {Applying to {{Ph}}.{{D}}. {{Programs}} in {{Computer Science}}},
  author = {{Harchol-Balter}, Mor},
  file = {D\:\\GDrive\\zotero\\Harchol-Balter\\harchol-balter_applying_to_ph.pdf},
  keywords = {research,to-read}
}

@article{hardjonoCloudBasedCommissioningConstrained,
  title = {Cloud-{{Based Commissioning}} of {{Constrained Devices}} Using {{Permissioned Blockchains}}},
  author = {Hardjono, Thomas and Smith, Ned},
  doi = {10.1145/2899007.2899012},
  abstract = {In this paper we describe a privacy-preserving method for commissioning an IoT device into a cloud ecosystem. The commissioning consists of the device proving its manufacturing provenance in an anonymous fashion without reliance on a trusted third party, and for the device to be anonymously registered through the use of a blockchain system. We introduce the ChainAnchor architecture that provides device commissioning in a privacy-preserving fashion. The goal of ChainAnchor is (i) to support anonymous device commissioning , (ii) to support device-owners being remunerated for selling their device sensor-data to service providers, and (iii) to incentivize device-owners and service providers to share sensor-data in a privacy-preserving manner.},
  file = {D\:\\GDrive\\zotero\\Hardjono\\hardjono_cloud-based_commissioning_of_constrained_devices_using_permissioned_blockchains.pdf},
  isbn = {9781450342834},
  keywords = {Blockchains,CCS Concepts •Security and privacy → Public key encryption,Identity Management,Keywords Internet of Things,Privacy,Security}
}

@article{Haribgpnotes,
  title = {Hari-Bgpnotes},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\FX2RZVBY\\hari-bgpnotes.pdf}
}

@misc{harperPracticalFoundationsProgramming2007,
  title = {Practical {{Foundations}} for {{Programming Languages}}},
  author = {Harper, Robert},
  year = {2007},
  month = jun,
  file = {D\:\\GDrive\\zotero\\Harper\\harper_2007_practical_foundations_for_programming_languages.pdf},
  howpublished = {https://web.archive.org/web/20070627041059/http://www.cs.cmu.edu/\textasciitilde rwh/plbook/book.pdf}
}

@article{harperProgrammingStandardML2011,
  title = {Programming in {{Standard ML}}},
  author = {Harper, Robert},
  year = {2011},
  pages = {297},
  file = {D\:\\GDrive\\zotero\\Harper\\harper_2011_programming_in_standard_ml.pdf},
  language = {en}
}

@book{harperTypeSystemsProgramming2000,
  title = {Type {{Systems}} for {{Programming Languages}}},
  author = {Harper, Robert},
  year = {2000},
  file = {D\:\\GDrive\\zotero\\Harper\\harper_2000_type_systems_for_programming_languages.pdf}
}

@techreport{hartMakingLocklessSynchronization,
  title = {Making {{Lockless Synchronization Fast}}: {{Performance Implications}} of {{Memory Reclamation}}},
  author = {Hart, Thomas E and Mckenney, Paul E and Brown, Angela Demke},
  abstract = {Achieving high performance for concurrent applications on modern multiprocessors remains challenging. Many programmers avoid locking to improve performance, while others replace locks with non-blocking synchronization to protect against deadlock, priority inversion, and convoying. In both cases, dynamic data structures that avoid locking, require a memory reclamation scheme that reclaims nodes once they are no longer in use. The performance of existing memory reclamation schemes has not been thoroughly evaluated. We conduct the first fair and comprehensive comparison of three recent schemes-quiescent-state-based reclamation, epoch-based reclamation, and hazard-pointer-based reclamation-using a flexible microbenchmark. Our results show that there is no globally optimal scheme. When evaluating lockless synchronization , programmers and algorithm designers should thus carefully consider the data structure, the workload, and the execution environment, each of which can dramatically affect memory reclamation performance.},
  file = {D\:\\GDrive\\zotero\\Hart\\hart_making_lockless_synchronization_fast.pdf}
}

@techreport{hartsonFULLPROTECTIONSPECIFICATIONS,
  title = {{{FULL PROTECTION SPECIFICATIONS IN THE SEMANTIC MODEL FOR DATABASE PROTECTION LANGUAGES}}*},
  author = {Hartson, H Rex and Hsiao, David K},
  file = {D\:\\GDrive\\zotero\\Hartson\\hartson_full_protection_specifications_in_the_semantic_model_for_database_protection.pdf}
}

@techreport{harveyIntegerMultiplicationTime2019,
  title = {Integer Multiplication in Time {{O}}(n Log n)},
  author = {Harvey, David and Van Der Hoeven, Joris},
  year = {2019},
  abstract = {We present an algorithm that computes the product of two n-bit integers in O(n log n) bit operations.},
  file = {D\:\\GDrive\\zotero\\Harvey\\harvey_2019_integer_multiplication_in_time_o(n_log_n).pdf}
}

@article{havelundModelCheckingJAVA2000,
  title = {Model Checking {{JAVA}} Programs Using {{JAVA PathFinder}}},
  author = {Havelund, Klaus and Pressburger, Thomas},
  year = {2000},
  volume = {2},
  pages = {366--381},
  issn = {14332779},
  doi = {10.1007/s100090050043},
  abstract = {This paper describes a translator called Java PathFinder (Jpf), which translates from Java to Promela, the modeling language of the Spin model checker. Jpf translates a given Java program into a Promela model, which then can be model checked using Spin. The Java program may contain assertions, which are translated into similar assertions in the Promela model. The Spin model checker will then look for deadlocks and violations of any stated assertions. Jpf generates a Promela model with the same state space characteristics as the Java program. Hence, the Java program must have a finite and tractable state space. This work should be seen in a broader attempt to make formal methods applicable within NASA's areas such as space, aviation, and robotics. The work is a continuation of an effort to formally analyze, using Spin, a multi-threaded operating system for the Deep-Space 1 space craft, and of previous work in applying existing model checkers and theorem provers to real applications. \textcopyright{} 2000 Springer-Verlag.},
  file = {D\:\\GDrive\\zotero\\Havelund\\havelund_2000_model_checking_java_programs_using_java_pathfinder.pdf},
  journal = {International Journal on Software Tools for Technology Transfer},
  keywords = {Assertions,Concurrent programming,Deadlocks,Java,Model checking,Program verification,Spin},
  number = {4}
}

@inproceedings{hawblitzelIronFleetProvingPractical2015,
  title = {{{IronFleet}}: {{Proving}} Practical Distributed Systems Correct},
  booktitle = {{{SOSP}} 2015 - {{Proceedings}} of the 25th {{ACM Symposium}} on {{Operating Systems Principles}}},
  author = {Hawblitzel, Chris and Howell, Jon and Kapritsos, Manos and Lorch, Jacob R. and Parno, Bryan and Roberts, Michael L. and Setty, Srinath and Zill, Brian},
  year = {2015},
  month = oct,
  pages = {1--17},
  publisher = {{Association for Computing Machinery, Inc}},
  doi = {10.1145/2815400.2815428},
  abstract = {Distributed systems are notorious for harboring subtle bugs. Verification can, in principle, eliminate these bugs a priori, but verification has historically been difficult to apply at full-program scale, much less distributed-system scale. We describe a methodology for building practical and provably correct distributed systems based on a unique blend of TLA-style state-machine refinement and Hoare-logic verification. We demonstrate the methodology on a complex implementation of a Paxos-based replicated state machine library and a lease-based sharded key-value store. We prove that each obeys a concise safety specification, as well as desirable liveness requirements. Each implementation achieves performance competitive with a reference system. With our methodology and lessons learned, we aim to raise the standard for distributed systems from ``tested'' to ``correct.''},
  file = {D\:\\GDrive\\zotero\\Hawblitzel\\hawblitzel_2015_ironfleet.pdf},
  isbn = {978-1-4503-3834-9}
}

@article{hawkingPropertiesExpandingUniverses1966,
  title = {Properties of Expanding Universes},
  author = {Hawking, Stephen},
  year = {1966},
  month = mar,
  publisher = {{Apollo - University of Cambridge Repository}},
  doi = {10.17863/CAM.11283},
  abstract = {Some implications and consequences of the expansion of the universe are examined. In Chapter 1 it is shown that this expansion creates grave difficulties for the Hoyle-Narlikar theory of gravitation. Chapter 2 deals with perturbations of an expanding homogeneous and isotropic universe. The conclusion is reached that galaxies cannot be formed as a result of the growth of perturbations that were initially small. The propagation and absorption of gravitational radiation is also investigated in this approximation. In Chapter 3 gravitational radiation in an expanding universe is examined by a method of asymptotic expansions. The 'peeling off' behaviour and the asymptotic group are derived. Chapter 4 deals with the occurrence of singularities in cosmological models. It is shown that a singularity is inevitable provided that certain very general conditions are satisfied.},
  collaborator = {{Apollo-University Of Cambridge Repository} and {Apollo-University Of Cambridge Repository}},
  copyright = {\textcopyright{} Stephen Hawking, All Rights Reserved, All Rights Reserved, https://www.rioxx.net/licenses/all-rights-reserved/},
  file = {D\:\\GDrive\\zotero\\Hawking\\hawking_1966_properties_of_expanding_universes.pdf}
}

@article{hazhirpasandJavaCryptographyUses2016,
  title = {Java {{Cryptography Uses}} in the {{Wild}}},
  author = {Hazhirpasand, Mohammadreza and Ghafari, Mohammad and Nierstrasz, Oscar},
  year = {2016},
  volume = {6},
  pages = {pages},
  doi = {10.1145/nnnnnnn.nnnnnnn},
  abstract = {[Background] Previous research has shown that developers commonly misuse cryptography APIs. [Aim] We have conducted an exploratory study to ond out how crypto APIs are used in open-source Java projects, what types of misuses exist, and why developers make such mistakes. [Method] We used a static analysis tool to analyze hundreds of open-source Java projects that rely on Java Cryptography Architecture, and manually inspected half of the analysis results to assess the tool results. We also contacted the maintainers of these projects by creating an issue on the GitHub repository of each project, and discussed the misuses with developers. [Results] We learned that 85\% of Cryptography APIs are misused, however, not every misuse has severe consequences. Developer feedback showed that security caveats in the documentation of crypto APIs are rare, developers may overlook misuses that originate in third-party code, and the context where a Crypto API is used should be taken into account. [Conclusion] We conclude that using Crypto APIs is still problematic for developers but blindly blaming them for such misuses may lead to erroneous conclusions.},
  file = {D\:\\GDrive\\zotero\\Hazhirpasand\\hazhirpasand_2016_java_cryptography_uses_in_the_wild.pdf},
  keywords = {empirical study,Java cryptography,security}
}

@article{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.}
}

@article{heilmanEclipseAttacksBitcoin,
  title = {Eclipse {{Attacks}} on {{Bitcoin}}'s {{Peer}}-to-{{Peer Network}}},
  author = {Heilman, Ethan and Kendler, Alison and Zohar, Aviv and Goldberg, Sharon},
  pages = {17},
  abstract = {We present eclipse attacks on bitcoin's peer-to-peer network. Our attack allows an adversary controlling a sufficient number of IP addresses to monopolize all connections to and from a victim bitcoin node. The attacker can then exploit the victim for attacks on bitcoin's mining and consensus system, including N-confirmation double spending, selfish mining, and adversarial forks in the blockchain. We take a detailed look at bitcoin's peerto-peer network, and quantify the resources involved in our attack via probabilistic analysis, Monte Carlo simulations, measurements and experiments with live bitcoin nodes. Finally, we present countermeasures, inspired by botnet architectures, that are designed to raise the bar for eclipse attacks while preserving the openness and decentralization of bitcoin's current network architecture.},
  file = {D\:\\GDrive\\zotero\\Heilman et al\\heilman_et_al_eclipse_attacks_on_bitcoin’s_peer-to-peer_network.pdf},
  language = {en}
}

@article{heinePracticalFlowSensitiveContextSensitive2003,
  title = {A {{Practical Flow}}-{{Sensitive}} and {{Context}}-{{Sensitive C}} and {{C}}++ {{Memory Leak Detector}}},
  author = {Heine, David L and Lam, Monica S},
  year = {2003},
  pages = {14},
  abstract = {This paper presents a static analysis tool that can automatically find memory leaks and deletions of dangling pointers in large C and C++ applications.},
  file = {D\:\\GDrive\\zotero\\Heine_Lam\\heine_lam_2003_a_practical_flow-sensitive_and_context-sensitive_c_and_c++_memory_leak_detector.pdf},
  language = {en}
}

@techreport{hellandLifeDistributedTransactions,
  title = {Life beyond {{Distributed Transactions}}: An {{Apostate}}'s {{Opinion Position Paper}}},
  author = {Helland, Pat},
  file = {D\:\\GDrive\\zotero\\Helland\\helland_life_beyond_distributed_transactions.pdf}
}

@incollection{heningerReconstructingRSAPrivate2009,
  title = {Reconstructing {{RSA Private Keys}} from {{Random Key Bits}}},
  booktitle = {Advances in {{Cryptology}} - {{CRYPTO}} 2009},
  author = {Heninger, Nadia and Shacham, Hovav},
  editor = {Halevi, Shai},
  year = {2009},
  volume = {5677},
  pages = {1--17},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03356-8_1},
  abstract = {We show that an RSA private key with small public exponent can be efficiently recovered given a 0.27 fraction of its bits at random. An important application of this work is to the ``cold boot'' attacks of Halderman et al. We make new observations about the structure of RSA keys that allow our algorithm to make use of the redundant information in the typical storage format of an RSA private key. Our algorithm itself is elementary and does not make use of the lattice techniques used in other RSA key reconstruction problems. We give an analysis of the running time behavior of our algorithm that matches the threshold phenomenon observed in our experiments.},
  file = {D\:\\GDrive\\zotero\\Heninger_Shacham\\heninger_shacham_2009_reconstructing_rsa_private_keys_from_random_key_bits.pdf},
  isbn = {978-3-642-03355-1 978-3-642-03356-8},
  language = {en}
}

@article{henryPAGAIPathSensitive2012,
  title = {{{PAGAI}}: {{A}} Path Sensitive Static Analyser},
  author = {Henry, Julien and Monniaux, David and Moy, Matthieu},
  year = {2012},
  volume = {289},
  pages = {15--25},
  issn = {15710661},
  doi = {10.1016/j.entcs.2012.11.003},
  abstract = {We describe the design and the implementation of PAGAI, a new static analyzer working over the LLVM compiler infrastructure, which computes inductive invariants on the numerical variables of the analyzed program. PAGAI implements various state-of-the-art algorithms combining abstract interpretation and decision procedures (SMT-solving), focusing on distinction of paths inside the control flow graph while avoiding systematic exponential enumerations. It is parametric in the abstract domain in use, the iteration algorithm, and the decision procedure. We compared the time and precision of various combinations of analysis algorithms and abstract domains, with extensive experiments both on personal benchmarks and widely available GNU programs. \textcopyright{} 2012 Elsevier B.V. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Henry\\henry_2012_pagai.pdf},
  journal = {Electronic Notes in Theoretical Computer Science},
  keywords = {Abstract Interpretation,Decision Procedure,Program Verification,Satisfiability Modulo Theories,Static Analysis}
}

@article{herleyResearchAgendaAcknowledging2012,
  title = {A Research Agenda Acknowledging the Persistence of Passwords},
  author = {Herley, Cormac and Van Oorschot, Paul},
  year = {2012},
  volume = {10},
  pages = {28--36},
  issn = {15407993},
  doi = {10.1109/MSP.2011.150},
  abstract = {Despite countless attempts and near-universal desire to replace passwords, they're more widely used than ever. Th e authors assert that, in many instances, passwords are the best-fi t solution and suggest better means to concretely identify actual requirements and weight their relative importance in target scenarios. \textcopyright{} 2012 IEEE.},
  file = {D\:\\GDrive\\zotero\\Herley\\herley_2012_a_research_agenda_acknowledging_the_persistence_of_passwords.pdf},
  journal = {IEEE Security and Privacy},
  keywords = {authentication alternatives,competing requirements,evaluation,passwords,supporting tools,systematic research},
  number = {1}
}

@article{hermannsProbabilisticCEGAR2008,
  title = {Probabilistic {{CEGAR}}},
  author = {Hermanns, Holger and Wachter, Bj{\"o}rn and Zhang, Lijun},
  year = {2008},
  volume = {5123 LNCS},
  pages = {162--175},
  issn = {03029743},
  doi = {10.1007/978-3-540-70545-1_16},
  abstract = {Counterexample-guided abstraction refinement (CEGAR) has been en vogue for the automatic verification of very large systems in the past years. When trying to apply CEGAR to the verification of probabilistic systems, various foundational questions arise. This paper explores them in the context of predicate abstraction. \textcopyright{} 2008 Springer-Verlag.},
  file = {D\:\\GDrive\\zotero\\Hermanns\\hermanns_2008_probabilistic_cegar.pdf},
  isbn = {3540705430},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{hermansenPreschoolChildrenRarely2021,
  title = {Preschool {{Children Rarely Seek Empirical Data That Could Help Them Complete}} a {{Task When Observation}} and {{Testimony Conflict}}},
  author = {Hermansen, Tone K. and Ronfard, Samuel and Harris, Paul L. and Zambrana, Imac M.},
  year = {2021},
  volume = {n/a},
  issn = {1467-8624},
  doi = {10.1111/cdev.13612},
  abstract = {Children (N = 278, 34\textendash 71 months, 54\% girls) were told which of two figurines turned on a music box and also observed empirical evidence either confirming or conflicting with that testimony. Children were then asked to sort novel figurines according to whether they could make the music box work or not. To see whether children would explore which figurine turned on the music box, especially when the observed and testimonial evidence conflicted, children were given access to the music box during their sorting. However, children rarely explored. Indeed, they struggled to disregard the misleading testimony both when sorting the figurines and when asked about a future attempt. In contrast, children who explored the effectiveness of the figurines dismissed the misleading testimony.},
  annotation = {\_eprint: https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/cdev.13612},
  file = {D\:\\GDrive\\zotero\\Hermansen et al\\hermansen_et_al_2021_preschool_children_rarely_seek_empirical_data_that_could_help_them_complete_a.pdf;C\:\\Users\\Admin\\Zotero\\storage\\JJUSFZII\\cdev.html},
  journal = {Child Development},
  language = {en},
  number = {n/a}
}

@techreport{herzbergCONCATENATIVEPROGRAMMINGOverlooked,
  title = {{{CONCATENATIVE PROGRAMMING An Overlooked Paradigm}} in {{Functional Programming}}},
  author = {Herzberg, Dominikus and Reichert, Tim},
  abstract = {Based on the state of our ongoing research into Language-Driven Software Development (LDSD) and Language-Oriented Programming (LOP) we argue that the yet relatively unknown paradigm of concatena-tive programming is valuable for fundamental software engineering research and might prove to be a suitable foundation for future programming. To be sound, we formally introduce Concat, our research prototype of a purely functional concatenative language. The simplicity of Concat is contrasted by its expressiveness and a richness of inspiring approaches. Concatenative languages contribute a fresh and different sight on functional programming, which might help tackle challenges in LDSD/LOP from a new viewpoint.},
  file = {D\:\\GDrive\\zotero\\Herzberg\\herzberg_concatenative_programming_an_overlooked_paradigm_in_functional_programming.pdf},
  keywords = {concatenative languages,functional programming,language-oriented programming}
}

@article{hewittViewingControlStructures1977,
  title = {Viewing Control Structures as Patterns of Passing Messages},
  author = {Hewitt, Carl},
  year = {1977},
  month = jun,
  volume = {8},
  pages = {323--364},
  issn = {00043702},
  doi = {10.1016/0004-3702(77)90033-9},
  file = {D\:\\GDrive\\zotero\\Hewitt\\hewitt_1977_viewing_control_structures_as_patterns_of_passing_messages.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {3}
}

@article{hickeyHistoryClojure2020,
  title = {A {{History}} of {{Clojure}}},
  author = {Hickey, Rich and Mezini, Mira},
  year = {2020},
  doi = {10.1145/3386321},
  file = {D\:\\GDrive\\zotero\\Hickey\\hickey_2020_a_history_of_clojure.pdf}
}

@article{hieronsUsingFormalSpecifications2009,
  title = {Using Formal Specifications to Support Testing},
  author = {Hierons, Robert M. and Bogdanov, Kirill and Bowen, Jonathan P. and Cleaveland, Rance and Derrick, John and Dick, Jeremy and Gheorghe, Marian and Harman, Mark and Kapoor, Kalpesh and Krause, Paul and L{\"u}ttgen, Gerald and Simons, Anthony J.H. and Vilkomir, Sergiy and Woodward, Martin R. and Zedan, Hussein},
  year = {2009},
  volume = {41},
  pages = {1--76},
  issn = {03600300},
  doi = {10.1145/1459352.1459354},
  abstract = {Formal methods and testing are two important approaches that assist in the development of high-quality software. While traditionally these approaches have been seen as rivals, in recent years a new consensus has developed in which they are seen as complementary. This article reviews the state of the art regarding ways in which the presence of a formal specification can be used to assist testing.\textcopyright{} 2009 ACM.},
  file = {D\:\\GDrive\\zotero\\Hierons\\hierons_2009_using_formal_specifications_to_support_testing.pdf},
  journal = {ACM Computing Surveys},
  keywords = {Formal methods,Software testing},
  number = {2}
}

@techreport{hindmanMesosPlatformFineGrained,
  title = {Mesos: {{A Platform}} for {{Fine}}-{{Grained Resource Sharing}} in the {{Data Center}}},
  author = {Hindman, Benjamin and Konwinski, Andy and Zaharia, Matei and Ghodsi, Ali and Joseph, Anthony D and Katz, Randy and Shenker, Scott},
  abstract = {We present Mesos, a platform for sharing commodity clusters between multiple diverse cluster computing frameworks, such as Hadoop and MPI. Sharing improves cluster utilization and avoids per-framework data repli-cation. Mesos shares resources in a fine-grained manner , allowing frameworks to achieve data locality by taking turns reading data stored on each machine. To support the sophisticated schedulers of today's frameworks , Mesos introduces a distributed two-level scheduling mechanism called resource offers. Mesos decides how many resources to offer each framework, while frameworks decide which resources to accept and which computations to run on them. Our results show that Mesos can achieve near-optimal data locality when sharing the cluster among diverse frameworks, can scale to 50,000 (emulated) nodes, and is resilient to failures.},
  file = {D\:\\GDrive\\zotero\\Hindman\\hindman_mesos.pdf},
  keywords = {distributed systems}
}

@inproceedings{hindPointerAnalysisHaven2001,
  title = {Pointer Analysis: Haven't We Solved This Problem Yet?},
  shorttitle = {Pointer Analysis},
  booktitle = {Proceedings of the 2001 {{ACM SIGPLAN}}-{{SIGSOFT}} Workshop on {{Program}} Analysis for Software Tools and Engineering  - {{PASTE}} '01},
  author = {Hind, Michael},
  year = {2001},
  pages = {54--61},
  publisher = {{ACM Press}},
  address = {{Snowbird, Utah, United States}},
  doi = {10.1145/379605.379665},
  file = {D\:\\GDrive\\zotero\\Hind\\hind_2001_pointer_analysis.pdf},
  isbn = {978-1-58113-413-1},
  language = {en}
}

@techreport{hinrichsHerbrandLogic2006,
  title = {Herbrand {{Logic}}},
  author = {Hinrichs, Timothy and Genesereth, Michael},
  year = {2006},
  file = {D\:\\GDrive\\zotero\\Hinrichs\\hinrichs_2006_herbrand_logic.pdf}
}

@article{hoareAxiomaticBasisComputer1969,
  title = {An Axiomatic Basis for Computer Programming},
  author = {Hoare, C. A.R.},
  year = {1969},
  volume = {12},
  pages = {576--580},
  issn = {15577317},
  doi = {10.1145/363235.363259},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics. \textcopyright{} 1969 ACM. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Hoare\\hoare_1969_an_axiomatic_basis_for_computer_programming.pdf;D\:\\GDrive\\zotero\\Hoare\\hoare_1969_an_axiomatic_basis_for_computer_programming2.pdf;D\:\\GDrive\\zotero\\Hoare\\hoare_an_axiomatic_basis_for_computer_programming.pdf},
  journal = {Communications of the ACM},
  keywords = {axiomatic method,formal language definition,machine-independent programming,program documentation,programming language design,theory of programming' proofs of programs},
  number = {10}
}

@techreport{hoareCommunicatingSequentialProcesses2004,
  title = {Communicating {{Sequential Processes}}},
  author = {Hoare, C A R},
  year = {2004},
  file = {D\:\\GDrive\\zotero\\Hoare\\hoare_2004_communicating_sequential_processes.pdf}
}

@techreport{hockmanEthnographicTechnologicalStudy2013,
  title = {An Ethnographic and Technological Study of Breakbeats in {{Hardcore}}, {{Jungle}}, and {{Drum}} \& {{Bass}}},
  author = {Hockman, Jason A},
  year = {2013},
  file = {D\:\\GDrive\\zotero\\Hockman\\hockman_2013_an_ethnographic_and_technological_study_of_breakbeats_in_hardcore,_jungle,_and.pdf}
}

@inproceedings{hoekstraUsingInnovativeInstructions2013,
  title = {Using Innovative Instructions to Create Trustworthy Software Solutions},
  booktitle = {Proceedings of the 2nd {{International Workshop}} on {{Hardware}} and {{Architectural Support}} for {{Security}} and {{Privacy}} - {{HASP}} '13},
  author = {Hoekstra, Matthew and Lal, Reshma and Pappachan, Pradeep and Phegade, Vinay and Del Cuvillo, Juan},
  year = {2013},
  pages = {1--1},
  publisher = {{ACM Press}},
  address = {{Tel-Aviv, Israel}},
  doi = {10.1145/2487726.2488370},
  abstract = {Software developers face a number of challenges when creating applications that attempt to keep important data confidential. Even diligent use of correct software design and implementation practices, can allow secrets to be exposed through a single flaw in any of the privileged code on the platform, code which may have been written by thousands of developers from hundreds of organizations throughout the world. Intel is developing innovative security technology that allows software developers control of the security of sensitive code and data by creating trusted domains within applications to protect critical information during execution and at rest. This paper will show how protection of private information, including enterprise rights management, video chat, trusted financial transactions, among others, has been demonstrated using this technology. Examples will include both protection of local processing and the establishment of secure communication with cloud services. It will illustrate useful software design patterns that can be followed to create many additional types of trusted software solutions.},
  file = {D\:\\GDrive\\zotero\\Hoekstra et al\\hoekstra_et_al_2013_using_innovative_instructions_to_create_trustworthy_software_solutions.pdf},
  isbn = {978-1-4503-2118-1},
  language = {en}
}

@techreport{hoffmanFormularyModelFlexible,
  title = {The Formulary Model for Flexible Privacy and Access Controls*},
  author = {Hoffman, Lance J},
  file = {D\:\\GDrive\\zotero\\Hoffman\\hoffman_the_formulary_model_for_flexible_privacy_and_access_controls.pdf}
}

@article{holtTypeAwareProgrammingModels2016,
  title = {Type-{{Aware Programming Models}} for {{Distributed Applications}}},
  author = {Holt, Brandon},
  year = {2016},
  pages = {145},
  file = {D\:\\GDrive\\zotero\\Holt\\holt_type-aware_programming_models_for_distributed_applications.pdf},
  language = {en}
}

@book{homExecutionContextOptimization,
  title = {Execution {{Context Optimization}} for {{Disk Energy}}},
  author = {Hom, Jerry and Kremer, Ulrich},
  abstract = {Power, energy, and thermal concerns have constrained embedded systems designs. Computing capability and storage density have increased dramatically, enabling the emergence of handheld devices from special to general purpose computing. In many mobile systems, the disk is among the top energy consumers. Many previous optimizations for disk energy have assumed uniprogramming environments. However , many optimizations degrade in multiprogramming because programs are unaware of other programs (execution context). We introduce a framework to make programs aware of and adapt to their runtime execution context. We evaluated real workloads by collecting user activity traces and characterizing the execution contexts. The study confirms that many users run a limited number of programs concurrently. We applied execution context optimizations to eight programs and tested ten combinations. The programs ran concurrently while the disk's power was measured. Our measurement infrastructure allows interactive sessions to be scripted, recorded, and replayed to compare the optimiza-tions' effects against the baseline. Our experiments covered two write cache policies. For write-through, energy savings was in the range 3-63\% with an average of 21\%. For write-back, energy savings was in the range-33-61\% with an average of 8\%. In all cases, our optimizations incurred less than 1\% performance penalty.},
  file = {D\:\\GDrive\\zotero\\Hom_Kremer\\hom_kremer_execution_context_optimization_for_disk_energy.pdf},
  isbn = {978-1-60558-469-0},
  keywords = {D33 [Language Constructs and Features]: Frameworks General Terms Languages,Experimentation,Measurement Keywords Multiprogramming,runtime adaptation,synchronization,user study}
}

@techreport{hongBehalfManyOthers,
  title = {On Behalf of Many Others in: {{Google Network Infrastructure}} and {{Network SREs}}},
  author = {Hong, Chi-yao and Mandal, Subhasree and {Al-Fares}, Mohammad and Zhu, Min and Alimi, Richard and Naidu, Kondapa B and Bhagat, Chandan and Jain, Sourabh and Kaimal, Jay and Liang, Shiyu and Mendelev, Kirill and Padgett, Steve and Rabe, Faro and Ray, Saikat and Tewari, Malveeka and Tierney, Matt and Zahn, Monika and Zolla, Jonathan and Ong, Joon and Vahdat, Amin},
  file = {D\:\\GDrive\\zotero\\Hong\\hong_on_behalf_of_many_others_in.pdf}
}

@article{hopcroftAlgorithm447Efficient1973,
  title = {Algorithm 447: Efficient Algorithms for Graph Manipulation},
  shorttitle = {Algorithm 447},
  author = {Hopcroft, John and Tarjan, Robert},
  year = {1973},
  month = jun,
  volume = {16},
  pages = {372--378},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/362248.362272},
  abstract = {Efficient algorithms are presented for partitioning a graph into connected components, biconnected components and simple paths. The algorithm for partitioning of a graph into simple paths is iterative and each iteration produces a new path between two vertices already on paths. (The start vertex can be specified dynamically.) If V is the number of vertices and E is the number of edges, each algorithm requires time and space proportional to max (V, E) when executed on a random access computer.},
  file = {D\:\\GDrive\\zotero\\Hopcroft_Tarjan\\hopcroft_tarjan_1973_algorithm_447.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@techreport{hopwoodZcashProtocolSpeciication2020,
  title = {Zcash {{Protocol Speciication Version}} 2020.1.2 [{{Overwinter}}+{{Sapling}}+{{Blossom}}+{{Heartwood}}]},
  author = {Hopwood, Daira and Bowe, Sean and Hornby, {\textdagger} -Taylor and Wilcox, {\textdagger} -Nathan},
  year = {2020},
  abstract = {Zcash is an implementation of the Decentralized Anonymous Payment scheme Zerocash, with security yxes and improvements to performance and functionality. It bridges the existing transparent payment scheme used by Bitcoin with a shielded payment scheme secured by zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs). It attempted to address the problem of mining centralization by use of the Equihash memory-hard proof-of-work algorithm. This speciication deenes the Zcash consensus protocol at launch, and after each of the upgrades codenamed Overwinter, Sapling, Blossom, and Heartwood. It is a work in progress. Protocol differences from Zerocash and Bitcoin are also explained.},
  file = {D\:\\GDrive\\zotero\\Hopwood\\hopwood_2020_zcash_protocol_speciication_version_2020.pdf},
  keywords = {anonymity,applications,cryptographic protocols,electronic commerce and payment,nancial privacy,proof of work,zero knowledge}
}

@techreport{horwitzInterproceduralSlicingUsing1990,
  title = {Interprocedural {{Slicing Using Dependence Graphs}}},
  author = {Horwitz, Susan and Reps, Thomas and Binkley, David},
  year = {1990},
  abstract = {A slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing-generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation. The chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some data-dependence edges that represent transitive dependencies due to the effects of procedure calls, in addition to the conventional direct-dependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameter-linkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitive-dependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals.},
  file = {D\:\\GDrive\\zotero\\Horwitz et al\\horwitz_et_al_interprocedural_slicing_using_dependence_graphs.pdf}
}

@article{houshmandBuildingBetterPasswords2012,
  title = {Building Better Passwords Using Probabilistic Techniques},
  author = {Houshmand, Shiva and Aggarwal, Sudhir},
  year = {2012},
  pages = {109--118},
  doi = {10.1145/2420950.2420966},
  abstract = {Password creation policies attempt to help users generate strong passwords but are generally not very effective and tend to frustrate users. The most popular policies are rule based which have been shown to have clear limitations. In this paper we consider a new approach that we term analyze-modify that ensures strong user passwords while maintaining usability. In our approach we develop a software system called AMP that first analyzes whether a user proposed password is weak or strong by estimating the probability of the password being cracked. AMP then modifies the password slightly (to maintain usability) if it is weak to create a strengthened password. We are able to estimate the strength of the password appropriately since we use a probabilistic password cracking system and associated probabilistic context-free grammar to model a realistic distribution of user passwords. In our experiments we were able to distinguish strong passwords from weak ones with an error rate of 1.43\%. In one of a series of experiments, our analyze-modify system was able to strengthen a set of weak passwords, of which 53\% could be easily cracked to a set of strong passwords of which only 0.27\% could be cracked with only a slight modification to the passwords. In our work, we also show how to compute and use various entropy measures from the grammar and show that our system remains effective with continued use through a dynamic updating capability. Copyright 2012 ACM.},
  file = {D\:\\GDrive\\zotero\\Houshmand\\houshmand_2012_building_better_passwords_using_probabilistic_techniques.pdf},
  isbn = {9781450313124},
  journal = {ACM International Conference Proceeding Series},
  keywords = {Information security,Password checking,Password creation policies,Strong authentication}
}

@misc{HowReadAcademic2010,
  title = {How to {{Read}} an {{Academic Article}}},
  year = {2010},
  month = sep,
  abstract = {| Peter Klein | This fall I'm teaching ``Economics of Institutions and Organizations'' to first-year graduate students. The reading list is rather heavy, compared to what most stude\ldots},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7JLMTUV9\\how-to-read-an-academic-article.html},
  journal = {Organizations and Markets},
  language = {en}
}

@article{HowReadResearch,
  title = {How to Read a Research Paper.},
  file = {D\:\\GDrive\\zotero\\_\\how_to_read_a_research_paper.pdf},
  keywords = {research}
}

@misc{HowReadUnderstand2016,
  title = {{How to read and understand a scientific paper: a guide for non-scientists}},
  shorttitle = {{How to read and understand a scientific paper}},
  year = {2016},
  month = may,
  abstract = {From vaccinations to climate change, getting science wrong has very real consequences. But journal articles, a primary way science is communicated in academia, are a different format to newspaper a\ldots},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\N2S9LZLG\\how-to-read-and-understand-a-scientific-paper-a-guide-for-non-scientists.html},
  journal = {Impact of Social Sciences},
  language = {"en-US"}
}

@misc{HowSeriouslyRead2016,
  title = {How to (Seriously) Read a Scientific Paper},
  year = {2016},
  month = mar,
  abstract = {Reading becomes easier with experience, but it is up to each scientist to identify the techniques that work best for them.},
  howpublished = {https://www.sciencemag.org/careers/2016/03/how-seriously-read-scientific-paper},
  journal = {Science | AAAS},
  language = {en}
}

@misc{HowTakeNotes2019,
  title = {How to {{Take Notes While Reading}}},
  year = {2019},
  month = jan,
  abstract = {Note-taking is a powerful tool. But it can be hard to take notes while reading. Here's how you can maximize your ability to remember, reference and learn.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\RGKJAJQ2\\take-notes-while-reading.html},
  howpublished = {https://www.scotthyoung.com/blog/2019/01/29/take-notes-while-reading/},
  journal = {Scott H Young},
  language = {en-US}
}

@misc{HowThinkBetter2018,
  title = {How to {{Think Better}}},
  year = {2018},
  month = dec,
  abstract = {Being able to think clearly is not easy. Working memory is limited, making thinking harder. Luckily, there's a tool to expand and improve your thinking.},
  howpublished = {https://www.scotthyoung.com/blog/2018/12/14/think-better/},
  journal = {Scott H Young},
  language = {en-US}
}

@article{huangBufferBasedApproachRate,
  title = {A {{Buffer}}-{{Based Approach}} to {{Rate Adaptation}}: {{Evidence}} from a {{Large Video Streaming Service}}},
  author = {Huang, Te-Yuan and Johari, Ramesh and Mckeown, Nick and Trunnell, Matthew and Watson, Mark},
  doi = {10.1145/2619239.2626296},
  abstract = {Existing ABR algorithms face a significant challenge in estimating future capacity: capacity can vary widely over time, a phenomenon commonly observed in commercial services. In this work, we suggest an alternative approach: rather than presuming that capacity estimation is required, it is perhaps better to begin by using only the buffer, and then ask when capacity estimation is needed. We test the viability of this approach through a series of experiments spanning millions of real users in a commercial service. We start with a simple design which directly chooses the video rate based on the current buffer occupancy. Our own investigation reveals that capacity estimation is unnecessary in steady state; however using simple capacity estimation (based on immediate past throughput) is important during the startup phase, when the buffer itself is growing from empty. This approach allows us to reduce the rebuffer rate by 10-20\% compared to Netflix's then-default ABR algorithm, while delivering a similar average video rate, and a higher video rate in steady state.},
  file = {D\:\\GDrive\\zotero\\Huang\\huang_a_buffer-based_approach_to_rate_adaptation.pdf},
  isbn = {9781450328364},
  keywords = {Video Rate Adaptation Al-gorithm}
}

@techreport{huangTrackingRansomwareEndtoend,
  title = {Tracking {{Ransomware End}}-to-End},
  author = {Huang, Danny Yuxing and Matthaios Aliapoulios, Maxwell and Li, Guo and Invernizzi, Luca and Mcroberts, Kylie and Bursztein, Elie and Levin, Jonathan and Levchenko, Kirill and Snoeren, Alex C and Mccoy, Damon},
  abstract = {Ransomware is a type of malware that encrypts the files of infected hosts and demands payment, often in a crypto-currency such as Bitcoin. In this paper, we create a measurement framework that we use to perform a large-scale, two-year, end-to-end measurement of ransomware payments, victims, and operators. By combining an array of data sources, including ransomware binaries, seed ransom payments, victim telemetry from infections, and a large database of Bitcoin addresses annotated with their owners, we sketch the outlines of this burgeoning ecosystem and associated third-party infrastructure. In particular, we trace the financial transactions, from the moment victims acquire bitcoins, to when ransomware operators cash them out. We find that many ransomware operators cashed out using BTC-e, a now-defunct Bitcoin exchange. In total we are able to track over \$16 million in likely ransom payments made by 19,750 potential victims during a two-year period. While our study focuses on ransomware, our methods are potentially applicable to other cybercriminal operations that have similarly adopted Bitcoin as their payment channel.},
  file = {D\:\\GDrive\\zotero\\Huang\\huang_tracking_ransomware_end-to-end.pdf}
}

@techreport{huDataOrientedProgrammingExpressiveness2016,
  title = {Data-{{Oriented Programming}}: {{On}} the {{Expressiveness}} of {{Non}}-{{Control Data Attacks}}},
  author = {Hu, Hong and Shinde, Shweta and Adrian, Sendroiu and Leong Chua, Zheng and Saxena, Prateek and Liang, Zhenkai},
  year = {2016},
  abstract = {As control-flow hijacking defenses gain adoption, it is important to understand the remaining capabilities of adversaries via memory exploits. Attacks targeting non-control data in memory can exhibit information leakage or privilege escalation. Compared to control-flow hijacking attacks, such non-control data exploits have limited expressiveness; however, the question is: what is the real expressive power of non-control data attacks? In this paper we show that such attacks are Turing-complete. We present a systematic technique called data-oriented programming (DOP) to construct expressive non-control data exploits for arbitrary x86 programs. In the experimental evaluation using 9 programs, we identified 7518 data-oriented x86 gadgets and 5052 gadget dispatchers, which are the building blocks for DOP. 8 out of 9 real-world programs have gadgets to simulate arbitrary computations and 2 of them are confirmed to be able to build Turing-complete attacks. We build 3 end-to-end attacks to bypass randomization defenses without leaking addresses, to run a network bot which takes commands from the attacker, and to alter the memory permissions. All the attacks work in the presence of ASLR and DEP, demonstrating how the expressiveness offered by DOP significantly empowers the attacker.},
  file = {D\:\\GDrive\\zotero\\Hu\\hu_data-oriented_programming.pdf;D\:\\GDrive\\zotero\\Hu\\hu_data-oriented_programming2.pdf}
}

@techreport{hudsonIncrementalCollectionMature,
  title = {Incremental {{Collection}} of {{Mature Objects}} ?},
  author = {Hudson, Richard L and Eliot, J and Moss, B},
  abstract = {We present a garbage collection algorithm that extends generational scav-enging to collect large older generations (mature objects) non-disruptively. The al-gorithm's approach is to process bounded-size pieces of mature object space at each collection; the subtleties lie in guaranteeing that it eventually collects any and all garbage. The algorithm does not assume any special hardware or operating system support, e.g., for forwarding pointers or protection traps. The algorithm copies objects, so it naturally supports compaction and reclustering.},
  file = {D\:\\GDrive\\zotero\\Hudson\\hudson_incremental_collection_of_mature_objects.pdf}
}

@article{huEnforcingUniqueCode2018,
  title = {Enforcing {{Unique Code Target Property}} for {{Control}}-{{Flow Integrity}}},
  author = {Hu, Hong and Qian, Chenxiong and Yagemann, Carter and Pak Ho Chung, Simon and Harris, William R and Kim, Taesoo and Lee, Wenke and Harris, R},
  year = {2018},
  pages = {17},
  publisher = {{ACM}},
  doi = {10.1145/3243734.3243797},
  abstract = {The goal of control-flow integrity (CFI) is to stop control-hijacking attacks by ensuring that each indirect control-flow transfer (ICT) jumps to its legitimate target. However, existing implementations of CFI have fallen short of this goal because their approaches are inaccurate and as a result, the set of allowable targets for an ICT instruction is too large, making illegal jumps possible. In this paper, we propose the Unique Code Target (UCT) property for CFI. Namely, for each invocation of an ICT instruction, there should be one and only one valid target. We develop a prototype called \textmu CFI to enforce this new property. During compilation, \textmu CFI identifies the sensitive instructions that influence ICT and instruments the program to record necessary execution context. At runtime, \textmu CFI monitors the program execution in a different process, and performs points-to analysis by interpreting sensitive instructions using the recorded execution context in a memory safe manner. It checks runtime ICT targets against the analysis results to detect CFI violations. We apply \textmu CFI to SPEC benchmarks and 2 servers (nginx and vsftpd) to evaluate its efficacy of enforcing UCT and its overhead. We also test \textmu CFI against control-hijacking attacks, including 5 real-world exploits, 1 proof of concept COOP attack, and 2 synthesized attacks that bypass existing defenses. The results show that \textmu CFI strictly enforces the UCT property for protected programs, successfully detects all attacks, and introduces less than 10\% performance overhead.},
  file = {D\:\\GDrive\\zotero\\Hu\\hu_enforcing_unique_code_tar-get_property_for_control-flow_integrity.pdf},
  isbn = {9781450356930},
  keywords = {CCS CONCEPTS • Security and privacy → Systems security,Intel PT,KEYWORDS Control-flow integrity,Performance,Software and ap-plication security,Unique code target}
}

@inproceedings{huEnforcingUniqueCode2018a,
  title = {Enforcing {{Unique Code Target Property}} for {{Control}}-{{Flow Integrity}}},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Hu, Hong and Qian, Chenxiong and Yagemann, Carter and Chung, Simon Pak Ho and Harris, William R. and Kim, Taesoo and Lee, Wenke},
  year = {2018},
  month = oct,
  pages = {1470--1486},
  publisher = {{ACM}},
  address = {{Toronto Canada}},
  doi = {10.1145/3243734.3243797},
  file = {D\:\\GDrive\\zotero\\Hu et al\\hu_et_al_2018_enforcing_unique_code_target_property_for_control-flow_integrity.pdf},
  isbn = {978-1-4503-5693-0},
  language = {en}
}

@techreport{hughesWhyFunctionalProgramming,
  title = {Why {{Functional Programming Matters}}},
  author = {Hughes, John and F{\"o}r Datavetenskap, Institutionen},
  abstract = {This paper dates from 1984, and circulated as a Chalmers memo for many years. Slightly revised versions appeared in 1989 and 1990 as [Hug90] and [Hug89]. This version is based on the original Chalmers memo nroff source, lightly edited for LaTeX and to bring it closer to the published versions , and with one or two errors corrected. Please excuse the slightly old-fashioned type-setting, and the fact that the examples are not in Haskell! Abstract As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place conceptual limits on the way problems can be modularised. Functional languages push those limits back. In this paper we show that two features of functional languages in particular, higher-order functions and lazy evaluation , can contribute greatly to modularity. As examples, we manipulate lists and trees, program several numerical algorithms, and implement the alpha-beta heuristic (an algorithm from Artificial Intelligence used in game-playing programs). Since modularity is the key to successful programming , functional languages are vitally important to the real world.},
  file = {D\:\\GDrive\\zotero\\Hughes\\hughes_why_functional_programming_matters.pdf}
}

@article{humayedCyberPhysicalSystemsSecurity2017,
  title = {Cyber-{{Physical Systems Security}} - {{A Survey}}},
  author = {Humayed, Abdulmalik and Lin, Jingqiang and Li, Fengjun and Luo, Bo},
  year = {2017},
  volume = {4},
  pages = {1802--1831},
  issn = {23274662},
  doi = {10.1109/JIOT.2017.2703172},
  abstract = {With the exponential growth of cyber-physical systems (CPSs), new security challenges have emerged. Various vulnerabilities, threats, attacks, and controls have been introduced for the new generation of CPS. However, there lacks a systematic review of the CPS security literature. In particular, the heterogeneity of CPS components and the diversity of CPS systems have made it difficult to study the problem with one generalized model. In this paper, we study and systematize existing research on CPS security under a unified framework. The framework consists of three orthogonal coordinates: 1) from the security perspective, we follow the well-known taxonomy of threats, vulnerabilities, attacks and controls; 2) from the CPS components perspective, we focus on cyber, physical, and cyber-physical components; and 3) from the CPS systems perspective, we explore general CPS features as well as representative systems (e.g., smart grids, medical CPS, and smart cars). The model can be both abstract to show general interactions of components in a CPS application, and specific to capture any details when needed. By doing so, we aim to build a model that is abstract enough to be applicable to various heterogeneous CPS applications; and to gain a modular view of the tightly coupled CPS components. Such abstract decoupling makes it possible to gain a systematic understanding of CPS security, and to highlight the potential sources of attacks and ways of protection. With this intensive literature review, we attempt to summarize the state-of-the-art on CPS security, provide researchers with a comprehensive list of references, and also encourage the audience to further explore this emerging field.},
  file = {D\:\\GDrive\\zotero\\Humayed\\humayed_2017_cyber-physical_systems_security_-_a_survey.pdf},
  journal = {IEEE Internet of Things Journal},
  keywords = {Attacks,controls,cyber physical systems (CPSs),industrial control systems (ICSs),medical devices,security,smart cars,smart grids,threats,vulnerabilities},
  number = {6}
}

@article{hundPracticalTimingSide2013,
  title = {Practical {{Timing Side Channel Attacks Against Kernel Space ASLR}}},
  author = {Hund, Ralf and Willems, Carsten and Holz, Thorsten},
  year = {2013},
  doi = {10.1109/SP.2013.23},
  abstract = {Due to the prevalence of control-flow hijacking attacks , a wide variety of defense methods to protect both user space and kernel space code have been developed in the past years. A few examples that have received widespread adoption include stack canaries, non-executable memory, and Address Space Layout Randomization (ASLR). When implemented correctly (i.e., a given system fully supports these protection methods and no information leak exists), the attack surface is significantly reduced and typical exploitation strategies are severely thwarted. All modern desktop and server operating systems support these techniques and ASLR has also been added to different mobile operating systems recently. In this paper, we study the limitations of kernel space ASLR against a local attacker with restricted privileges. We show that an adversary can implement a generic side channel attack against the memory management system to deduce information about the privileged address space layout. Our approach is based on the intrinsic property that the different caches are shared resources on computer systems. We introduce three implementations of our methodology and show that our attacks are feasible on four different x86-based CPUs (both 32-and 64-bit architectures) and also applicable to virtual machines. As a result, we can successfully circumvent kernel space ASLR on current operating systems. Furthermore, we also discuss mitigation strategies against our attacks, and propose and implement a defense solution with negligible performance overhead.},
  file = {D\:\\GDrive\\zotero\\Hund\\hund_2013_practical_timing_side_channel_attacks_against_kernel_space_aslr.pdf;D\:\\GDrive\\zotero\\Hund\\hund_2013_practical_timing_side_channel_attacks_against_kernel_space_aslr2.pdf},
  keywords = {Address Space Layout Randomization,Exploit Mitigation,Kernel Vulnerabilities,side-channel,ss,Timing Attacks}
}

@techreport{huntZooKeeperWaitfreeCoordination,
  title = {{{ZooKeeper}}: {{Wait}}-Free Coordination for {{Internet}}-Scale Systems},
  author = {Hunt, Patrick and Konar, Mahadev and Grid, Yahoo ! and Junqueira, Flavio P and Reed, Benjamin and Research, Yahoo !},
  abstract = {In this paper, we describe ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client. It incorporates elements from group messaging, shared registers, and distributed lock services in a repli-cated, centralized service. The interface exposed by Zoo-Keeper has the wait-free aspects of shared registers with an event-driven mechanism similar to cache invalidations of distributed file systems to provide a simple, yet powerful coordination service. The ZooKeeper interface enables a high-performance service implementation. In addition to the wait-free property, ZooKeeper provides a per client guarantee of FIFO execution of requests and linearizability for all requests that change the ZooKeeper state. These design decisions enable the implementation of a high performance processing pipeline with read requests being satisfied by local servers. We show for the target workloads, 2:1 to 100:1 read to write ratio, that ZooKeeper can handle tens to hundreds of thousands of transactions per second. This performance allows ZooKeeper to be used extensively by client applications.},
  file = {D\:\\GDrive\\zotero\\Hunt\\hunt_zookeeper.pdf},
  keywords = {distributed systems}
}

@inproceedings{huoLAPELightweightAttestation2020,
  title = {{{LAPE}}: {{A Lightweight Attestation}} of {{Program Execution Scheme}} for {{Bare}}-{{Metal Systems}}},
  shorttitle = {{{LAPE}}},
  booktitle = {2020 {{IEEE}} 22nd {{International Conference}} on {{High Performance Computing}} and {{Communications}}; {{IEEE}} 18th {{International Conference}} on {{Smart City}}; {{IEEE}} 6th {{International Conference}} on {{Data Science}} and {{Systems}} ({{HPCC}}/{{SmartCity}}/{{DSS}})},
  author = {Huo, Dongdong and Wang, Yu and Liu, Chao and Li, Mingxuan and Wang, Yazhe and Xu, Zhen},
  year = {2020},
  month = dec,
  pages = {78--86},
  doi = {10.1109/HPCC-SmartCity-DSS50907.2020.00011},
  abstract = {Unlike traditional processors, Internet of Things (IoT) devices are short of resources to incorporate mature protections (e.g. MMU, TrustZone) against modern control-flow attacks. Remote (control-flow) attestation is fast becoming a key instrument in securing such devices as it has proven the effectiveness on not only detecting runtime malware infestation of a remote device, but also saving the computing resources by moving the costly verification process away. However, few control-flow attestation schemes have been able to draw on any systematic research into the software specificity of bare-metal systems, which are widely deployed on resource-constrained IoT devices. To our knowledge, the unique design patterns of the system limit implementations of such expositions. In this paper, we present the design and proof-of-concept implementation of LAPE, a lightweight attestation of program execution scheme that enables detecting control-flow attacks for bare-metal systems without requiring hardware modification. With rudimentary memory protection support found in modern IoT-class microcontrollers, LAPE leverages software instrumentation to compartmentalize the firmware functions into several ''attestation compartments''. It then continuously tracks the control-flow events of each compartment and periodically reports them to the verifier. The PoC of the scheme is incorporated into an LLVM-based compiler to generate the LAPE-enabled firmware. By taking experiments with several real-world IoT firmware, the results show both the efficiency and practicality of LAPE.},
  file = {D\:\\GDrive\\zotero\\Huo et al\\huo_et_al_2020_lape.pdf},
  keywords = {Attestation Compartments,Bare-Metal Systems,Control Flow Attestation,Hardware,Instruments,Performance evaluation,Process control,Program processors,Runtime,Systematics}
}

@inproceedings{hussain5GReasoner2019,
  title = {{{5GReasoner}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}  - {{CCS}} '19},
  author = {Hussain, Syed Rafiul and Echeverria, Mitziu and Karim, Imtiaz and Chowdhury, Omar and Bertino, Elisa},
  year = {2019},
  pages = {669--684},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3319535.3354263},
  file = {D\:\\GDrive\\zotero\\Hussain\\hussain_2019_5greasoner.pdf},
  isbn = {978-1-4503-6747-9}
}

@article{huSurveySoftwaredefinedNetwork2014,
  title = {A Survey on Software-Defined Network and {{OpenFlow}}: {{From}} Concept to Implementation},
  author = {Hu, Fei and Hao, Qi and Bao, Ke},
  year = {2014},
  volume = {16},
  pages = {2181--2206},
  publisher = {{IEEE}},
  issn = {1553877X},
  doi = {10.1109/COMST.2014.2326417},
  abstract = {Software-defined network (SDN) has become one of the most important architectures for the management of largescale complex networks, which may require repolicing or reconfigurations from time to time. SDN achieves easy repolicing by decoupling the control plane from data plane. Thus, the network routers/switches just simply forward packets by following the flow table rules set by the control plane. Currently, OpenFlow is the most popular SDN protocol/standard and has a set of design specifications. Although SDN/OpenFlow is a relatively new area, it has attracted much attention from both academia and industry. In this paper, we will conduct a comprehensive survey of the important topics in SDN/OpenFlow implementation, including the basic concept, applications, language abstraction, controller, virtualization, quality of service, security, and its integration with wireless and optical networks. We will compare the pros and cons of different schemes and discuss the future research trends in this exciting area. This survey can help both industry and academia R\&D people to understand the latest progress of SDN/OpenFlow designs.},
  file = {D\:\\GDrive\\zotero\\Hu\\hu_2014_a_survey_on_software-defined_network_and_openflow.pdf},
  journal = {IEEE Communications Surveys and Tutorials},
  keywords = {network virtualization,OpenFlow,QoS,security,Software-defined network (SDN)},
  number = {4}
}

@book{hutchisonHowForgeTimeStamp2014,
  title = {How to {{Forge}} a {{Time}}-{{Stamp Which Adobe}} ' s {{Acrobat Accepts}}},
  author = {Hutchison, David},
  year = {2014},
  doi = {10.1007/978-3-540-77272-9},
  file = {D\:\\GDrive\\zotero\\Hutchison\\hutchison_2014_how_to_forge_a_time-stamp_which_adobe_’_s_acrobat_accepts.pdf},
  isbn = {978-3-540-77272-9},
  number = {December 2007}
}

@techreport{hwangCycleapproximateRetargetablePerformance,
  title = {Cycle-Approximate {{Retargetable Performance Estimation}} at the {{Transaction Level}}},
  author = {Hwang, Yonghyun and Abdi, Samar and Gajski, Daniel},
  abstract = {This paper presents a novel cycle-approximate performance estimation technique for automatically generated transaction level models (TLMs) for heterogeneous multi-core designs. The inputs are application C processes and their mapping to processing units in the platform. The processing unit model consists of pipelined datapath, memory hierarchy and branch delay model. Using the processing unit model, the basic blocks in the C processes are analyzed and annotated with estimated delays. This is followed by a code generation phase where delay-annotated C code is generated and linked with a SystemC wrapper consisting of inter-process communication channels. The generated TLM is compiled and executed natively on the host machine. Our key contribution is that the estimation technique is close to cycle-accurate, it can be applied to any multi-core platform and it produces high-speed native compiled TLMs. For experiments , timed TLMs for industrial scale designs such as MP3 decoder were automatically generated for 4 heterogeneous multi-processor platforms with up to 5 PEs under 1 minute. Each TLM simulated under 1 second, compared to 3-4 hrs of instruction set simulation (ISS) and 15-18 hrs of RTL simulation. Comparison to on-board measurement showed only 8\% error on average in estimated number of cycles.},
  file = {D\:\\GDrive\\zotero\\Hwang\\hwang_cycle-approximate_retargetable_performance_estimation_at_the_transaction_level.pdf}
}

@article{Idsevasionptaceknewsham,
  title = {Ids-Evasion-Ptacek-Newsham},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\A8T6DCMC\\ids-evasion-ptacek-newsham.pdf}
}

@book{ieeecomputersociety.technicalcommitteeondependablecomputingandfaulttolerance.ZabHighperformanceBroadcast2011,
  title = {Zab: {{High}}-Performance Broadcast for Primary-Backup Systems},
  author = {{IEEE Computer Society. Technical Committee on Dependable Computing and Fault Tolerance.} and {IFIP Working Group 10.4 on Dependable Computing and Fault Tolerance.}},
  year = {2011},
  publisher = {{IEEE}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HFFCFUYS\\IEEE Computer Society. Technical Committee on Dependable Computing and Fault Tolerance., IFIP Working Group 10.4 on Dependable Computing.pdf;D\:\\MEGA\\zotero\\IEEE Computer Society. Technical Committee on Dependable Computing and Fault Tolerance.\\ieee_computer_society._technical_committee_on_dependable_computing_and_fault_tolerance._2011_zab.pdf},
  isbn = {978-1-4244-9233-6}
}

@article{ImplementingTransactionControl,
  title = {Implementing {{Transaction Control Expressions}} by {{Checking}} for {{Absence}} of {{Access Rights}}},
  file = {D\:\\GDrive\\zotero\\undefined\\implementing_transaction_control_expressions_by_checking_for_absence_of_access.pdf}
}

@article{inciCacheAttacksEnable2016,
  title = {Cache Attacks Enable Bulk Key Recovery on the Cloud},
  author = {{\.I}nci, Mehmet Sinan and Gulmezoglu, Berk and Irazoqui, Gorka and Eisenbarth, Thomas and Sunar, Berk},
  year = {2016},
  volume = {9813 LNCS},
  pages = {368--388},
  issn = {16113349},
  doi = {10.1007/978-3-662-53140-2_18},
  abstract = {Cloud services keep gaining popularity despite the security concerns. While non-sensitive data is easily trusted to cloud, security critical data and applications are not. The main concern with the cloud is the shared resources like the CPU, memory and even the network adapter that provide subtle side-channels to malicious parties. We argue that these side-channels indeed leak fine grained, sensitive information and enable key recovery attacks on the cloud. Even further, as a quick scan in one of the Amazon EC2 regions shows, high percentage \textendash{} 55\% \textendash{} of users run outdated, leakage prone libraries leaving them vulnerable to mass surveillance. The most commonly exploited leakage in the shared resource systems stem from the cache and the memory. High resolution and the stability of these channels allow the attacker to extract fine grained information. In this work, we employ the Prime and Probe attack to retrieve an RSA secret key from a co-located instance. To speed up the attack, we reverse engineer the cache slice selection algorithm for the Intel Xeon E5-2670 v2 that is used in our cloud instances. Finally we employ noise reduction to deduce the RSA private key from the monitored traces. By processing the noisy data we obtain the complete 2048-bit RSA key used during the decryption.},
  file = {D\:\\GDrive\\zotero\\İnci\\i̇nci_2016_cache_attacks_enable_bulk_key_recovery_on_the_cloud.pdf},
  isbn = {9783662531396},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Amazon EC2,Co-location detection,Prime and probe attack,RSA key recovery,Virtualization}
}

@techreport{IncrementalMatureGarbage,
  title = {Incremental {{Mature Garbage Collection Using}} the {{Train Algorithm}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LDN4NPFV\\incremental_mature_garbage_collection_using_the_train_algorithm.pdf}
}

@techreport{ingallsBackFutureStory,
  title = {Back to the {{Future The Story}} of {{Squeak}}, {{A Practical Smalltalk Written}} in {{Itself}}},
  author = {Ingalls, Dan and Kaehler, Ted and Maloney, John and Wallace, Scott and Kay, Alan and Disney, Walt},
  abstract = {Squeak is an open, highly-portable Smalltalk implementation whose virtual machine is written entirely in Smalltalk, making it easy to debug, analyze, and change. To achieve practical performance, a translator produces an equivalent C program whose performance is comparable to commercial Smalltalks. Other noteworthy aspects of Squeak include: a compact object format that typically requires only a single word of overhead per object; a simple yet efficient incremental garbage collector for 32-bit direct pointers; efficient bulk-mutation of objects; extensions of BitBlt to handle color of any depth and anti-aliased image rotation and scaling; and real-time sound and music synthesis written entirely in Smalltalk.},
  file = {D\:\\GDrive\\zotero\\Ingalls\\ingalls_back_to_the_future_the_story_of_squeak,_a_practical_smalltalk_written_in_itself.pdf}
}

@techreport{ingallsDesignPrinciplesSmalltalk2009,
  title = {Design {{Principles Behind Smalltalk}}},
  author = {Ingalls, Daniel H H},
  year = {2009},
  abstract = {The purpose of the Smalltalk project is to provide computer support for the creative spirit in everyone. Our work flows from a vision that includes a creative individual and the best computing hardware available. We have chosen to concentrate on two principle areas of research: a language of description (programming language) that serves as an interface between the models in the human mind and those in computing hardware, and a language of interaction (user interface) that matches the human communication system to that of the computer. Our work has followed a two-to four-year cycle that can be seen to parallel the scientific method: Build an application program within the current system (make an observation) Based on that experience, redesign the language (formulate a theory) Build a new system based on the new design (make a prediction that can be tested)},
  file = {D\:\\GDrive\\zotero\\Ingalls\\ingalls_2009_design_principles_behind_smalltalk.pdf}
}

@book{instituteofelectricalandelectronicsengineers.RealByzantineGenerals2004,
  title = {The {{Real Byzantine Generals}}},
  author = {{Institute of Electrical and Electronics Engineers.}},
  year = {2004},
  publisher = {{IEEE}},
  abstract = {"IEEE Catalog Number: 04CH37576"--V. 1, p. ii.},
  file = {D\:\\GDrive\\zotero\\Institute of Electrical and Electronics Engineers.\\institute_of_electrical_and_electronics_engineers._2004_the_real_byzantine_generals.pdf;D\:\\GDrive\\zotero\\Institute of Electrical and Electronics Engineers.\\institute_of_electrical_and_electronics_engineers._2004_the_real_byzantine_generals.pdf},
  isbn = {0-7803-8539-X}
}

@book{instituteofelectricalandelectronicsengineersShapeSizeThreats,
  title = {The {{Shape}} and {{Size}} of {{Threats}}: {{Defining}} a {{Networked System}}'s {{Attack Surface}}},
  author = {{Institute of Electrical and Electronics Engineers} and {IEEE Computer Society}},
  abstract = {IEEE Computer Society Order Number E5363.},
  file = {D\:\\GDrive\\zotero\\Institute of Electrical and Electronics Engineers\\institute_of_electrical_and_electronics_engineers_the_shape_and_size_of_threats.pdf},
  isbn = {978-1-4799-6204-4}
}

@techreport{intanagonwiwatDirectedDiffusionScalable2000,
  title = {Directed {{Diffusion}}: {{A Scalable}} and {{Robust Communication Paradigm}} for {{Sensor Networks}}},
  author = {Intanagonwiwat, Chalermek and Govindan, Ramesh and Estrin, Deborah},
  year = {2000},
  abstract = {Advances in processor, memory and radio technology will enable small arid cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.},
  file = {D\:\\GDrive\\zotero\\Intanagonwiwat\\intanagonwiwat_2000_directed_diffusion.pdf}
}

@article{intelIntelAnalysisSpeculative2018,
  title = {Intel {{Analysis}} of {{Speculative Execution Side Channels}}},
  author = {{Intel}},
  year = {2018},
  pages = {1--12},
  file = {D\:\\GDrive\\zotero\\Intel\\intel_2018_intel_analysis_of_speculative_execution_side_channels.pdf},
  journal = {Technical report},
  keywords = {Intel Analysis of Speculative Execution Side Chann},
  number = {1}
}

@book{internationalfederationforinformationprocessing.technicalcommittee6CanSPDYReally,
  title = {Can {{SPDY Really Make}} the {{Web Faster}}?},
  author = {{International Federation for Information Processing. Technical Committee 6} and {IEEE Computer Society} and {Norges teknisk-naturvitenskapelige universitet}},
  abstract = {Proceedings of a meeting held 2-4 June 2014, Trondheim, Norway. Annotation The main objectives of Networking 2014 are to bring together members of the networking community, from both academia and industry, to discuss recent advances in the broad and quickly evolving field of computer and communication networks, and to highlight key issues, identify trends, and develop visions for the networking domain The areas of work and research will contribute to the knowledge base of the IEEE Computer Society In addition, the Networking conference series has a very high reputation as well as standing in the international community on networking experts.},
  file = {D\:\\GDrive\\zotero\\International Federation for Information Processing. Technical Committee 6\\international_federation_for_information_processing._technical_committee_6_can_spdy_really_make_the_web_faster.pdf},
  isbn = {978-3-901882-58-6}
}

@techreport{INTERNETWORMMicroscope,
  title = {{{THE INTERNET WORM With Microscope}} and {{Tweezers}}: {{The Worm}} from {{MITS Perspective}}},
  file = {D\:\\GDrive\\zotero\\undefined\\the_internet_worm_with_microscope_and_tweezers.pdf}
}

@article{irazoquiCrossProcessorCache2016,
  title = {Cross Processor Cache Attacks},
  author = {Irazoqui, Gorka and Eisenbarth, Thomas and Sunar, Berk},
  year = {2016},
  pages = {353--364},
  doi = {10.1145/2897845.2897867},
  abstract = {Multi-processor systems are becoming the de-facto standard across different computing domains, ranging from high-end multi-tenant cloud servers to low-power mobile platforms. The denser integration of CPUs creates an opportunity for great economic savings achieved by packing processes of multiple tenants or by bundling all kinds of tasks at various privilege levels to share the same platform. This level of sharing carries with it a serious risk of leaking sensitive information through the shared microarchitectural components. Microarchitectural attacks initially only exploited core-private resources, but were quickly generalized to resources shared within the CPU. We present the first fine grain side channel attack that works across processors. The attack does not require CPU colocation of the attacker and the victim. The novelty of the proposed work is that, for the first time the directory protocol of high efficiency CPU interconnects is targeted. The directory protocol is common to all modern multi-CPU systems. Examples include AMD's HyperTransport, Intel's Quickpath, and ARM's AMBA Coherent Interconnect. The proposed attack does not rely on any specific characteristic of the cache hierarchy, e.g. inclusiveness. Note that inclusiveness was assumed in all earlier works. Furthermore, the viability of the proposed covert channel is demonstrated with two new attacks: by recovering a full AES key in OpenSSL, and a full ElGamal key in libgcrypt within the range of seconds on a shared AMD Opteron server.},
  file = {D\:\\GDrive\\zotero\\Irazoqui\\irazoqui_2016_cross_processor_cache_attacks.pdf},
  isbn = {9781450342339},
  journal = {ASIA CCS 2016 - Proceedings of the 11th ACM Asia Conference on Computer and Communications Security},
  keywords = {Cache Attacks,Cross-CPU attack,HyperTransport,Invalidate+Transfer}
}

@article{irazoquiSharedCacheAttack2015,
  title = {S\${{A}}: {{A}} Shared Cache Attack That Works across Cores and Defies {{VM}} Sandboxing - {{And}} Its Application to {{AES}}},
  author = {Irazoqui, Gorka and Eisenbarth, Thomas and Sunar, Berk},
  year = {2015},
  volume = {2015-July},
  pages = {591--604},
  issn = {10816011},
  doi = {10.1109/SP.2015.42},
  abstract = {The cloud computing infrastructure relies on virtualized servers that provide isolation across guest OS's through sand boxing. This isolation was demonstrated to be imperfect in past work which exploited hardware level information leakages to gain access to sensitive information across co-located virtual machines (VMs). In response virtualization companies and cloud services providers have disabled features such as deduplication to prevent such attacks. In this work, we introduce a fine-grain cross-core cache attack that exploits access time variations on the last level cache. The attack exploits huge pages to work across VM boundaries without requiring deduplication. No configuration changes on the victim OS are needed, making the attack quite viable. Furthermore, only machine co-location is required, while the target and victim OS can still reside on different cores of the machine. Our new attack is a variation of the prime and probe cache attack whose applicability at the time is limited to L1 cache. In contrast, our attack works in the spirit of the flush and reload attack targeting the shared L3 cache instead. Indeed, by adjusting the huge page size our attack can be customized to work virtually at any cache level/size. We demonstrate the viability of the attack by targeting an Open SSL1.0.1f implementation of AES. The attack recovers AES keys in the cross-VM setting on Xen 4.1 with deduplication disabled, being only slightly less efficient than the flush and reload attack. Given that huge pages are a standard feature enabled in the memory management unit of OS's and that besides co-location no additional assumptions are needed, the attack we present poses a significant risk to existing cloud servers.},
  file = {D\:\\GDrive\\zotero\\Irazoqui\\irazoqui_2015_s$a.pdf},
  isbn = {9781467369497},
  journal = {Proceedings - IEEE Symposium on Security and Privacy},
  keywords = {cache attacks,Cross-VM,flush+reload,huge pages,memory deduplication,prime and probe}
}

@article{irazoquiWaitMinuteFast2014,
  title = {Wait a Minute! {{A}} Fast, Cross-{{VM}} Attack on {{AES}}},
  author = {Irazoqui, Gorka and Inci, Mehmet Sinan and Eisenbarth, Thomas and Sunar, Berk},
  year = {2014},
  volume = {8688 LNCS},
  pages = {299--319},
  issn = {16113349},
  doi = {10.1007/978-3-319-11379-1_15},
  abstract = {In cloud computing, efficiencies are reaped by resource sharing such as co-location of computation and deduplication of data. This work exploits resource sharing in virtualization software to build a powerful cache-based attack on AES. We demonstrate the vulnerability by mounting Cross-VM Flush+Reload cache attacks in VMware VMs to recover the keys of an AES implementation of OpenSSL 1.0.1 running inside the victim VM. Furthermore, the attack works in a realistic setting where different VMs are located on separate cores. The modified flush+reload attack we present, takes only in the order of seconds to minutes to succeed in a cross-VM setting. Therefore long term co-location, as required by other fine grain attacks in the literature, are not needed. The results of this study show that there is a great security risk to OpenSSL AES implementation running on VMware cloud services when the deduplication is not disabled. \textcopyright{} 2014 Springer International Publishing.},
  file = {D\:\\GDrive\\zotero\\Irazoqui\\irazoqui_2014_wait_a_minute.pdf},
  isbn = {9783319113784},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {cache attacks,Cross-VM,memory deduplication},
  number = {Vmm}
}

@article{islamRiskAssessmentFramework2016,
  title = {A {{Risk Assessment Framework}} for {{Automotive Embedded Systems}}},
  author = {Islam, Mafijul Md and Lautenbach, Aljoscha and Sandberg, Christian and Olovsson, Tomas},
  year = {2016},
  doi = {10.1145/2899015.2899018},
  abstract = {The automotive industry is experiencing a paradigm shift towards autonomous and connected vehicles. Coupled with the increasing usage and complexity of electrical and/or electronic systems, this introduces new safety and security risks. Encouragingly, the automotive industry has relatively well-known and standardised safety risk management practices, but security risk management is still in its infancy. In order to facilitate the derivation of security requirements and security measures for automotive embedded systems, we propose a specifically tailored risk assessment framework, and we demonstrate its viability with an industry use-case. Some of the key features are alignment with existing processes for functional safety, and usability for non-security specialists. The framework begins with a threat analysis to identify the assets, and threats to those assets. The following risk assessment process consists of an estimation of the threat level and of the impact level. This step utilises several existing standards and methodologies, with changes where necessary. Finally, a security level is estimated which is used to formulate high-level security requirements. The strong alignment with existing standards and processes should make this framework well-suited for the needs in the automotive industry. CCS Concepts \textbullet Security and privacy \textrightarrow{} Security requirements; Embedded systems security; \textbullet Computer systems organization \textrightarrow{} Embedded software; Dependable and fault-tolerant systems and networks;},
  file = {D\:\\GDrive\\zotero\\Islam et al\\islam_et_al_2016_a_risk_assessment_framework_for_automotive_embedded_systems.pdf},
  isbn = {978-1-4503-4288-9},
  keywords = {CPSS'16; Keywords Automotive Security,Risk Assessment,Security Requirements,Threat Analysis}
}

@article{itrustlabsSecureWaterTreatment2015,
  title = {Secure {{Water Treatment}} ({{SWaT}})},
  author = {{iTrust Labs}},
  year = {2015},
  pages = {1--18},
  number = {July}
}

@article{jaaskelainenRetargetableCompilerBackend2010,
  title = {Retargetable {{Compiler Backend}} for {{Transport Triggered Architectures}}},
  author = {Jaaskelainen, Veli-Pekka},
  year = {2010},
  pages = {65},
  file = {D\:\\GDrive\\zotero\\Jaaskelainen\\jaaskelainen_2010_retargetable_compiler_backend_for_transport_triggered_architectures.pdf},
  journal = {Tampere University}
}

@phdthesis{jaaskelainenRetargetableCompilerBackend2011,
  title = {Retargetable Compiler Backend for Transport Triggered Architectures},
  author = {J{\"a}{\"a}skel{\"a}inen, Veli-Pekka},
  year = {2011},
  file = {D\:\\GDrive\\zotero\\undefined\\veli-pekka_jääskeläinen_retargetable_compiler_backend_for_transport_triggered.pdf}
}

@techreport{jacobsonCongestionAvoidanceControl,
  title = {Congestion {{Avoidance}} and {{Control}}},
  author = {Jacobson, V},
  file = {D\:\\GDrive\\zotero\\Jacobson\\jacobson_congestion_avoidance_and_control.pdf}
}

@phdthesis{jaddooDetectionCategorizationAnomalous,
  title = {Detection and Categorization of Anomalous Behavior in Enterprise Network Traffic Using Machine Learning Techniques},
  author = {Jaddoo, Yeaz Elias},
  abstract = {Consequently, we generate a set of 80 features from our network data capture. Using a smaller subset of more relevant features is not only more resource efficient but also leads to improved detection results on our learning techniques. We then evaluate the performance of our feature subset with our machine learning algorithms to indicate the best set of features for detecting our diverse attack categories.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7XKQPSC2\\Yeaz - Presentation.pptx;D\:\\GDrive\\zotero\\Jaddoo\\jaddoo_detection_and_categorization_of_anomalous_behavior_in_enterprise_network.pdf},
  language = {en}
}

@article{jaddooDetectionCategorizationAnomalous2018,
  title = {Detection and Categorization of Anomalous Behavior in Enterprise Network Traffic Using Machine Learning Techniques},
  author = {JADDOO, Yeaz Elias},
  year = {2018},
  number = {2010}
}

@article{jainB4ExperienceGloballyDeployed2013,
  title = {B4: {{Experience}} with a {{Globally}}-{{Deployed Software Defined WAN}}},
  author = {Jain, Sushant and Kumar, Alok and Mandal, Subhasree and Ong, Joon and Poutievski, Leon and Singh, Arjun and Venkata, Subbaiah and Wanderer, Jim and Zhou, Junlan and Zhu, Min and Zolla, Jonathan and H{\"o}lzle, Urs and Stuart, Stephen and Vahdat, Amin},
  year = {2013},
  abstract = {We present the design, implementation, and evaluation of BB, a private WAN connecting Google's data centers across the planet. BB has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traf-c demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. ese characteristics led to a Sooware Deened Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. BB's centralized traac engineering service drives links to near rrrr utilization , while splitting application nows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of BB production deployment, lessons learned, and areas for future work.},
  file = {D\:\\GDrive\\zotero\\Jain\\jain_2013_b4.pdf},
  keywords = {C [Network Protocols]: Routing Protocols Keywords Centralized Traac Engineering,OpenFlow,Routing,Sooware-Deened Networking,Wide-Area Networks}
}

@article{jangAnalyzingCrossdomainPolicies2011,
  title = {Analyzing the {{Crossdomain Policies}} of {{Flash Applications}}},
  author = {Jang, Dongseok and Venkataraman, Aishwarya and Shacham, Hovav and Sawka, G Michael},
  year = {2011},
  pages = {13},
  abstract = {Adobe Flash is a rich Internet application platform. Flash applications are often deployed to the Web; The Flash Player plugin is installed on a large fraction of all Webconnected PCs. Flash provides a mechanism by which sites can opt in to more expressive information sharing regimes than the same-origin policy for JavaScript allows. A site that wishes to share its content can host a crossdomain policy file, crossdomain.xml, which lists sites authorized to access the sharing site's content, or even a wildcard to allow all access. Because browsers will typically attach cookies to crossdomain URL requests made by the Flash Player plugin, a site that publishes a crossdomain policy effectively opts out from some of the confidentiality guarantees of the same-origin policy. In some cases, a misconfigured, overly permissive crossdomain policy can expose a site to attacks such as information disclosure or CSRF.},
  file = {D\:\\GDrive\\zotero\\Jang et al\\jang_et_al_2011_analyzing_the_crossdomain_policies_of_flash_applications.pdf},
  language = {en}
}

@article{jangBreakingKernelAddress2016,
  title = {Breaking {{Kernel Address Space Layout Randomization}} with {{Intel TSX}}},
  author = {Jang, Yeongjin and Lee, Sangho and Kim, Taesoo},
  year = {2016},
  volume = {24-28-Octo},
  pages = {380--392},
  issn = {15437221},
  doi = {10.1145/2976749.2978321},
  abstract = {Kernel hardening has been an important topic since many applications and security mechanisms often consider the kernel as part of their Trusted Computing Base (TCB). Among various hardening techniques, Kernel Address Space Layout Randomization (KASLR) is the most effective and widely adopted defense mechanism that can practically mitigate various memory corruption vulnerabilities, such as buffer overfow and use-after-free. In principle, KASLR is secure as long as no memory leak vulnerability exists and high entropy is ensured. In this paper, we introduce a highly stable timing attack against KASLR, called DrK, that can precisely de-randomize the memory layout of the kernel without violating any such assumptions. DrK exploits a hardware feature called Intel Transactional Synchronization Extension (TSX) that is readily available in most modern commodity CPUs. One surprising behavior of TSX, which is essentially the root cause of this security loophole, is that it aborts a transaction without notifying the underlying kernel even when the transaction fails due to a critical error, such as a page fault or an access violation, which traditionally requires kernel intervention. DrK turned this property into a precise timing channel that can determine the mapping status (i.e., mapped versus unmapped) and execution status (i.e., executable versus non-executable) of the privileged kernel address space. In addition to its surprising accuracy and precision, DrK is universally applicable to all OSes, even in virtualized environments, and generates no visible footprint, making it diffcult to detect in practice. We demonstrated that DrK can break the KASLR of all major OSes (i.e., Windows, Linux, and OS X) with near-perfect accuracy in under a second. Finally, we propose potential countermeasures that can effectively prevent or mitigate the DrK attack. We urge our community to be aware of the potential threat of having Intel TSX, which is present in most recent Intel CPUs-100\% in workstation and 60\% in high-end Intel CPUs since Skylake- and is even available on Amazon EC2 (X1).},
  file = {D\:\\GDrive\\zotero\\Jang\\jang_2016_breaking_kernel_address_space_layout_randomization_with_intel_tsx.pdf},
  isbn = {9781450341394},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security}
}

@inproceedings{jangEmpiricalStudyPrivacyviolating2010,
  title = {An Empirical Study of Privacy-Violating Information Flows in {{JavaScript}} Web Applications},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Computer}} and Communications Security - {{CCS}} '10},
  author = {Jang, Dongseok and Jhala, Ranjit and Lerner, Sorin and Shacham, Hovav},
  year = {2010},
  pages = {270},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1866307.1866339},
  abstract = {The dynamic nature of JavaScript web applications has given rise to the possibility of privacy violating information flows. We present an empirical study of the prevalence of such flows on a large number of popular websites. We have (1) designed an expressive, fine-grained information flow policy language that allows us to specify and detect different kinds of privacy-violating flows in JavaScript code, (2) implemented a new rewriting-based JavaScript information flow engine within the Chrome browser, and (3) used the enhanced browser to conduct a large-scale empirical study over the Alexa global top 50,000 websites of four privacyviolating flows: cookie stealing, location hijacking, history sniffing, and behavior tracking. Our survey shows that several popular sites, including Alexa global top-100 sites, use privacy-violating flows to exfiltrate information about users' browsing behavior. Our findings show that steps must be taken to mitigate the privacy threat from covert flows in browsers.},
  file = {D\:\\GDrive\\zotero\\Jang et al\\jang_et_al_2010_an_empirical_study_of_privacy-violating_information_flows_in_javascript_web.pdf},
  isbn = {978-1-4503-0245-6},
  language = {en}
}

@article{jangoldbergDesignCustom32bit2017,
  title = {The {{Design}} of a {{Custom}} 32-Bit {{RISC CPU}} and {{LLVM Compiler The Design}} of a {{Custom}} 32-Bit {{RISC CPU}} and {{LLVM Compiler Backend Backend}}},
  author = {Jan Goldberg, Connor},
  year = {2017},
  file = {D\:\\GDrive\\zotero\\Jan Goldberg\\jan_goldberg_2017_the_design_of_a_custom_32-bit_risc_cpu_and_llvm_compiler_the_design_of_a_custom.pdf},
  keywords = {None provided}
}

@article{javadiDiscoveringStatisticalModels2011,
  title = {Discovering Statistical Models of Availability in Large Distributed Systems: {{An}} Empirical Study of {{SETI}}@home},
  author = {Javadi, Bahman and Kondo, Derrick and Vincent, Jean Marc and Anderson, David P.},
  year = {2011},
  volume = {22},
  pages = {1896--1903},
  issn = {10459219},
  doi = {10.1109/TPDS.2011.50},
  abstract = {In the age of cloud, Grid, P2P, and volunteer distributed computing, large-scale systems with tens of thousands of unreliable hosts are increasingly common. Invariably, these systems are composed of heterogeneous hosts whose individual availability often exhibit different statistical properties (for example stationary versus nonstationary behavior) and fit different models (for example exponential, Weibull, or Pareto probability distributions). In this paper, we describe an effective method for discovering subsets of hosts whose availability have similar statistical properties and can be modeled with similar probability distributions. We apply this method with about 230,000 host availability traces obtained from a real Internet-distributed system, namely SETI@home. We find that about 21 percent of hosts exhibit availability, that is, a truly random process, and that these hosts can often be modeled accurately with a few distinct distributions from different families. We show that our models are useful and accurate in the context of a scheduling problem that deals with resource brokering. We believe that these methods and models are critical for the design of stochastic scheduling algorithms across large systems where host availability is uncertain.},
  file = {D\:\\GDrive\\zotero\\Javadi\\javadi_2011_discovering_statistical_models_of_availability_in_large_distributed_systems.pdf},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  keywords = {reliability,resource failures,Statistical availability models,stochastic scheduling.},
  number = {11}
}

@article{jawaheriDeanonymizingTorHidden2020,
  title = {Deanonymizing {{Tor}} Hidden Service Users through {{Bitcoin}} Transactions Analysis},
  author = {Jawaheri, Husam Al and Sabah, Mashael Al and Boshmaf, Yazan and Erbad, Aiman},
  year = {2020},
  volume = {89},
  issn = {01674048},
  doi = {10.1016/j.cose.2019.101684},
  abstract = {With the rapid increase of threats on the Internet, people are continuously seeking privacy and anonymity. Services such as Bitcoin and Tor were introduced to provide anonymity for online transactions and Web browsing. Due to its pseudonymity model, Bitcoin lacks retroactive operational security, which means historical pieces of information could be used to identify a certain user. By exploiting publicly available information, we show how relying on Bitcoin for payments on Tor hidden services could lead to deanonymization of these services' users. Such linking is possible by finding at least one past transaction in the Blockchain that involves their publicly declared Bitcoin addresses. To demonstrate the consequences of this deanonymization approach, we carried out a real-world experiment simulating a passive, limited adversary. We crawled 1.5K hidden services and collected 88 unique and active Bitcoin addresses. We then crawled 5B tweets and 1M BitcoinTalk forum pages and collected 4.2K and 41K unique Bitcoin addresses, respectively. Each user address was associated with an online identity along with its public profile information. By analyzing the transactions in the Blockchain, we were able to link 125 unique users to 20 hidden services, including sensitive ones, such as The Pirate Bay and Silk Road. We also analyzed two case studies in detail to demonstrate the implications of the information leakage on users anonymity. In particular, we confirm that Bitcoin addresses should be considered exploitable, as they can be used to deanonymize users retroactively. This is especially important for Tor hidden service users who actively seek and expect privacy and anonymity.},
  file = {D\:\\GDrive\\zotero\\Jawaheri\\jawaheri_2020_deanonymizing_tor_hidden_service_users_through_bitcoin_transactions_analysis.pdf},
  journal = {Computers and Security},
  keywords = {Attack,Bitcoin,Deanonymization,Privacy,Tor hidden services}
}

@article{jayasingheOptimisticFairexchangeAnonymity2014,
  title = {Optimistic Fair-Exchange with Anonymity for Bitcoin Users},
  author = {Jayasinghe, Danushka and Markantonakis, Konstantinos and Mayes, Keith},
  year = {2014},
  pages = {44--51},
  doi = {10.1109/ICEBE.2014.20},
  abstract = {Fair-exchange and anonymity are two important attributes in e-commerce. It is much more difficult to expect fairness in e-commerce transactions using Bit coin due to anonymity and transaction irreversibility. Genuine consumers and merchants who would like to make and receive payments using Bit coin may be reluctant to do so due to this uncertainty. The proposed protocol guarantees strong-fairness while preserving anonymity of the consumer and the merchant, using Bit coin as a payment method which addresses the aforementioned concern. The involvement of the trusted third party (TTP) is kept to a minimum, which makes the protocol optimistic and the exchanged product is not revealed to TTP. It achieves dispute resolution within the protocol run without any intervention of an external judge. Finally we show how the protocol can be easily adapted to use other digital cash systems designed using public ledgers such as Zerocoin/Zerocash.},
  file = {D\:\\GDrive\\zotero\\Jayasinghe\\jayasinghe_2014_optimistic_fair-exchange_with_anonymity_for_bitcoin_users.pdf},
  isbn = {9781479965632},
  journal = {Proceedings - 11th IEEE International Conference on E-Business Engineering, ICEBE 2014 - Including 10th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 2014 and 1st Workshop on E-Commerce Engineering, ECE 2014},
  keywords = {Anonymity,Bitcoin,Optimistic Fair-Exchange}
}

@techreport{jeanRepresentationAnalysisSoftware,
  title = {Representation and {{Analysis}} of {{Software}}},
  author = {Jean, Mary and Rothermel, Harrold Gregg and Orso, Alex and Tech, Georgia},
  file = {D\:\\GDrive\\zotero\\Jean\\jean_representation_and_analysis_of_software.pdf}
}

@misc{jeanUpdatedSporadicallyBest2016,
  title = {Updated Sporadically at Best: {{What My PhD Was Like}}},
  shorttitle = {Updated Sporadically at Best},
  author = {Jean},
  year = {2016},
  month = feb,
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LLEDZMKZ\\my-phd-abridged.html},
  journal = {updated sporadically at best}
}

@article{JeffreyDeanCSE,
  title = {Jeffrey {{Dean CSE}} Summa Sum1990},
  file = {D\:\\GDrive\\zotero\\undefined\\jeffrey_dean_cse_summa_sum1990.pdf}
}

@incollection{jiangHyperCropHypervisorBasedCountermeasure2011,
  title = {{{HyperCrop}}: {{A Hypervisor}}-{{Based Countermeasure}} for {{Return Oriented Programming}}},
  shorttitle = {{{HyperCrop}}},
  booktitle = {Information and {{Communications Security}}},
  author = {Jiang, Jun and Jia, Xiaoqi and Feng, Dengguo and Zhang, Shengzhi and Liu, Peng},
  editor = {Qing, Sihan and Susilo, Willy and Wang, Guilin and Liu, Dongmei},
  year = {2011},
  volume = {7043},
  pages = {360--373},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25243-3_29},
  abstract = {Return oriented programming (ROP) has recently caught great attention of both academia and industry. It reuses existing binary code instead of injecting its own code and is able to perform arbitrary computation due to its Turing-completeness. Hence, It can successfully bypass state-of-the-art code integrity mechanisms such as NICKLE and SecVisor. In this paper, we present HyperCrop, a hypervisor-based approach to counter such attacks. Since ROP attackers extract short instruction sequences ending in ret called ``gadgets'' and craft stack content to ``chain'' these gadgets together, our method recognizes that the key characteristics of ROP is to fill the stack with plenty of addresses that are within the range of libraries (e.g. libc). Accordingly, we inspect the content of the stack to see if a potential ROP attack exists. We have implemented a proof-of-concept system based on the open source Xen hypervisor. The evaluation results exhibit that our solution is effective and efficient.},
  file = {D\:\\GDrive\\zotero\\Jiang et al\\jiang_et_al_2011_hypercrop.pdf},
  isbn = {978-3-642-25242-6 978-3-642-25243-3},
  language = {en}
}

@techreport{johnConsensusRoutingInternet,
  title = {Consensus {{Routing}}: {{The Internet}} as a {{Distributed System}}},
  author = {John, John P and {Katz-Bassett}, Ethan and Krishnamurthy, Arvind and Anderson, Thomas and Venkataramani, Arun},
  abstract = {Internet routing protocols (BGP, OSPF, RIP) have traditionally favored responsiveness over consistency. A router applies a received update immediately to its forwarding table before propagating the update to other routers, including those that potentially depend upon the outcome of the update. Responsiveness comes at the cost of routing loops and blackholes-a router A thinks its route to a destination is via B but B disagrees. By favoring responsiveness (a liveness property) over consistency (a safety property), Internet routing has lost both. Our position is that consistent state in a distributed system makes its behavior more predictable and securable. To this end, we present consensus routing, a consistency-first approach that cleanly separates safety and liveness using two logically distinct modes of packet delivery: a stable mode where a route is adopted only after all dependent routers have agreed upon it, and a transient mode that heuristically forwards the small fraction of packets that encounter failed links. Somewhat surprisingly, we find that consensus routing improves overall availability when used in conjunction with existing transient mode heuris-tics such as backup paths, deflections, or detouring. Experiments on the Internet's AS-level topology show that consensus routing eliminates nearly all transient discon-nectivity in BGP.},
  file = {D\:\\GDrive\\zotero\\John\\john_consensus_routing.pdf}
}

@techreport{johnsonEllipticCurveDigital2001,
  title = {The {{Elliptic Curve Digital Signature Algorithm}} ({{ECDSA}})},
  author = {Johnson, Don and Menezes, Alfred and Vanstone, Scott},
  year = {2001},
  abstract = {The Elliptic Curve Digital Signature Algorithm (ECDSA) is the elliptic curve analogue of the Digital Signature Algorithm (DSA). It was accepted in 1999 as an ANSI standard, and was accepted in 2000 as IEEE and NIST standards. It was also accepted in 1998 as an ISO standard, and is under consideration for inclusion in some other ISO standards. Unlike the ordinary discrete logarithm problem and the integer factorization problem, no subexponential-time algorithm is known for the elliptic curve discrete logarithm problem. For this reason, the strength-per-key-bit is substantially greater in an algorithm that uses elliptic curves. This paper describes the ANSI X9.62 ECDSA, and discusses related security, implementation, and interoperability issues.},
  file = {D\:\\GDrive\\zotero\\Johnson et al\\johnson_et_al_2001_the_elliptic_curve_digital_signature_algorithm_(ecdsa).pdf}
}

@techreport{johnsonTheoreticianGuideExperimental2001,
  title = {A {{Theoretician}}'s {{Guide}} to the {{Experimental Analysis}} of {{Algorithms}}},
  author = {Johnson, David S},
  year = {2001},
  abstract = {This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally. It is based on lessons learned by the author over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists. Although written from the perspective of a theoretical computer scientist, it is intended to be of use to researchers from all fields who want to study algorithms experimentally. It has two goals: first, to provide a useful guide to new experimen-talists about how such work can best be performed and written up, and second, to challenge current researchers to think about whether their own work might be improved from a scientific point of view. With the latter purpose in mind, the author hopes that at least a few of his recommendations will be considered controversial.}
}

@techreport{johnsonX3LowOverhead,
  title = {X3: {{A Low Overhead High Performance Buffer Management Replacement Algorithm}} *},
  author = {Johnson, Theodore},
  abstract = {In a path-breaking paper last year Pat and Betty O'Neil and Gerhard Weikum pro posed a self-tuning improvement to the Least Recently Used (LRU) buffer management algorithm[l5]. Their improvement is called LRU/k and advocates giving priority to buffer pages baaed on the kth most recent access. (The standard LRU algorithm is denoted LRU/l according to this terminology.) If Pl's kth most recent access is more more recent than P2's, then Pl will be replaced after P2. Intuitively, LRU/k for k {$>$} 1 is a good strategy, because it gives low priority to pages that have been scanned or to pages that belong to a big randomly accessed file (e.g., the account file in TPC/A). They found that LRU/S achieves most of the advantage of their method.},
  file = {D\:\\GDrive\\zotero\\Johnson\\johnson_x3.PDF}
}

@misc{jonesHowWriteGreat,
  title = {How to Write a Great Research Paper {{Seven}} Simple Suggestions},
  author = {Jones, Simon Peyton},
  file = {D\:\\GDrive\\zotero\\Jones\\jones_how_to_write_a_great_research_paper_seven_simple_suggestions.pdf},
  language = {en}
}

@misc{jonesHowWriteGreata,
  title = {How to Write a Great Research Paper},
  author = {Jones, Simon},
  annotation = {https://www.youtube.com/watch?v=VK51E3gHENc\&ab\_channel=MicrosoftResearch https://sms.cam.ac.uk/media/1464870},
  file = {D\:\\GDrive\\zotero\\Jones\\jones_how_to_write_a_great_research_paper.pdf}
}

@book{jonesParallelGenerationalCopyingGarbage2008,
  title = {Parallel {{Generational}}-{{Copying Garbage Collection}} with {{aBlock}}-{{Structured Heap}}},
  author = {Jones, Richard. and {ACM Digital Library.} and {ACM Special Interest Group on Programming Languages.}},
  year = {2008},
  publisher = {{ACM}},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Jones\\jones_2008_parallel_generational-copying_garbage_collection_with_ablock-structured_heap.pdf},
  isbn = {978-1-60558-134-7}
}

@techreport{jonesTacklingAwkwardSquad2010,
  title = {Tackling the {{Awkward Squad}}: Monadic Input/Output, Concurrency, Exceptions, and Foreign-Language Calls in {{Haskell}}},
  author = {Jones, Simon Peyton},
  year = {2010},
  abstract = {Functional programming may be beautiful, but to write real applications we must grapple with awkward real-world issues: input/output, robustness, concurrency, and interfacing to programs written in other languages. These lecture notes give an overview of the techniques that have been developed by the Haskell community to address these problems. I introduce various proposed extensions to Haskell along the way, and I offer an operational semantics that explains what these extensions mean. M Broy, and R Steinbrueggen, NATO ASI Series, IOS Press, 2001, pp47-96. This version has a few errors corrected compared with the published version. Change summary: \textbullet{} Jan 2009: Clarify {$\nu$} and fn () in Section 3.5; reword the one occurrence of fv () in Section 2.7 \textbullet{} Feb 2008: Fix typo in Section 3.5 \textbullet{} May 2005: Section 6: correct the way in which the FFI declares an imported function to be pure (no "unsafe" necessary). \textbullet{} Apr 2005: Section 5.2.2: some examples added to clarify evaluate. \textbullet{} March 2002: substantial revision},
  file = {D\:\\GDrive\\zotero\\Jones\\jones_2010_tackling_the_awkward_squad.pdf}
}

@misc{jonesWritingGreatResearch,
  title = {Writing a Great Research Proposal},
  author = {Jones, Simon},
  file = {D\:\\GDrive\\zotero\\Jones\\jones_writing_a_great_research_proposal.pdf}
}

@phdthesis{josephlalVulnerabilityAnalysisAutomotive,
  title = {Vulnerability {{Analysis}} of an {{Automotive Infotainment System}}},
  author = {Josephlal, Edwin Franco Myloth},
  file = {D\:\\GDrive\\zotero\\Josephlal\\josephlal_vulnerability_analysis_of_an_automotive_infotainment_system.pdf},
  language = {en}
}

@techreport{joshiTASKOPTIMIZATIONFRAMEWORK,
  title = {A {{TASK OPTIMIZATION FRAMEWORK FOR MSSP}}},
  author = {Joshi, Rahul Ulhas},
  file = {D\:\\GDrive\\zotero\\Joshi\\joshi_a_task_optimization_framework_for_mssp.pdf}
}

@article{jovanovicPixyStaticAnalysis2006,
  title = {Pixy: {{A}} Static Analysis Tool for Detecting Web Application Vulnerabilities ({{Short}} Paper)},
  author = {Jovanovic, Nenad and Kruegel, Christopher and Kirda, Engin},
  year = {2006},
  volume = {2006},
  pages = {258--263},
  issn = {10816011},
  doi = {10.1109/SP.2006.29},
  abstract = {The number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three web applications, and reconstructed 36 known vulnerabilities in three other web applications. The observed false positive rate is at around 50\% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits. \textcopyright{} 2006 IEEE.},
  file = {D\:\\GDrive\\zotero\\Jovanovic\\jovanovic_2006_pixy.pdf},
  isbn = {0769525741},
  journal = {Proceedings - IEEE Symposium on Security and Privacy}
}

@inproceedings{juelsHoneywords2013,
  title = {Honeywords},
  booktitle = {Proceedings of the 2013 {{ACM SIGSAC}} Conference on {{Computer}} \& Communications Security - {{CCS}} '13},
  author = {Juels, Ari and Rivest, Ronald L.},
  year = {2013},
  pages = {145--160},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  issn = {15437221},
  doi = {10.1145/2508859.2516671},
  abstract = {We propose a simple method for improving the security of hashed passwords: the maintenance of additional ``honey- words''(false passwords) associated with each user's account. An adversary who steals a file of hashed passwords and in- verts the hash function cannot tell if he has found the pass- word or a honeyword. The attempted use of a honeyword for login sets off an alarm. An auxiliary server (the ``hon- eychecker'') can distinguish the user password from honey- words for the login routine, and will set off an alarm if a honeyword is submitted.},
  file = {D\:\\GDrive\\zotero\\Juels\\juels_2013_honeywords.pdf},
  isbn = {978-1-4503-2477-9},
  keywords = {authentication,chaffing,honeywords,login,password cracking,password hashes,passwords,ss}
}

@article{junodObfuscatorLLVMSoftwareProtectionMasses2015,
  title = {Obfuscator-{{LLVM}}-{{Software Protection}} for the {{Masses}}},
  author = {Junod, Pascal and Rinaldini, Julien and Wehrli, Johan and Michielin, Julie},
  year = {2015},
  pages = {3--9},
  publisher = {{IEEE}},
  doi = {10.1109/SPRO.2015.10},
  abstract = {Software security with respect to reverse-engineering is a challenging discipline that has been researched for several years and which is still active. At the same time, this field is inherently practical, and thus of industrial relevance: indeed, protecting a piece of software against tampering, malicious modifications or reverse-engineering is a very difficult task. In this paper, we present and discuss a software obfuscation prototype tool based on the LLVM compilation suite. Our tool is built as different passes, where some of them have been open-sourced and are freely available, that work on the LLVM Intermediate Representation (IR) code. This approach brings several advantages, including the fact that it is language-agnostic and mostly independent of the target architecture. Our current prototype supports basic instruction substitutions, insertion of bogus control-flow constructs mixed with opaque predicates, control-flow flattening, procedures merging as well as a code tamper-proofing algorithm embedding code and data checksums directly in the control-flow flattening mechanism.},
  file = {D\:\\GDrive\\zotero\\Junod\\junod_2015_obfuscator-llvm-software_protection_for_the_masses.pdf},
  isbn = {9781467370943},
  journal = {Proceedings - International Workshop on Software Protection, SPRO 2015}
}

@article{kaijanahoEvidenceBasedProgrammingLanguage2015,
  title = {Evidence-{{Based Programming Language Design}}. {{A Philosophical}} and {{Methodological Exploration}}},
  author = {Kaijanaho, Antti-Juhani},
  year = {2015},
  pages = {259},
  file = {D\:\\GDrive\\zotero\\Kaijanaho\\kaijanaho_2015_evidence-based_programming_language_design.pdf},
  language = {en}
}

@article{kalyanaramanCThroughParttimeOptics2010,
  title = {C-{{Through}}: {{Part}}-Time {{Optics}} in {{Data Centers}}},
  author = {Kalyanaraman, Shiv. and Ramakrishnan, K. K. and Voelker, Geoffrey M. and {Association for Computing Machinery. Special Interest Group on Data Communications.} and {Association for Computing Machinery.}},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Kalyanaraman\\kalyanaraman_2010_c-through.pdf}
}

@techreport{kambadurParaSharesFindingImportant,
  title = {{{ParaShares}}: {{Finding}} the {{Important Basic Blocks}} in {{Multithreaded Programs}}},
  author = {Kambadur, Melanie and Tang, Kui and Kim, Martha},
  abstract = {Understanding and optimizing multithreaded execution is a significant challenge. Numerous research and industrial tools debug parallel performance by combing through program source or thread traces for pathologies including communication overheads, data dependencies, and load imbalances. This work takes a new approach: it ignores any underlying pathologies, and focuses instead on pinpointing the exact locations in source code that consume the largest share of execution. Our new metric, ParaShares, scores and ranks all basic blocks in a program based on their share of parallel execution. For the eight benchmarks examined in this paper, ParaShare rankings point to just a few important blocks per application. The paper demonstrates two uses of this information , exploring how the important blocks vary across thread counts and input sizes, and making modest source code changes (fewer than 10 lines of code) that result in 14-92\% savings in parallel program runtime.},
  file = {D\:\\GDrive\\zotero\\Kambadur\\kambadur_parashares.pdf}
}

@techreport{kampJailsConfiningOmnipotent,
  title = {Jails: {{Confining}} the Omnipotent Root},
  author = {Kamp, Poul-Henning and Org{$>$} and Watson, Robert N M},
  file = {D\:\\GDrive\\zotero\\Kamp\\kamp_jails.pdf}
}

@article{kandulaSocialNetworkDatacenter2015,
  title = {Inside the {{Social Network}} ' s ( {{Datacenter}} ) {{Network}} \textendash{} {{Public Review}}},
  author = {Kandula, Srikanth},
  year = {2015},
  pages = {123--137},
  isbn = {9781450335423},
  journal = {Sigcomm 2015},
  keywords = {datacenter traffic patterns}
}

@techreport{kannanMakingProgramsForget,
  title = {Making {{Programs Forget}}: {{Enforcing Lifetime For Sensitive Data}}},
  author = {Kannan, Jayanthkumar and Altekar, Gautam and Maniatis, Petros and Chun, Byung-Gon},
  abstract = {This paper introduces guaranteed data lifetime, a novel system property ensuring that sensitive data cannot be retrieved from a system beyond a specified time. The trivial way to achieve this is to "reboot"; however, this is disruptive from the user's perspective, and may not even eliminate disk copies. We discuss an alternate approach based on state reincarnation where data expiry is completely transparent to the user, and can be used even if the system is not designed a priori to provide the property.},
  file = {D\:\\GDrive\\zotero\\Kannan\\kannan_making_programs_forget.pdf}
}

@article{kapposEmpiricalAnalysisAnonymity,
  title = {An {{Empirical Analysis}} of {{Anonymity}} in {{Zcash}}},
  author = {Kappos, George and Yousaf, Haaroon and Maller, Mary and Meiklejohn, Sarah},
  abstract = {Among the now numerous alternative cryptocurren-cies derived from Bitcoin, Zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. In this paper, we examine the extent to which anonymity is achieved in the deployed version of Zcash. We investigate all facets of anonymity in Zcash's transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuris-tics based on identifiable patterns of usage.},
  file = {D\:\\GDrive\\zotero\\Kappos\\kappos_an_empirical_analysis_of_anonymity_in_zcash.pdf}
}

@article{karelScholarlyCommonsSubheapAugmentedGarbage2019,
  title = {{{ScholarlyCommons Subheap}}-{{Augmented Garbage Collection Subheap}}-{{Augmented Garbage Collection}}},
  author = {Karel, Benjamin},
  year = {2019},
  file = {D\:\\GDrive\\zotero\\Karel\\karel_2019_scholarlycommons_subheap-augmented_garbage_collection_subheap-augmented_garbage.pdf}
}

@techreport{kargerConsistentHashingRandom,
  title = {Consistent {{Hashing}} and {{Random Trees}}: {{Distributed Caching Protocols}} for {{Relieving Hot Spots}} on the {{World Wide Web}}},
  author = {Karger, David and Lehman, Eric and Leighton, Tom and {\textcent}{\textcurrency}, {\textexclamdown} {\textsterling} and Levine, Matthew and Lewin, Daniel and Panigrahy, Rina},
  file = {D\:\\GDrive\\zotero\\Karger\\karger_consistent_hashing_and_random_trees.pdf}
}

@article{karimovBenchmarkingDistributedStream2018,
  title = {Benchmarking {{Distributed Stream Data Processing Systems}}},
  author = {Karimov, Jeyhun and Rabl, Tilmann and Katsifodimos, Asterios and Samarev, Roman and Heiskanen, Henri and Markl, Volker},
  year = {2018},
  month = feb,
  doi = {10.1109/ICDE.2018.00169},
  abstract = {The need for scalable and efficient stream analysis has led to the development of many open-source streaming data processing systems (SDPSs) with highly diverging capabilities and performance characteristics. While first initiatives try to compare the systems for simple workloads, there is a clear gap of detailed analyses of the systems' performance characteristics. In this paper, we propose a framework for benchmarking distributed stream processing engines. We use our suite to evaluate the performance of three widely used SDPSs in detail, namely Apache Storm, Apache Spark, and Apache Flink. Our evaluation focuses in particular on measuring the throughput and latency of windowed operations, which are the basic type of operations in stream analytics. For this benchmark, we design workloads based on real-life, industrial use-cases inspired by the online gaming industry. The contribution of our work is threefold. First, we give a definition of latency and throughput for stateful operators. Second, we carefully separate the system under test and driver, in order to correctly represent the open world model of typical stream processing deployments and can, therefore, measure system performance under realistic conditions. Third, we build the first benchmarking framework to define and test the sustainable performance of streaming systems. Our detailed evaluation highlights the individual characteristics and use-cases of each system.},
  file = {D\:\\GDrive\\zotero\\Karimov\\karimov_2018_benchmarking_distributed_stream_data_processing_systems.pdf}
}

@techreport{karlofConditionedsafeCeremoniesUser,
  title = {Conditioned-Safe {{Ceremonies}} and a {{User Study}} of an {{Application}} to {{Web Authentication}}},
  author = {Karlof, Chris and Tygar, J D and Wagner, David},
  abstract = {We introduce the notion of a conditioned-safe ceremony. A "ceremony" is similar to the conventional notion of a protocol , except that a ceremony explicitly includes human participants. Our formulation of a conditioned-safe ceremony draws on several ideas and lessons learned from the human factors and human reliability community: forcing functions, defense in depth, and the use of human tendencies, such as rule-based decision making. We propose design principles for building conditioned-safe ceremonies and apply these principles to develop a registration ceremony for machine authentication based on email. We evaluated our email registration ceremony with a user study of 200 participants. We designed our study to be as ecologically valid as possible: we employed deception, did not use a laboratory environment , and attempted to create an experience of risk. We simulated attacks against the users and found that email registration was significantly more secure than challenge question based registration. We also found evidence that conditioning helped email registration users resist attacks, but contributed towards making challenge question users more vulnerable.},
  file = {D\:\\GDrive\\zotero\\Karlof\\karlof_conditioned-safe_ceremonies_and_a_user_study_of_an_application_to_web.pdf}
}

@techreport{karlssonELBtreesEfficientLockfreeTrees2016,
  title = {{{ELB}}-Trees-{{Efficient Lock}}-Free {{B}}+trees},
  author = {Karlsson, L F and Probst, S W},
  year = {2016},
  institution = {{ACACES}},
  abstract = {As computer systems scale in the number of processors, data structures with good parallel performance become increasingly important. Lock-free data structures promise improved parallel performance at the expense of higher complexity and sequential execution time. We present ELB-trees, a new lock-free dictionary with simple synchronization in the common case, making it almost 30 times faster than sequential library implementations at 24 threads.},
  file = {D\:\\GDrive\\zotero\\Karlsson\\karlsson_2016_elb-trees-efficient_lock-free_b+trees.pdf}
}

@techreport{karpenkoDigitalVideoStabilization,
  title = {Digital {{Video Stabilization}} and {{Rolling Shutter Correction}} Using {{Gyroscopes}}},
  author = {Karpenko, Alexandre and Jacobs, David and Baek, Jongmin and Levoy, Marc},
  abstract = {(a) (b) 0 0.5 1 1.5 0.5 0 0.5 time (s) (rad/s) (c) (d) Figure 1: (a) Videos captured with a cell-phone camera tend to be shaky due to the device's size and weight. (b) The rolling shutter used by sensors in these cameras also produces warping in the output frames (we have exagerrated the effect for illustrative purposes). (c) We use gyroscopes to measure the camera's rotations during video capture. (d) We use the measured camera motion to stabilize the video and to rectify the rolling shutter. Abstract In this paper we present a robust, real-time video stabilization and rolling shutter correction technique based on commodity gy-roscopes. First, we develop a unified algorithm for modeling camera motion and rolling shutter warping. We then present a novel framework for automatically calibrating the gyroscope and camera outputs from a single video capture. This calibration allows us to use only gyroscope data to effectively correct rolling shutter warping and to stabilize the video. Using our algorithm, we show results for videos featuring large moving foreground objects, parallax, and low-illumination. We also compare our method with commercial image-based stabilization algorithms. We find that our solution is more robust and computationally inexpensive. Finally, we implement our algorithm directly on a mobile phone. We demonstrate that by using the phone's inbuilt gyroscope and GPU, we can remove camera shake and rolling shutter artifacts in real-time.},
  keywords = {CR Categories: I43 [Computing Methodologies]: Image Processing and Computer Vision-Enhancement; I41 [Comput-ing Methodologies]: Image Processing and Computer Vision-Digitization and Image Capture Keywords: video stabilization,gyro-scopes,mobile devices,rolling shutter correction}
}

@article{karrenbergAutomaticPacketization2009,
  title = {Automatic {{Packetization}}},
  author = {Karrenberg, Ralf},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Karrenberg\\karrenberg_automatic_packetization.pdf}
}

@phdthesis{kasfunEffectivenessOpensourceLeading,
  title = {Effectiveness of Open-Source and Leading Industry Web Application Firewalls against Client and Server-Side Attacks},
  author = {Kasfun, Muhammad},
  file = {D\:\\GDrive\\zotero\\Kasfun\\kasfun_effectiveness_of_open-source_and_leading_industry_web_application_firewalls.pdf}
}

@techreport{katabiInternetCongestionControl,
  title = {Internet {{Congestion Control}} for {{Future High Bandwidth}}-{{Delay Product Environments}}},
  author = {Katabi, Dina and Handley, Mark and Rohrs, Charlie},
  abstract = {Theory and experiments show that as the per-flow product of bandwidth and latency increases, TCP becomes inefficient and prone to instability, regardless of the queuing scheme. This failing becomes increasingly important as the Internet evolves to incorporate very high-bandwidth optical links and more large-delay satellite links. To address this problem, we develop a novel approach to Internet congestion control that outperforms TCP in conventional environments, and remains efficient, fair, scalable, and stable as the bandwidth-delay product increases. This new eXplicit Control Protocol, XCP, generalizes the Explicit Congestion Notification proposal (ECN). In addition, XCP introduces the new concept of decoupling utilization control from fairness control. This allows a more flexible and analytically tractable protocol design and opens new avenues for service differentiation. Using a control theory framework, we model XCP and demonstrate it is stable and efficient regardless of the link capacity , the round trip delay, and the number of sources. Extensive packet-level simulations show that XCP outperforms TCP in both conventional and high bandwidth-delay environments. Further, XCP achieves fair bandwidth allocation, high utilization, small standing queue size, and near-zero packet drops, with both steady and highly varying traffic. Additionally , the new protocol does not maintain any per-flow state in routers and requires few CPU cycles per packet, which makes it implementable in high-speed routers.},
  file = {D\:\\GDrive\\zotero\\Katabi\\katabi_internet_congestion_control_for_future_high_bandwidth-delay_product_environments.pdf}
}

@techreport{kattiXORsAirPractical2006,
  title = {{{XORs}} in {{The Air}}: {{Practical Wireless Network Coding}}},
  author = {Katti, Sachin and Rahul, Hariharan and Hu Dina Katabi, Wenjun and Crowcroft, Jon},
  year = {2006},
  abstract = {This paper proposes COPE, a new architecture for wireless mesh networks. In addition to forwarding packets, routers mix (i.e., code) packets from different sources to increase the information content of each transmission. We show that intelligently mixing packets increases network throughput. Our design is rooted in the theory of network coding. Prior work on network coding is mainly theoretical and focuses on multicast traffic. This paper aims to bridge theory with practice; it addresses the common case of unicast traffic , dynamic and potentially bursty flows, and practical issues facing the integration of network coding in the current network stack. We evaluate our design on a 20-node wireless network, and discuss the results of the first testbed deployment of wireless network coding. The results show that COPE largely increases network throughput. The gains vary from a few percent to several folds depending on the traffic pattern, congestion level, and transport protocol.},
  file = {D\:\\GDrive\\zotero\\Katti\\katti_2006_xors_in_the_air.pdf},
  keywords = {C22 [Computer Systems Organization]: Computer-Communications Networks General Terms Algorithms,Design,Performance,Theory Keywords Network Coding,Wireless Networks}
}

@article{kaushikLoopFusionLLVM2015,
  title = {Loop {{Fusion}} in {{LLVM Compiler}}},
  author = {Kaushik, Madhura Dinesh},
  year = {2015},
  file = {D\:\\GDrive\\zotero\\Kaushik\\kaushik_loop_fusion_in_llvm_compiler.pdf}
}

@techreport{kayEarlyHistorySmalltalk,
  title = {The {{Early History}} of {{Smalltalk}}},
  author = {Kay, Alan C},
  abstract = {Most ideas come from previous ideas. The sixties, particularly in the  community , gave rise to a host of notions about "human-computer symbiosis" through interactive time-shared computers, graphics screens and pointing devices. Advanced computer languages were invented to simulate complex systems such as oil refineries and semi-intelligent behavior. The soon-to-follow paradigm shift of modern personal computing, overlapping window interfaces, and object-oriented design came from seeing the work of the sixties as something more than a "better old thing." This is, more than a better way: to do mainframe computing; for end-users to invoke functionality; to make data structures more abstract. Instead the promise of exponential growth in computing volume demanded that the sixties be regarded as "almost a new thing" and to find out what the actual "new things" might be. For example, one would computer with a handheld "Dynabook" in a way that would not be possible on a shared mainframe; millions of potential users meant that the user interface would have to become a learning environment along the lines of Montessori and Bruner; and needs for large scope, reduction in complexity, and end-user literacy would require that data and control structures be done away with in favor of a more biological scheme of protected universal cells interacting only through messages that could mimic any desired behavior. Early Smalltalk was the first complete realization of these new points of view as parented by its many predecessors in hardware, language and user interface design. It became the exemplar of the new computing, in part, because we were actually trying for a qualitative shift in belief structures-a new Kuhnian paradigm in the same spirit as the invention of the printing press-and thus took highly extreme positions which almost forced these new styles to be invented.},
  file = {D\:\\GDrive\\zotero\\Kay\\kay_the_early_history_of_smalltalk.pdf}
}

@article{kelleyGuessAgainAgain2012,
  title = {Guess Again (and Again and Again): {{Measuring}} Password Strength by Simulating Password-Cracking Algorithms},
  author = {Kelley, Patrick Gage and Komanduri, Saranga and Mazurek, Michelle L. and Shay, Richard and Vidas, Timothy and Bauer, Lujo and Christin, Nicolas and Cranor, Lorrie Faith and L{\'o}pez, Julio},
  year = {2012},
  pages = {523--537},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2012.38},
  abstract = {Text-based passwords remain the dominant authentication method in computer systems, despite significant advancement in attackers' capabilities to perform password cracking. In response to this threat, password composition policies have grown increasingly complex. However, there is insufficient research defining metrics to characterize password strength and using them to evaluate password-composition policies. In this paper, we analyze 12,000 passwords collected under seven composition policies via an online study. We develop an efficient distributed method for calculating how effectively several heuristic password-guessing algorithms guess passwords. Leveraging this method, we investigate (a) the resistance of passwords created under different conditions to guessing, (b) the performance of guessing algorithms under different training sets, (c) the relationship between passwords explicitly created under a given composition policy and other passwords that happen to meet the same requirements, and (d) the relationship between guess ability, as measured with password-cracking algorithms, and entropy estimates. Our findings advance understanding of both password-composition policies and metrics for quantifying password security. \textcopyright{} 2012 IEEE.},
  file = {D\:\\GDrive\\zotero\\Kelley et al\\kelley_et_al_2012_guess_again_(and_again_and_again).pdf},
  isbn = {9780769546810},
  journal = {Proceedings - IEEE Symposium on Security and Privacy},
  keywords = {authentication,passwords,user study}
}

@techreport{kentSecureBorderGateway2000,
  title = {Secure {{Border Gateway Protocol}} ({{S}}-{{BGP}})-{{Real World Performance}} and {{Deployment Issues}}},
  author = {Kent, Stephen and Lynn, Charles and Mikkelson, Joanne and Seo, Karen},
  year = {2000},
  abstract = {The Border Gateway Protocol (BGP), which is used to distribute routing information between autonomous systems, is an important component of the Internet's routing infrastructure. Secure BGP (S-BGP) addresses critical BGP vulnerabilities by providing a scalable means of verifying the authenticity and authorization of BGP control traffic. To facilitate widespread adoption, S-BGP must avoid introducing undue overhead (processing, bandwidth, storage) and must be incrementally deployable, i.e., interoperable with BGP. To provide a proof of concept demonstration, we developed a prototype implementation of S-BGP and deployed it in DARPA's CAIRN testbed. Real Internet BGP traffic was fed to the testbed routers via replay of a recorded BGP peering session with an ISP's BGP router. This document describes the results of these experiments-examining interoperability, the efficacy of the S-BGP countermeasures in securing BGP control traffic, and their impact on BGP performance, and thus evaluating the feasibility of deployment in the Internet.},
  file = {D\:\\GDrive\\zotero\\Kent\\kent_2000_secure_border_gateway_protocol_(s-bgp)-real_world_performance_and_deployment.pdf}
}

@phdthesis{keplerRealTimeRay2008,
  title = {Real \textendash{} Time {{Ray Tracing}} of {{Dynamic Scenes Diplomingenieur Informatik}}},
  author = {Kepler, Johannes and Linz, T and Universit, A},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Kepler\\kepler_2008_real_–_time_ray_tracing_of_dynamic_scenes_diplomingenieur_informatik.pdf}
}

@techreport{keshavHowReadPaper2016,
  title = {How to {{Read}} a {{Paper}}},
  author = {Keshav, S and Cheriton, David R},
  year = {2016},
  abstract = {Researchers spend a great deal of time reading research papers. However, this skill is rarely taught, leading to much wasted effort. This article outlines a practical and efficient three-pass method for reading research papers. I also describe how to use this method to do a literature survey.},
  file = {D\:\\GDrive\\zotero\\Keshav\\keshav_2016_how_to_read_a_paper.pdf},
  keywords = {research}
}

@techreport{kharrazCuttingGordianKnot2015,
  title = {Cutting the {{Gordian Knot}}: {{A Look Under}} the {{Hood}} of {{Ransomware Attacks}}},
  author = {Kharraz, Amin and Robertson, William and Balzarotti, Davide and Bilge, Leyla and Kirda, Engin},
  year = {2015},
  abstract = {In this paper, we present the results of a long-term study of ransomware attacks that have been observed in the wild between 2006 and 2014. We also provide a holistic view on how ransomware attacks have evolved during this period by analyzing 1,359 samples that belong to 15 different ransomware families. Our results show that, despite a continuous improvement in the encryption, deletion, and communication techniques in the main ransomware families, the number of families with sophisticated destructive capabilities remains quite small. In fact, our analysis reveals that in a large number of samples, the malware simply locks the victim's computer desktop or attempts to encrypt or delete the victim's files using only superficial techniques. Our analysis also suggests that stopping advanced ransomware attacks is not as complex as it has been previously reported. For example, we show that by monitoring abnormal file system activity, it is possible to design a practical defense system that could stop a large number of ran-somware attacks, even those using sophisticated encryption capabilities. A close examination on the file system activities of multiple ransomware samples suggests that by looking at I/O requests and protecting Master File Table (MFT) in the NTFS file system, it is possible to detect and prevent a significant number of zero-day ransomware attacks.},
  file = {D\:\\GDrive\\zotero\\Kharraz et al\\kharraz_et_al_2015_cutting_the_gordian_knot.pdf},
  keywords = {Bitcoin,Malicious Activities,Malware,Ransomware,Underground Econ-omy}
}

@book{khatibHadoopDistributedFile2010,
  title = {The {{Hadoop Distributed File System}}},
  author = {Khatib, Mohammed G. and He, Xubin. and Factor, Michael. and {IEEE Computer Society. Mass Storage Systems and Technology Technical Committee.}},
  year = {2010},
  publisher = {{IEEE}},
  abstract = {IEEE Catalog Number CFP10257-ART. Title from PDF title page (IEEE Xplore, viewed Jun. 16, 2011).},
  file = {D\:\\GDrive\\zotero\\Khatib\\khatib_2010_the_hadoop_distributed_file_system.pdf},
  isbn = {978-1-4244-7153-9}
}

@inproceedings{khattabColBERTEfficientEffective2020,
  title = {{{ColBERT}}: {{Efficient}} and {{Effective Passage Search}} via {{Contextualized Late Interaction}} over {{BERT}}},
  shorttitle = {{{ColBERT}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Khattab, Omar and Zaharia, Matei},
  year = {2020},
  month = jul,
  pages = {39--48},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3397271.3401075},
  abstract = {Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query\textendash document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their finegrained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.},
  file = {D\:\\GDrive\\zotero\\Khattab_Zaharia\\khattab_zaharia_2020_colbert.pdf},
  isbn = {978-1-4503-8016-4},
  language = {en}
}

@techreport{kholiaLookingDropBox,
  title = {Looking inside the ({{Drop}}) Box},
  author = {Kholia, Dhiru},
  abstract = {Dropbox is a cloud based file storage service used by more than 100 million users. In spite of its widespread popularity, we believe that Dropbox as a platform hasn't been analyzed extensively enough from a security standpoint. Also, the previous work on the security analysis of Dropbox has been heavily censored. Moreover, the existing Python bytecode reversing techniques are not enough for reversing hardened applications like Dropbox. This paper presents new and generic techniques, to reverse engineer frozen Python applications, which are not limited to just the Dropbox world. We describe a method to bypass Dropbox's two factor authentication and hijack Dropbox accounts. Additionally, generic techniques to intercept SSL data using code injection techniques and monkey patching are presented. We believe that our biggest contribution is to open up the Dropbox platform to further security analysis and research. Dropbox will / should no longer be a black box. Finally, we describe the design and implementation of an open-source version of Dropbox client (and yes, it runs on ARM too).},
  file = {D\:\\GDrive\\zotero\\Kholia\\kholia_looking_inside_the_(drop)_box.pdf}
}

@phdthesis{khooAutomatedModelBased,
  title = {Automated {{Model}} Based {{Analysis}} and {{Testing}} of {{Cyber Physical Systems}}: {{Theory}} and {{Practice}}},
  author = {Khoo, Teck Ping},
  file = {D\:\\GDrive\\zotero\\Khoo\\khoo_automated_model_based_analysis_and_testing_of_cyber_physical_systems.pdf},
  language = {en}
}

@techreport{khurshidGeneralizedSymbolicExecution,
  title = {Generalized {{Symbolic Execution}} for {{Model Checking}} and {{Testing}}},
  author = {Khurshid, Sarfraz and P{\textasciibreve} As{\textasciibreve} Areanu, Corina S and Visser, Willem},
  abstract = {Modern software systems, which often are concurrent and manipulate complex data structures must be extremely reliable. We present a novel framework based on symbolic execution, for automated checking of such systems. We provide a twofold generalization of traditional symbolic execution based approaches. First, we define a source to source translation to instrument a program, which enables standard model checkers to perform symbolic execution of the program. Second, we give a novel symbolic execution algorithm that handles dynamically allocated structures (e.g., lists and trees), method preconditions (e.g., acyclicity), data (e.g., integers and strings) and concurrency. The program instrumentation enables a model checker to automatically explore different program heap configurations and manipulate logical formulae on program data (using a decision procedure). We illustrate two applications of our framework: checking correctness of multi-threaded programs that take inputs from unbounded domains with complex structure and generation of non-isomorphic test inputs that satisfy a testing criterion. Our implementation for Java uses the Java PathFinder model checker.},
  file = {D\:\\GDrive\\zotero\\Khurshid\\khurshid_generalized_symbolic_execution_for_model_checking_and_testing.pdf}
}

@techreport{khurshidVeriFlowVerifyingNetworkWide,
  title = {{{VeriFlow}}: {{Verifying Network}}-{{Wide Invariants}} in {{Real Time}}},
  author = {Khurshid, Ahmed and Zou, Xuan and Zhou, Wenxuan and Caesar, Matthew and Godfrey, P Brighten},
  pages = {15},
  abstract = {Networks are complex and prone to bugs. Existing tools that check network configuration files and the data-plane state operate offline at timescales of seconds to hours, and cannot detect or prevent bugs as they arise. Is it possible to check network-wide invariants in real time, as the network state evolves? The key challenge here is to achieve extremely low latency during the checks so that network performance is not affected. In this paper, we present a design, VeriFlow, which achieves this goal. VeriFlow is a layer between a software-defined networking controller and network devices that checks for network-wide invariant violations dynamically as each forwarding rule is inserted, modified or deleted. VeriFlow supports analysis over multiple header fields, and an API for checking custom invariants. Based on a prototype implementation integrated with the NOX OpenFlow controller, and driven by a Mininet OpenFlow network and Route Views trace data, we find that Veri-Flow can perform rigorous checking within hundreds of microseconds per rule insertion or deletion.},
  file = {D\:\\GDrive\\zotero\\Khurshid\\khurshid_veriflow.pdf}
}

@phdthesis{kiatAutomatedMethodologyInspecting,
  title = {Automated Methodology for Inspecting and Modifying {{TLS}} Packets despite Certificate Pinning in {{Android}} Applications},
  author = {Kiat, Lim Choon and Szalachowski, Pawel},
  file = {D\:\\GDrive\\zotero\\Kiat_Szalachowski\\kiat_szalachowski_automated_methodology_for_inspecting_and_modifying_tls_packets_despite.pdf},
  language = {en}
}

@techreport{kilAddressSpaceLayout2006,
  title = {Address {{Space Layout Permutation}} ({{ASLP}}): {{Towards Fine}}-{{Grained Randomization}} of {{Commodity Software}}},
  author = {Kil, Chongkyung and Jun, Jinsuk and Bookholt, Christopher and Xu, Jun and Ning, Peng},
  year = {2006},
  abstract = {Address space randomization is an emerging and promising method for stopping a broad range of memory corruption attacks. By randomly shifting critical memory regions at process initialization time, address space ran-domization converts an otherwise successful malicious attack into a benign process crash. However, existing approaches either introduce insufficient randomness, or require source code modification. While insufficient random-ness allows successful brute-force attacks, as shown in recent studies, the required source code modification prevents this effective method from being used for commodity software , which is the major source of exploited vulnerabilities on the Internet. We propose Address Space Layout Permutation (ASLP) that introduces high degree of randomness (or high entropy) with minimal performance overhead. Essential to ASLP is a novel binary rewriting tool that can place the static code and data segments of a compiled exe-cutable to a randomly specified location and performs fine-grained permutation of procedure bodies in the code segment as well as static data objects in the data segment. We have also modified the Linux operating system kernel to per-mute stack, heap, and memory mapped regions. Together, ASLP completely permutes memory regions in an application. Our security and performance evaluation shows minimal performance overhead with orders of magnitude improvement in randomness (e.g., up to 29 bits of randomness on a 32-bit architecture).},
  file = {D\:\\GDrive\\zotero\\Kil\\kil_address_space_layout_permutation_(aslp).pdf}
}

@article{kilRemoteAttestationDynamic2009,
  title = {Remote Attestation to Dynamic System Properties: {{Towards}} Providing Complete System Integrity Evidence},
  author = {Kil, Chongkyung and Sezer, Emre C. and Azab, Ahmed M. and Ning, Peng and Zhang, Xiaolan},
  year = {2009},
  pages = {115--124},
  publisher = {{IEEE}},
  doi = {10.1109/DSN.2009.5270348},
  abstract = {Remote attestation of system integrity is an essential part of trusted computing. However, current remote attestation techniques only provide integrity proofs of static properties of the system. To address this problem we present a novel remote dynamic attestation system named ReDAS (Remote Dynamic Attestation System) that provides integrity evidence for dynamic system properties. Such dynamic system properties represent the runtime behavior of the attested system, and enable an attester to prove its runtime integrity to a remote party. ReDAS currently provides two types of dynamic system properties for running applications: structural integrity and global data integrity. In this work, we present the challenges of remote dynamic attestation, provide an in-depth security analysis and introduce a first step towards providing a complete runtime dynamic attestation framework. Our prototype implementation and evaluation with real-world applications show that we can improve on current static attestation techniques with an average performance overhead of 8\%. \textcopyright 2009 IEEE.},
  file = {D\:\\GDrive\\zotero\\Kil\\kil_2009_remote_attestation_to_dynamic_system_properties.pdf},
  isbn = {9781424444212},
  journal = {Proceedings of the International Conference on Dependable Systems and Networks},
  keywords = {Dynamic attestation,Remote attestation,Runtime integrity,System security,Trusted computing}
}

@article{kimFlippingBitsMemory2014,
  title = {Flipping Bits in Memory without Accessing Them: {{An}} Experimental Study of {{DRAM}} Disturbance Errors},
  author = {Kim, Yoongu and Daly, Ross and Kim, Jeremie and Fallin, Chris and Lee, Ji Hye and Lee, Donghyuk and Wilkerson, Chris and Lai, Konrad and Mutlu, Onur},
  year = {2014},
  pages = {361--372},
  issn = {10636897},
  doi = {10.1109/ISCA.2014.6853210},
  abstract = {Memory isolation is a key property of a reliable and secure computing system - an access to one memory address should not have unintended side effects on data stored in other addresses. However, as DRAM process technology scales down to smaller dimensions, it becomes more difficult to prevent DRAM cells from electrically interacting with each other. In this paper, we expose the vulnerability of commodity DRAM chips to disturbance errors. By reading from the same address in DRAM, we show that it is possible to corrupt data in nearby addresses. More specifically, activating the same row in DRAM corrupts data in nearby rows. We demonstrate this phenomenon on Intel and AMD systems using a malicious program that generates many DRAM accesses. We induce errors in most DRAM modules (110 out of 129) from three major DRAM manufacturers. From this we conclude that many deployed systems are likely to be at risk. We identify the root cause of disturbance errors as the repeated toggling of a DRAM row's wordline, which stresses inter-cell coupling effects that accelerate charge leakage from nearby rows. We provide an extensive characterization study of disturbance errors and their behavior using an FPGA-based testing platform. Among our key findings, we show that (i) it takes as few as 139K accesses to induce an error and (ii) up to one in every 1.7K cells is susceptible to errors. After examining various potential ways of addressing the problem, we propose a low-overhead solution to prevent the errors. \textcopyright{} 2014 IEEE.},
  file = {D\:\\GDrive\\zotero\\Kim\\kim_2014_flipping_bits_in_memory_without_accessing_them.pdf},
  isbn = {9781479943968},
  journal = {Proceedings - International Symposium on Computer Architecture}
}

@techreport{kimFloodlessSEATTLEScalable,
  title = {Floodless in {{SEATTLE}}: {{A Scalable Ethernet Architecture}} for {{Large Enterprises}}},
  author = {Kim, Changhoon and Caesar, Matthew and Rexford, Jennifer},
  abstract = {IP networks today require massive effort to configure and manage. Ethernet is vastly simpler to manage, but does not scale beyond small local area networks. This paper describes an alternative network architecture called SEATTLE that achieves the best of both worlds: The scalability of IP combined with the simplicity of Ethernet. SEATTLE provides plug-and-play functionality via flat addressing, while ensuring scalability and efficiency through shortest-path routing and hash-based resolution of host information. In contrast to previous work on identity-based routing, SEATTLE ensures path predictability and stability, and simplifies network management. We performed a simulation study driven by real-world traffic traces and network topologies, and used Emu-lab to evaluate a prototype of our design based on the Click and XORP open-source routing platforms. Our experiments show that SEATTLE efficiently handles network failures and host mobility, while reducing control overhead and state requirements by roughly two orders of magnitude compared with Ethernet bridging.},
  file = {D\:\\GDrive\\zotero\\Kim\\kim_floodless_in_seattle.pdf}
}

@techreport{kingmaADAMMETHODSTOCHASTIC,
  title = {{{ADAM}}: {{A METHOD FOR STOCHASTIC OPTIMIZATION}}},
  author = {Kingma, Diederik P and Lei Ba, Jimmy},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  file = {D\:\\GDrive\\zotero\\Kingma\\kingma_adam.pdf}
}

@techreport{kingPPCoinPeertoPeerCryptoCurrency2012,
  title = {{{PPCoin}}: {{Peer}}-to-{{Peer Crypto}}-{{Currency}} with {{Proof}}-of-{{Stake}}},
  author = {King, Sunny and Nadal, Scott},
  year = {2012},
  abstract = {A peer-to-peer crypto-currency design derived from Satoshi Nakamoto's Bitcoin. Proof-of-stake replaces proof-of-work to provide most of the network security. Under this hybrid design proof-of-work mainly provides initial minting and is largely non-essential in the long run. Security level of the network is not dependent on energy consumption in the long term thus providing an energy-efficient and more cost-competitive peer-to-peer crypto-currency. Proof-of-stake is based on coin age and generated by each node via a hashing scheme bearing similarity to Bitcoin's but over limited search space. Block chain history and transaction settlement are further protected by a centrally broadcasted checkpoint mechanism.},
  file = {D\:\\GDrive\\zotero\\King\\king_2012_ppcoin.pdf}
}

@techreport{kingPrimecoinCryptocurrencyPrime2013,
  title = {Primecoin: {{Cryptocurrency}} with {{Prime Number Proof}}-of-{{Work}}},
  author = {King, Sunny},
  year = {2013},
  abstract = {A new type of proof-of-work based on searching for prime numbers is introduced in peer-to-peer cryptocurrency designs.},
  file = {D\:\\GDrive\\zotero\\King\\king_2013_primecoin.pdf}
}

@techreport{kingSymbolicExecutionProgram1976,
  title = {Symbolic {{Execution}} and {{Program Testing}}},
  author = {King, James C},
  year = {1976},
  abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may he symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described, it interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
  file = {D\:\\GDrive\\zotero\\King\\king_1976_symbolic_execution_and_program_testing.pdf}
}

@misc{kistlerContinuousProgramOptimization2003,
  title = {Continuous {{Program Optimization}}: {{A Case}}},
  shorttitle = {Continuous {{Program Optimization}}},
  author = {Kistler, Thomas and Franz, Michael},
  year = {2003},
  abstract = {Much of the software in everyday operation is not making optimal use of the hardware on which it actually runs. Among the reasons for this discrepancy are hardware/software mismatches, modularization overheads introduced by software engineering considerations, and the inability of systems to adapt to users ' behaviors. A solution to these problems is to delay code generation until load time. This is the earliest point at which a piece of software can be fine-tuned to the actual capabilities of the hardware on which it is about to be executed, and also the earliest point at wich modularization overheads can be overcome by global optimization. A still better match between software and hardware can be achieved by replacing the already executing software at regular intervals by new versions constructed on-the-fly using a background code re-optimizer. This not only enables the use of live profiling data to guide optimization decisions, but also facilitates adaptation to changing usage patterns and the late addition of dynamic link libraries. This paper presents a system that provides code generation at load-time and continuous program optimization at run-time. First, the architecture of the system is presented. Then, two},
  file = {D\:\\GDrive\\zotero\\Kistler_Franz\\kistler_franz_2003_continuous_program_optimization.pdf;C\:\\Users\\Admin\\Zotero\\storage\\F8UAPNAZ\\summary.html}
}

@techreport{kleinbergAuthoritativeSourcesHyperlinked1997,
  title = {Authoritative {{Sources}} in a {{Hyperlinked Environment}} *},
  author = {Kleinberg, Jon M},
  year = {1997},
  abstract = {The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of "authoritative" information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of "hub pages" that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.},
  file = {D\:\\GDrive\\zotero\\Kleinberg\\kleinberg_1997_authoritative_sources_in_a_hyperlinked_environment.pdf}
}

@techreport{kleinCommonGroundCoordination2004,
  title = {Common {{Ground}} and {{Coordination}} in {{Joint Activity}}},
  author = {Klein, Gary and Feltovich, Paul J and Bradshaw, Jeffrey M and Woods, David D},
  year = {2004},
  abstract = {Generalizing the concepts of joint activity developed by Clark (1996), we describe key aspects of team coordination. Joint activity depends on interpredictability of the participants' attitudes and actions. Such interpredictability is based on common ground-pertinent knowledge, beliefs and assumptions that are shared among the involved parties. Joint activity assumes a basic compact, which is an agreement (often tacit) to facilitate coordination and prevent its breakdown. One aspect of the Basic Compact is the commitment to some degree of aligning multiple goals. A second aspect is that all parties are expected to bear their portion of the responsibility to establish and sustain common ground and to repair it as needed. We apply our understanding of these features of joint activity to account for issues in the design of automation. Research in software and robotic agents seeks to understand and satisfy requirements for the basic aspects of joint activity. Given the widespread demand for increasing the effectiveness of team play for complex systems that work closely and collaboratively with people, observed shortfalls in these current research efforts are ripe for further exploration and study. Common Ground P. 3}
}

@book{kleppmannDesigningDataintensiveApplications2017,
  title = {Designing Data-Intensive Applications: The Big Ideas behind Reliable, Scalable, and Maintainable Systems},
  shorttitle = {Designing Data-Intensive Applications},
  author = {Kleppmann, Martin},
  year = {2017},
  edition = {First edition},
  publisher = {{O'Reilly Media}},
  address = {{Boston}},
  abstract = {Data is at the center of many challenges in system design today. Difficult issues need to be figured out, such as scalability, consistency, reliability, efficiency, and mainteinability. In addition, we have an overwhelming variet of tools, including relational databases, NoSQL datastores, stream or batch processors, and message brokers. What are the right choices for your application? How do you make sense of all these buzzwords? In this practical and comprehensive gjuide, author Martin Kleppmann helps you navigate this diverse landscape by examining the pros and cons of various technologies for processing and storing data. Software keeps changing, but the fundamental principles remain the same. With this book, software engineers and architects will learn how to apply those ideas in practice, and how to make full use of data in modern applications},
  annotation = {OCLC: ocn893895983},
  file = {D\:\\GDrive\\zotero\\Kleppmann\\kleppmann_2017_designing_data-intensive_applications.pdf},
  isbn = {978-1-4493-7332-0},
  keywords = {Application software,Database management,Development,Web site development},
  lccn = {QA76.76.A65 K612 2017}
}

@inproceedings{kleppmannLocalfirstSoftwareYou2019,
  title = {Local-First Software: You Own Your Data, in Spite of the Cloud},
  booktitle = {Proceedings of the 2019 {{ACM SIGPLAN International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}  - {{Onward}}! 2019},
  author = {Kleppmann, Martin and Wiggins, Adam and {van Hardenberg}, Peter and McGranaghan, Mark},
  year = {2019},
  pages = {154--178},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3359591.3359737},
  file = {D\:\\GDrive\\zotero\\Kleppmann\\kleppmann_2019_local-first_software.pdf},
  isbn = {978-1-4503-6995-4}
}

@article{klieberFormalVerificationUsing2014,
  title = {Formal {{Verification Using Quantified Boolean Formulas}} ({{QBF}})},
  author = {Klieber, William},
  year = {2014},
  file = {D\:\\GDrive\\zotero\\Klieber\\klieber_2014_formal_verification_using_quantified_boolean_formulas_(qbf).pdf},
  number = {May}
}

@techreport{knuthMathematicalWriting,
  title = {Mathematical {{Writing}}},
  author = {Knuth, Donald E and Larrabee, Tracy and Roberts, Paul M},
  abstract = {This report is based on a course of the same name given at Stanford University during autumn quarter, 1987. Here's the catalog description: CS 209. Mathematical Writing-Issues of technical writing and the effective presentation of mathematics and computer science. Preparation of theses, papers, books, and "literate" computer programs. A term paper on a topic of your choice; this paper may be used for credit in another course. The first three lectures were a "minicourse" that summarized the basics. About two hundred people attended those three sessions, which were devoted primarily to a discussion of the points in \textsection 1 of this report. An exercise (\textsection 2) and a suggested solution (\textsection 3) were also part of the minicourse. The remaining 28 lectures covered these and other issues in depth. We saw many examples of "before" and "after" from manuscripts in progress. We learned how to avoid excessive subscripts and superscripts. We discussed the documentation of algorithms, computer programs, and user manuals. We considered the process of refereeing and editing. We studied how to make effective diagrams and tables, and how to find appropriate quotations to spice up a text.},
  file = {D\:\\GDrive\\zotero\\Knuth\\knuth_mathematical_writing_by.pdf}
}

@techreport{kobayashiMaturingOpenFlowSoftware,
  title = {Maturing of {{OpenFlow}} and {{Software Defined Networking}} through {{Deployments}}},
  author = {Kobayashi, Masayoshi and Seetharaman, Srini and Parulkar, Guru and Appenzeller, Guido and Little, Joseph and Van Reijendam, Johan and Weissmann, Paul and Mckeown, Nick},
  abstract = {Software-defined networking (SDN) has emerged as a new paradigm of networking that enables evolvable and programmable networks allowing network operators , owners, vendors, and even third parties to innovate and create new capabilities at a faster pace. The SDN paradigm shows potential for all domains of use including the data center, cellular, service provider, enterprise, and home. In this paper we present SDN deployments that we did on our campus and other deployments that we led with partners over four years in four phases. Our deployments include the first ever SDN prototype in a lab to a (small) global deployment. The deployments combined with demonstrations of new networking capabilities enabled by SDN played an important role in maturing of SDN and its ecosystem so far. We share our experiences and lessons learned that have to do with demonstration of SDN potential; influence on successive OpenFlow specifications; evolution of SDN architecture; performance of SDN and various components; and growing the ecosystem.},
  file = {D\:\\GDrive\\zotero\\Kobayashi et al\\kobayashi_et_al_maturing_of_openflow_and_software_defined_networking_through_deployments.pdf}
}

@article{kobayashiPredicateAbstractionCEGAR2011,
  title = {Predicate Abstraction and {{CEGAR}} for Higher-Order Model Checking},
  author = {Kobayashi, Naoki and Sato, Ryosuke and Unno, Hiroshi},
  year = {2011},
  pages = {222--233},
  doi = {10.1145/1993498.1993525},
  abstract = {Higher-order model checking (more precisely, the model checking of higher-order recursion schemes) has been extensively studied recently, which can automatically decide properties of programs written in the simply-typed {$\lambda$}-calculus with recursion and finite data domains. This paper formalizes predicate abstraction and counterexample-guided abstraction refinement (CEGAR) for higher-order model checking, enabling automatic verification of programs that use infinite data domains such as integers. A prototype verifier for higher-order functional programs based on the formalization has been implemented and tested for several programs. \textcopyright{} 2011 ACM.},
  file = {D\:\\GDrive\\zotero\\Kobayashi\\kobayashi_2011_predicate_abstraction_and_cegar_for_higher-order_model_checking.pdf},
  isbn = {9781450306638},
  journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  keywords = {cegar,dependent types,higher-order model checking,predicate abstraction}
}

@article{kocherSecurityNewDimension2004,
  title = {Security as a New Dimension in Embedded System Design},
  author = {Kocher, Paul and Lee, Ruby and McGraw, Gary and Raghunathan, Anand and Ravi, Srivaths},
  year = {2004},
  pages = {753--760},
  issn = {0738100X},
  doi = {10.1145/996566.996771},
  abstract = {The growing number of instances of breaches in information security in the last few years has created a compelling case for efforts towards secure electronic systems. Embedded systems, which will be ubiquitously used to capture, store, manipulate, and access data of a sensitive nature, pose several unique and interesting security challenges. Security has been the subject of intensive research in the areas of cryptography, computing, and networking. However, security is often mis-construed by embedded system designers as the addition of features, such as specific cryptographic algorithms and security protocols, to the system. In reality, it is an entirely new metric that designers should consider throughout the design process, along with other metrics such as cost, performance, and power. This paper is intended to introduce embedded system designers and design tool developers to the challenges involved in designing secure embedded systems. We attempt to provide a unified view of embedded system security by first analyzing the typical functional security requirements for embedded systems from an end-user perspective. We then identify the implied challenges for embedded system architects, as well as hardware and software designers (e.g., tamper-resistant embedded system design, processing requirements for security, impact of security on battery life for battery-powered systems, etc.). We also survey solution techniques to address these challenges, drawing from both current practice and emerging research, and identify open research problems that will require innovations in embedded system architecture and design methodologies.},
  file = {D\:\\GDrive\\zotero\\Kocher\\kocher_2004_security_as_a_new_dimension_in_embedded_system_design.pdf},
  isbn = {1581138288},
  journal = {Proceedings - Design Automation Conference},
  keywords = {Architectures,Battery Life,Cryptography,Design,Design Methodologies,Digital Rights Management,Embedded Systems,Performance,Security,Security Processing,Security Protocols,Software Attacks,Tamper Resistance,Trusted Computing,Viruses}
}

@article{kocherSpectreAttacksExploiting2018,
  title = {Spectre {{Attacks}}: {{Exploiting Speculative Execution}}},
  author = {Kocher, Paul and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
  year = {2018},
  month = jan,
  volume = {32},
  pages = {811--829},
  issn = {0164-0291},
  doi = {10.1007/s10764-011-9503-1},
  abstract = {Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.},
  file = {D\:\\GDrive\\zotero\\Kocher\\kocher_2018_spectre_attacks.pdf},
  isbn = {0123704901},
  journal = {International Journal of Primatology},
  keywords = {ss},
  number = {4},
  pmid = {15003161}
}

@article{kocherTimingAttacksImplement1996,
  title = {Timing {{Attacks}} on {{Implement}} at Ions Of},
  author = {Kocher, Paul C},
  year = {1996},
  pages = {104--113},
  abstract = {By carefully measuring the amourit of time required tm per-forin private key operalions, attackers m a y t) P able to find fixed Diffie-IieUirian exponents, fact ,or RSA keys, aid break other crypt,osysteins. Against, a vrilnerablc system, t,he atlack is corriprit,atiorially inexpensive and ofteri requires only known ciphertext. Actual systems are potentially at risk, ind I idi ng cryptographic t okeris, net work-based cryptosystems, arid other applica1,ions where attackers can make reasonably accilrate timing measurements. Techniques for preventing the attack for RSA and Iliffie-Hellman are presented. Some cryptJosysterrrs will need to be revised to protect against thc: at,tack. and new protocols and algorithms may need to incorporate measures t o prevenl timing attacks.},
  file = {D\:\\GDrive\\zotero\\Kocher\\kocher_1996_timing_attacks_on_implement_at_ions_of.pdf},
  journal = {Advances in Cryptology - CRYPTO'96},
  keywords = {cryptaiialysis,diffie-hellman,dss,g attack,rsa,t h i n}
}

@techreport{kocherTimingAttacksImplementations,
  title = {Timing {{Attacks}} on {{Implementations}} of {{Diie}}-{{Hellman}}, {{RSA}}, {{DSS}}, and {{Other Systems}}},
  author = {Kocher, Paul C},
  abstract = {By carefully measuring the amount of time required to perform private key operations, attackers may be able to ond dxed Diie-Hellman exponents, factor RSA keys, and break other cryptosystems. Against a vulnerable system, the attack is computationally inexpensive and often requires only known ciphertext. Actual systems are potentially at risk, including cryptographic tokens, network-based cryptosystems, and other applications where attackers can make reasonably accurate timing measurements. Techniques for preventing the attack for RSA and Diie-Hellman are presented. Some cryptosystems will need to be revised to protect against the attack, and new protocols and algorithms may need to incorporate measures to prevent timing attacks.},
  file = {D\:\\GDrive\\zotero\\Kocher\\kocher_timing_attacks_on_implementations_of_diie-hellman,_rsa,_dss,_and_other_systems.pdf},
  keywords = {side-channel,ss}
}

@article{kodialamObliviousRoutingHighly2009,
  title = {Oblivious Routing of Highly Variable Traffic in Service Overlays and {{IP}} Backbones},
  author = {Kodialam, Murali and Lakshman, T. V. and Orlin, James B. and Sengupta, Sudipta},
  year = {2009},
  volume = {17},
  pages = {459--472},
  issn = {10636692},
  doi = {10.1109/TNET.2008.927257},
  abstract = {The emergence of new applications on the Internet like voice-over-IP, peer-to-peer, and video-on-demand has created highly dynamic and changing traffic patterns. In order to route such traffic with quality-of-service (QoS) guarantees without requiring detection of traffic changes in real-time or reconfiguring the network in response to it, a routing and bandwidth allocation scheme has been recently proposed that allows preconfiguration of the network such that all traffic patterns permissible within the network's natural ingress-egress capacity constraints can be handled in a capacity efficient manner. The scheme routes traffic in two phases. In the first phase, incoming traffic is sent from the source to a set of intermediate nodes and then, in the second phase, from the intermediate nodes to the final destination. The traffic in the first phase is distributed to the intermediate nodes in predetermined proportions that depend on the intermediate nodes. In this paper, we develop linear programming formulations and a fast combinatorial algorithm for routing under the scheme so as to maximize throughput (or, minimize maximum link utilization). We compare the throughput performance of the scheme with that of the optimal scheme among the class of all schemes that are allowed to even make the routing dependent on the traffic matrix. For our evaluations, we use actual Internet Service Provider topologies collected for the Rocketfuel project. We also bring out the versatility of the scheme in not only handling widely fluctuating traffic but also accommodating applicability to several widely differing networking scenarios, including i) economical Virtual Private Networks (VPNs); ii) supporting indirection in specialized service overlay models like Internet Indirection Infrastructure (i3); iii) adding QoS guarantees to services that require routing through a network-based middlebox; and iv) reducing IP layer transit traffic and handling extreme traffic variability in IP-over-optical networks without dynamic reconfiguration of the optical layer. The two desirable properties of supporting indirection in specialized service overlay models and static optical layer provisioning in IP-over-optical networks are not present in other approaches for routing variable traffic, such as direct source-destination routing along fixed paths. \textcopyright{} 2008 IEEE.},
  journal = {IEEE/ACM Transactions on Networking},
  keywords = {Hose traffic model,IP/MPLS; IP-over-optical,Oblivious routing,Service overlays,Two-phase routing,Valiant load balancing,Variable traffic},
  number = {2}
}

@techreport{kodialamVersatileSchemeRouting,
  title = {A {{Versatile Scheme}} for {{Routing Highly Variable Traffic}} in {{Service Overlays}} and {{IP Backbones}}},
  author = {Kodialam, M and Lakshman, T V and Orlin, James B and Sengupta, Sudipta},
  abstract = {The emergence of new applications on the Internet like voice-over-IP, peer-to-peer, and video-on-demand has created highly dynamic and changing traffic patterns. In order to route such traffic with Quality-of-Service (QoS) guarantees without requiring detection of traffic changes in real-time or reconfiguring the network in response to it, we consider a routing and bandwidth allocation scheme that allows preconfiguration of the network such that all traffic patterns permissible within the network's natural ingress-egress capacity constraints can be handled in a capacity efficient manner. The scheme routes traffic in two phases. In the first phase, incoming traffic is sent from the source to a set of intermediate nodes and then, in the second phase, from the intermediate nodes to the final destination. The traffic in the first phase is distributed to the intermediate nodes in predetermined proportions that depend on the intermediate nodes. In this paper, we develop linear programming formulations and a fast combinatorial algorithm for routing under the scheme so as to maximize throughput (or, minimize maximum link utilization). We compare the throughput performance of the scheme with that of the optimal scheme among the class of all schemes that are allowed to even make the routing dependent on the traffic matrix. For our evaluations, we use actual Internet Service Provider topologies collected for the Rocketfuel project. We also bring out the versatility of the scheme in not only handling widely fluctuating traffic but also accommodating applicability to several widely differing networking scenarios, including (i) economical Virtual Private Networks (VPNs), (ii) supporting indirection in specialized service overlay models like Internet Indirection Infrastructure (i3), (iii) adding QoS guarantees to services that require routing through a network-based middlebox, and (iv) reducing IP layer transit traffic and handling extreme traffic variability in IP-over-Optical networks without dynamic reconfiguration of the optical layer. The two desirable properties of supporting indirection in specialized service overlay models and static optical layer provisioning in IP-over-Optical networks are not present in other approaches for routing variable traffic, such as direct source-destination routing along fixed paths.},
  file = {D\:\\GDrive\\zotero\\Kodialam\\kodialam_a_versatile_scheme_for_routing_highly_variable_traffic_in_service_overlays_and.pdf}
}

@article{koeberlTrustLiteSecurityArchitecture2014,
  title = {{{TrustLite}}: {{A Security Architecture}} for {{Tiny Embedded Devices}}},
  author = {Koeberl, Patrick and Schulz, Steffen and Sadeghi, Ahmad-Reza and Varadharajan, Vijay},
  year = {2014},
  doi = {10.1145/2592798.2592824},
  abstract = {Embedded systems are increasingly pervasive, interdependent and in many cases critical to our every day life and safety. Tiny devices that cannot afford sophisticated hardware security mechanisms are embedded in complex control infrastructures, medical support systems and entertainment products [51]. As such devices are increasingly subject to attacks, new hardware protection mechanisms are needed to provide the required resilience and dependency at low cost. In this work, we present the TrustLite security architecture for flexible, hardware-enforced isolation of software modules. We describe mechanisms for secure exception handling and communication between protected modules, enabling seamless interoperability with untrusted operating systems and tasks. TrustLite scales from providing a simple protected firmware runtime to advanced functionality such as attestation and trusted execution of userspace tasks. Our FPGA prototype shows that these capabilities are achievable even on low-cost embedded systems.},
  file = {D\:\\GDrive\\zotero\\Koeberl\\koeberl_trustlite.pdf},
  isbn = {9781450327046}
}

@article{kohlbrennerEffectivenessMitigationsFloatingpoint2017,
  title = {On the Effectiveness of Mitigations against Floating-Point Timing Channels},
  author = {Kohlbrenner, David and Shacham, Hovav},
  year = {2017},
  pages = {15},
  abstract = {The duration of floating-point instructions is a known timing side channel that has been used to break SameOrigin Policy (SOP) privacy on Mozilla Firefox and the Fuzz differentially private database. Several defenses have been proposed to mitigate these attacks.},
  file = {D\:\\GDrive\\zotero\\Kohlbrenner_Shacham\\kohlbrenner_shacham_2017_on_the_effectiveness_of_mitigations_against_ﬂoating-point_timing_channels.pdf},
  language = {en}
}

@article{kohlbrennerTrustedBrowsersUncertain2016,
  title = {Trusted Browsers for Uncertain Times},
  author = {Kohlbrenner, David and Shacham, Hovav},
  year = {2016},
  pages = {19},
  abstract = {JavaScript in one origin can use timing channels in browsers to learn sensitive information about a user's interaction with other origins, violating the browser's compartmentalization guarantees. Browser vendors have attempted to close timing channels by trying to rewrite sensitive code to run in constant time and by reducing the resolution of reference clocks.},
  file = {D\:\\GDrive\\zotero\\Kohlbrenner_Shacham\\kohlbrenner_shacham_2016_trusted_browsers_for_uncertain_times.pdf},
  language = {en}
}

@article{kokoris-kogiasEnhancingBitcoinSecurity,
  title = {Enhancing {{Bitcoin Security}} and {{Performance}} with {{Strong Consistency}} via {{Collective Signing}}},
  author = {{Kokoris-Kogias}, Eleftherios and Jovanovic, Philipp and Gailly, Nicolas and Khoffi, Ismail and Gasser, Linus and Ford, Bryan},
  pages = {19},
  abstract = {While showing great promise, Bitcoin requires users to wait tens of minutes for transactions to commit, and even then, offering only probabilistic guarantees. This paper introduces ByzCoin, a novel Byzantine consensus protocol that leverages scalable collective signing to commit Bitcoin transactions irreversibly within seconds. ByzCoin achieves Byzantine consensus while preserving Bitcoin's open membership by dynamically forming hash power-proportionate consensus groups that represent recently-successful block miners. ByzCoin employs communication trees to optimize transaction commitment and verification under normal operation while guaranteeing safety and liveness under Byzantine faults, up to a near-optimal tolerance of f faulty group members among 3 f + 2 total. ByzCoin mitigates double spending and selfish mining attacks by producing collectively signed transaction blocks within one minute of transaction submission. Tree-structured communication further reduces this latency to less than 30 seconds. Due to these optimizations, ByzCoin achieves a throughput higher than Paypal currently handles, with a confirmation latency of 15-20 seconds.},
  file = {D\:\\GDrive\\zotero\\Kokoris-Kogias et al\\kokoris-kogias_et_al_enhancing_bitcoin_security_and_performance_with_strong_consistency_via.pdf},
  language = {en}
}

@techreport{koNoninterferenceIntrusionDetection2002,
  title = {Noninterference and {{Intrusion Detection}}},
  author = {Ko, Calvin and Redmond, Timothy},
  year = {2002},
  abstract = {This paper presents an intrusion detection methodology based on the concept of noninterference for detecting race-condition attacks. In general, this type of attack occurs when an unprivileged process causes a privileged process to perform illegal operations by executing strategic operations in the appropriate timing window. We apply the non-interference model in a novel way that allows us to formally represent valid interleaving between privileged and unprivi-leged processes. Instead of proving a system satisfies nonin-terference assertions, we derive an algorithm for checking the assertions at run-time based on the developed theory and a formal model of Unix system calls. Our methodology can detect unknown race-condition attacks. In addition, this work provides an example of the application of formal specification and reasoning in intrusion detection.},
  file = {D\:\\GDrive\\zotero\\Ko\\ko_2002_noninterference_and_intrusion_detection.pdf}
}

@book{kopkaGuideLATEXDocument2004,
  title = {A Guide to {{LATEX}}: Document Preparation for Beginners and Advanced Users},
  shorttitle = {A Guide to {{LATEX}}},
  author = {Kopka, Helmut and Daly, Patrick W.},
  year = {2004},
  edition = {4th ed},
  publisher = {{Addison-Wesley}},
  address = {{Harlow, England ; Reading, Mass}},
  file = {D\:\\GDrive\\zotero\\Kopka_Daly\\kopka_daly_1999_a_guide_to_latex.pdf},
  isbn = {978-0-201-39825-0},
  keywords = {Computerized typesetting,LaTeX (Computer file)},
  language = {en},
  lccn = {Z253.4.L38 K66 1999}
}

@article{koponenNetworkVirtualizationMultitenant,
  title = {Network {{Virtualization}} in {{Multi}}-Tenant {{Datacenters Network Virtualization}} in {{Multi}}-Tenant {{Datacenters}}},
  author = {Koponen, Teemu and Amidon, Keith and Balland, Peter and Casado, Mart{\'i}n and Chanda, Anupam and Fulton, Bryan and Ganichev, Igor and Gross, Jesse and Gude, Natasha and Ingram, Paul and Jackson, Ethan and Lambeth, Andrew and Lenglet, Romain and Li, Shih-Hao and Padmanabhan, Amar and Pettit, Justin and Pfaff, Ben and Ramanathan, Rajiv and Shenker, Scott and Shieh, Alan and Stribling, Jeremy and Thakkar, Pankaj and Wendlandt, Dan and Yip, Alexander and Zhang, Ronghua},
  abstract = {Multi-tenant datacenters represent an extremely challenging networking environment. Tenants want the ability to migrate unmodified workloads from their enterprise networks to service provider datacenters, retaining the same networking configurations of their home network. The service providers must meet these needs without operator intervention while preserving their own operational flexibility and efficiency. Traditional networking approaches have failed to meet these tenant and provider requirements. Responding to this need, we present the design and implementation of a network virtualization solution for multi-tenant datacenters.},
  file = {D\:\\GDrive\\zotero\\Koponen\\koponen_network_virtualization_in_multi-tenant_datacenters_network_virtualization_in.pdf}
}

@techreport{koponenOnixDistributedControl,
  title = {Onix: {{A Distributed Control Platform}} for {{Large}}-Scale {{Production Networks}}},
  author = {Koponen, Teemu and Casado, Martin and Gude, Natasha and Stribling, Jeremy and Poutievski, Leon and Zhu, Min and Ramanathan, Rajiv and Iwata, Yuichiro and Inoue, Hiroaki and Hama, Takayuki and Shenker, Scott},
  abstract = {Computer networks lack a general control paradigm, as traditional networks do not provide any network-wide management abstractions. As a result, each new function (such as routing) must provide its own state distribution, element discovery, and failure recovery mechanisms. We believe this lack of a common control platform has significantly hindered the development of flexible, reliable and feature-rich network control planes. To address this, we present Onix, a platform on top of which a network control plane can be implemented as a distributed system. Control planes written within Onix operate on a global view of the network, and use basic state distribution primitives provided by the platform. Thus Onix provides a general API for control plane implementations, while allowing them to make their own trade-offs among consistency, durability, and scalability.},
  file = {D\:\\GDrive\\zotero\\Koponen et al\\koponen_et_al_onix.pdf}
}

@techreport{kornauReturnOrientedProgramming2009,
  title = {Return {{Oriented Programming}} for the {{ARM Architecture Diplomarbeit Ruhr}}-{{Universit\"at Bochum Lehrstuhl}} F\"ur {{Netz}}-Und {{Datensicherheit}}},
  author = {Kornau, Tim and Schwenk, J{\"o}rg},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Kornau\\kornau_2009_return_oriented_programming_for_the_arm_architecture_diplomarbeit.pdf}
}

@techreport{korobeynikovImprovingSwitchLowering,
  title = {Improving {{Switch Lowering}} for {{The LLVM Compiler System}}},
  author = {Korobeynikov, Anton},
  abstract = {Switch-case statements (or switches) provide a natural way to express multiway branching control flow semantics. They are common in many applications including compilers, parsers, text processing programs, virtual machines. Various optimizations for switches has been studied for many years. This paper presents the description of switch lowering refactoring recently made for the LLVM Compiler System [1].},
  file = {D\:\\GDrive\\zotero\\Korobeynikov\\korobeynikov_improving_switch_lowering_for_the_llvm_compiler_system.pdf}
}

@techreport{kosbaHawkBlockchainModel,
  title = {Hawk: {{The Blockchain Model}} of {{Cryptography}} and {{Privacy}}-{{Preserving Smart Contracts}}},
  author = {Kosba, Ahmed and Miller, Andrew and Shi, Elaine and Wen, Zikai and Papamanthou, Charalampos},
  abstract = {Emerging smart contract systems over decentralized cryptocurrencies allow mutually distrustful parties to transact safely without trusted third parties. In the event of contractual breaches or aborts, the decentralized blockchain ensures that honest parties obtain commensurate compensation. Existing systems, however, lack transactional privacy. All transactions, including flow of money between pseudonyms and amount transacted, are exposed on the blockchain. We present Hawk, a decentralized smart contract system that does not store financial transactions in the clear on the block-chain, thus retaining transactional privacy from the public's view. A Hawk programmer can write a private smart contract in an intuitive manner without having to implement cryptography, and our compiler automatically generates an efficient cryptographic protocol where contractual parties interact with the blockchain, using cryptographic primitives such as zero-knowledge proofs. To formally define and reason about the security of our protocols, we are the first to formalize the blockchain model of cryptography. The formal modeling is of independent interest. We advocate the community to adopt such a formal model when designing applications atop decentralized blockchains.},
  file = {D\:\\GDrive\\zotero\\Kosba\\kosba_hawk.pdf}
}

@article{koscherExperimentalSecurityAnalysis2010,
  title = {Experimental {{Security Analysis}} of a {{Modern Automobile}}},
  author = {Koscher, Karl and Czeskis, Alexei and Roesner, Franziska and Patel, Shwetak and Kohno, Tadayoshi and Checkoway, Stephen and McCoy, Damon and Kantor, Brian and Anderson, Danny and Shacham, Hovav and Savage, Stefan},
  year = {2010},
  pages = {16},
  abstract = {Modern automobiles are no longer mere mechanical devices; they are pervasively monitored and controlled by dozens of digital computers coordinated via internal vehicular networks. While this transformation has driven major advancements in efficiency and safety, it has also introduced a range of new potential risks. In this paper we experimentally evaluate these issues on a modern automobile and demonstrate the fragility of the underlying system structure. We demonstrate that an attacker who is able to infiltrate virtually any Electronic Control Unit (ECU) can leverage this ability to completely circumvent a broad array of safety-critical systems. Over a range of experiments, both in the lab and in road tests, we demonstrate the ability to adversarially control a wide range of automotive functions and completely ignore driver input \textemdash including disabling the brakes, selectively braking individual wheels on demand, stopping the engine, and so on. We find that it is possible to bypass rudimentary network security protections within the car, such as maliciously bridging between our car's two internal subnets. We also present composite attacks that leverage individual weaknesses, including an attack that embeds malicious code in a car's telematics unit and that will completely erase any evidence of its presence after a crash. Looking forward, we discuss the complex challenges in addressing these vulnerabilities while considering the existing automotive ecosystem.},
  file = {D\:\\GDrive\\zotero\\Koscher et al\\koscher_et_al_2010_experimental_security_analysis_of_a_modern_automobile.pdf},
  language = {en}
}

@article{kovacsFindingLoopInvariants2009,
  title = {Finding Loop Invariants for Programs over Arrays Using a Theorem Prover},
  author = {Kov{\'a}cs, Laura and Voronkov, Andrei},
  year = {2009},
  volume = {5503},
  pages = {470--485},
  issn = {03029743},
  doi = {10.1007/978-3-642-00593-0_33},
  abstract = {We present a new method for automatic generation of loop invariants for programs containing arrays. Unlike all previously known methods, our method allows one to generate first-order invariants containing alternations of quantifiers. The method is based on the automatic analysis of the so-called update predicates of loops. An update predicate for an array A expresses updates made to A. We observe that many properties of update predicates can be extracted automatically from the loop description and loop properties obtained by other methods such as a simple analysis of counters occurring in the loop, recurrence solving and quantifier elimination over loop variables. We run the theorem prover Vampire on some examples and show that non-trivial loop invariants can be generated.},
  file = {D\:\\GDrive\\zotero\\Kovács\\kovács_2009_finding_loop_invariants_for_programs_over_arrays_using_a_theorem_prover.pdf},
  isbn = {9783642005923},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  number = {026133}
}

@techreport{kowshikEnsuringCodeSafety,
  title = {Ensuring {{Code Safety Without Runtime Checks}} for {{Real}}-{{Time Control Systems}}},
  author = {Kowshik, Sumant and Dhurjati, Dinakar and Adve, Vikram},
  abstract = {This paper considers the problem of providing safe programming support and enabling secure online software upgrades for control software in real-time control systems. In such systems, offline techniques for ensuring code safety are greatly preferable to online techniques. We propose a language called Control-C that is essentially a subset of C, but with key restrictions designed to ensure that memory safety of code can be verified entirely by static checking, under certain system assumptions. The language permits pointer-based data structures, restricted dynamic memory allocation , and restricted array operations, without requiring any runtime checks on memory operations and without garbage collection. The language restrictions have been chosen based on an understanding of both compiler technology and the needs of real-time control systems. The paper describes the language design and a compiler implementation for Control-C. We use control codes from three different experimental control systems to evaluate the suitability of the language for these codes, the effort required to port them to Control-C, and the effectiveness of the compiler in detecting a wide range of potential security violations for one of the systems.},
  file = {D\:\\GDrive\\zotero\\Kowshik\\kowshik_ensuring_code_safety_without_runtime_checks_for_real-time_control_systems.pdf},
  keywords = {C3 [Computer Systems Organization]: Special-Purpose and Application-based Systems,D3 [Software]: Programming Lan-guages,D46 [Software]: Operating Systems-Security and Pro-tection General Terms Security; Languages Keywords real-time; control; compiler; programming language; static analy-sis; security}
}

@techreport{kportsSerializableSnapshotIsolation,
  title = {Serializable {{Snapshot Isolation}} in {{PostgreSQL}}},
  author = {K Ports, Dan R and Grittner, Kevin},
  volume = {5},
  abstract = {This paper describes our experience implementing PostgreSQL's new serializable isolation level. It is based on the recently-developed Serializable Snapshot Isolation (SSI) technique. This is the first implementation of SSI in a production database release as well as the first in a database that did not previously have a lock-based serializ-able isolation level. We reflect on our experience and describe how we overcame some of the resulting challenges, including the implementation of a new lock manager, a technique for ensuring memory usage is bounded, and integration with other PostgreSQL features. We also introduce an extension to SSI that improves performance for read-only transactions. We evaluate PostgreSQL's serializable isolation level using several benchmarks and show that it achieves performance only slightly below that of snapshot isolation, and significantly outperforms the traditional two-phase locking approach on read-intensive workloads. 1. OVERVIEW Serializable isolation for transactions is an important property: it allows application developers to write transactions as though they will execute sequentially, without regard for interactions with concurrently-executing transactions. Until recently, PostgreSQL, a popular open-source database, did not provide a serializable isolation level because the standard two-phase locking mechanism was seen as too expensive. Its highest isolation level was snapshot isolation , which offers greater performance but allows certain anomalies. In the latest PostgreSQL 9.1 release, 1 we introduced a serializable isolation level that retains many of the performance benefits of snapshot isolation while still guaranteeing true serializability. It uses an extension of the Serializable Snapshot Isolation (SSI) technique from current research [7]. SSI runs transactions using snapshot isolation, but checks at runtime for conflicts between concurrent transactions, and aborts transactions when anomalies are possible. We extended SSI to improve performance for read-only transactions, an important part of many workloads. 1 PostgreSQL 9.1 is available for download from http://www.postgresql.org/. This paper describes our experiences implementing SSI in Post-greSQL. Our experience is noteworthy for several reasons: It is the first implementation of SSI in a production database release. Accordingly, it must address interactions with other database features that previous research prototypes have ignored. For example , we had to integrate SSI with PostgreSQL's support for repli-cation systems, two-phase commit, and subtransactions. We also address memory usage limitations, an important practical concern; we describe a transaction summarization technique that ensures that the SSI implementation uses a bounded amount of RAM without limiting the number of concurrent transactions. Ours is also the first implementation of SSI for a purely snapshot-based DBMS. Although SSI seems especially suited for such databases , earlier SSI implementations were based on databases that already supported serializable isolation via two-phase locking, such as MySQL. As a result, they were able to take advantage of existing predicate locking mechanisms to detect conflicting transactions for SSI. Lacking this infrastructure, we were required to build a new lock manager. Our lock manager is specifically optimized for tracking SSI read dependencies, making it simpler in some respects than a classic lock manager but also introducing some unusual challenges. PostgreSQL 9.1 uses this lock manager, along with multiversion concurrency control data, to detect conflicts between concurrent transactions. We also introduce a safe retry rule, which resolves conflicts by aborting transactions in such a way that an immediately retried transaction does not fail in the same way. Read-only transactions are common, so PostgreSQL 9.1 optimizes for them. We extend SSI by deriving a result in multiversion serializability theory and applying it to reduce the rate of false positive serialization failures. We also introduce safe snapshots and deferrable transactions, which allow certain read-only transactions to execute without the overhead of SSI by identifying cases where snapshot isolation anomalies cannot occur. PostgreSQL 9.1's serializable isolation level is effective: it provides true serializability but allows more concurrency than two-phase locking. Our experiments with a transaction processing and a web application benchmark show that our serializable mode has a performance cost of less than 7\% relative to snapshot isolation, and outperforms two-phase locking significantly on some workloads. This paper begins with an explanation of how snapshot isolation differs from serializability and why we view serializability as an important DBMS feature in Section 2. Section 3 describes the SSI technique and reviews the previous work. Section 4 extends SSI with new optimizations for read-only transactions. We then turn to the implementation of SSI in PostgreSQL 9.1, with Section 5 giving an overview of the implementation and Section 6 discussing techniques for reducing its memory usage. Section 7 examines how SSI interacts with other PostgreSQL features. Finally, in Section 8},
  file = {D\:\\GDrive\\zotero\\K Ports\\k_ports_serializable_snapshot_isolation_in_postgresql.pdf},
  number = {12}
}

@article{kranchUpgradingHTTPSMidair2015,
  title = {Upgrading {{HTTPS}} in Mid-Air: {{An Empirical Study}} of {{Strict Transport Security}} and {{Key Pinning}}},
  author = {Kranch, Michael and Bonneau, Joseph},
  year = {2015},
  doi = {10.14722/ndss.2015.23162},
  abstract = {NDSS '15},
  number = {Section IX}
}

@techreport{krawczykSIGMASIGnandMAcApproach2003,
  title = {{{SIGMA}}: The '{{SIGn}}-and-{{MAc}}' {{Approach}} to {{Authenticated Diffie}}-{{Hellman}} and Its {{Use}} in the {{IKE Protocols}} *},
  author = {Krawczyk, Hugo},
  year = {2003},
  abstract = {We present the SIGMA family of key-exchange protocols and the "SIGn-and-MAc" approach to authenticated Diffie-Hellman underlying its design. The SIGMA protocols provide perfect forward secrecy via a Diffie-Hellman exchange authenticated with digital signatures, and are specifically designed to ensure sound cryptographic key exchange while supporting a variety of features and trade-offs required in practical scenarios (such as optional identity protection and reduced number of protocol rounds). As a consequence, the SIGMA protocols are very well suited for use in actual applications and for standardized key exchange. In particular, SIGMA serves as the cryptographic basis for the signature-based modes of the standardized Internet Key Exchange (IKE) protocol (versions 1 and 2). This paper describes the design rationale behind the SIGMA approach and protocols, and points out to many subtleties surrounding the design of secure key-exchange protocols in general, and identity-protecting protocols in particular. We motivate the design of SIGMA by comparing it to other protocols, most notable the STS protocol and its variants. In particular, it is shown how SIGMA solves some of the security shortcomings found in previous protocols. * A shortened version of this paper appears in the proceedings of CRYPTO'03. For further information related to the SIGMA protocols see},
  file = {D\:\\GDrive\\zotero\\Krawczyk\\krawczyk_2003_sigma.pdf}
}

@book{krepsKafkaDistributedMessaging,
  title = {Kafka: A {{Distributed Messaging System}} for {{Log Processing}}},
  author = {Kreps, Jay and Narkhede, Neha and Rao, Jun},
  abstract = {"Co-located with: ICMI 2014."},
  file = {D\:\\GDrive\\zotero\\SIGCHI (Group  U.S.)\\sigchi_(group_u.s.)_kafka.pdf},
  isbn = {978-1-4503-0652-2},
  keywords = {distributed systems}
}

@techreport{kreutzSoftwareDefinedNetworkingComprehensive,
  title = {Software-{{Defined Networking}}: {{A Comprehensive Survey}}},
  author = {Kreutz, Diego and V Ramos, Fernando M and Verissimo, Paulo and Esteve Rothenberg, Christian and Azodolmolky, Siamak and Member, Senior and Uhlig, Steve and Kreutz, D and Ramos, F},
  abstract = {The Internet has led to the creation of a digital society, where (almost) everything is connected and is accessible from anywhere. However, despite their widespread adoption, traditional IP networks are complex and very hard to manage. It is both difficult to configure the network according to pre-defined policies, and to reconfigure it to respond to faults, load and changes. To make matters even more difficult, current networks are also vertically integrated: the control and data planes are bundled together. Software-Defined Networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network's control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound APIs, network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms-with a focus on aspects such as resiliency, scalability, performance, security and dependability-as well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined environment.},
  file = {D\:\\GDrive\\zotero\\Kreutz et al\\kreutz_et_al_software-defined_networking.pdf},
  keywords = {carrier-grade net-works,dependability,flow-based networking,Index Terms-Software-defined networking,net-work virtualization,network hypervisor,network operating systems,OpenFlow,programmable networks,programming languages,scalability,software-defined environments}
}

@techreport{kripkeCompletenessTheoremModal1959,
  title = {A {{Completeness Theorem}} in {{Modal Logic}}},
  author = {Kripke, Saul A},
  year = {1959},
  volume = {24},
  pages = {1--14},
  file = {D\:\\GDrive\\zotero\\Kripke\\kripke_1959_a_completeness_theorem_in_modal_logic.pdf},
  journal = {Source: The Journal of Symbolic Logic},
  number = {1}
}

@techreport{krishnamurthiProgrammingLanguagesApplication2012,
  title = {Programming {{Languages}}: {{Application}} and {{Interpretation Version Second Edition}}},
  author = {Krishnamurthi, Shriram},
  year = {2012},
  file = {D\:\\GDrive\\zotero\\Krishnamurthi\\krishnamurthi_2012_programming_languages.pdf}
}

@techreport{krizhevskyImageNetClassificationDeep,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {D\:\\GDrive\\zotero\\Krizhevsky\\krizhevsky_imagenet_classification_with_deep_convolutional_neural_networks.pdf}
}

@techreport{kumarExploitingUnderlyingStructure,
  title = {Exploiting {{Underlying Structure}} for {{Detailed Reconstruction}} of an {{Internet}}-Scale {{Event}}},
  author = {Kumar, Abhishek and Paxson, Vern and Weaver, Nicholas},
  abstract = {Network "telescopes" that record packets sent to unused blocks of Internet address space have emerged as an important tool for observing Internet-scale events such as the spread of worms and the backscatter from flooding attacks that use spoofed source addresses. Current telescope analyses produce detailed tabulations of packet rates, victim population, and evolution over time. While such cataloging is a crucial first step in studying the telescope observations , incorporating an understanding of the underlying processes generating the observations allows us to construct detailed inferences about the broader "universe" in which the Internet-scale activity occurs, greatly enriching and deepening the analysis in the process. In this work we apply such an analysis to the propagation of the Witty worm, a malicious and well-engineered worm that when released in March 2004 infected more than 12,000 hosts worldwide in 75 minutes. We show that by carefully exploiting the structure of the worm, especially its pseudo-random number generation , from limited and imperfect telescope data we can with high fidelity: extract the individual rate at which each infectee injected packets into the network prior to loss; correct distortions in the telescope data due to the worm's volume overwhelming the monitor; reveal the worm's inability to fully reach all of its potential victims; determine the number of disks attached to each infected machine; compute when each infectee was last booted, to sub-second accuracy; explore the "who infected whom" infection tree; uncover that the worm specifically targeted hosts at a US military base; and pinpoint Patient Zero, the initial point of infection, i.e., the IP address of the system the attacker used to unleash Witty.},
  file = {D\:\\GDrive\\zotero\\Kumar\\kumar_exploiting_underlying_structure_for_detailed_reconstruction_of_an.pdf}
}

@book{kumarProfileDrivenStatisticalAnalysis,
  title = {A {{Profile}}-{{Driven Statistical Analysis Framework}} for the {{Design Optimization}} of {{Soft Real}}-{{Time Applications}}},
  author = {Kumar, Tushar and Sreeram, Jaswanth and Cledat, Romain and Pande, Santosh},
  abstract = {Soft real-time applications lack a formal methodology for their design optimization. Well-established techniques from hard real-time systems cannot be directly applied to soft real-time applications, without losing key benefits of the soft real-time paradigm. We introduce a statistical analysis framework that is well-suited for discovering opportunities for optimization of soft real-time applications. We demonstrate how programmers can use the analysis provided by our framework to perform aggressive soft real-time design optimizations on their applications. The paper introduces the Context Execution Tree (CET) representation for capturing the statistical properties of function calls in the context of their execution in the program. The CET is constructed from an offline-profile of the application. Statistical measures are coupled with techniques that extract runtime distinguishable call-chains. This combination of techniques is applied to the CET to find statistically significant patterns of activity that i) expose slack in the execution of the application with respect to its soft real-time requirements, and ii) can be predicted with low overhead and high reliability during the normal execution of the application.},
  file = {D\:\\GDrive\\zotero\\Kumar\\kumar_a_profile-driven_statistical_analysis_framework_for_the_design_optimization_of.pdf},
  isbn = {978-1-59593-812-1},
  keywords = {Behavior Prediction,D22 [Software Engineering]: Design Tools and Tech-niques General Terms Design,Measurement Keywords Statistical Analysis,Pattern Detection,Performance,Profiling,Signatures,Soft Real-time}
}

@article{kumarVectorizationObfuscationP42018,
  title = {Vectorization, {{Obfuscation}} and {{P4 LLVM Tool}}-Chain},
  author = {Kumar, D T and Upadrasta, R},
  year = {2018},
  abstract = {This thesis broadly focuses on three different areas: Loop Vectorization, Code Obfuscation, and P4LLVM compiler. The work in Loop vectorization starts with a comparison of Auto-vectorization of GCC, ICC and LLVM compilers and show their strengths and weakness. As \ldots},
  file = {D\:\\GDrive\\zotero\\Kumar\\kumar_2018_vectorization,_obfuscation_and_p4_llvm_tool-chain.pdf},
  number = {July}
}

@techreport{kuzmanovicLowRateTCPTargetedDenial2003,
  title = {Low-{{Rate TCP}}-{{Targeted Denial}} of {{Service Attacks}} ({{The Shrew}} vs. the {{Mice}} and {{Elephants}}) \textcent{} \textexclamdown},
  author = {Kuzmanovic, Aleksandar and Knightly, Edward W},
  year = {2003},
  abstract = {Denial of Service attacks are presenting an increasing threat to the global inter-networking infrastructure. While TCP's congestion control algorithm is highly robust to diverse network conditions, its implicit assumption of end-system cooperation results in a well-known vulnerability to attack by high-rate non-responsive flows. In this paper, we investigate a class of low-rate denial of service attacks which, unlike high-rate attacks, are difficult for routers and counter-DoS mechanisms to detect. Using a combination of analytical modeling, simulations, and Internet experiments, we show that maliciously chosen low-rate DoS traffic patterns that exploit TCP's retransmission time-out mechanism can throttle TCP flows to a small fraction of their ideal rate while eluding detection. Moreover , as such attacks exploit protocol homogeneity, we study fundamental limits of the ability of a class of randomized time-out mechanisms to thwart such low-rate DoS attacks. A shrew is a small but aggressive mammal that ferociously attacks and kills much larger animals with a venomous bite.},
  file = {D\:\\GDrive\\zotero\\Kuzmanovic\\kuzmanovic_2003_low-rate_tcp-targeted_denial_of_service_attacks_(the_shrew_vs.pdf},
  keywords = {C20 [Security and Protection]: Denial of Service; C22 [Computer-Communication Networks]: Network Proto-cols General Terms Algorithms,Performance,retransmission timeout,Security Keywords Denial of Service,TCP}
}

@article{kuznetsovCodePointerIntegrity2018,
  title = {Code-{{Pointer Integrity}}},
  author = {Kuznetsov, Volodymyr and Szekeres, L{\'a}szl{\'o} and Payer, Mathias and Candea, George and Sekar, R and Song, Dawn},
  year = {2018},
  abstract = {Systems code is often written in low-level languages like C/C++, which offer many benefits but also delegate memory management to programmers. This invites memory safety bugs that attackers can exploit to divert control flow and compromise the system. Deployed defense mechanisms (e.g., ASLR, DEP) are incomplete, and stronger defense mechanisms (e.g., CFI) often have high overhead and limited guarantees [19, 15, 9]. We introduce code-pointer integrity (CPI), a new design point that guarantees the integrity of all code pointers in a program (e.g., function pointers, saved return addresses) and thereby prevents all control-flow hijack attacks , including return-oriented programming. We also introduce code-pointer separation (CPS), a relaxation of CPI with better performance properties. CPI and CPS offer substantially better security-to-overhead ratios than the state of the art, they are practical (we protect a complete FreeBSD system and over 100 packages like apache and postgresql), effective (prevent all attacks in the RIPE benchmark), and efficient: on SPEC CPU2006, CPS averages 1.2\% overhead for C and 1.9\% for C/C++, while CPI's overhead is 2.9\% for C and 8.4\% for C/C++. A prototype implementation of CPI and CPS can be obtained from http://levee.epfl.ch.},
  file = {D\:\\GDrive\\zotero\\Kuznetsov et al\\kuznetsov_et_al_code-pointer_integrity.pdf}
}

@article{kvarnstromStaticCodeAnalysis2016,
  title = {Static {{Code Analysis}} of {{C}} ++ in {{LLVM}}},
  author = {Kvarnstr{\"o}m, Olle},
  year = {2016},
  file = {D\:\\GDrive\\zotero\\Kvarnström\\kvarnström_2016_static_code_analysis_of_c_++_in_llvm.pdf}
}

@techreport{kwonTendermintConsensusMining,
  title = {Tendermint: {{Consensus}} without {{Mining}}},
  author = {Kwon, Jae},
  abstract = {Cryptocurrencies such as Bitcoin enable users to submit payment transactions without going through a centralized trusted organization. Bitcoin relies on proof-of-work mining to secure consensus which is problematic; mining requires a massive expenditure of energy, confirmation of transactions is slow, and security is difficult to quantify. We propose a solution to the blockchain consensus problem that does not require mining by adapting an existing solution to the Byzantine Generals Problem.},
  file = {D\:\\GDrive\\zotero\\Kwon\\kwon_tendermint.pdf}
}

@techreport{labovitzInternetRoutingInstability,
  title = {Internet {{Routing Instability}}},
  author = {Labovitz, Craig and Malan, G Robert and Jahanian, Farnam},
  abstract = {This paper examines the network inter-domain routing information exchanged between backbone service providers at the major U.S. public Internet exchange points. Internet routing instability, or the rapid uctuation of network reach-ability information, is an important problem currently facing the Internet engineering community. High levels of network instability can lead to packet loss, increased network latency and time to convergence. At the extreme, high levels of routing instability h a v e lead to the loss of internal connectivity in wide-area, national networks. In this paper, we describe several unexpected trends in routing instability, and examine a number of anomalies and pathologies observed in the exchange of inter-domain routing information. The analysis in this paper is based on data collected from BGP routing messages generated by border routers at ve of the Internet core's public exchange points during a nine month period. We show that the volume of these routing updates is several orders of magnitude more than expected and that the majority of this routing information is redundant, or pathological. Furthermore, our analysis reveals several unexpected trends and ill-behaved systematic properties in Internet routing. We nally posit a number of explanations for these anomalies and evaluate their potential impact on the Internet infrastructure.},
  file = {D\:\\GDrive\\zotero\\Labovitz\\labovitz_1998_internet_routing_instability.pdf;D\:\\GDrive\\zotero\\Labovitz\\labovitz_internet_routing_instability.pdf},
  keywords = {communication system routing,computer network,Index Terms-Communication system,Internet,routing,stability}
}

@techreport{labrecqueCompilationInfrastructureNetwork,
  title = {Towards a {{Compilation Infrastructure}} for {{Network Processors}}},
  author = {Labrecque, Martin},
  file = {D\:\\GDrive\\zotero\\Labrecque\\labrecque_towards_a_compilation_infrastructure_for_network_processors.pdf}
}

@article{labrecqueCompilationInfrastructureNetwork2006,
  title = {Towards a {{Compilation Infrastructure}} for {{Network Processors}} By},
  author = {Labrecque, Martin},
  year = {2006},
  file = {D\:\\GDrive\\zotero\\Labrecque\\labrecque_2006_towards_a_compilation_infrastructure_for_network_processors_by.pdf},
  journal = {Computer Engineering}
}

@techreport{labrecqueScalingTaskGraphs,
  title = {Scaling {{Task Graphs}} for {{Network Processors}}},
  author = {Labrecque, Martin and Steffan, J Gregory},
  abstract = {Modern network processors (NPs) are highly multithreaded chip multiprocessors (CMPs), supporting a wide variety of mechanisms for on-chip storage and inter-task communication. Real network processor applications are hard to program and must be tailored to fit the resources of the underlying NP, motivating an automated approach to mapping multithreaded applications to NPs. In this paper we propose and evaluate compiler-based automated task and data management techniques to scale the throughput of network processing task graphs onto NPs. We evaluate these techniques using a NP simulation infrastructure based on realistic NP applications, and present an approach to discovering performance bottlenecks. Finally we demonstrate how our techniques enhance throughput-scaling for NPs.},
  file = {D\:\\GDrive\\zotero\\Labrecque\\labrecque_scaling_task_graphs_for_network_processors.pdf},
  keywords = {Compiler techniques,Concurrency,Feedback-directed optimization,Methodology,Multiprocessors,Multithreading,Network Processors,Simulation}
}

@techreport{lakshmanCassandraADecentralizedStructured,
  title = {Cassandra-{{A Decentralized Structured Storage System}}},
  author = {Lakshman, Avinash and Prashant, Facebook and Facebook, Malik},
  doi = {10.1145/1773912.1773922},
  abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full rela-tional data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write through-put while not sacrificing read efficiency.},
  file = {D\:\\GDrive\\zotero\\Lakshman\\lakshman_cassandra-a_decentralized_structured_storage_system.pdf},
  keywords = {distributed systems}
}

@article{lamNumbaLLVMbasedPython,
  title = {Numba: {{A LLVM}}-Based {{Python JIT Compiler}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  doi = {10.1145/2833157.2833162},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition , we share our experience in building a JIT compiler using LLVM[1].},
  file = {D\:\\GDrive\\zotero\\Lam et al\\lam_et_al_numba.pdf},
  isbn = {9781450340052},
  keywords = {Compiler,D34 [Programming Languages]: Processors-Compil-ers,Optimization General Terms Languages,Performance Keywords LLVM,Python}
}

@techreport{lamportBuridanPrinciple1984,
  title = {Buridan's {{Principle}}},
  author = {Lamport, Leslie},
  year = {1984},
  file = {D\:\\GDrive\\zotero\\Lamport\\lamport_1984_buridan's_principle.pdf}
}

@techreport{lamportByzantineGeneralsProblem1982,
  title = {The {{Byzantine Generals Problem}}},
  author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
  year = {1982},
  abstract = {Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one or more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.},
  file = {D\:\\GDrive\\zotero\\Lamport\\lamport_1982_the_byzantine_generals_problem.pdf;D\:\\GDrive\\zotero\\Lamport\\lamport_1982_the_byzantine_generals_problem2.pdf},
  keywords = {C24 [Computer-Communication Networks]: Distributed Systems-network operating systems,D44 [Operating Systems]: Communications Management-network communication,D45 [Operating Systems]: Reliability-fault tolerance General Terms: Algorithms; Reliability Additional Key Words and Phrases: Interactive consistency /}
}

@techreport{lamportOperatingStocktonGaines1978,
  title = {Operating {{R}}. {{Stockton Gaines Systems Editor Time}}, {{Clocks}}, and the {{Ordering}} of {{Events}} in a {{Distributed System}}},
  author = {Lamport, Leslie},
  year = {1978},
  abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
  file = {D\:\\GDrive\\zotero\\Lamport\\lamport_1978_operating_r.pdf},
  keywords = {529,and Phrases: distributed systems,clock synchronization,computer networks,multiprocess systems CR Categories: 432}
}

@techreport{lamportPartTimeParliament1998,
  title = {The {{Part}}-{{Time Parliament}}},
  author = {Lamport, Leslie},
  year = {1998},
  volume = {16},
  pages = {133--169},
  file = {D\:\\GDrive\\zotero\\Lamport\\lamport_1998_the_part-time_parliament.pdf;D\:\\GDrive\\zotero\\Lamport\\lamport_1998_the_part-time_parliament2.pdf},
  journal = {ACM Transactions on Computer Sys-tems},
  number = {2}
}

@techreport{lamportPaxosMadeSimple2001,
  title = {Paxos {{Made Simple}}},
  author = {Lamport, Leslie},
  year = {2001},
  abstract = {The Paxos algorithm, when presented in plain English, is very simple.},
  file = {D\:\\GDrive\\zotero\\Lamport\\lamport_2001_paxos_made_simple.pdf}
}

@techreport{lamportTimeClocksOrdering1978,
  title = {Time, {{Clocks}}, and the {{Ordering}} of {{Events}} in a {{Distributed System}}},
  author = {Lamport, Leslie},
  year = {1978},
  abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
  file = {D\:\\GDrive\\zotero\\Lamport\\lamport_1978_time,_clocks,_and_the_ordering_of_events_in_a_distributed_system.pdf},
  keywords = {529,and Phrases: distributed systems,clock synchronization,computer networks,multiprocess systems CR Categories: 432}
}

@article{lampropoulosRandomTestingLanguage2018,
  title = {Random {{Testing}} for {{Language Design}}},
  author = {Lampropoulos, Leonidas},
  year = {2018},
  pages = {268},
  abstract = {Property-based random testing can facilitate formal verification, exposing errors early on in the proving process and guiding users towards correct specifications and implementations. However, effective random testing often requires users to write custom generators for well-distributed random data satisfying complex logical predicates, a task which can be tedious and error prone.    In this work, I aim to reduce the cost of property-based testing by making such generators easier to write, read and maintain. I present a domain-specific language, called Luck, in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated.    I also aim to increase the applicability of testing to formal verification by bringing advanced random testing techniques to the Coq proof assistant. I describe QuickChick, a QuickCheck clone for Coq, and improve it by incorporating ideas explored in the context of Luck to automatically derive provably correct generators for data constrained by inductive relations.    Finally, I evaluate both QuickChick and Luck in a variety of complex case studies from programming languages literature, such as information-flow abstract machines and type systems for lambda calculi.},
  file = {D\:\\GDrive\\zotero\\Lampropoulos\\lampropoulos_2018_random_testing_for_language_design.pdf},
  isbn = {9780438424401},
  journal = {ProQuest Dissertations and Theses},
  keywords = {0723:Information science,0984:Computer science,Applied sciences,Communication and the arts,Computer science,Coq,Information science,Property based testing,Quickcheck,Quickchick}
}

@techreport{lampsonExperienceProcessesMonitors,
  title = {Experience with {{Processes}} and {{Monitors}} in {{Mesa Experience}} with {{Processes}} and {{Monitors}} in {{Mesa}} 1},
  author = {Lampson, Butler W and Redell, David D},
  abstract = {The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.},
  file = {D\:\\GDrive\\zotero\\Lampson\\lampson_experience_with_processes_and_monitors_in_mesa_experience_with_processes_and.pdf},
  keywords = {and Phrases: concurrency,condition variable,deadlock,module,monitor,operating system,process,synchronization,task}
}

@techreport{lampsonHintsComputerSystem1983,
  title = {Hints for {{Computer System Design}}},
  author = {Lampson, Butler W},
  year = {1983},
  abstract = {Studying the design and implementation of a number of computer has led to some general hints for system design. They are described here and illustrated by many examples, ranging from hardware such as the Alto and the Dorado to application programs such as Bravo and Star.},
  file = {D\:\\GDrive\\zotero\\Lampson\\lampson_1983_hints_for_computer_system_design.pdf}
}

@article{landauNewProofEquation2010,
  title = {New {{Proof}} of the {{Equation}} \$\textbackslash sum\_\{k=1\}\^\textbackslash infty \textbackslash frac\{\textbackslash mu(k)\}\{k\}=0.\$},
  author = {Landau, Edmund},
  year = {2010},
  month = jan,
  abstract = {This is an English translation of Edmund Landau's Doctoral Dissertation.},
  archiveprefix = {arXiv},
  eprint = {0803.3787},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Landau\\landau_2010_new_proof_of_the_equation_$-sum_ k=1 ^-infty_-frac -mu(k) k =0.pdf;C\:\\Users\\Admin\\Zotero\\storage\\WIGIT7AT\\0803.html},
  journal = {arXiv:0803.3787 [math]},
  keywords = {01A75,Mathematics - History and Overview},
  primaryclass = {math}
}

@techreport{landiUndecidabilityStaticAnalysis1992,
  title = {Undecidability of Static Analysis},
  author = {Landi, William},
  year = {1992},
  file = {D\:\\GDrive\\zotero\\Landi\\landi_1992_undecidability_of_static_analysis.pdf}
}

@article{langleyQUICTransportProtocol2017,
  title = {The {{QUIC Transport Protocol}}: {{Design}} and {{Internet}}-{{Scale Deployment}}},
  author = {Langley, Adam and Riddoch, Alistair and Wilk, Alyssa and Vicente, Antonio and Krasic, Charles and Zhang, Dan and Yang, Fan and Kouranov, Fedor and Swett, Ian and Iyengar, Janardhan and Bailey, Jeff and Dorfman, Jeremy and Roskind, Jim and Kulik, Joanna and Westin, Patrik and Tenneti, Raman and Shade, Robbie and Hamilton, Ryan and Vasiliev, Victor and Chang, Wan-Teh and Shi, Zhongyi},
  year = {2017},
  volume = {14},
  doi = {10.1145/3098822.3098842},
  abstract = {We present our experience with QUIC, an encrypted, multiplexed, and low-latency transport protocol designed from the ground up to improve transport performance for HTTPS traffic and to enable rapid deployment and continued evolution of transport mechanisms. QUIC has been globally deployed at Google on thousands of servers and is used to serve traffic to a range of clients including a widely-used web browser (Chrome) and a popular mobile video streaming app (YouTube). We estimate that 7\% of Internet traffic is now QUIC. We describe our motivations for developing a new transport, the principles that guided our design, the Internet-scale process that we used to perform iterative experiments on QUIC, performance improvements seen by our various services, and our experience deploying QUIC globally. We also share lessons about transport design and the Internet ecosystem that we learned from our deployment.},
  file = {D\:\\GDrive\\zotero\\Langley\\langley_2017_the_quic_transport_protocol.pdf},
  isbn = {9781450346535}
}

@article{laraNetworkInnovationUsing2014,
  title = {Network Innovation Using Open Flow: {{A}} Survey},
  author = {Lara, Adrian and Kolasani, Anisha and Ramamurthy, Byrav},
  year = {2014},
  volume = {16},
  pages = {493--512},
  publisher = {{IEEE}},
  issn = {1553877X},
  doi = {10.1109/SURV.2013.081313.00105},
  abstract = {OpenFlow is currently the most commonly deployed Software Defined Networking (SDN) technology. SDN consists of decoupling the control and data planes of a network. A software-based controller is responsible for managing the forwarding information of one or more switches; the hardware only handles the forwarding of traffic according to the rules set by the controller. OpenFlow is an SDN technology proposed to standardize the way that a controller communicates with network devices in an SDN architecture. It was proposed to enable researchers to test new ideas in a production environment. OpenFlow provides a specification to migrate the control logic from a switch into the controller. It also defines a protocol for the communication between the controller and the switches. As discussed in this survey paper, OpenFlow-based architectures have specific capabilities that can be exploited by researchers to experiment with new ideas and test novel applications. These capabilities include software-based traffic analysis, centralized control, dynamic updating of forwarding rules and flow abstraction. OpenFlow-based applications have been proposed to ease the configuration of a network, to simplify network management and to add security features, to virtualize networks and data centers and to deploy mobile systems. These applications run on top of networking operating systems such as Nox, Beacon, Maestro, Floodlight, Trema or Node.Flow. Larger scale OpenFlow infrastructures have been deployed to allow the research community to run experiments and test their applications in more realistic scenarios. Also, studies have measured the performance of OpenFlow networks through modelling and experimentation. We describe the challenges facing the large scale deployment of OpenFlow-based networks and we discuss future research directions of this technology. \textcopyright{} 2014 IEEE.},
  file = {D\:\\GDrive\\zotero\\Lara\\lara_2014_network_innovation_using_open_flow.pdf},
  journal = {IEEE Communications Surveys and Tutorials},
  keywords = {Applications,Capabilities,Deployments,Networking Challenges,OpenFlow,Software Defined Networking},
  number = {1}
}

@article{larsenSoKAutomatedSoftware2014,
  title = {{{SoK}}: {{Automated}} Software Diversity},
  author = {Larsen, Per and Homescu, Andrei and Brunthaler, Stefan and Franz, Michael},
  year = {2014},
  pages = {276--291},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2014.25},
  abstract = {The idea of automatic software diversity is at least two decades old. The deficiencies of currently deployed defenses and the transition to online software distribution (the "App store" model) for traditional and mobile computers has revived the interest in automatic software diversity. Consequently, the literature on diversity grew by more than two dozen papers since 2008. Diversity offers several unique properties. Unlike other defenses, it introduces uncertainty in the target. Precise knowledge of the target software provides the underpinning for a wide range of attacks. This makes diversity a broad rather than narrowly focused defense mechanism. Second, diversity offers probabilistic protection similar to cryptography-attacks may succeed by chance so implementations must offer high entropy. Finally, the design space of diversifying program transformations is large. As a result, researchers have proposed multiple approaches to software diversity that vary with respect to threat models, security, performance, and practicality. In this paper, we systematically study the state-of-the-art in software diversity and highlight fundamental trade-offs between fully automated approaches. We also point to open areas and unresolved challenges. These include "hybrid solutions", error reporting, patching, and implementation disclosure attacks on diversified software.},
  file = {D\:\\GDrive\\zotero\\Larsen\\larsen_2014_sok.pdf},
  isbn = {9781479946860},
  journal = {Proceedings - IEEE Symposium on Security and Privacy}
}

@techreport{LATEX2eCheat,
  title = {{{LATEX}} 2{$\epsilon$} {{Cheat Sheet Document}} Classes},
  file = {D\:\\GDrive\\zotero\\undefined\\latex_2ε_cheat_sheet_document_classes.pdf}
}

@techreport{LATEXMathUndergrads,
  title = {{{LATEX Math}} for {{Undergrads}}},
  file = {D\:\\GDrive\\zotero\\undefined\\latex_math_for_undergrads.pdf}
}

@techreport{lattnerArchitectureNextGenerationGCC,
  title = {Architecture for a {{Next}}-{{Generation GCC}}},
  author = {Lattner, Chris and Adve, Vikram},
  abstract = {This paper presents a design and implementation of a whole-program interprocedural optimizer built in the GCC framework. Through the introduction of a new language-independent intermediate representation, we extend the current GCC architecture to include a powerful mid-level optimizer and add link-time interprocedural analysis and optimization capabilities. This intermediate representation is an SSA-based, low-level, strongly-typed, representation which is designed to support both efficient global optimizations and high-level analyses. Because most of the program is available at link-time, aggressive "whole-program" optimizations and analyses are possible, improving the time and space requirements of compiled programs. The final proposed organization of GCC retains the important features which make it successful today, requires almost no modification to either the front-or back-ends of GCC, and is completely compatible with user makefiles.},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_architecture_for_a_next-generation_gcc.pdf}
}

@techreport{lattnerAutomaticPoolAllocation,
  title = {Automatic {{Pool Allocation}}: {{Improving Performance}} by {{Controlling Data Structure Layout}} in the {{Heap}}},
  author = {Lattner, Chris and Adve, Vikram},
  abstract = {This paper describes Automatic Pool Allocation, a transformation framework that segregates distinct instances of heap-based data structures into seperate memory pools and allows heuristics to be used to partially control the internal layout of those data structures. The primary goal of this work is performance improvement, not automatic memory management, and the paper makes several new contributions. The key contribution is a new compiler algorithm for partitioning heap objects in imperative programs based on a context-sensitive pointer analysis, including a novel strategy for correct handling of indirect (and potentially unsafe) function calls. The transformation does not require type safe programs and works for the full generality of C and C++. Second, the paper describes several optimizations that exploit data structure partitioning to further improve program performance. Third, the paper evaluates how memory hierarchy behavior and overall program performance are impacted by the new transformations. Using a number of benchmarks and a few applications, we find that compilation times are extremely low, and overall running times for heap intensive programs speed up by 10-25\% in many cases, about 2x in two cases, and more than 10x in two small benchmarks. Overall, we believe this work provides a new framework for optimizing pointer intensive programs by segregating and controlling the layout of heap-based data structures.},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_automatic_pool_allocation.pdf},
  keywords = {cache,D34 [Processors]: Compil-ers,data layout,Memory management General Terms Algorithms,Optimization,Performance Keywords Recursive data structure,pool allocation,static analysis}
}

@techreport{lattnerAutomaticPoolAllocation2002,
  title = {Automatic {{Pool Allocation}} for {{Disjoint Data Structures}}},
  author = {Lattner, Chris and Adve, Vikram},
  year = {2002},
  abstract = {This paper presents an analysis technique and a novel program transformation that can enable powerful optimizations for entire linked data structures. The fully automatic transformation converts ordinary programs to use pool (aka region) allocation for heap-based data structures. The transformation relies on an efficient link-time interprocedural analysis to identify disjoint data structures in the program, to check whether these data structures are accessed in a type-safe manner, and to construct a Disjoint Data Structure Graph that describes the connectivity pattern within such structures. We present preliminary experimental results showing that the data structure analysis and pool allocation are effective for a set of pointer intensive programs in the Olden benchmark suite. To illustrate the optimizations that can be enabled by these techniques, we describe a novel pointer compression transformation and briefly discuss several other optimization possibilities for linked data structures.},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_automatic_pool_allocation_for_disjoint_data_structures_£.pdf}
}

@techreport{lattnerAutomaticPoolAllocation2005,
  title = {Automatic {{Pool Allocation}}: {{Automatic Pool Allocation}}: {{Improving Performance}} by {{Controlling Improving Performance}} by {{Controlling Data Structure Layout}} in the {{Heap Data Structure Layout}} in the {{Heap Vikram Vikram Adve Adve}}},
  author = {Lattner, Chris},
  year = {2005},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2005_automatic_pool_allocation.pdf}
}

@techreport{lattnerDataStructureAnalysis,
  title = {Data {{Structure Analysis}}: {{A Fast}} and {{Scalable Context}}-{{Sensitive Heap Analysis}}},
  author = {Lattner, Chris and Adve, Vikram},
  abstract = {This paper describes a scalable heap analysis algorithm, Data Structure Analysis, designed to enable analyses and transformations of programs at the level of entire logical data structures. Data Structure Analysis attempts to identify dis-joint instances of logical program data structures and their internal and external connectivity properties (without trying to categorize their "shape"). To achieve this, Data Structure Analysis is fully context-sensitive (in the sense that it names memory objects by entire acyclic call paths), is field-sensitive, builds an explicit model of the heap, and is robust enough to handle the full generality of C. Despite these aggressive features, the algorithm is both extremely fast (requiring 2-7 seconds for C programs in the range of 100K lines of code) and is scalable in practice. It has three features we believe are novel: (a) it incrementally builds a precise program call graph during the analysis; (b) it distinguishes complete and incomplete information in a manner that simplifies analysis of libraries or other portions of programs; and (c) it uses speculative field-senstivity in type-unsafe programs in order to preserve efficiency and scalabil-ity. Finally, it shows that the key to achieving scalability in a fully context-sensitive algorithm is the use of a unification-based approach, a combination that has been used before but whose importance has not been clearly articulated.},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_data_structure_analysis.pdf}
}

@phdthesis{lattnerIMPLEMENTATIONSWINGMODULO2005,
  title = {{{AN IMPLEMENTATION OF SWING MODULO SCHEDULING WITH EXTENSIONS FOR SUPERBLOCKS}}},
  author = {Lattner, Tanya M},
  year = {2005},
  abstract = {This thesis details the implementation of Swing Modulo Scheduling, a Software Pipelining technique , that is both effective and efficient in terms of compile time and generated code. Software Pipelining aims to expose Instruction Level Parallelism in loops which tend to help scientific and graphical applications. Modulo Scheduling is a category of algorithms that attempt to overlap iterations of single basic block loops and schedule instructions based upon a priority (derived from a set of heuristics). The approach used by Swing Modulo Scheduling is designed to achieve a highly optimized schedule, keeping register pressure low, and does both in a reasonable amount of compile time. One drawback of Swing Modulo Scheduling, (and all Modulo Scheduling algorithms) is that they are missing opportunities for further Instruction Level Parallelism by only handling single basic block loops. This thesis details extensions to the Swing Modulo Scheduling algorithm to handle multiple basic block loops in the form of a superblock. A superblock is group of basic blocks that have a single entry and multiple exits. Extending Swing Modulo Scheduling to support these types of loops increases the number of loops Swing Modulo Scheduling can be applied to. In addition, it allows Modulo Scheduling to be performed on hot paths (also single entry, multiple exit), found with profile information to be optimized later offline or at runtime. Our implementation of Swing Modulo Scheduling and extensions to the algorithm for superblock loops were evaluated and found to be both effective and efficient. For the original algorithm, benchmarks were transformed to have performance gains of 10-33\%, while the extended algorithm increased benchmark performance from 7-22\%. iii},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2005_an_implementation_of_swing_modulo_scheduling_with_extensions_for_superblocks.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_2005_an_implementation_of_swing_modulo_scheduling_with_extensions_for_superblocks2.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_an_implementation_of_swing_modulo_scheduling_with_extensions_for_superblocks.pdf},
  keywords = {llvm,modulo scheduling,superblocks}
}

@techreport{lattnerIntroductionLLVMCompiler2006,
  title = {Introduction to the {{LLVM Compiler Infrastructure}}},
  author = {Lattner, Chris and Computer, Apple},
  year = {2006},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2006_introduction_to_the_llvm_compiler_infrastructure.pdf}
}

@misc{lattnerIntroductionLLVMCompiler2008,
  title = {Introduction to the {{LLVM Compiler System}}},
  author = {Lattner, Chris},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2008_introduction_to_the_llvm_compiler_system.pdf}
}

@techreport{lattnerLLVMClangNext2008,
  title = {{{LLVM}} and {{Clang}}: {{Next Generation Compiler Technology LLVM}}: {{Low Level Virtual Machine}}},
  author = {Lattner, Chris},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2008_llvm_and_clang.pdf}
}

@inproceedings{lattnerLLVMCompilationFramework2004a,
  title = {{{LLVM}}: {{A}} Compilation Framework for Lifelong Program Analysis \& Transformation},
  shorttitle = {{{LLVM}}},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2004. {{CGO}} 2004.},
  author = {Lattner, C. and Adve, V.},
  year = {2004},
  pages = {75--86},
  publisher = {{IEEE}},
  address = {{San Jose, CA, USA}},
  doi = {10.1109/CGO.2004.1281665},
  abstract = {This paper describes LLVM (Low Level Virtual Machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in Static Single Assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
  file = {D\:\\GDrive\\zotero\\Lattner_Adve\\lattner_adve_2004_llvm.pdf},
  isbn = {978-0-7695-2102-2},
  language = {en}
}

@phdthesis{lattnerLLVMINFRASTRUCTUREMULTISTAGE2002,
  title = {{{LLVM}}: {{AN INFRASTRUCTURE FOR MULTI}}-{{STAGE OPTIMIZATION}}},
  author = {Lattner, Chris Arthur},
  year = {2002},
  abstract = {Modern programming languages and software engineering principles are causing increasing problems for compiler systems. Traditional approaches, which use a simple compile-link-execute model, are unable to provide adequate application performance under the demands of the new conditions. Traditional approaches to interprocedural and profile-driven compilation can provide the application performance needed, but require infeasible amounts of compilation time to build the application. This thesis presents LLVM, a design and implementation of a compiler infrastructure which supports a unique multi-stage optimization system. This system is designed to support extensive interprocedural and profile-driven optimizations, while being efficient enough for use in commercial compiler systems. The LLVM virtual instruction set is the glue that holds the system together. It is a low-level representation, but with high-level type information. This provides the benefits of a low-level representation (compact representation, wide variety of available transformations, etc.) as well as providing high-level information to support aggressive interprocedural optimizations at link-and post-link time. In particular, this system is designed to support optimization in the field, both at run-time and during otherwise unused idle time on the machine. This thesis also describes an implementation of this compiler design, the LLVM compiler infrastructure , proving that the design is feasible. The LLVM compiler infrastructure is a maturing and efficient system, which we show is a good host for a variety of research. More information about LLVM can be found on its web site at:},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2002_llvm.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_llvm2.pdf},
  keywords = {LLVM Introduction}
}

@techreport{lattnerLLVMInstructionSet2002,
  title = {The {{LLVM Instruction Set}} and {{Compilation Strategy}}},
  author = {Lattner, Chris and Adve, Vikram},
  year = {2002},
  abstract = {This document introduces the LLVM compiler infrastructure and instruction set, a simple approach that enables  sophisticated code transformations at link time, runtime, and in the field. It is a pragmatic approach to compilation,  interfering with programmers and tools as little as possible, while still retaining extensive high-level information from  source-level compilers for later stages of an application's lifetime. We describe the LLVM instruction set, the design  of the LLVM system, and some of its key components.},
  file = {D\:\\GDrive\\zotero\\Lattner_Adve\\lattner_adve_2002_the_llvm_instruction_set_and_compilation_strategy.pdf;C\:\\Users\\Admin\\Zotero\\storage\\24QC8GGD\\summary.html}
}

@techreport{lattnerMacroscopicDataStructure,
  title = {Macroscopic {{Data Structure Analysis}} \& {{Optimization DSA Algorithm Highlights Extremely}} Fast Compiler Transform: 1.3s for {{100K}} Loc {{Optimize}} Based on Pool Properties {{Pool Allocation Performance Effect Pool Allocation}} \& Optzns Improve {{RDS}} Performance},
  author = {Lattner, Chris and Adve, Vikram},
  abstract = {Basic algorithm design: \textbullet{} Context-sensitive, unification-based, flow-insensitive algorithm \textbullet{} Provides speculative type information and field-sensitivity \textbullet{} Computes which memory is passed into/out of the analysis region Bottom-Up phase computes Fn behavior with all callees \textbullet{} Computes "total effect" of calling the function \textbullet{} Incrementally constructs program call graph \textbullet{} BU results are used by Pool Allocation \& Pointer Compression Top-Down phase adds information from callees \textbullet{} BU computes no information about callers of a function \textbullet{} TD pass is useful for alias analysis clients See llvm-tv demo for more examples of graphs Automatic Pool Allocation [PLDI'05] Allocate memory from pool instead of the heap: \textbullet{} Partition distinct data structures in memory-Better for cache, locality, allocation speed, etc \textbullet{} Give compiler information about dynamic location of memory-Needed to perform memory layout optimizations at runtime \textbullet{} Give compiler control over layout of data structure-Can segregate or collocate nodes in the RDS-Can optimize away inter-object padding in many cases (below)},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_macroscopic_data_structure_analysis_&_optimization_dsa_algorithm_highlights.pdf}
}

@phdthesis{lattnerMACROSCOPICDATASTRUCTURE2000,
  title = {{{MACROSCOPIC DATA STRUCTURE ANALYSIS AND OPTIMIZATION}}},
  author = {Lattner, Chris},
  year = {2000},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2000_macroscopic_data_structure_analysis_and_optimization.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_2000_macroscopic_data_structure_analysis_and_optimization2.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_2000_macroscopic_data_structure_analysis_and_optimization3.pdf}
}

@article{lattnerMacroscopicDataStructure2005b,
  title = {Macroscopic {{Data Structure Analysis}} \& {{Optimization}}},
  author = {Lattner, Chris and Adve, Prof Vikram},
  year = {2005},
  pages = {64},
  abstract = {Providing high performance for pointer-intensive programs on modern architectures is an increasingly difficult problem for compilers. Pointer-intensive programs are often bound by memory latency and cache performance, but traditional approaches to these problems usually fail: Pointer-intensive programs are often highly-irregular and the compiler has little control over the layout of heap allocated objects. This thesis presents a new class of techniques named ``Macroscopic Data Structure Analyses and Optimizations'', which is a new approach to the problem of analyzing and optimizing pointer-intensive programs. Instead of analyzing individual load/store operations or structure definitions, this approach identifies, analyzes, and transforms entire memory structures as a unit. The foundation of the approach is an analysis named Data Structure Analysis and a transformation named Automatic Pool Allocation. Data Structure Analysis is a context-sensitive pointer analysis which identifies data structures on the heap and their important properties (such as type safety). Automatic Pool Allocation uses the results of Data Structure Analysis to segregate dynamically allocated objects on the heap, giving control over the layout of the data structure in memory to the compiler. Based on these two foundation techniques, this thesis describes several performance improving optimizations for pointer-intensive programs. First, Automatic Pool Allocation itself provides important locality improvements for the program. Once the program is pool allocated, several pool-specific optimizations can be performed to reduce inter-object padding and pool overhead. Second, we describe an aggressive technique, Automatic Pointer Compression, which reduces the size of pointers on 64-bit targets to 32-bits or less, increasing effective cache capacity and memory bandwidth for pointer-intensive programs. This thesis describes the approach, analysis, and transformation of programs with macroscopic techniques, and evaluates the net performance impact of the transformations. Finally, it describes a large class of potential applications for the work in fields such as heap safety and reliability, program understanding, distributed computing, and static garbage collection.},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_2005_macroscopic_data_structure_analysis_&_optimization.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_2005_macroscopic_data_structure_analysis_&_optimization2.pdf;D\:\\GDrive\\zotero\\Lattner\\lattner_2005_macroscopic_data_structure_analysis_&_optimization3.pdf},
  journal = {Llvm},
  number = {2005-05}
}

@book{lattnerMakingContextsensitivePointsto,
  title = {Making {{Context}}-Sensitive {{Points}}-to {{Analysis}} with {{Heap Cloning Practical For The Real World}} *},
  author = {Lattner, Chris and Lenharth, Andrew and Adve, Vikram},
  abstract = {Context-sensitive pointer analysis algorithms with full "heap cloning" are powerful but are widely considered to be too expensive to include in production compilers. This paper shows, for the first time, that a context-sensitive, field-sensitive algorithm with full heap cloning (by acyclic call paths) can indeed be both scal-able and extremely fast in practice. Overall, the algorithm is able to analyze programs in the range of 100K-200K lines of C code in 1-3 seconds, takes less than 5\% of the time it takes for GCC to compile the code (which includes no whole-program analysis), and scales well across five orders of magnitude of code size. It is also able to analyze the Linux kernel (about 355K lines of code) in 3.1 seconds. The paper describes the major algorithmic and engineering design choices that are required to achieve these results, including (a) using flow-insensitive and unification-based analysis, which are essential to avoid exponential behavior in practice; (b) sacrificing context-sensitivity within strongly connected components of the call graph; and (c) carefully eliminating several kinds of O(N 2) behaviors (largely without affecting precision). The techniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are generalizable to other context-sensitive algorithms. We show that the engineering choices collectively reduce analysis time by factors of up to 3x-21x in our ten largest programs, and that the savings grow strongly with program size. Finally, we briefly summarize results demonstrating the precision of the analysis.},
  file = {D\:\\GDrive\\zotero\\Lattner\\lattner_making_context-sensitive_points-to_analysis_with_heap_cloning_practical_for_the.pdf},
  isbn = {978-1-59593-633-2},
  keywords = {context-sensitive,D34 [Processors]: Compil-ers General Terms Algorithms Keywords Pointer analysis,field-sensitive,in-terprocedural,recursive data structure *,static analysis}
}

@techreport{lattnersabreLLVMCompilerSystem2007,
  title = {The {{LLVM Compiler System LLVM}}: {{Low Level Virtual Machine}}},
  author = {{Lattner sabre}, Chris},
  year = {2007},
  file = {D\:\\GDrive\\zotero\\Lattner sabre\\lattner_sabre_2007_the_llvm_compiler_system_llvm.pdf}
}

@techreport{laurieProofofWorkProvesNot2004,
  title = {"{{Proof}}-of-{{Work}}" {{Proves Not}} to {{Work}}},
  author = {Laurie, Ben and Clayton, Richard},
  year = {2004},
  abstract = {A frequently proposed method of reducing unsolicited bulk email ("spam") is for senders to pay for each email they send. Proof-of-work schemes avoid charging real money by requiring senders to demonstrate that they have expended processing time in solving a cryptographic puzzle. We attempt to determine how difficult that puzzle should be so as to be effective in preventing spam. We analyse this both from an economic perspective, "how can we stop it being cost-effective to send spam", and from a security perspective, "spammers can access insecure end-user machines and will steal processing cycles to solve puzzles". Both analyses lead to similar values of puzzle difficulty. Unfortunately, real-world data from a large ISP shows that these difficulty levels would mean that significant numbers of senders of legitimate email would be unable to continue their current levels of activity. We conclude that proof-of-work will not be a solution to the problem of spam.},
  file = {D\:\\GDrive\\zotero\\Laurie_Clayton\\laurie_clayton_2004_proof-of-work_proves_not_to_work.pdf}
}

@techreport{lazowskaCyberSecurityCrisis2005,
  title = {Cyber {{Security}}: {{A Crisis}} of {{Prioritization}}},
  author = {Lazowska, Edward D},
  year = {2005},
  file = {D\:\\GDrive\\zotero\\Lazowska\\lazowska_2005_cyber_security.pdf}
}

@techreport{leavlineHardwareImplementationLZMA2013,
  title = {Hardware {{Implementation}} of {{LZMA Data Compression Algorithm}}},
  author = {Leavline, E Jebamalar and Asir, D and Singh, Antony Gnana},
  year = {2013},
  volume = {5},
  abstract = {Data transmission, storage and processing are the integral parts of today's information systems. Transmission and storage of huge volume of data is a critical task in spite of the advancements in the integrated circuit technology and communication. In order to store and transmit such a data as it is, requires larger memory and increased bandwidth utilization. This in turn increases the hardware and transmission cost. Hence, before storage or transmission the size of data has to be reduced without affecting the information content of the data. Among the various encoding algorithms, the Lempel Ziv Marcov chain Algorithm (LZMA) algorithm which is used in 7zip was proved to be effective in unknown byte stream compression for reliable lossless data compression. However the encoding speed of software based coder is slow compared to the arrival time of real time data. Hence hardware implementation is needed since number of instructions processed per unit time depends directly on system clock. The aim of this work is to implement the LZMA algorithm on SPARTAN 3E FPGA to design hardware encoder/decoder with reduces circuit size and cost of storage.},
  file = {D\:\\GDrive\\zotero\\Leavline et al\\leavline_et_al_2013_hardware_implementation_of_lzma_data_compression_algorithm.pdf},
  journal = {International Journal of Applied Information Systems (IJAIS)},
  keywords = {compression ratio,decoding,encoding,FPGA,General Terms Data Compression,LZMA algorithm,unknown byte stream,VLSI Keywords Data compression},
  number = {4}
}

@techreport{leeGamificationEducationWhat2011,
  title = {Gamification in {{Education}}: {{What}}, {{How}}, {{Why Bother}}?},
  author = {Lee, J J and Hammer, J},
  year = {2011},
  pages = {15},
  abstract = {Today's schools face major problems around student motivation and engagement. Gamification, or the incorporation of game elements into non-game settings, provides an opportunity to help schools solve these difficult problems. However, if gamification is to be of use to schools, we must better understand what gamification is, how it functions, and why it might be useful. This article addresses all three questions-what, how, and why bother?-while exploring both the potential benefits and pitfalls of gamification.},
  file = {D\:\\GDrive\\zotero\\Lee\\lee_2011_gamification_in_education.pdf},
  number = {2}
}

@article{leeInferringFinegrainedControl2016,
  title = {Inferring {{Fine}}-Grained {{Control Flow Inside SGX Enclaves}} with {{Branch Shadowing}}},
  author = {Lee, Sangho and Shih, Ming-Wei and Gera, Prasun and Kim, Taesoo and Kim, Hyesoon and Peinado, Marcus},
  year = {2016},
  month = nov,
  pages = {557--574},
  abstract = {In this paper, we explore a new, yet critical, side-channel attack against Intel Software Guard Extension (SGX), called a branch shadowing attack, which can reveal fine-grained control flows (i.e., each branch) of an enclave program running on real SGX hardware. The root cause of this attack is that Intel SGX does not clear the branch history when switching from enclave mode to non-enclave mode, leaving the fine-grained traces to the outside world through a branch-prediction side channel. However, exploiting the channel is not so straightforward in practice because 1) measuring branch prediction/misprediction penalties based on timing is too inaccurate to distinguish fine-grained control-flow changes and 2) it requires sophisticated control over the enclave execution to force its execution to the interesting code blocks. To overcome these challenges, we developed two novel exploitation techniques: 1) Intel PT- and LBR-based history-inferring techniques and 2) APIC-based technique to control the execution of enclave programs in a fine-grained manner. As a result, we could demonstrate our attack by breaking recent security constructs, including ORAM schemes, Sanctum, SGX-Shield, and T-SGX. Not limiting our work to the attack itself, we thoroughly studied the feasibility of hardware-based solutions (e.g., branch history clearing) and also proposed a software-based countermeasure, called Zigzagger, to mitigate the branch shadowing attack in practice.},
  file = {D\:\\GDrive\\zotero\\Lee\\lee_2016_inferring_fine-grained_control_flow_inside_sgx_enclaves_with_branch_shadowing.pdf},
  isbn = {9781931971409},
  journal = {Proceedings of the 26th USENIX Security Symposium}
}

@article{legayStatisticalModelChecking2010,
  title = {Statistical Model Checking: {{An}} Overview},
  author = {Legay, Axel and Delahaye, Beno{\^i}t and Bensalem, Saddek},
  year = {2010},
  volume = {6418 LNCS},
  pages = {122--135},
  issn = {03029743},
  doi = {10.1007/978-3-642-16612-9_11},
  abstract = {Quantitative properties of stochastic systems are usually specified in logics that allow one to compare the measure of executions satisfying certain temporal properties with thresholds. The model checking problem for stochastic systems with respect to such logics is typically solved by a numerical approach [31,8,35,22,21,5] that iteratively computes (or approximates) the exact measure of paths satisfying relevant subformulas; the algorithms themselves depend on the class of systems being analyzed as well as the logic used for specifying the properties. Another approach to solve the model checking problem is to simulate the system for finitely many executions, and use hypothesis testing to infer whether the samples provide a statistical evidence for the satisfaction or violation of the specification. In this tutorial, we survey the statistical approach, and outline its main advantages in terms of efficiency, uniformity, and simplicity. \textcopyright{} 2010 Springer-Verlag.},
  isbn = {3642166113},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{leiBlockchainBasedDynamicKey2017,
  title = {Blockchain-{{Based Dynamic Key Management}} for {{Heterogeneous Intelligent Transportation Systems}}},
  author = {Lei, Ao and Cruickshank, Haitham and Cao, Yue and Asuquo, Philip and Ogah, Chibueze P.Anyigor and Sun, Zhili},
  year = {2017},
  month = dec,
  volume = {4},
  pages = {1832--1843},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {23274662},
  doi = {10.1109/JIOT.2017.2740569},
  abstract = {As modern vehicle and communication technologies advanced apace, people begin to believe that the Intelligent Transportation System (ITS) would be achievable in one decade. ITS introduces information technology to the transportation infrastructures and aims to improve road safety and traffic efficiency. However, security is still a main concern in vehicular communication systems (VCSs). This can be addressed through secured group broadcast. Therefore, secure key management schemes are considered as a critical technique for network security. In this paper, we propose a framework for providing secure key management within the heterogeneous network. The security managers (SMs) play a key role in the framework by capturing the vehicle departure information, encapsulating block to transport keys and then executing rekeying to vehicles within the same security domain. The first part of this framework is a novel network topology based on a decentralized blockchain structure. The blockchain concept is proposed to simplify the distributed key management in heterogeneous VCS domains. The second part of the framework uses the dynamic transaction collection period to further reduce the key transfer time during vehicles handover. Extensive simulations and analysis show the effectiveness and efficiency of the proposed framework, in which the blockchain structure performs better in term of key transfer time than the structure with a central manager, while the dynamic scheme allows SMs to flexibly fit various traffic levels.},
  file = {D\:\\GDrive\\zotero\\Lei\\lei_2017_blockchain-based_dynamic_key_management_for_heterogeneous_intelligent.pdf;D\:\\GDrive\\zotero\\Lei\\lei_2017_blockchain-based_dynamic_key_management_for_heterogeneous_intelligent2.pdf},
  journal = {IEEE Internet of Things Journal},
  keywords = {Blockchain,dynamic key management,handover,Intelligent Transportation System (ITS),vehicular communication systems (VCSs)},
  number = {6}
}

@article{leightonImprovingPerformanceInternet2008,
  title = {Improving Performance on the Internet},
  author = {Leighton, Tom},
  year = {2008},
  volume = {6},
  pages = {20--29},
  issn = {15427730},
  doi = {10.1145/1466443.1466449},
  abstract = {Even though we've seen dramatic advances in the ubiquity and usefulness of the Internet over the past decade, the real growth in bandwidth-intensive Web content, rich media, and Web- and IP-based applications is just beginning. The challenges presented by this growth are many: as businesses move more of their critical functions online, and as consumer entertainment (games, movies, sports) shifts to the Internet from other broadcast media, the stresses placed on the Internet's middle mile will become increasingly apparent and detrimental. As such, we believe that the issues raised in this article and the benefits of a highly distributed approach to content delivery will only grow in importance as we collectively work to enable the Internet to scale to the requirements of the next generation of users. \textcopyright{} 2008 ACM.},
  file = {D\:\\GDrive\\zotero\\Leighton\\leighton_2008_improving_performance_on_the_internet.pdf},
  journal = {Queue},
  number = {6}
}

@techreport{leitaoEpidemicBroadcastTrees2007,
  title = {Epidemic {{Broadcast Trees Epidemic Broadcast Trees}} *},
  author = {Leit{\~a}o, Jo{\~a}o and Pereira, Jos{\'e} and Rodrigues, Lu{\'i}s},
  year = {2007},
  abstract = {There is an inherent trade-off between epidemic and deterministic tree-based broadcast primitives. Tree-based approaches have a small message complexity in steady-state but are very fragile in the presence of faults. Gossip, or epidemic, protocols have a higher message complexity but also offer much higher resilience. This paper proposes an integrated broadcast scheme that combines both approaches. We use a low cost scheme to build and maintain broadcast trees embedded on a gossip-based overlay. The protocol sends the message payload preferably via tree branches but uses the remaining links of the gossip overlay for fast recovery and expedite tree healing. Experimental evaluation presented in the paper shows that our new strategy has a low overhead and that is able to support large number of faults while maintaining a high reliability.},
  file = {D\:\\GDrive\\zotero\\Leitão\\leitão_2007_epidemic_broadcast_trees_epidemic_broadcast_trees.pdf}
}

@article{leiteSurveyDevOpsConcepts2019,
  title = {A {{Survey}} of {{DevOps Concepts}} and {{Challenges}}},
  author = {Leite, Leonardo and Rocha, Carla and Kon, Fabio and Milojicic, Dejan},
  year = {2019},
  volume = {52},
  doi = {10.1145/3359981},
  abstract = {DevOpsis a collaborative and multidisciplinary organizational effort to automate continuous delivery of new software updates while guaranteeing their correctness and reliability. The present survey investigates and discusses DevOps challenges from the perspective of engineers, managers, and researchers. We review the literature and develop a DevOps conceptual map, correlating the DevOps automation tools with these concepts. We then discuss their practical implications for engineers, managers, and researchers. Finally, we critically explore some of the most relevant DevOps challenges reported by the literature.},
  file = {D\:\\GDrive\\zotero\\Leite\\leite_2019_a_survey_of_devops_concepts_and_challenges.pdf},
  journal = {ACM Comput. Surv},
  keywords = {Additional Key Words and Phrases: DevOps; continuous (delivery; deployment; integration); release process; versioning; configuration management; and build process ACM Reference format:,CCS Concepts: • Software and its engineering → Software development process management,Pro-gramming teams,Software post-development issues}
}

@article{leroyCompCertFormallyVerified,
  title = {{{CompCert}} - {{A Formally Verified Optimizing Compiler}}},
  author = {Leroy, Xavier and Blazy, Sandrine and K{\"a}stner, Daniel and Schommer, Bernhard and Pister, Markus and Ferdinand, Christian},
  pages = {9},
  abstract = {CompCert is the first commercially available optimizing compiler that is formally verified, using machineassisted mathematical proofs, to be exempt from miscompilation. The executable code it produces is proved to behave exactly as specified by the semantics of the source C program. This article gives an overview of the design of CompCert and its proof concept and then focuses on aspects relevant for industrial application. We briefly summarize practical experience and give an overview of recent CompCert development aiming at industrial usage. CompCert's intended use is the compilation of life-critical and mission-critical software meeting high levels of assurance. In this context tool qualification is of paramount importance. We summarize the confidence argument of CompCert and give an overview of relevant qualification strategies.},
  file = {D\:\\GDrive\\zotero\\Leroy et al\\leroy_et_al_compcert_-_a_formally_verified_optimizing_compiler.pdf},
  language = {en}
}

@article{letichevskyCyberPhysicalSystems2017,
  title = {Cyber-{{Physical Systems}}},
  author = {Letichevsky, A. A. and Letychevskyi, O. O. and Skobelev, V. G. and Volkov, V. A.},
  year = {2017},
  volume = {53},
  pages = {821--834},
  issn = {15738337},
  doi = {10.1007/s10559-017-9984-9},
  abstract = {A retrospective analysis of cyber-physical systems theory is given, and its current state is characterized. A number of problems arising in the theory of hybrid automata is investigated. A semigroup transition system is considered, which underlies the extension of the algebraic theory of interaction of labeled transition systems to cyber-physical systems.},
  file = {D\:\\GDrive\\zotero\\Letichevsky\\letichevsky_2017_cyber-physical_systems.pdf},
  journal = {Cybernetics and Systems Analysis},
  keywords = {cyber-physical systems,hybrid automata,verification},
  number = {6}
}

@techreport{leurentSHA1ShamblesFirst,
  title = {{{SHA}}-1 Is a {{Shambles First Chosen}}-{{Prefix Collision}} on {{SHA}}-1 and {{Application}} to the {{PGP Web}} of {{Trust}}},
  author = {Leurent, Ga{\"e}tan and Peyrin, Thomas},
  abstract = {The SHA-1 hash function was designed in 1995 and has been widely used during two decades. A theoretical collision attack was first proposed in 2004 [WYY05], but due to its high complexity it was only implemented in practice in 2017, using a large GPU cluster [SBK + 17]. More recently, an almost practical chosen-prefix collision attack against SHA-1 has been proposed [LP19]. This more powerful attack allows to build colliding messages with two arbitrary prefixes, which is much more threatening for real protocols. In this paper, we report the first practical implementation of this attack, and its impact on real-world security with a PGP/GnuPG impersonation attack. We managed to significantly reduce the complexity of collisions attack against SHA-1: on an Nvidia GTX 970, identical-prefix collisions can now be computed with a complexity of 2 61.2 rather than 2 64.7 , and chosen-prefix collisions with a complexity of 2 63.4 rather than 2 67.1. When renting cheap GPUs, this translates to a cost of 11k US\$ for a collision, and 45k US\$ for a chosen-prefix collision, within the means of academic researchers. Our actual attack required two months of computations using 900 Nvidia GTX 1060 GPUs (we paid 75k US\$ because GPU prices were higher, and we wasted some time preparing the attack). Therefore, the same attacks that have been practical on MD5 since 2009 are now practical on SHA-1. In particular, chosen-prefix collisions can break signature schemes and handshake security in secure channel protocols (TLS, SSH). We strongly advise to remove SHA-1 from those type of applications as soon as possible. We exemplify our cryptanalysis by creating a pair of PGP/GnuPG keys with different identities, but colliding SHA-1 certificates. A SHA-1 certification of the first key can therefore be transferred to the second key, leading to a forgery. This proves that SHA-1 signatures now offers virtually no security in practice. The legacy branch of GnuPG still uses SHA-1 by default for identity certifications, but after notifying the authors, the modern branch now rejects SHA-1 signatures (the issue is tracked as CVE-2019-14855).},
  file = {D\:\\GDrive\\zotero\\Leurent\\leurent_sha-1_is_a_shambles_first_chosen-prefix_collision_on_sha-1_and_application_to.pdf}
}

@techreport{levchenkoClickTrajectoriesEndtoEnd,
  title = {Click {{Trajectories}}: {{End}}-to-{{End Analysis}} of the {{Spam Value Chain}}},
  author = {Levchenko, Kirill and Pitsillidis, Andreas and Chachra, Neha and Enright, Brandon and F{\'e}legyh{\'a}zi, M{\'a}rk and Grier, Chris and Halvorson, Tristan and Kanich, Chris and Kreibich, Christian and Liu, He and Mccoy, Damon and Weaver, Nicholas and Paxson, Vern and Voelker, Geoffrey M and Savage, Stefan},
  abstract = {Spam-based advertising is a business. While it has engendered both widespread antipathy and a multi-billion dollar anti-spam industry, it continues to exist because it fuels a profitable enterprise. We lack, however, a solid understanding of this enterprise's full structure, and thus most anti-spam interventions focus on only one facet of the overall spam value chain (e.g., spam filtering, URL blacklisting, site takedown). In this paper we present a holistic analysis that quantifies the full set of resources employed to monetize spam email-including naming, hosting, payment and fulfillment-using extensive measurements of three months of diverse spam data, broad crawling of naming and hosting infrastructures, and over 100 purchases from spam-advertised sites. We relate these resources to the organizations who administer them and then use this data to characterize the relative prospects for defensive interventions at each link in the spam value chain. In particular, we provide the first strong evidence of payment bottlenecks in the spam value chain; 95\% of spam-advertised pharmaceutical, replica and software products are monetized using merchant services from just a handful of banks.},
  file = {D\:\\GDrive\\zotero\\Levchenko\\levchenko_click_trajectories.pdf}
}

@techreport{lewyckyCheckerStaticProgram2006,
  title = {Checker: A {{Static Program Checker}}},
  author = {Lewycky, Nicholas},
  year = {2006},
  abstract = {Automated software analysis is the process of testing program source code against a set of conditions. These may be as simple as verifying the coding standards, or as complicated as new languages which are formally verifiable by a theorem solver. Checker is able to find two small classes of errors, one is memory faults, the other, non-deterministic behaviour. Lacking interprocedu-ral analysis, checker can not be applied to real-world software.},
  file = {D\:\\GDrive\\zotero\\Lewycky\\lewycky_2006_checker.pdf;D\:\\GDrive\\zotero\\Lewycky\\lewycky_2006_checker2.pdf;D\:\\GDrive\\zotero\\Lewycky\\lewycky_2006_checker3.pdf}
}

@inproceedings{lianCallARMsUnderstanding2017,
  title = {A {{Call}} to {{ARMs}}: {{Understanding}} the {{Costs}} and {{Benefits}} of {{JIT Spraying Mitigations}}},
  shorttitle = {A {{Call}} to {{ARMs}}},
  booktitle = {Proceedings 2017 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Lian, Wilson and Shacham, Hovav and Savage, Stefan},
  year = {2017},
  publisher = {{Internet Society}},
  address = {{San Diego, CA}},
  doi = {10.14722/ndss.2017.23108},
  abstract = {JIT spraying allows an attacker to subvert a JustIn-Time compiler, introducing instruction sequences useful to the attacker into executable regions of the victim program's address space as a side effect of compiling seemingly innocuous code in a safe language like JavaScript.},
  file = {D\:\\GDrive\\zotero\\Lian et al\\lian_et_al_2017_a_call_to_arms.pdf},
  isbn = {978-1-891562-46-4},
  language = {en}
}

@techreport{liangEfficientPointstoAnalysis1990,
  title = {Efficient {{Points}}-to {{Analysis}} for {{Whole}}-{{Program Analysis}}},
  author = {Liang, Donglin and Harrold, Mary Jean},
  year = {1990},
  abstract = {TO function on programs written in languages such as C that make extensive use of pointers, automated software engineering tools require safe alias information. Existing alias-analysis techniques that are sufficiently efficient for analysis on large software systems may provide alias information that is too imprecise for tools that use it: the impreci-sion of the alias information may (1) reduce the precision of the information provided by the tools and (2) increase the cost of the tools. This paper presents a flow-insensitive, context-sensitive points-to analysis algorithm that computes alias information that is almost as precise as that computed by Andersen's algorithm-the most precise flow-and context-insensitive algorithm-and almost as efficient as Steensgaard's algorithm-the most efficient flow-and context-insensitive algorithm. Our empirical studies show that our algorithm scales to large programs better than Andersen's algorithm and show that flow-insensitive alias analysis algorithms , such as our algorithm and Andersen's algorithm, can compute alias information that is close in precision to that computed by the more expensive flow-and context-sensitive alias analysis algorithms.},
  file = {D\:\\GDrive\\zotero\\Liang_Harrold\\liang_harrold_efficient_points-to_analysis_for_whole-program_analysis.pdf},
  keywords = {Aliasing analysis,pointer analysis,points-to graph}
}

@article{lianMeasuringPracticalImpact,
  title = {Measuring the Practical Impact of {{DNSSEC Deployment}}},
  author = {Lian, Wilson and Rescorla, Eric and Shacham, Hovav and Savage, Stefan},
  abstract = {DNSSEC extends DNS with a public-key infrastructure , providing compatible clients with cryptographic assurance for DNS records they obtain, even in the presence of an active network attacker. As with many Internet protocol deployments, administrators deciding whether to deploy DNSSEC for their DNS zones must perform cost/benefit analysis. For some fraction of clients-those that perform DNSSEC validation-the zone will be protected from malicious hijacking. But another fraction of clients-those whose DNS resolvers are buggy and incompatible with DNSSEC-will no longer be able to connect to the zone. Deploying DNSSEC requires making a cost-benefit decision, balancing security for some users with denial of service for others. We have performed a large-scale measurement of the effects of DNSSEC on client name resolution using an ad network to collect results from over 500,000 geographically-distributed clients. Our findings corroborate those of previous researchers in showing that a relatively small fraction of users are protected by DNSSEC-validating resolvers. And we show, for the first time, that enabling DNSSEC measurably increases end-to-end resolution failures. For every 10 clients that are protected from DNS tampering when a domain deploys DNSSEC, approximately one ordinary client (primarily in Asia) becomes unable to access the domain.},
  file = {D\:\\GDrive\\zotero\\Lian\\lian_measuring_the_practical_impact_of_dnssec_deployment.pdf},
  isbn = {978-1-931971-03-4}
}

@article{lianMeasuringPracticalImpact2013,
  title = {Measuring the Practical Impact of {{DNSSEC Deployment}}},
  author = {Lian, Wilson and Rescorla, Eric and Shacham, Hovav and Savage, Stefan},
  year = {2013},
  pages = {15},
  abstract = {DNSSEC extends DNS with a public-key infrastructure, providing compatible clients with cryptographic assurance for DNS records they obtain, even in the presence of an active network attacker. As with many Internet protocol deployments, administrators deciding whether to deploy DNSSEC for their DNS zones must perform cost/benefit analysis. For some fraction of clients \textemdash{} those that perform DNSSEC validation \textemdash{} the zone will be protected from malicious hijacking. But another fraction of clients \textemdash{} those whose DNS resolvers are buggy and incompatible with DNSSEC \textemdash{} will no longer be able to connect to the zone. Deploying DNSSEC requires making a cost-benefit decision, balancing security for some users with denial of service for others.},
  file = {D\:\\GDrive\\zotero\\Lian et al\\lian_et_al_2013_measuring_the_practical_impact_of_dnssec_deployment.pdf},
  language = {en}
}

@inproceedings{lianTooLeJITQuit2015,
  title = {Too {{LeJIT}} to {{Quit}}: {{Extending JIT Spraying}} to {{ARM}}},
  shorttitle = {Too {{LeJIT}} to {{Quit}}},
  booktitle = {Proceedings 2015 {{Network}} and {{Distributed System Security Symposium}}},
  author = {Lian, Wilson and Shacham, Hovav and Savage, Stefan},
  year = {2015},
  publisher = {{Internet Society}},
  address = {{San Diego, CA}},
  doi = {10.14722/ndss.2015.23288},
  abstract = {In the face of widespread DEP and ASLR deployment, JIT spraying brings together the best of code injection and code reuse attacks to defeat both defenses. However, to date, JIT spraying has been an x86-only attack thanks to its reliance on variable-length, unaligned instructions. In this paper, we finally extend JIT spraying to a RISC architecture by introducing a novel technique called gadget chaining, whereby high level code invokes short sequences of unintended and intended instructions called gadgets just like a function call. We demonstrate gadget chaining in an end-to-end JIT spraying attack against WebKit's JavaScriptCore JS engine on ARM and found that existing JIT spray mitigations that were sufficient against the x86 version of the JIT spraying attack fall short in the face of gadget chaining.},
  file = {D\:\\GDrive\\zotero\\Lian et al\\lian_et_al_2015_too_lejit_to_quit.pdf},
  isbn = {978-1-891562-38-9},
  language = {en}
}

@techreport{LibraMissionEnable,
  title = {Libra's Mission Is to Enable a Simple Global Currency and Financial Infrastructure That Empowers Billions of People},
  abstract = {This document outlines our plans for a new decentralized blockchain, a low-volatility cryptocurrency, and a smart contract platform that together aim to create a new opportunity for responsible financial services innovation. Problem Statement The advent of the internet and mobile broadband has empowered billions of people globally to have access to the world's knowledge and information, high-fidelity communications, and a wide range of lower-cost, more convenient services. These services are now accessible using a \$40 smartphone from almost anywhere in the world. 1 This connectivity has driven economic empowerment by enabling more people to access the financial ecosystem. Working together, technology companies and financial institutions have also found solutions to help increase economic empowerment around the world. Despite this progress, large swaths of the world's population are still left behind-1.7 billion adults globally remain outside of the financial system with no access to a traditional bank, even though one billion have a mobile phone and nearly half a billion have internet access. 2 For too many, parts of the financial system look like telecommunication networks pre-internet. Twenty years ago, the average price to send a text message in Europe was 16 cents per message. 3 Now everyone with a smartphone can communicate across the world for free with a basic data plan. Back then, telecommunications prices were high but uniform, whereas today, access to financial services is limited or restricted for those who need it most-those impacted by cost, reliability, and the ability to seamlessly send money. All over the world, people with less money pay more for financial services. Hard-earned income is eroded by fees, from remittances and wire costs to overdraft and ATM charges. Payday loans can charge annualized interest rates of 400 percent or more, and finance charges can be as high as \$30 just to borrow \$100. 4 When people are asked why they remain on the fringe of the existing financial system, those who remain "unbanked" point to not having sufficient funds, high and unpredictable fees, banks being too far away, and lacking the necessary documentation. 5 Blockchains and cryptocurrencies have a number of unique properties that can potentially address some of the problems of accessibility and trustworthiness. These include distributed governance, which ensures that no single entity controls the network; open access, which allows anybody with an internet connection to participate; and security through cryptography, which protects the integrity of funds.},
  file = {D\:\\GDrive\\zotero\\undefined\\libra's_mission_is_to_enable_a_simple_global_currency_and_financial.pdf}
}

@book{liebeherrNetworkingNamedContent2009,
  title = {Networking {{Named Content}}},
  author = {Liebeherr, J{\"o}rg. and Ventre, G. (Giorgio) and Biersack, Ernst.},
  year = {2009},
  publisher = {{ACM Press}},
  abstract = {" ... the CoNEXT 2009 Conference \& co-located workshops' compilation proceedings ... ACM order number 534095"--Front matter, p. [ii].},
  file = {D\:\\GDrive\\zotero\\Liebeherr\\liebeherr_2009_networking_named_content.pdf},
  isbn = {978-1-60558-636-6}
}

@techreport{liFirstPrinciplesApproachUnderstanding2004,
  title = {A {{First}}-{{Principles Approach}} to {{Understanding}} the {{Internet}}'s {{Router}}-Level {{Topology}}},
  author = {Li, Lun and Alderson, David and Willinger, Walter and Doyle, John},
  year = {2004},
  abstract = {A detailed understanding of the many facets of the Internet's topo-logical structure is critical for evaluating the performance of networking protocols, for assessing the effectiveness of proposed techniques to protect the network from nefarious intrusions and attacks , or for developing improved designs for resource provision-ing. Previous studies of topology have focused on interpreting measurements or on phenomenological descriptions and evaluation of graph-theoretic properties of topology generators. We propose a complementary approach of combining a more subtle use of statistics and graph theory with a first-principles theory of router-level topology that reflects practical constraints and tradeoffs. While there is an inevitable tradeoff between model complexity and fidelity , a challenge is to distill from the seemingly endless list of potentially relevant technological and economic issues the features that are most essential to a solid understanding of the intrinsic fundamentals of network topology. We claim that very simple models that incorporate hard technological constraints on router and link bandwidth and connectivity, together with abstract models of user demand and network performance, can successfully address this challenge and further resolve much of the confusion and controversy that has surrounded topology generation and evaluation.},
  file = {D\:\\GDrive\\zotero\\Li\\li_2004_a_first-principles_approach_to_understanding_the_internet's_router-level.pdf},
  keywords = {C21 [Communication Networks]: Architecture and Design-topology General Terms Performance,degree-based generators,Design,Economics Keywords Network topology,heuris-tically optimal topology,topology metrics}
}

@article{LightweightCrossprocedureTracing2003,
  title = {Lightweight, {{Cross}}-Procedure {{Tracing For Runtime Optimization}}},
  year = {2003},
  file = {D\:\\GDrive\\zotero\\undefined\\2003_lightweight,_cross-procedure_tracing_for_runtime_optimization.pdf}
}

@techreport{lihuProofUsefulWork,
  title = {A {{Proof}} of {{Useful Work}} for {{Artificial Intelligence}} on the {{Blockchain}}},
  author = {Lihu, Andrei and Du, Jincheng and Barjaktarevi{\textasciiacute}cbarjaktarevi{\textasciiacute}c, Igor and Gerzanics, Patrick and Harvilla, Mark},
  abstract = {Bitcoin mining is a wasteful and resource-intensive process. To add a block of transactions to the blockchain, miners spend a considerable amount of energy. The Bitcoin protocol, named 'proof of work' (PoW), resembles a lottery and the underlying computational work is not useful otherwise. In this paper, we describe a novel 'proof of useful work' (PoUW) protocol based on training a machine learning model on the blockchain. Miners get a chance to create new coins after performing honest ML training work. Clients submit tasks and pay all training contributors. This is an extra incentive to participate in the network because the system does not rely only on the lottery procedure. Using our consensus protocol, interested parties can order, complete, and verify useful work in a distributed environment. We outline mechanisms to reward useful work and punish malicious actors. We aim to build better AI systems using the security of the blockchain.},
  file = {D\:\\GDrive\\zotero\\Lihu\\lihu_a_proof_of_useful_work_for_artificial_intelligence_on_the_blockchain.pdf}
}

@techreport{liIndustrialStrengthAudioSearch,
  title = {An {{Industrial}}-{{Strength Audio Search Algorithm}}},
  author = {Li, Avery and Wang, -Chun},
  abstract = {We have developed and commercially deployed a flexible audio search engine. The algorithm is noise and distortion resistant, computationally efficient, and massively scalable, capable of quickly identifying a short segment of music captured through a cellphone microphone in the presence of foreground voices and other dominant noise, and through voice codec compression, out of a database of over a million tracks. The algorithm uses a combinatorially hashed time-frequency constellation analysis of the audio, yielding unusual properties such as transparency, in which multiple tracks mixed together may each be identified. Furthermore, for applications such as radio monitoring, search times on the order of a few milliseconds per query are attained, even on a massive music database.},
  file = {D\:\\GDrive\\zotero\\Li\\li_an_industrial-strength_audio_search_algorithm.pdf}
}

@article{lindholmJavaVirtualMachine1997,
  title = {The {{Java}}\texttrademark{} {{Virtual Machine Specification}}},
  author = {Lindholm, Tim and Yellin, Frank},
  year = {1997},
  pages = {491},
  file = {D\:\\GDrive\\zotero\\Lindholm_Yellin\\lindholm_yellin_1997_the_java™_virtual_machine_specification.pdf},
  language = {en}
}

@article{linSurveyBlockchainInternet2019,
  title = {Survey on Blockchain for Internet of Things},
  author = {Lin, Fuhong and Hui, Hongwen and An, Xingshuo and Wang, Haoyu and Ju, Weijia and Yang, Huixuan and Gao, Hongjie},
  year = {2019},
  volume = {9},
  pages = {1--30},
  publisher = {{IEEE}},
  issn = {21822077},
  doi = {10.22667/JISIS.2019.05.31.001},
  abstract = {The Internet of Things (IoT) refers to a network concept that extends its clients to any item for information exchange and communication. Blockchain is a new application paradigm of distributed data storage, point-to-point transmission, consensus mechanism, asymmetric encryption, intelligent contract and other computer technologies. Blockchain technology for IoT has been a hot topic in academia and industry. This paper mainly summarizes the main existing research results of the challenges faced by IoT, the technical characteristics of blockchain, the advantages of applying blockchain technology in IoT, security challenges (key management, intrusion detection, access control, privacy protection) and the technology to deal with them. Finally, the prospect of this industry would be summarized and discussed. On the basis of this study, we hope to put forward suggestions and provide references for future research orientation.},
  file = {D\:\\GDrive\\zotero\\Lin\\lin_2019_survey_on_blockchain_for_internet_of_things.pdf},
  journal = {Journal of Internet Services and Information Security},
  keywords = {Access control,Blockchain,Internet of Things (IoT),Intrusion detection},
  number = {2}
}

@article{linSurveyBlockchainSecurity2017,
  title = {A {{Survey}} of {{Blockchain Security Issues}} and {{Challenges}}},
  author = {Lin, Iuon-Chang and Liao, Tzu-Chun},
  year = {2017},
  volume = {19},
  pages = {653--659},
  doi = {10.6633/IJNS.201709.19(5).01},
  abstract = {Blockchain technologies is one of the most popular issue in recent years, it has already changed people's lifestyle in some area due to its great influence on many business or industry, and what it can do will still continue cause impact in many places. Although the feature of blockchain technologies may bring us more reliable and convenient services, the security issues and challenges behind this innovative technique is also an important topic that we need to concern.},
  file = {D\:\\GDrive\\zotero\\Lin\\lin_2017_a_survey_of_blockchain_security_issues_and_challenges.pdf},
  journal = {International Journal of Network Security},
  keywords = {Blockchain,Security,Smart Contracts},
  number = {5}
}

@article{lippARMageddonCacheAttacks2016,
  title = {{{ARMageddon}} : {{Cache Attacks}} on {{Mobile Devices This}} Paper Is Included in the {{Proceedings}} of the {{ARMageddon}} : {{Cache Attacks}} on {{Mobile Devices}}},
  author = {Lipp, Moritz and Gruss, Daniel and Spreitzer, Raphael and Maurice, Cl{\'e}mentine and Mangard, Stefan and Lipp, Moritz and Gruss, Daniel and Spreitzer, Raphael and Mangard, Stefan},
  year = {2016},
  pages = {897--912},
  abstract = {In the last 10 years, cache attacks on Intel x86 CPUs have gained increasing attention among the scientific com-munity and powerful techniques to exploit cache side channels have been developed. However, modern smart-phones use one or more multi-core ARM CPUs that have a different cache organization and instruction set than Intel x86 CPUs. So far, no cross-core cache attacks have been demonstrated on non-rooted Android smartphones. In this work, we demonstrate how to solve key chal-lenges to perform the most powerful cross-core cache at-tacks Prime+Probe, Flush+Reload, Evict+Reload, and Flush+Flush on non-rooted ARM-based devices without any privileges. Based on our techniques, we demonstrate covert channels that outperform state-of-the-art covert channels on Android by several orders of magnitude. Moreover, we present attacks to monitor tap and swipe events as well as keystrokes, and even derive the lengths of words entered on the touchscreen. Eventually, we are the first to attack cryptographic primitives implemented in Java. Our attacks work across CPUs and can even monitor cache activity in the ARM TrustZone from the normal world. The techniques we present can be used to attack hundreds of millions of Android devices.},
  file = {D\:\\GDrive\\zotero\\Lipp\\lipp_2016_armageddon.pdf},
  isbn = {9781931971324},
  journal = {Usenix Sec}
}

@article{lippARMageddonCacheAttacks2016a,
  title = {{{ARMageddon}}: {{Cache Attacks}} on {{Mobile Devices}}},
  author = {Lipp, Moritz and Gruss, Daniel and Spreitzer, Raphael and Maurice, Clementine and Mangard, Stefan},
  year = {2016},
  pages = {17},
  abstract = {In the last 10 years, cache attacks on Intel x86 CPUs have gained increasing attention among the scientific community and powerful techniques to exploit cache side channels have been developed. However, modern smartphones use one or more multi-core ARM CPUs that have a different cache organization and instruction set than Intel x86 CPUs. So far, no cross-core cache attacks have been demonstrated on non-rooted Android smartphones. In this work, we demonstrate how to solve key challenges to perform the most powerful cross-core cache attacks Prime+Probe, Flush+Reload, Evict+Reload, and Flush+Flush on non-rooted ARM-based devices without any privileges. Based on our techniques, we demonstrate covert channels that outperform state-of-the-art covert channels on Android by several orders of magnitude. Moreover, we present attacks to monitor tap and swipe events as well as keystrokes, and even derive the lengths of words entered on the touchscreen. Eventually, we are the first to attack cryptographic primitives implemented in Java. Our attacks work across CPUs and can even monitor cache activity in the ARM TrustZone from the normal world. The techniques we present can be used to attack hundreds of millions of Android devices.},
  file = {D\:\\GDrive\\zotero\\Lipp et al\\lipp_et_al_2016_armageddon.pdf},
  language = {en}
}

@inproceedings{lippMeltdownReadingKernel2018,
  title = {Meltdown: {{Reading}} Kernel Memory from User Space},
  booktitle = {Proceedings of the 27th {{USENIX Security Symposium}}},
  author = {Lipp, Moritz and Schwarz, Michael and Gruss, Daniel and Prescher, Thomas and Haas, Werner and Fogh, Anders and Horn, Jann and Mangard, Stefan and Kocher, Paul and Genkin, Daniel and Yarom, Yuval and Hamburg, Mike},
  year = {2018},
  pages = {973--990},
  abstract = {The security of computer systems fundamentally relies on memory isolation, e.g., kernel address ranges are marked as non-accessible and are protected from user access. In this paper, we present Meltdown. Meltdown exploits side effects of out-of-order execution on modern processors to read arbitrary kernel-memory locations including personal data and passwords. Out-of-order execution is an indispensable performance feature and present in a wide range of modern processors. The attack is independent of the operating system, and it does not rely on any software vulnerabilities. Meltdown breaks all security guarantees provided by address space isolation as well as paravirtualized environments and, thus, every security mechanism building upon this foundation. On affected systems, Meltdown enables an adversary to read memory of other processes or virtual machines in the cloud without any permissions or privileges, affecting millions of customers and virtually every user of a personal computer. We show that the KAISER defense mechanism for KASLR has the important (but inadvertent) side effect of impeding Meltdown. We stress that KAISER must be deployed immediately to prevent large-scale exploitation of this severe information leakage.},
  file = {D\:\\GDrive\\zotero\\Lipp\\lipp_2018_meltdown.pdf},
  isbn = {978-1-939133-04-5},
  keywords = {ss}
}

@incollection{lippPracticalKeystrokeTiming2017,
  title = {Practical {{Keystroke Timing Attacks}} in {{Sandboxed JavaScript}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Lipp, Moritz and Gruss, Daniel and Schwarz, Michael and Bidner, David and Maurice, Cl{\'e}mentine and Mangard, Stefan},
  year = {2017},
  volume = {10493 LNCS},
  pages = {191--209},
  issn = {16113349},
  doi = {10.1007/978-3-319-66399-9_11},
  abstract = {Keystrokes trigger interrupts which can be detected through software side channels to reconstruct keystroke timings. Keystroke timing attacks use these side channels to infer typed words, passphrases, or create user fingerprints. While keystroke timing attacks are considered harmful, they typically require native code execution to exploit the side channels and, thus, may not be practical in many scenarios. In this paper, we present the first generic keystroke timing attack in sandboxed JavaScript, targeting arbitrary other tabs, processes and programs. This violates same-origin policy, HTTPS security model, and process isolation. Our attack is based on the interrupt-timing side channel which has previously only been exploited using native code. In contrast to previous attacks, we do not require the victim to run a malicious binary or interact with the malicious website. Instead, our attack runs in a background tab, possibly in a minimized browser window, displaying a malicious online advertisement. We show that we can observe the exact inter-keystroke timings for a user's PIN or password, infer URLs entered by the user, and distinguish different users time-sharing a computer. Our attack works on personal computers, laptops and smartphones, with different operating systems and browsers. As a solution against all known JavaScript timing attacks, we propose a fine-grained permission model.},
  file = {D\:\\GDrive\\zotero\\Lipp\\lipp_2017_practical_keystroke_timing_attacks_in_sandboxed_javascript.pdf},
  isbn = {978-3-319-66398-2},
  keywords = {Fingerprint,Interrupt,JavaScript,Keystroke,Side channel}
}

@techreport{lippPushPull,
  title = {{{PushPull}} ++},
  author = {Lipp, Markus and Wonka, Peter and M{\"u}ller, Pascal},
  abstract = {(a) Where should new faces be inserted? (b) How should adjacent faces be updated, keeping them planar? (c) How should edge collapses be handled? (d) Example showing all features Figure 1: There are multiple challenges when a PushPull operation is performed on a face or edge. Case (a): New faces can either be inserted for all edges (left) or not at all by adjusting adjacent faces (middle). In addition, our solution can adaptively add new faces where needed (right). New faces are blue and modified adjacent faces are green. In (b-d), the left figure is the input, the middle is the degenerate result by previous approaches, and the right is our result. Non-planar or self-intersecting faces are red and edge collapses are blue dots. Abstract PushPull tools are implemented in most commercial 3D modeling suites. Their purpose is to intuitively transform a face, edge, or ver-tex, and then to adapt the polygonal mesh locally. However, previous approaches have limitations: Some allow adjustments only when adjacent faces are orthogonal; others support slanted surfaces but never create new details. Moreover, self-intersections and edge-collapses during editing are either ignored or work only partially for solid geometry. To overcome these limitations, we introduce the PushPull++ tool for rapid polygonal modeling. In our solution, we contribute novel methods for adaptive face insertion, adjacent face updates, edge collapse handling, and an intuitive user interface that automatically proposes useful drag directions. We show that PushPull++ reduces the complexity of common modeling tasks by up to an order of magnitude when compared with existing tools.},
  file = {D\:\\GDrive\\zotero\\Lipp et al\\lipp_et_al_pushpull_++.pdf},
  keywords = {DL PDF * mlipp@esricom †,local mesh editing Links,Polygonal modeling}
}

@article{liReadingTeaLeaves2019,
  title = {Reading the Tea Leaves: {{A}} Comparative Analysis of Threat Intelligence},
  author = {Li, Vector Guo and Dunn, Matthew and Pearce, Paul and McCoy, Damon and Voelker, Geoffrey M. and Savage, Stefan and Levchenko, Kirill},
  year = {2019},
  pages = {851--867},
  abstract = {The term ``threat intelligence'' has swiftly become a staple buzzword in the computer security industry. The entirely reasonable premise is that, by compiling up-to-date information about known threats (i.e., IP addresses, domain names, file hashes, etc.), recipients of such information may be able to better defend their systems from future attacks. Thus, today a wide array of public and commercial sources distribute threat intelligence data feeds to support this purpose. However, our understanding of this data, its characterization and the extent to which it can meaningfully support its intended uses, is still quite limited. In this paper, we address these gaps by formally defining a set of metrics for characterizing threat intelligence data feeds and using these measures to systematically characterize a broad range of public and commercial sources. Further, we ground our quantitative assessments using external measurements to qualitatively investigate issues of coverage and accuracy. Unfortunately, our measurement results suggest that there are significant limitations and challenges in using existing threat intelligence data for its purported goals.},
  file = {D\:\\GDrive\\zotero\\Li\\li_2019_reading_the_tea_leaves.pdf},
  isbn = {9781939133069},
  journal = {Proceedings of the 28th USENIX Security Symposium}
}

@techreport{liSemanticsEvaluationTechniques,
  title = {Semantics and {{Evaluation Techniques}} for {{Window Aggregates}} in {{Data Streams}}},
  author = {Li, Jin and Maier, David and Tufte, Kristin and Papadimos, Vassilis and Tucker, Peter A},
  abstract = {A windowed query operator breaks a data stream into possibly overlapping subsets of data and computes results over each. Many stream systems can evaluate window aggregate queries. However, current stream systems suffer from a lack of an explicit definition of window semantics. As a result, their implementations unnecessarily confuse window definition with physical stream properties. This confusion complicates the stream system, and even worse, can hurt performance both in terms of memory usage and execution time. To address this problem, we propose a framework for defining window semantics, which can be used to express almost all types of windows of which we are aware, and which is easily extensible to other types of windows that may occur in the future. Based on this definition, we explore a one-pass query evaluation strategy, the Window-ID (WID) approach, for various types of window aggregate queries. WID significantly reduces both required memory space and execution time for a large class of window definitions. In addition, WID can leverage punctuations to gracefully handle disorder. Our experimental study shows that WID has better execution-time performance than existing window aggregate query evaluation options that retain and reprocess tuples, and has better latency-accuracy tradeoff performance for disordered input streams compared to using a fixed delay for disorder handling.},
  file = {D\:\\GDrive\\zotero\\Li\\li_semantics_and_evaluation_techniques_for_window_aggregates_in_data_streams.pdf}
}

@article{LispIiGarbage,
  title = {The\_lisp\_ii\_garbage\_collector},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\2U24AU4H\\the_lisp_ii_garbage_collector.pdf}
}

@article{liSteeringSymbolicExecution2013,
  title = {Steering Symbolic Execution to Less Traveled Paths},
  author = {Li, You and Su, Zhendong and Wang, Linzhang and Li, Xuandong},
  year = {2013},
  volume = {48},
  pages = {19--31},
  issn = {15232867},
  doi = {10.1145/2544173.2509553},
  abstract = {Symbolic execution is a promising testing and analysis methodology. It systematically explores a program's execution space and can generate test cases with high coverage. One significant practical challenge for symbolic execution is how to effectively explore the enormous number of program paths in real-world programs. Various heuristics have been proposed for guiding symbolic execution, but they are generally inefficient and ad-hoc. In this paper, we introduce a novel, unified strategy to guide symbolic execution to less explored parts of a program. Our key idea is to exploit a specific type of path spectra, namely the length-n subpath program spectra, to systematically approximate full path information for guiding path exploration. In particular, we use frequency distributions of explored length-n subpaths to prioritize "less traveled" parts of the program to improve test coverage and error detection. We have implemented our general strategy in KLEE, a state-of-the-art symbolic execution engine. Evaluation results on the GNU Coreutils programs show that (1) varying the length n captures program-specific information and exhibits different degrees of effectiveness, and (2) our general approach outperforms traditional strategies in both coverage and error detection. Copyright \textcopyright{} 2013. Copyright \textcopyright{} 2013 ACM.},
  file = {D\:\\GDrive\\zotero\\Li\\li_2013_steering_symbolic_execution_to_less_traveled_paths.pdf},
  isbn = {9781450323741},
  journal = {ACM SIGPLAN Notices},
  keywords = {Less traveled,Path spectra,Symbolic execution},
  number = {10}
}

@article{liSurveyOpenFlowbasedSoftware2016,
  title = {A Survey on {{OpenFlow}}-Based {{Software Defined Networks}}: {{Security}} Challenges and Countermeasures},
  author = {Li, Wenjuan and Meng, Weizhi and Kwok, Lam For},
  year = {2016},
  doi = {10.1016/j.jnca.2016.04.011},
  abstract = {Software-Defined Networking (SDN) has been proposed as an emerging network architecture, which consists of decoupling the control planes and data planes of a network. Due to its openness and standardization , SDN enables researchers to design and implement new innovative network functions and protocols in a much easier and flexible way. In particular, OpenFlow is currently the most deployed SDN concept, which provides communication between the controller and the switches. However, the dynamism of programmable networks also brings potential new security challenges relating to various attacks such as scanning, spoofing attacks, denial-of-service (DoS) attacks and so on. In this survey, we aim to give particular attention to OpenFlow-based SDN and present an up-to-date view to existing security challenges and countermeasures in the literature. This effort attempts to simulate more research attention to these issues in future OpenFlow and\& SDN development.},
  file = {D\:\\GDrive\\zotero\\Li\\li_2016_a_survey_on_openflow-based_software_defined_networks.pdf},
  keywords = {Control and data planes,Defined,Flow,Networking,Open,Review and survey,Security challenges and countermeasures,Software-}
}

@article{liSurveySecurityBlockchain2020,
  title = {A Survey on the Security of Blockchain Systems},
  author = {Li, Xiaoqi and Jiang, Peng and Chen, Ting and Luo, Xiapu and Wen, Qiaoyan},
  year = {2020},
  volume = {107},
  pages = {841--853},
  doi = {10.1016/j.future.2017.08.020},
  abstract = {h i g h l i g h t s \textbullet{} We conduct the first systematic examination on security risks to popular blockchain systems. \textbullet{} We survey the real attacks on blockchain systems and analyze related vulnerabilities exploited. \textbullet{} We summarize practical academic achievements for enhancing the security of blockchain. \textbullet{} We suggest a few future directions in the area of blockchain security. a b s t r a c t Since its inception, the blockchain technology has shown promising application prospects. From the initial cryptocurrency to the current smart contract, blockchain has been applied to many fields. Although there are some studies on the security and privacy issues of blockchain, there lacks a systematic examination on the security of blockchain systems. In this paper, we conduct a systematic study on the security threats to blockchain and survey the corresponding real attacks by examining popular blockchain systems. We also review the security enhancement solutions for blockchain, which could be used in the development of various blockchain systems, and suggest some future directions to stir research efforts into this area.},
  file = {D\:\\GDrive\\zotero\\Li\\li_2020_a_survey_on_the_security_of_blockchain_systems.pdf},
  journal = {Future Generation Computer Systems},
  keywords = {Blockchain,Cryptocurrency,Security,Smart contract}
}

@techreport{liuAnalysisThreeBayesian2004,
  title = {Analysis of {{Three Bayesian Network Inference Algorithms}}: {{Variable Elimination}}, {{Likelihood Weighting}}, and {{Gibbs Sampling}}},
  author = {Liu, Rose F and Soetjipto, Rusmin},
  year = {2004},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2004_analysis_of_three_bayesian_network_inference_algorithms.pdf}
}

@article{liuFocusedCrawlerDark2013,
  title = {A {{Focused Crawler}} for {{Dark Web Forums}}},
  author = {Liu, Xiaozhong},
  year = {2013},
  volume = {64},
  pages = {1852--1863},
  abstract = {In this article, we use innovative full-text citation analysis along with supervised topic modeling and network-analysis algorithms to enhance classical bibliometric analysis and publication/author/venue ranking. By utilizing citation contexts extracted from a large number of full-text publications, each citation or publication is represented by a probability distribution over a set of predefined topics, where each topic is labeled by an author-contributed keyword. We then used publication/citation topic distribution to generate a citation graph with vertex prior and edge transitioning probability distributions. The publication importance score for each given topic is calculated by PageRank with edge and vertex prior distributions. To evaluate this work, we sampled 104 topics (labeled with keywords) in review papers. The cited publications of each review paper are assumed to be ``important publications'' for the target topic (keyword), and we use these cited publications to validate our topic-ranking result and to compare different publication-ranking lists. Evaluation results show that full-text citation and publication content prior topic distribution, along with the classical PageRank algorithm can significantly enhance bibliometric analysis and scientific publication ranking performance, comparing with term frequency\textendash inverted document frequency (tf\textendash idf), language model, BM25, PageRank, and PageRank + language model (p {$<$} .001), for academic information retrieval (IR) systems.},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2013_a_focused_crawler_for_dark_web_forums.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  keywords = {bibliometrics,text mining},
  number = {July}
}

@inproceedings{liuLastLevelCacheSideChannel2015,
  title = {Last-{{Level Cache Side}}-{{Channel Attacks}} Are {{Practical}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Liu, Fangfei and Yarom, Yuval and Ge, Qian and Heiser, Gernot and Lee, Ruby B.},
  year = {2015},
  month = may,
  volume = {2015-July},
  pages = {605--622},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2015.43},
  abstract = {We present an effective implementation of the Prime Probe side-channel attack against the last-level cache. We measure the capacity of the covert channel the attack creates and demonstrate a cross-core, cross-VM attack on multiple versions of GnuPG. Our technique achieves a high attack resolution without relying on weaknesses in the OS or virtual machine monitor or on sharing memory between attacker and victim.},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2015_last-level_cache_side-channel_attacks_are_practical.pdf;D\:\\GDrive\\zotero\\Liu\\liu_2015_last-level_cache_side-channel_attacks_are_practical2.pdf},
  isbn = {978-1-4673-6949-7},
  keywords = {covert channel,cross-VM side channel,ElGamal,last-level cache,side-channel attack}
}

@article{liUnderstandingPropagationHard,
  title = {Understanding the {{Propagation}} of {{Hard Errors}} to {{Software}} and {{Implications}} for {{Resilient System Design}} *},
  author = {Li, Man-Lap and Ramachandran, Pradeep and Sahoo, Swarup K and Adve, Sarita V and Adve, Vikram S and Zhou, Yuanyuan},
  abstract = {With continued CMOS scaling, future shipped hardware will be increasingly vulnerable to in-the-field faults. To be broadly deploy-able, the hardware reliability solution must incur low overheads, precluding use of expensive redundancy. We explore a cooperative hardware-software solution that watches for anomalous software behavior to indicate the presence of hardware faults. Fundamental to such a solution is a characterization of how hardware faults in different microarchitectural structures of a modern processor propagate through the application and OS. This paper aims to provide such a characterization, resulting in identifying low-cost detection methods and providing guidelines for implementation of the recovery and diagnosis components of such a reliability solution. We focus on hard faults because they are increasingly important and have different system implications than the much studied transients. We achieve our goals through fault injection experiments with a microarchitecture-level full system timing simulator. Our main results are: (1) we are able to detect 95\% of the unmasked faults in 7 out of 8 studied microarchitectural structures with simple detectors that incur zero to little hardware overhead; (2) over 86\% of these detections are within latencies that existing hardware checkpointing schemes can handle, while others require software checkpointing; and (3) a surprisingly large fraction of the detected faults corrupt OS state, but almost all of these are detected with latencies short enough to use hardware checkpointing, thereby enabling OS recovery in virtually all such cases.},
  file = {D\:\\GDrive\\zotero\\Li\\li_understanding_the_propagation_of_hard_errors_to_software_and_implications_for.pdf},
  keywords = {Architecture,B81 [Reliability,Design Keywords Error detection,Experimentation,Fault injection *,Permanent fault,Testing and Fault-Tolerance] General Terms Reliability}
}

@article{liuRobustEfficientDefense2018,
  title = {A Robust and Efficient Defense against Use-after-Free Exploits via Concurrent Pointer Sweeping},
  author = {Liu, Daiping and Zhang, Mingwei and Wang, Haining},
  year = {2018},
  pages = {1635--1648},
  issn = {15437221},
  doi = {10.1145/3243734.3243826},
  abstract = {Applications in C/C++ are notoriously prone to memory corruptions. With significant research efforts devoted to this area of study, the security threats posed by previously popular vulnerabilities, such as stack and heap overflows, are not as serious as before. Instead, we have seen the meteoric rise of attacks exploiting use-after-free (UaF) vulnerabilities in recent years, which root in pointers pointing to freed memory (i.e., dangling pointers). Although various approaches have been proposed to harden software against UaF, none of them can achieve robustness and efficiency at the same time. In this paper, we present a novel defense called pSweeper to robustly protect against UaF exploits with low overhead, and pinpoint the root-causes of UaF vulnerabilities with one safe crash. The success of pSweeper lies in its two unique and innovative design ideas, concurrent pointer sweeping (CPW) and object origin tracking (OOT). CPW exploits the increasingly available multi-cores on modern PCs and outsources the heavyweight security checks and enforcement to dedicated threads that can run on spare cores. Specifically, CPW iteratively sweeps all live pointers in a concurrent thread to find dangling pointers. This design is quite different from previous work that requires to track every pointer propagation to maintain accurate point-to relationship between pointers and objects. OOT can help to pinpoint the root-causes of UaF by informing developers of how a dangling pointer is created. We implement a prototype of pSweeper and validate its efficacy in real scenarios. Our experimental results show that pSweeper is effective in defeating real-world UaF exploits and efficient when deployed in production runs.},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2018_a_robust_and_efficient_defense_against_use-after-free_exploits_via_concurrent.pdf;D\:\\GDrive\\zotero\\Liu\\liu_2018_a_robust_and_efficient_defense_against_use-after-free_exploits_via_concurrent2.pdf},
  isbn = {9781450356930},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security}
}

@article{liuSpeculativeExecutionGPU2010,
  title = {Speculative {{Execution}} on {{GPU}}: {{An Exploratory Study}}},
  author = {Liu, Shaoshan and Eisenbeis, Christine and Gaudiot, Jean-Luc},
  year = {2010},
  doi = {10.1109/ICPP.2010.53},
  abstract = {We explore the possibility of using GPUs for speculative execution: we implement software value prediction techniques to accelerate programs with limited parallelism, and software speculation techniques to accelerate programs that contain runtime parallelism, which are hard to parallelize statically. Our experiment results show that due to the relatively high overhead, mapping software value prediction techniques on existing GPUs may not bring any immediate performance gain. On the other hand, although software speculation techniques introduce some overhead as well, mapping these techniques to existing GPUs can already bring some performance gain over CPU.},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2010_speculative_execution_on_gpu.pdf}
}

@article{liuStaticInformationFlow2010a,
  title = {Static Information Flow Analysis with Handling of Implicit Flows and a Study on Effects of Implicit Flows vs Explicit Flows},
  author = {Liu, Yin and Milanova, Ana},
  year = {2010},
  pages = {146--155},
  issn = {15345351},
  doi = {10.1109/CSMR.2010.26},
  abstract = {Reasoning about information flow can help software engineering. Static information flow inference analysis is a technique which automatically infers information flows based on data or control dependence. It can be utilized for the purposes of general program understanding, detection of security attacks and security vulnerabilities, and type inference for security type systems. This paper proposes a new static information flow inference analysis, which unlike most other information flow analyses, handles both explicit and implicit information flows. The analysis does not require annotations and it is relatively precise and practical. We illustrate the usage of the static information flow analysis on three applications. The first application of information flow analysis is security violation detection.We perform experiments on a set of Java web applications and the experiments show that our analysis effectively detects security violations. The second application is type inference. Our experiments on the Java web applications successfully infer security types. The last application studies the effect of thread-shared variables on thread-local variables. Our experiments on a set of multithread programs show that most of the thread-local variables are affected by the thread-shared variables. We study the impact of implicit flow versus explicit flow in these applications. Implicit flow has significant impact on all these applications. In security violation detection, implicit flow detects more security violations than explicit flow. In type inference, implicit flow infers more untrusted type variables. In the study of the effect of thread-shared variables, implicit flow detects more affected variables than explicit flow. \textcopyright{} 2010 IEEE.},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2010_static_information_flow_analysis_with_handling_of_implicit_flows_and_a_study_on.pdf;D\:\\GDrive\\zotero\\Liu\\liu_2010_static_information_flow_analysis_with_handling_of_implicit_flows_and_a_study_on2.pdf},
  isbn = {9780769543215},
  journal = {Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
  keywords = {Automatic inference,Information flow,Security vulnerability,Type inference}
}

@article{liuUncoveringDarkWebCase2013,
  title = {Uncovering the {{DarkWeb}}: {{A Case Study}} of {{Jihad}} on the {{Web}}},
  author = {Liu, Xiaozhong},
  year = {2013},
  volume = {64},
  pages = {1852--1863},
  doi = {10.1002/asi},
  abstract = {In this article, we use innovative full-text citation analysis along with supervised topic modeling and network-analysis algorithms to enhance classical bibliometric analysis and publication/author/venue ranking. By utilizing citation contexts extracted from a large number of full-text publications, each citation or publication is represented by a probability distribution over a set of predefined topics, where each topic is labeled by an author-contributed keyword. We then used publication/citation topic distribution to generate a citation graph with vertex prior and edge transitioning probability distributions. The publication importance score for each given topic is calculated by PageRank with edge and vertex prior distributions. To evaluate this work, we sampled 104 topics (labeled with keywords) in review papers. The cited publications of each review paper are assumed to be ``important publications'' for the target topic (keyword), and we use these cited publications to validate our topic-ranking result and to compare different publication-ranking lists. Evaluation results show that full-text citation and publication content prior topic distribution, along with the classical PageRank algorithm can significantly enhance bibliometric analysis and scientific publication ranking performance, comparing with term frequency\textendash inverted document frequency (tf\textendash idf), language model, BM25, PageRank, and PageRank + language model (p {$<$} .001), for academic information retrieval (IR) systems.},
  file = {D\:\\GDrive\\zotero\\Liu\\liu_2013_uncovering_the_darkweb.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  keywords = {bibliometrics,text mining},
  number = {July}
}

@book{liVIPERVerifyingIntegrity2011,
  title = {{{VIPER}}: {{Verifying}} the {{Integrity}} of {{PERipherals}}' {{Firmware}}},
  author = {Li, Yanlin and Mccune, Jonathan M and Perrig, Adrian},
  year = {2011},
  abstract = {Recent research demonstrates that malware can infect pe-ripherals' firmware in a typical x86 computer system, e.g., by exploiting vulnerabilities in the firmware itself or in the firmware update tools. Verifying the integrity of peripher-als' firmware is thus an important challenge. We propose software-only attestation protocols to verify the integrity of peripherals' firmware, and show that they can detect all known software-based attacks. We implement our scheme using a Netgear GA620 network adapter in an x86 PC, and evaluate our system with known attacks.},
  file = {D\:\\GDrive\\zotero\\Li\\li_2011_viper.pdf},
  isbn = {978-1-4503-0948-6},
  keywords = {D46 [Software]: Operating Systems-Security and Pro-tection General Terms Security,Proxy Attack,Software-based Attestation,Verification Keywords Integrity of Peripherals' Firmware}
}

@techreport{livshitsFindingSecurityVulnerabilities2005,
  title = {Finding {{Security Vulnerabilities}} in {{Java Applications}} with {{Static Analysis}}},
  author = {Livshits, V Benjamin and Lam, Monica S},
  year = {2005},
  abstract = {This paper proposes a static analysis technique for detecting many recently discovered application vulner-abilities such as SQL injections, cross-site scripting, and HTTP splitting attacks. These vulnerabilities stem from unchecked input, which is widely recognized as the most common source of security vulnerabilities in Web applications. We propose a static analysis approach based on a scalable and precise points-to analysis. In our system, user-provided specifications of vulnerabilities are automatically translated into static analyzers. Our approach finds all vulnerabilities matching a specification in the statically analyzed code. Results of our static analysis are presented to the user for assessment in an auditing interface integrated within Eclipse, a popular Java development environment. Our static analysis found 29 security vulnerabilities in nine large, popular open-source applications, with two of the vulnerabilities residing in widely-used Java libraries. In fact, all but one application in our benchmark suite had at least one vulnerability.Context sensitivity, combined with improved object naming, proved instrumental in keeping the number of false positives low. Our approach yielded very few false positives in our experiments: in fact, only one of our benchmarks suffered from false alarms.},
  file = {D\:\\GDrive\\zotero\\Livshits_Lam\\livshits_lam_2005_finding_security_vulnerabilities_in_java_applications_with_static_analysis.pdf}
}

@article{liWhatDistinguishesGreat2019,
  title = {What Distinguishes Great Software Engineers?},
  author = {Li, Paul Luo and Ko, Amy J and Begel, {$\cdot$} Andrew},
  year = {2019},
  doi = {10.1007/s10664-019-09773-y},
  abstract = {Great software engineers are essential to the creation of great software. However, today, we lack an understanding of what distinguishes great engineers from ordinary ones. We address this knowledge gap by conducting one of the largest mixed-method studies of experienced engineers to date. We surveyed 1,926 expert engineers, including senior engineers, architects , and technical fellows, asking them to judge the importance of a comprehensive set of 54 attributes of great engineers. We then conducted 77 email interviews to interpret our findings and to understand the influence of contextual factors on the ratings. After synthesizing the findings, we believe that the top five distinguishing characteristics of great engineers are writing good code, adjusting behaviors to account for future value and costs, practicing informed decision-making, avoiding making others' jobs harder, and learning continuously. We relate the findings to prior work, and discuss implications for researchers, practitioners, and educators.},
  file = {D\:\\GDrive\\zotero\\Li\\li_what_distinguishes_great_software_engineers.pdf},
  keywords = {Collaboration,Computer science education,Software development management,Software engineering}
}

@article{liWhatMakesGreat2015,
  title = {What Makes a Great Software Engineer?},
  author = {Li, Paul Luo and Ko, Andrew J. and Zhu, Jiamin},
  year = {2015},
  volume = {1},
  pages = {700--710},
  issn = {02705257},
  doi = {10.1109/ICSE.2015.335},
  abstract = {Good software engineers are essential to the creation of good software. However, most of what we know about softwareengineering expertise are vague stereotypes, such as 'excellent communicators' and 'great teammates'. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.},
  file = {D\:\\GDrive\\zotero\\Li et al\\li_et_al_2015_what_makes_a_great_software_engineer_appendix.pdf;D\:\\GDrive\\zotero\\Li et al\\li_et_al_2015_what_makes_a_great_software_engineer.pdf},
  isbn = {9781479919345},
  journal = {Proceedings - International Conference on Software Engineering},
  keywords = {expertise,Expertise,Software engineers,teamwork,Teamwork}
}

@phdthesis{liWhatMakesGreat2016,
  title = {What {{Makes}} a {{Great Software Engineer}}},
  author = {Li, Paul Luo},
  year = {2016},
  file = {D\:\\GDrive\\zotero\\Li\\li_2016_what_makes_a_great_software_engineer.pdf}
}

@techreport{lloydStrongerSemanticsLowLatency2013,
  title = {Stronger {{Semantics}} for {{Low}}-{{Latency Geo}}-{{Replicated Storage}}},
  author = {Lloyd, Wyatt and Freedman, Michael J and Kaminsky, Michael and Andersen, David G},
  year = {2013},
  abstract = {We present the first scalable, geo-replicated storage system that guarantees low latency, offers a rich data model, and provides "stronger" semantics. Namely, all client requests are satisfied in the local datacenter in which they arise; the system efficiently supports useful data model abstractions such as column families and counter columns; and clients can access data in a causally-consistent fashion with read-only and write-only transac-tional support, even for keys spread across many servers. The primary contributions of this work are enabling scalable causal consistency for the complex column-family data model, as well as novel, non-blocking algorithms for both read-only and write-only transactions. Our evaluation shows that our system, Eiger, achieves low latency (single-ms), has throughput competitive with eventually-consistent and non-transactional Cassandra (less than 7\% overhead for one of Facebook's real-world workloads), and scales out to large clusters almost linearly (averaging 96\% increases up to 128 server clusters).},
  file = {D\:\\GDrive\\zotero\\Lloyd\\lloyd_2013_stronger_semantics_for_low-latency_geo-replicated_storage.pdf}
}

@article{LLVMBackendOpen2019,
  title = {An {{LLVM}} Backend for the {{Open}}- {{Modelica Compiler}}},
  year = {2019},
  file = {D\:\\GDrive\\zotero\\undefined\\2019_an_llvm_backend_for_the_open-_modelica_compiler.pdf}
}

@misc{LLVMDoxygen,
  title = {{{LLVM Doxygen}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\H56SPRE7\\doxygen.html},
  howpublished = {https://llvm.org/doxygen/}
}

@misc{LLVMGradStudents,
  title = {{{LLVM}} for {{Grad Students}}},
  abstract = {LLVM is a godsend of a research tool. Here are some detailed notes on what LLVM is, why you would want to use it for research, and how to get started as a compiler hacker.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JJ6NVUTF\\llvm.html},
  howpublished = {https://www.cs.cornell.edu/\textasciitilde asampson/blog/llvm.html}
}

@article{LLVMInfrastructureMultiStage2002,
  title = {{{LLVM}}: {{An Infrastructure}} for {{Multi}}-{{Stage Optimization}}},
  year = {2002},
  file = {D\:\\GDrive\\zotero\\undefined\\2002_llvm.pdf}
}

@misc{LLVMLanguageReference,
  title = {{{LLVM Language Reference Manual}} \textemdash{} {{LLVM}} 13 Documentation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\6I6PLUWX\\LangRef.html},
  howpublished = {https://llvm.org/docs/LangRef.html}
}

@misc{LLVMLanguageReferencea,
  title = {{{LLVM Language Reference Manual}} \textemdash{} {{LLVM}} 13 Documentation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\RYHMB9JZ\\LangRef.html},
  howpublished = {https://llvm.org/docs/LangRef.html}
}

@misc{LLVMLLVMa,
  title = {{{LLVM}}: {{LLVM}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZHRWP3KM\\doxygen.html},
  howpublished = {https://llvm.org/doxygen/}
}

@misc{LLVMProgrammerManual,
  title = {{{LLVM Programmer}}'s {{Manual}} \textemdash{} {{LLVM}} 13 Documentation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\XHX8TS7M\\ProgrammersManual.html},
  howpublished = {https://llvm.org/docs/ProgrammersManual.html}
}

@misc{LLVMProgrammerManuala,
  title = {{{LLVM Programmer}}'s {{Manual}} \textemdash{} {{LLVM}} 13 Documentation},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\HEVKFYAP\\ProgrammersManual.html},
  howpublished = {https://llvm.org/docs/ProgrammersManual.html}
}

@phdthesis{LLVMTransformationsModel2016,
  title = {{{LLVM Transformations}} for {{Model Checking}}},
  year = {2016},
  file = {D\:\\GDrive\\zotero\\undefined\\2016_llvm_transformations_for_model_checking.pdf}
}

@article{longFullyConvolutionalNetworks,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolu-tional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmen-tation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolu-tional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolu-tional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.}
}

@techreport{LOOKINGP2P,
  title = {{{LOOKING UP}} in {{P2P}}},
  abstract = {T he main challenge in P2P computing is to design and implement a robust and scalable distributed system composed of inexpensive, individually unreliable computers in unrelated administrative domains. The participants in a typical P2P system might include computers at homes, schools, and businesses, and can grow to several million concurrent participants. P2P systems are attractive for several reasons: \textbullet{} The barriers to starting and growing such systems are low, since they usually don't require any special administrative or financial arrangements, unlike centralized facilities; \textbullet{} P2P systems offer a way to aggregate and make use of the tremendous computation and storage resources on computers across the Internet; and \textbullet{} The decentralized and distributed nature of P2P systems gives them the potential to be robust to faults or intentional attacks, making them ideal for long-term storage as well as for lengthy computations. P2P computing raises many interesting research problems in distributed systems. In this article we will look at one of them, the lookup problem. How do you find any given data item in a large P2P system in a scalable manner, without any centralized servers or hierarchy? This problem is at the heart of any P2P system. It is not addressed well by most popular systems currently in use, and it provides a good example of how the challenges of designing P2P systems can be addressed. The recent algorithms developed by several research groups for the lookup problem present a simple and general interface, a distributed hash table (DHT). Data items are inserted in a DHT and found by specifying a unique key DATA B y H a r i B a l a k r i s h n a n , M. F r a n s K a a s h o e k , D a v i d K a r g e r , R o b e r t M o r r i s , a n d I o n S t o i c a Designing and implementing a robust distribution system composed of inexpensive computers in unrelated administrative domains.},
  file = {D\:\\GDrive\\zotero\\undefined\\looking_up_in_p2p.pdf}
}

@article{lopatovskaTheoriesMethodsCurrent2011,
  title = {Theories, Methods and Current Research on Emotions in Library and Information Science, Information Retrieval and Human-Computer Interaction},
  author = {Lopatovska, Irene and Arapakis, Ioannis},
  year = {2011},
  month = jul,
  volume = {47},
  pages = {575--592},
  issn = {03064573},
  doi = {10.1016/j.ipm.2010.09.001},
  abstract = {Emotions are an integral component of all human activities, including human-computer interactions. This article reviews literature on the theories of emotions, methods for studying emotions, and their role in human information behaviour. It also examines current research on emotions in library and information science, information retrieval and human-computer interaction, and outlines some of the challenges and directions for future work. \textcopyright{} 2010 Elsevier Ltd. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Lopatovska\\lopatovska_2011_theories,_methods_and_current_research_on_emotions_in_library_and_information.pdf},
  journal = {Information Processing and Management},
  keywords = {Affect,Affective computing,Affective feedback,Emotion,Emotion research,Feelings,Methods for studying affect,Subjective variables,Theories of emotion},
  number = {4}
}

@article{loukasProtectionDenialService,
  title = {Protection against {{Denial}} of {{Service Attacks}}: {{A Survey}}},
  author = {Loukas, Georgios and Ulay{\textasciidieresis}okeulay{\textasciidieresis} Ulay{\textasciidieresis}oke, G {\textasciidieresis}},
  doi = {10.1093/comjnl/bxh000},
  abstract = {Denial of Service (DoS) is a prevalent threat in today's networks because DoS attacks are easy to launch, while defending a network resource against them is disproportionately difficult. Despite the extensive research in recent years, DoS attacks continue to harm, as the attackers adapt to the newer protection mechanisms. For this reason, we start our survey with a historical timeline of DoS incidents, where we illustrate the variety of types, targets and motives for such attacks and how they evolved during the last two decades. We then provide an extensive literature review on the existing research on denial of service protection with an emphasis on the research of the last years and the most demanding aspects of defence. These include traceback, detection, classification of incoming traffic, response in the presence of an attack, and mathematical modelling of attack and defence mechanisms. Our discussion aims to identify the trends in DoS attacks, the weaknesses of protection approaches and the qualities that modern ones should exhibit, so as to suggest new directions that DoS research can follow.},
  file = {D\:\\GDrive\\zotero\\Loukas\\loukas_protection_against_denial_of_service_attacks.pdf}
}

@article{luDataflowBendingEffectiveness2019,
  title = {Data-Flow Bending: {{On}} the Effectiveness of Data-Flow Integrity},
  shorttitle = {Data-Flow Bending},
  author = {Lu, Tingting and Wang, Junfeng},
  year = {2019},
  month = jul,
  volume = {84},
  pages = {365--375},
  issn = {01674048},
  doi = {10.1016/j.cose.2019.04.002},
  file = {D\:\\GDrive\\zotero\\Lu_Wang\\lu_wang_2019_data-flow_bending.pdf},
  journal = {Computers \& Security},
  language = {en}
}

@article{lukPinBuildingCustomized2005,
  title = {Pin: {{Building Customized Program Analysis Tools}} with {{Dynamic Instrumentation}}},
  author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim and Lowney, Geoff},
  year = {2005},
  pages = {11},
  file = {D\:\\GDrive\\zotero\\Luk et al\\luk_et_al_2005_pin.pdf},
  language = {en}
}

@article{lundellLLVMBackendTimber2010,
  title = {{{LLVM}} Back-End for the {{Timber}} Compiler},
  author = {Lundell, Mattias},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Lundell\\lundell_2010_llvm_back-end_for_the_timber_compiler.pdf},
  number = {May}
}

@article{luSequentialAggregateSignatures2013,
  title = {Sequential {{Aggregate Signatures}}, {{Multisignatures}}, and {{Verifiably Encrypted Signatures Without Random Oracles}}},
  author = {Lu, Steve and Ostrovsky, Rafail and Sahai, Amit and Shacham, Hovav and Waters, Brent},
  year = {2013},
  month = apr,
  volume = {26},
  pages = {340--373},
  issn = {0933-2790, 1432-1378},
  doi = {10.1007/s00145-012-9126-5},
  abstract = {We present the first aggregate signature, the first multisignature, and the first verifiably encrypted signature provably secure without random oracles. Our constructions derive from a novel application of a recent signature scheme due to Waters. Signatures in our aggregate signature scheme are sequentially constructed, but knowledge of the order in which messages were signed is not necessary for verification. The aggregate signatures obtained are shorter than Lysyanskaya et al. sequential aggregates and can be verified more efficiently than Boneh et al. aggregates. We also consider applications to secure routing and proxy signatures.},
  file = {D\:\\GDrive\\zotero\\Lu et al\\lu_et_al_2013_sequential_aggregate_signatures,_multisignatures,_and_verifiably_encrypted.pdf},
  journal = {Journal of Cryptology},
  language = {en},
  number = {2}
}

@inproceedings{luuMakingSmartContracts2016,
  title = {Making {{Smart Contracts Smarter}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Luu, Loi and Chu, Duc-Hiep and Olickel, Hrishi and Saxena, Prateek and Hobor, Aquinas},
  year = {2016},
  month = oct,
  pages = {254--269},
  publisher = {{ACM}},
  address = {{Vienna Austria}},
  doi = {10.1145/2976749.2978309},
  abstract = {Cryptocurrencies record transactions in a decentralized data structure called a blockchain. Two of the most popular cryptocurrencies, Bitcoin and Ethereum, support the feature to encode rules or scripts for processing transactions. This feature has evolved to give practical shape to the ideas of smart contracts, or full-fledged programs that are run on blockchains. Recently, Ethereum's smart contract system has seen steady adoption, supporting tens of thousands of contracts, holding millions dollars worth of virtual coins.},
  file = {D\:\\GDrive\\zotero\\Luu et al\\luu_et_al_2016_making_smart_contracts_smarter.pdf},
  isbn = {978-1-4503-4139-4},
  language = {en}
}

@incollection{lysyanskayaSequentialAggregateSignatures2004,
  title = {Sequential {{Aggregate Signatures}} from {{Trapdoor Permutations}}},
  booktitle = {Advances in {{Cryptology}} - {{EUROCRYPT}} 2004},
  author = {Lysyanskaya, Anna and Micali, Silvio and Reyzin, Leonid and Shacham, Hovav},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Cachin, Christian and Camenisch, Jan L.},
  year = {2004},
  volume = {3027},
  pages = {74--90},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24676-3_5},
  abstract = {An aggregate signature scheme (recently proposed by Boneh, Gentry, Lynn, and Shacham) is a method for combining n signatures from n different signers on n different messages into one signature of unit length. We propose sequential aggregate signatures, in which the set of signers is ordered. The aggregate signature is computed by having each signer, in turn, add his signature to it. We show how to realize this in such a way that the size of the aggregate signature is independent of n. This makes sequential aggregate signatures a natural primitive for certificate chains, whose length can be reduced by aggregating all signatures in a chain. We give a construction in the random oracle model based on families of certified trapdoor permutations, and show how to instantiate our scheme based on RSA.},
  file = {D\:\\GDrive\\zotero\\Lysyanskaya et al\\lysyanskaya_et_al_2004_sequential_aggregate_signatures_from_trapdoor_permutations.pdf},
  isbn = {978-3-540-21935-4 978-3-540-24676-3},
  language = {en}
}

@article{maggioniLLVMDatastructuresOverview,
  title = {{{LLVM Data}}-Structures Overview},
  author = {Maggioni, Marcello},
  pages = {31},
  file = {D\:\\GDrive\\zotero\\Maggioni\\maggioni_llvm_data-structures_overview.pdf},
  language = {en}
}

@techreport{maggsrameshksitaramandukeAlgorithmicNuggetsContent,
  title = {Algorithmic {{Nuggets}} in {{Content Delivery}}},
  author = {Maggs Ramesh K Sitaraman Duke, Bruce M and UMass, Akamai},
  abstract = {This paper "peeks under the covers" at the subsystems that provide the basic functionality of a leading content delivery network. Based on our experiences in building one of the largest distributed systems in the world, we illustrate how sophisticated algorithmic research has been adapted to balance the load between and within server clusters, manage the caches on servers, select paths through an overlay routing network, and elect leaders in various contexts. In each instance, we first explain the theory underlying the algorithms, then introduce practical considerations not captured by the theoretical models, and finally describe what is implemented in practice. Through these examples, we highlight the role of algorithmic research in the design of complex networked systems. The paper also illustrates the close synergy that exists between research and industry where research ideas cross over into products and product requirements drive future research.},
  file = {D\:\\GDrive\\zotero\\Maggs Ramesh K Sitaraman Duke\\maggs_ramesh_k_sitaraman_duke_algorithmic_nuggets_in_content_delivery.pdf}
}

@book{magnoRegisterAllocationPuzzle,
  title = {Register {{Allocation}} by {{Puzzle Solving}}},
  author = {Magno, Fernando and Pereira, Quint{\~a}o and Palsberg, Jens},
  abstract = {We show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; pre-coloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, state-of-the-art iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.},
  file = {D\:\\GDrive\\zotero\\Magno\\magno_register_allocation_by_puzzle_solving.pdf},
  isbn = {978-1-59593-860-2},
  keywords = {D34 [Processors]: Code generation General Terms Algorithms,puzzle solving,register aliasing,Theory Keywords Register allocation}
}

@techreport{maierDeprecatingObserverPattern,
  title = {Deprecating the {{Observer Pattern}}},
  author = {Maier, Ingo and Rompf, Tiark and Odersky, Martin},
  abstract = {Programming interactive systems by means of the observer pattern is hard and error-prone yet is still the implementation standard in many production environments. We present an approach to gradually deprecate observers in favor of re-active programming abstractions. Several library layers help programmers to smoothly migrate existing code from call-backs to a more declarative programming model. Our central high-level API layer embeds an extensible higher-order data-flow DSL into our host language. This embedding is enabled by a continuation passing style transformation.},
  file = {D\:\\GDrive\\zotero\\Maier\\maier_deprecating_the_observer_pattern.pdf},
  keywords = {General Terms Design,Languages Keywords data-flow language,reactive programming,Scala,user interface programming}
}

@article{maisuradzeSpeculoseAnalyzingSecurity2018,
  title = {Speculose: {{Analyzing}} the {{Security Implications}} of {{Speculative Execution}} in {{CPUs}}},
  author = {Maisuradze, Giorgi and Rossow, Christian},
  year = {2018},
  abstract = {Whenever modern CPUs encounter a conditional branch for which the condition cannot be evaluated yet, they predict the likely branch target and speculatively execute code. Such pipelining is key to optimizing runtime performance and is incorporated in CPUs for more than 15 years. In this paper, to the best of our knowledge, we are the first to study the inner workings and the security implications of such speculative execution. We revisit the assumption that speculatively executed code leaves no traces in case it is not committed. We reveal several measurable side effects that allow adversaries to enumerate mapped memory pages and to read arbitrary memory---all using only speculated code that was never fully executed. To demonstrate the practicality of such attacks, we show how a user-space adversary can probe for kernel pages to reliably break kernel-level ASLR in Linux in under three seconds and reduce the Windows 10 KASLR entropy by 18\textasciitilde bits in less than a second.},
  file = {D\:\\GDrive\\zotero\\Maisuradze\\maisuradze_2018_speculose.pdf}
}

@article{majumdarHybridConcolicTesting2007a,
  title = {Hybrid Concolic Testing},
  author = {Majumdar, Rupak and Sen, Koushik},
  year = {2007},
  pages = {416--425},
  issn = {02705257},
  doi = {10.1109/ICSE.2007.41},
  abstract = {We present hybrid concolic testing, an algorithm that interleaves random testing with concolic execution to obtain both a deep and a wide exploration of program state space. Our algorithm generates test inputs automatically by interleaving random testing until saturation with bounded exhaustive symbolic exploration of program points. It thus combines the ability of random search to reach deep program states quickly together with the ability of concolic testing to explore states in a neighborhood exhaustively. We have implemented our algorithm on top of CUTE and applied it to obtain better branch coverage for an editor implementation (VIM 5.7, 150K lines of code) as well as a data structure implementation in C. Our experiments suggest that hybrid concolic testing can handle large programs and provide, for the same testing budget, almost 4\texttimes{} the branch coverage than random testing and almost 2 \texttimes{} that of concolic testing. \textcopyright{} 2007 IEEE.},
  file = {D\:\\GDrive\\zotero\\Majumdar\\majumdar_2007_hybrid_concolic_testing.pdf;D\:\\GDrive\\zotero\\Majumdar\\majumdar_2007_hybrid_concolic_testing2.pdf},
  isbn = {0769528287},
  journal = {Proceedings - International Conference on Software Engineering},
  keywords = {Concolic testing,Directed random testing}
}

@article{Makingafastcurrypushenterversusevalapplyforhigherorderlanguages,
  title = {Making-a-Fast-Curry-Push-Enter-versus-Eval-Apply-for-Higher-Order-Languages},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PPDZANFE\\making-a-fast-curry-push-enter-versus-eval-apply-for-higher-order-languages.pdf}
}

@book{malewiczPregelSystemLargeScale2010,
  title = {Pregel: {{A System}} for {{Large}}-{{Scale Graph Processing}}},
  author = {Malewicz, Grzegorz and Austern, Matthew H and Bik, Aart J C and Dehnert, James C and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
  year = {2010},
  doi = {10.1145/1807167.1807184},
  abstract = {Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs-in some cases billions of vertices, trillions of edges-poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scal-able and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronic-ity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.},
  file = {D\:\\GDrive\\zotero\\Elmagarmid\\elmagarmid_2010_pregel.pdf},
  isbn = {978-1-4503-0032-2},
  keywords = {distributed systems}
}

@techreport{manichandyDistributedSnapshotsDetermining,
  title = {Distributed {{Snapshots}}: {{Determining Global States}} of {{Distributed Systems}}},
  author = {Mani Chandy, K},
  abstract = {This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are "computation has terminated," " the system is deadlocked" and "all tokens in a token ring have disappeared." The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing.},
  file = {D\:\\GDrive\\zotero\\Mani Chandy\\mani_chandy_distributed_snapshots.pdf},
  keywords = {C24 [Computer-Communication Networks]: Distributed Systems-distributed applications,checkpoint/restart,D41 [Operating Systems]: Process Management-concurrency,D45 [Operating Systems]: Reliability-backup procedures,deadlocks; multiprocessing/multiprogramming,distributed databases,fault-tolerance,mutual exclusion,network operating systems,scheduling,synchronization,verification General Terms: Algorithms Additional Key Words and Phrases: Global States; Distributed deadlock detection; distributed systems; message communication systems}
}

@phdthesis{mansinghkaNativelyProbabilisticComputation2009a,
  title = {Natively {{Probabilistic Computation}}},
  author = {Mansinghka, Vikash Kumar},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Mansinghka\\mansinghka_2009_natively_probabilistic_computation.pdf;D\:\\GDrive\\zotero\\Mansinghka\\mansinghka_2009_natively_probabilistic_computation2.pdf}
}

@phdthesis{mantriSecureDelegatedQuantum,
  title = {Secure Delegated Quantum Computing},
  author = {Mantri, Atul},
  abstract = {Consider a scenario where a computationally weak party, the client, wishes to delegate his/her desired quantum computation to a more powerful party, the server. While delegating expensive tasks to quantum computers in the cloud seems promising, it raises an inevitable concern: How can a client ensure the privacy of their computation as well as the input data and at the same time verify that the output is indeed correct? In this dissertation, I investigate the task of the secure delegation of quantum computation or blind quantum computation. To do this, I introduce secure protocols in the client-server setting and analyze them under the information-theoretic security approach.},
  file = {D\:\\GDrive\\zotero\\Mantri\\mantri_secure_delegated_quantum_computing.pdf},
  language = {en}
}

@article{manualDDI0388ECortexA92009,
  title = {{{DDI0388E}}\_cortex\_a9\_r2p0\_trm},
  author = {Manual, Technical Reference},
  year = {2009},
  pages = {1--266},
  file = {D\:\\GDrive\\zotero\\Manual\\manual_2009_ddi0388e_cortex_a9_r2p0_trm.pdf}
}

@techreport{marangetCompilingPatternMatching,
  title = {Compiling {{Pattern Matching}} to Good {{Decision Trees}}},
  author = {Maranget, Luc},
  abstract = {We address the issue of compiling ML pattern matching to efficient decisions trees. Traditionally, compilation to decision trees is optimized by (1) implementing decision trees as dags with maximal sharing; (2) guiding a simple compiler with heuristics. We first design new heuristics that are inspired by necessity, a notion from lazy pattern matching that we rephrase in terms of decision tree semantics. Thereby, we simplify previous semantical frameworks and demonstrate a direct connection between necessity and decision tree runtime efficiency. We complete our study by experiments, showing that optimized compilation to decision trees is competitive. We also suggest some heuristics precisely.},
  file = {D\:\\GDrive\\zotero\\Maranget\\maranget_compiling_pattern_matching_to_good_decision_trees.pdf},
  keywords = {D 3 3 [Programming Lan-guages]: Language Constructs and Features-Patterns General Terms Design,Decision Trees,Heuristics,Performance,Sequentiality Keywords Match Compilers}
}

@techreport{marangetWarningsPatternMatching,
  title = {Warnings for Pattern Matching},
  author = {Maranget, Luc},
  abstract = {We examine the ML pattern-matching anomalies of useless clauses and non-exhaustive matches. We state the definition of these anomalies, building upon pattern matching semantics , and propose a simple algorithm to detect them. We have integrated the algorithm in the Objective Caml compiler, but we show that the same algorithm is also usable in a non-strict language such as Haskell. Or-patterns are considered for both strict and non-strict languages.},
  file = {D\:\\GDrive\\zotero\\Maranget\\maranget_warnings_for_pattern_matching.pdf}
}

@article{marforioApplicationCollusionAttack2011,
  title = {Application {{Collusion Attack}} on the {{Permission}}-{{Based Security Model}} and Its {{Implications}} for {{Modern Smartphone Systems}}},
  author = {Marforio, Claudio and Francillon, Aur{\'e}lien and Capkun, Srdjan},
  year = {2011},
  volume = {15},
  pages = {12--19},
  doi = {10.3929/ethz-a-010782581},
  abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Es- tablished leaders in the field are the SIFT and SURF al- gorithms which exhibit great performance under a variety of image transformations, with SURF in particular consid- ered as the most computationally efficient amongst the high- performance methods to date. In this paper we propose BRISK1, a novel method for keypoint detection, description and matching. A compre- hensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art al- gorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.},
  file = {D\:\\GDrive\\zotero\\Marforio\\marforio_2011_application_collusion_attack_on_the_permission-based_security_model_and_its.pdf},
  isbn = {8610828378018},
  journal = {BRISK Binary Robust Invariant Scalable Keypoints},
  number = {3}
}

@article{marinescuHighcoverageSymbolicPatch2012,
  title = {High-Coverage Symbolic Patch Testing},
  author = {Marinescu, Paul Dan and Cadar, Cristian},
  year = {2012},
  volume = {7385 LNCS},
  pages = {7--21},
  issn = {03029743},
  doi = {10.1007/978-3-642-31759-0_2},
  abstract = {Software patches are often poorly tested, with many of them containing faults that affect the correct operation of the software. In this paper, we propose an automatic technique based on symbolic execution, that aims to increase the quality of patches by providing developers with an automated mechanism for generating a set of comprehensive test cases covering all or most of the statements in a software patch. Our preliminary evaluation of this technique has shown promising results on several real patches from the lighttpd web server. \textcopyright{} 2012 Springer-Verlag Berlin Heidelberg.},
  file = {D\:\\GDrive\\zotero\\Marinescu\\marinescu_2012_high-coverage_symbolic_patch_testing.pdf},
  isbn = {9783642317583},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{martinImplementingOmegaFailure2009,
  title = {Implementing the {{Omega}} Failure Detector in the Crash-Recovery Failure Model},
  author = {Mart{\'i}n, Cristian and Larrea, Mikel and Jim{\'e}nez, Ernesto},
  year = {2009},
  month = may,
  volume = {75},
  pages = {178--189},
  issn = {00220000},
  doi = {10.1016/j.jcss.2008.10.002},
  abstract = {Unreliable failure detectors are mechanisms providing information about process failures, that allow to solve several problems in asynchronous systems, e.g., Consensus. A particular failure detector, Omega, provides an eventual leader election functionality. This paper addresses the implementation of Omega in the crash-recovery failure model. We first propose an algorithm assuming that processes are reachable from the correct process that crashes and recovers a minimum number of times. Then, we propose two algorithms which assume only that processes are reachable from some correct process. Besides this, one of the algorithms requires the membership to be known a priori, while the other two do not. \textcopyright{} 2008 Elsevier Inc. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Martín\\martín_2009_implementing_the_omega_failure_detector_in_the_crash-recovery_failure_model.pdf},
  journal = {Journal of Computer and System Sciences},
  keywords = {Consensus,Distributed algorithms,Eventual leader election,Omega failure detector},
  number = {3}
}

@techreport{MartinLofMEANINGSLOGICAL,
  title = {Per {{Martin}}-{{L\"of ON THE MEANINGS OF THE LOGICAL CONSTANTS AND THE JUSTIFICATIONS OF THE LOGICAL LAWS}}},
  abstract = {Preface The following three lectures were given in the form of a short course at the meeting Teoria della Dimostrazione e Filosofia della Logica, organized in Siena, 6-9 April 1983, by the Scuola di Specializzazione in Logica Matematica of the Universit\`a degli Studi di Siena. I am very grateful to Giovanni Sambin and Aldo Ursini of that school, not only for recording the lectures on tape, but, above all, for transcribing the tapes produced by the recorder: no machine could have done that work. This written version of the lectures is based on their transcription. The changes that I have been forced to make have mostly been of a stylistic nature, except at one point. In the second lecture, as I actually gave it, the order of conceptual priority between the notions of proof and immediate inference was wrong. Since I discovered my mistake later the same month as the meeting was held, I thought it better to let the written text diverge from the oral presentation rather than possibly confusing others by letting the mistake remain. The oral origin of these lectures is the source of the many redundancies of the written text. It is also my sole excuse for the lack of detailed references. First lecture When I was asked to give these lectures about a year ago, I suggested the title On the Meanings of the Logical Constants and the Justifications of the Logical Laws. So that is what I shall talk about, eventually, but, first of all, I shall have to say something about, on the one hand, the things that the logical operations operate on, which we normally call propositions and propositional functions, and, on the Principia, for instance. Thus we have two kinds of entities here: we have the entities that the logical operations operate on, which we call propositions, and we have those that we prove and that appear as premises and conclusion of a logical inference, which we call assertions. It turns out that, in order to clarify the meanings of the logical constants and justify the logical laws, a considerable portion of the philosophical work lies already in clarifying the notion of proposition and the notion of assertion. Accordingly, a large part of my lectures will be taken up by a philosophical analysis of these two notions. Let us first look at the term proposition. It has its origin in the Gr. , used by Aristotle in the Prior Analytics, the third part of the Organon. It was translated, apparently by Cicero, into Lat. propositio, which has its modern counterparts in It. proposizione, Eng. proposition and Ger. Satz. In the old, traditional use of the word proposition, propositions are the things that we prove. We talk about proposition and proof, of course, in mathematics: we put up a proposition and let it be followed by its proof. In particular, the premises and conclusion of an inference were propositions in this old terminology. It was the standard use of the word up to the last century. And it is this use which is retained in mathematics, where a theorem is sometimes called a proposition, sometimes a theorem. Thus we have two words for the things that we prove, proposition and theorem. The word proposition, Gr. , comes from Aristotle and has dominated the logical tradition , whereas the word theorem, Gr. , is in Euclid, I believe, and has dominated the mathematical tradition. With Kant, something important happened, namely, that the term judgement, Ger. Urteil, came to be used instead of proposition.},
  file = {D\:\\GDrive\\zotero\\undefined\\per_martin-löf_on_the_meanings_of_the_logical_constants_and_the_justifications.pdf}
}

@article{maskiewiczMouseTrapExploiting2014,
  title = {Mouse {{Trap}}: {{Exploiting Firmware Updates}} in {{USB Peripherals}}},
  author = {Maskiewicz, Jacob and Ellis, Benjamin and Mouradian, James and Shacham, Hovav},
  year = {2014},
  pages = {10},
  abstract = {Although many users are aware of the threats that malware pose, users are unaware that malware can infect peripheral devices. Many embedded devices support firmware update capabilities, yet they do not authenticate such updates; this allows adversaries to infect peripherals with malicious firmware. We present a case study of the Logitech G600 mouse, demonstrating attacks on networked systems which are also feasible against airgapped systems.},
  file = {D\:\\GDrive\\zotero\\Maskiewicz et al\\maskiewicz_et_al_2014_mouse_trap.pdf},
  language = {en}
}

@article{mathisCongestionControlHigh2002,
  title = {Congestion {{Control}} for {{High Bandwidth}}-{{Delay Product Networks}}},
  author = {Mathis, Matt. and {ACM Digital Library.} and {ACM Special Interest Group on Data Communication.}},
  year = {2002},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Mathis\\mathis_2002_congestion_control_for_high_bandwidth-delay_product_networks.pdf}
}

@article{mathisNewDirectionsTraffic2002,
  title = {New {{Directions}} in {{Traffic Measurement}} and {{Accounting}}},
  author = {Mathis, Matt. and {ACM Digital Library.} and {ACM Special Interest Group on Data Communication.}},
  year = {2002},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Mathis\\mathis_2002_new_directions_in_traffic_measurement_and_accounting.pdf}
}

@article{mathisTussleCyberspaceDefining2002,
  title = {Tussle in {{Cyberspace}}: {{Defining Tomorrow}}'s {{Internet}}},
  author = {Mathis, Matt. and {ACM Digital Library.} and {ACM Special Interest Group on Data Communication.}},
  year = {2002},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Mathis\\mathis_2002_tussle_in_cyberspace.pdf}
}

@article{mathurSWaTWaterTreatment2016,
  title = {{{SWaT}}: {{A}} Water Treatment Testbed for Research and Training on {{ICS}} Security},
  author = {Mathur, Aditya P. and Tippenhauer, Nils Ole},
  year = {2016},
  pages = {31--36},
  publisher = {{IEEE}},
  doi = {10.1109/CySWater.2016.7469060},
  abstract = {This paper presents the SWaT testbed, a modern industrial control system (ICS) for security research and training. SWaT is currently in use to (a) understand the impact of cyber and physical attacks on a water treatment system, (b) assess the effectiveness of attack detection algorithms, (c) assess the effectiveness of defense mechanisms when the system is under attack, and (d) understand the cascading effects of failures in one ICS on another dependent ICS. SWaT consists of a 6-stage water treatment process, each stage is autonomously controlled by a local PLC. The local fieldbus communications between sensors, actuators, and PLCs is realized through alternative wired and wireless channels. While the experience with the testbed indicates its value in conducting research in an active and realistic environment, it also points to design limitations that make it difficult for system identification and attack detection in some experiments.},
  file = {D\:\\GDrive\\zotero\\Mathur\\mathur_2016_swat.pdf},
  isbn = {9781509011612},
  journal = {2016 International Workshop on Cyber-physical Systems for Smart Water Networks, CySWater 2016},
  keywords = {Cyber Attacks,Cyber Defense,Cyber Physical Systems,Industrial Control Systems,Water Testbed},
  number = {Figure 1}
}

@article{mauriceHelloOtherSide2017,
  title = {Hello from the {{Other Side}}: {{SSH}} over {{Robust Cache Covert Channels}} in the {{Cloud}}},
  author = {Maurice, Clementine and Weber, Manuel and Schwarz, Michael and Giner, Lukas and Gruss, Daniel and Boano, Carlo Alberto and Mangard, Stefan and Roemer, Kay and Mangard, Stefan},
  year = {2017},
  doi = {10.14722/ndss.2017.23294},
  abstract = {Covert channels evade isolation mechanisms be- tween multiple parties in the cloud. Especially cache covert channels allow the transmission of several hundred kilobits per second between unprivileged user programs in separate virtual machines. However, caches are small and shared and thus cache-based communication is susceptible to noise from any system activity and interrupts. The feasibility of a reliable cache covert channel under a severe noise scenario has not been demonstrated yet. Instead, previous work relies on either of the two contradicting assumptions: the assumption of direct applicability of error-correcting codes, or the assumption that noise effectively prevents covert channels. In this paper, we show that both assumptions are wrong. First, error-correcting codes cannot be applied directly, due to the noise characteristics. Second, even with extraordinarily high system activity, we demonstrate an error-free and high- throughput covert channel. We provide the first comprehensive characterization of noise on cache covert channels due to cache activity and interrupts. We build the first robust covert channel based on established techniques from wireless transmission proto- cols, adapted for our use in microarchitectural attacks. Our error- correcting and error-handling high-throughput covert channel can sustain transmission rates of more than 45 KBps on Amazon EC2, which is 3 orders of magnitude higher than previous covert channels demonstrated on Amazon EC2. Our robust and error- free channel even allows us to build an SSH connection between two virtual machines, where all existing covert channels fail.},
  file = {D\:\\GDrive\\zotero\\Maurice\\maurice_2017_hello_from_the_other_side.pdf},
  number = {March}
}

@techreport{mayerECDSASecurityBitcoin2016,
  title = {{{ECDSA Security}} in {{Bitcoin}} and {{Ethereum}}: A {{Research Survey}}},
  author = {Mayer, Hartwig},
  year = {2016},
  abstract = {This survey discusses the security level of the signature scheme implemented in Bitcoin and Ethereum.},
  file = {D\:\\GDrive\\zotero\\Mayer\\mayer_2016_ecdsa_security_in_bitcoin_and_ethereum.pdf}
}

@techreport{mayerTranscendence2006,
  title = {The {{Transcendence}} of {$\pi$}},
  author = {Mayer, Steve},
  year = {2006},
  abstract = {The proof that {$\pi$} is transcendental is not well-known despite the fact that it isn't too difficult for a university mathematics student to follow. The purpose of this paper is to make the proof more widely available. A bonus is that the proof also shows that e is transcendental as well. The material in these notes are not mine; it is taken from a supplement issued by Ian Stewart as an adjunct to a Rings and Fields course in 1970 at the University of Warwick.},
  file = {D\:\\GDrive\\zotero\\Mayer\\mayer_2006_the_transcendence_of_π.pdf}
}

@techreport{mcbrideDerivativeRegularType,
  title = {The {{Derivative}} of a {{Regular Type}} Is Its {{Type}} of {{One}}-{{Hole Contexts}}},
  author = {Mcbride, Conor},
  abstract = {Polymorphic regular types are tree-like datatypes generated by polynomial type expressions over a set of free variables and closed under least fixed point. The 'equal-ity types' of Core ML can be expressed in this form. Given such a type expression \textexclamdown{} with \textcent{} free, this paper shows a way to represent the one-hole contexts for elements of \textcent{} within elements of \textexclamdown{} , together with an operation which will plug an element of \textcent{} into the hole of such a context. One-hole contexts are given as inhabitants of a regular type \textsterling{} \textyen{} \textcurrency{} \textexclamdown{} , computed generically from the syntactic structure of \textexclamdown{} by a mechanism better known as partial differentiation. The relevant notion of containment is shown to be appropriately characterized in terms of derivatives and plugging in. The technology is then exploited to give the one-hole contexts for sub-elements of recursive types in a manner similar to Huet's 'zippers'[Hue97].},
  file = {D\:\\GDrive\\zotero\\Mcbride\\mcbride_the_derivative_of_a_regular_type_is_its_type_of_one-hole_contexts.pdf}
}

@techreport{mccanneBSDPacketFilter1992,
  title = {The {{BSD Packet Filter}}: {{A New Architecture}} for {{User}}-Level {{Packet Capture}}},
  author = {Mccanne, Steven and Jacobson, Van},
  year = {1992},
  abstract = {Many versions of Unix provide facilities for user-level packet capture, making possible the use of general purpose workstations for network monitoring. Because network monitors run as user-level processes, packets must be copied across the kernel/user-space protection boundary. This copying can be minimized by deploying a kernel agent called a packet filter, which discards unwanted packets as early as possible. The original Unix packet filter was designed around a stack-based filter evaluator that performs sub-optimally on current RISC CPUs. The BSD Packet Filter (BPF) uses a new, register-based filter evaluator that is up to 20 times faster than the original design. BPF also uses a straightforward buffering strategy that makes its overall performance up to 100 times faster than Sun's NIT running on the same hardware.}
}

@misc{MccannePdf,
  title = {Mccanne.{{Pdf}}}
}

@techreport{mccarthyRecursiveFunctionsSymbolic1960,
  title = {Recursive {{Functions}} of {{Symbolic Expressions}} and {{Their Computation}} by {{Machine}}, {{Part I}}},
  author = {Mccarthy, John},
  year = {1960},
  file = {D\:\\GDrive\\zotero\\Mccarthy\\mccarthy_1960_recursive_functions_of_symbolic_expressions_and_their_computation_by_machine,.pdf}
}

@article{mccorrySoKChainTransactions2019,
  title = {{{SoK}}: {{Off The Chain Transactions}}},
  author = {McCorry, Patrick and {Moreno-Sanchez}, Pedro and Wien, TU and Gervais, Arthur and Roos, Stefanie and Delft, TU},
  year = {2019},
  pages = {17},
  abstract = {Blockchains have the potential to revolutionize markets and services, yet, currently exhibit high latencies and fail to handle loads comparable to those managed by traditional custodian financial systems. Layer-two protocols, built on top of (layer-one) blockchains, avoid disseminating every transaction to the whole network by sending transactions off-chain and instead utilize the blockchain only as a recourse for disputes. The promise of layer-two protocols is to complete transactions in sub-seconds, reduce fees, and allow blockchains to scale.},
  file = {D\:\\GDrive\\zotero\\McCorry et al\\mccorry_et_al_2019_sok.pdf},
  language = {en}
}

@inproceedings{mccuneFlicker2008,
  title = {Flicker},
  booktitle = {Proceedings of the 3rd {{ACM SIGOPS}}/{{EuroSys European Conference}} on {{Computer Systems}} 2008  - {{Eurosys}} '08},
  author = {McCune, Jonathan M. and Parno, Bryan J. and Perrig, Adrian and Reiter, Michael K. and Isozaki, Hiroshi},
  year = {2008},
  pages = {315},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/1352592.1352625},
  file = {D\:\\GDrive\\zotero\\McCune\\mccune_2008_flicker.pdf},
  isbn = {978-1-60558-013-5}
}

@techreport{mccuneTrustVisorEfficientTCB2009,
  title = {{{TrustVisor}}: {{Efficient TCB Reduction}} and {{Attestation TrustVisor}}: {{Efficient TCB Reduction}} and {{Attestation}} *},
  author = {Mccune, Jonathan M and Qu, Ning and Li, Yanlin and Datta, Anupam and Gligor, Virgil D and Perrig, Adrian and Zhou, Zongwei and Gligor, Virgil},
  year = {2009},
  abstract = {An important security challenge is to protect the execution of security-sensitive code on legacy systems from malware that may infect the OS, applications, or system devices. Prior work experienced a tradeoff between the level of security achieved and efficiency. In this work, we leverage the features of modern processors from AMD and Intel to overcome the tradeoff to simultaneously achieve a high level of security and high performance. We present TrustVisor, a special-purpose hypervisor that provides code integrity as well as data integrity and secrecy for selected portions of an application. TrustVisor achieves a high level of security, first because it can protect sensitive code at a very fine granularity, and second because it has a very small code base (only around 6K lines of code) that makes verification feasible. TrustVisor can also attest the existence of isolated execution to an external entity. We have implemented TrustVisor to protect security-sensitive code blocks while imposing less than 7\% overhead on the legacy OS and its applications in the common case.},
  file = {D\:\\MEGA\\zotero\\Mccune\\mccune_2009_trustvisor.pdf}
}

@techreport{mckeenInnovativeInstructionsSoftware,
  title = {Innovative {{Instructions}} and {{Software Model}} for {{Isolated Execution}}},
  author = {Mckeen, Frank and Alexandrovich, Ilya and Berenzon, Alex and Rozas, Carlos and Shafi, Hisham and Shanbhogue, Vedvyas and Savagaonkar, Uday},
  abstract = {For years the PC community has struggled to provide secure solutions on open platforms. Intel has developed innovative new technology to enable SW developers to develop and deploy secure applications on open platforms. The technology enables applications to execute with confidentiality and integrity in the native OS environment. It does this by providing ISA extensions for generating hardware enforceable containers at a granularity determined by the developer. These containers while opaque to the operating system are managed by the OS. This paper analyzes the threats and attacks to applications. It then describes the ISA extension for generating a HW based container. Finally it describes the programming model of this container.}
}

@techreport{mckeownOpenFlowEnablingInnovation,
  title = {{{OpenFlow}}: {{Enabling Innovation}} in {{Campus Networks}}},
  author = {Mckeown, Nick and Anderson, Tom and Balakrishnan, Hari and Parulkar, Guru and Peterson, Larry and Rexford, Jennifer and Shenker, Scott and Turner, Jonathan},
  abstract = {This whitepaper proposes OpenFlow: a way for researchers to run experimental protocols in the networks they use every day. OpenFlow is based on an Ethernet switch, with an internal flow-table, and a standardized interface to add and remove flow entries. Our goal is to encourage networking vendors to add OpenFlow to their switch products for deployment in college campus backbones and wiring closets. We believe that OpenFlow is a pragmatic compromise: on one hand, it allows researchers to run experiments on heterogeneous switches in a uniform way at line-rate and with high port-density; while on the other hand, vendors do not need to expose the internal workings of their switches. In addition to allowing researchers to evaluate their ideas in real-world traffic settings, OpenFlow could serve as a useful campus component in proposed large-scale testbeds like GENI. Two buildings at Stanford University will soon run OpenFlow networks, using commercial Ethernet switches and routers. We will work to encourage deployment at other schools; and We encourage you to consider deploying OpenFlow in your university network too.},
  file = {D\:\\GDrive\\zotero\\Mckeown\\mckeown_openflow.pdf},
  keywords = {C2 [Internetworking]: Routers General Terms Experimentation,Design Keywords Ethernet switch,flow-based,virtualization}
}

@article{mcmillanInterpolationModelChecking2018,
  title = {Interpolation and Model Checking},
  author = {McMillan, Kenneth L.},
  year = {2018},
  pages = {421--446},
  doi = {10.1007/978-3-319-10575-8_14},
  abstract = {In this chapter we consider applications of logical proofs in model checking. Here we are not concerned with using model checking to verify steps in a larger proof but rather with ways in which logical proof methods can aid model checking, particularly in focusing model-checking methods on relevant facts. We introduce a framework for abstraction refinement based on a concept of deductive generalization. We then show how various abstraction refinement schemes can be understood in this framework in terms of local proofs and Craig interpolation. This unifying view exposes the trade-offs made in different systems between the quality and cost of refinements, and also leads to novel model-checking approaches.},
  file = {D\:\\GDrive\\zotero\\McMillan\\mcmillan_2018_interpolation_and_model_checking.pdf},
  isbn = {9783319105758},
  journal = {Handbook of Model Checking}
}

@techreport{mcsherryScalabilityWhatCOST,
  title = {Scalability! {{But}} at What {{COST}}?},
  author = {Mcsherry, Frank and Isard, Michael and Murray, Derek G and Microsoft, Unaffiliated and Unaffiliated, Research},
  abstract = {We offer a new metric for big data platforms, COST, or the Configuration that Outperforms a Single Thread. The COST of a given platform for a given problem is the hardware configuration required before the platform out-performs a competent single-threaded implementation. COST weighs a system's scalability against the overheads introduced by the system, and indicates the actual performance gains of the system, without rewarding systems that bring substantial but parallelizable overheads. We survey measurements of data-parallel systems recently reported in SOSP and OSDI, and find that many systems have either a surprisingly large COST, often hundreds of cores, or simply underperform one thread for all of their reported configurations.},
  file = {D\:\\GDrive\\zotero\\Mcsherry\\mcsherry_scalability.pdf}
}

@article{mcsherryScalabilityWhatCOSTa,
  title = {Scalability! {{But}} at What {{COST}}?},
  author = {McSherry, Frank and Isard, Michael and Murray, Derek G},
  pages = {6},
  abstract = {We offer a new metric for big data platforms, COST, or the Configuration that Outperforms a Single Thread. The COST of a given platform for a given problem is the hardware configuration required before the platform outperforms a competent single-threaded implementation. COST weighs a system's scalability against the overheads introduced by the system, and indicates the actual performance gains of the system, without rewarding systems that bring substantial but parallelizable overheads.},
  file = {D\:\\GDrive\\zotero\\McSherry et al\\mcsherry_et_al_scalability.pdf},
  language = {en}
}

@techreport{mechanismsozalpCMConsistentGlobal1993,
  title = {{{CM Consistent Global States}} of {{Distributed Systems}}: {{Fundamental Concepts}} and {{Mechanisms}}\textasciidieresis{{Ozalp}}},
  author = {Mechanisms{\textasciidieresis}ozalp, Mechanisms{\textasciidieresis} and Glu, Babao {\textasciibreve} and Marzullo, Keith},
  year = {1993},
  file = {D\:\\GDrive\\zotero\\Mechanisms¨ozalp\\mechanisms¨ozalp_1993_cm_consistent_global_states_of_distributed_systems.pdf}
}

@techreport{meijerFunctionalProgrammingBananas,
  title = {Functional {{Programming}} with {{Bananas}}, {{Lenses}}, {{Envelopes}} and {{Barbed Wire}}},
  author = {Meijer, Erik and Fokkinga, Maarten and Paterson, Ross},
  abstract = {We develop a calculus for lazy functional programming based on recursion operators associated with data type deenitions. For these operators we derive various algebraic laws that are useful in deriving and manipulating programs. We shall show that all example functions in Bird and Wadler's \textbackslash Introduction to Functional Programming" can be expressed using these operators.}
}

@misc{meijerTechnicalOverviewCommon2000,
  title = {Technical {{Overview}} of the {{Common Language Runtime}}},
  author = {Meijer, Erik and Wa, Redmond and Gough, John},
  year = {2000},
  abstract = {The functionality of the recently announced Microsoft .NET system is founded on the capabilities of the Common Language Infrastructure (CLI). Unlike some other recent systems based on virtual machines, the CLI was designed from the start to support a wide range of programming languages. It is also expected that ECMA standardization will make the CLI available on a wide range of computing platforms. This combination of multi-language capability and multiplatform implementation make the CLI an important target for future language compilers.},
  file = {D\:\\GDrive\\zotero\\Meijer et al\\meijer_et_al_2000_technical_overview_of_the_common_language_runtime.pdf;C\:\\Users\\Admin\\Zotero\\storage\\I5FMRET4\\summary.html}
}

@article{meiklejohnFistfulBitcoinsCharacterizing2013,
  title = {A {{Fistful}} of {{Bitcoins}}: {{Characterizing Payments Among Men}} with {{No Names}}},
  author = {Meiklejohn, Sarah and Pomarole, Marjori and Jordan, Grant and Levchenko, Kirill and Mccoy, Damon and Voelker, Geoffrey M and Savage, Stefan},
  year = {2013},
  doi = {10.1145/2504730.2504747},
  abstract = {Bitcoin is a purely online virtual currency, unbacked by either physical commodities or sovereign obligation; instead, it relies on a combination of cryptographic protection and a peer-to-peer protocol for witnessing settlements. Consequently, Bitcoin has the un-intuitive property that while the ownership of money is implicitly anonymous, its flow is globally visible. In this paper we explore this unique characteristic further, using heuristic clustering to group Bitcoin wallets based on evidence of shared authority, and then using re-identification attacks (i.e., empirical purchasing of goods and services) to classify the operators of those clusters. From this analysis , we characterize longitudinal changes in the Bitcoin market, the stresses these changes are placing on the system, and the challenges for those seeking to use Bitcoin for criminal or fraudulent purposes at scale.},
  file = {D\:\\GDrive\\zotero\\Meiklejohn\\meiklejohn_2013_a_fistful_of_bitcoins.pdf},
  isbn = {9781450319539},
  keywords = {Anonymity,K44 [Electronic Commerce]: Payment schemes Keywords Bitcoin,Measurement}
}

@incollection{meiklejohnLimitationsTransformationsCompositeOrder2010,
  title = {Limitations on {{Transformations}} from {{Composite}}-{{Order}} to {{Prime}}-{{Order Groups}}: {{The Case}} of {{Round}}-{{Optimal Blind Signatures}}},
  shorttitle = {Limitations on {{Transformations}} from {{Composite}}-{{Order}} to {{Prime}}-{{Order Groups}}},
  booktitle = {Advances in {{Cryptology}} - {{ASIACRYPT}} 2010},
  author = {Meiklejohn, Sarah and Shacham, Hovav and Freeman, David Mandell},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Abe, Masayuki},
  year = {2010},
  volume = {6477},
  pages = {519--538},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17373-8_30},
  abstract = {Beginning with the work of Groth and Sahai, there has been much interest in transforming pairing-based schemes in composite-order groups to equivalent ones in prime-order groups. A method for achieving such transformations has recently been proposed by Freeman, who identified two properties of pairings using composite-order groups \textemdash{} ``cancelling'' and ``projecting'' \textemdash on which many schemes rely, and showed how either of these properties can be obtained using prime-order groups.},
  file = {D\:\\GDrive\\zotero\\Meiklejohn et al\\meiklejohn_et_al_2010_limitations_on_transformations_from_composite-order_to_prime-order_groups.pdf},
  isbn = {978-3-642-17372-1 978-3-642-17373-8},
  language = {en}
}

@incollection{meiklejohnLimitationsTransformationsCompositeOrder2010a,
  title = {Limitations on {{Transformations}} from {{Composite}}-{{Order}} to {{Prime}}-{{Order Groups}}: {{The Case}} of {{Round}}-{{Optimal Blind Signatures}}},
  shorttitle = {Limitations on {{Transformations}} from {{Composite}}-{{Order}} to {{Prime}}-{{Order Groups}}},
  booktitle = {Advances in {{Cryptology}} - {{ASIACRYPT}} 2010},
  author = {Meiklejohn, Sarah and Shacham, Hovav and Freeman, David Mandell},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Abe, Masayuki},
  year = {2010},
  volume = {6477},
  pages = {519--538},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17373-8_30},
  abstract = {Beginning with the work of Groth and Sahai, there has been much interest in transforming pairing-based schemes in composite-order groups to equivalent ones in prime-order groups. A method for achieving such transformations has recently been proposed by Freeman, who identified two properties of pairings using composite-order groups \textemdash{} ``cancelling'' and ``projecting'' \textemdash on which many schemes rely, and showed how either of these properties can be obtained using prime-order groups.},
  file = {D\:\\GDrive\\zotero\\Meiklejohn et al\\meiklejohn_et_al_2010_limitations_on_transformations_from_composite-order_to_prime-order_groups2.pdf},
  isbn = {978-3-642-17372-1 978-3-642-17373-8},
  language = {en}
}

@article{meiklejohnPhantomTollboothPrivacyPreserving2011,
  title = {The {{Phantom Tollbooth}}: {{Privacy}}-{{Preserving Electronic Toll Collection}} in the {{Presence}} of {{Driver Collusion}}},
  author = {Meiklejohn, Sarah and Mowery, Keaton and Checkoway, Stephen and Shacham, Hovav},
  year = {2011},
  pages = {16},
  abstract = {In recent years, privacy-preserving toll collection has been proposed as a way to resolve the tension between the desire for sophisticated road pricing schemes and drivers' interest in maintaining the privacy of their driving patterns. Two recent systems in particular, VPriv (USENIX Security 2009) and PrETP (USENIX Security 2010), use modern cryptographic primitives to solve this problem. In order to keep drivers honest in paying for their usage of the roads, both systems rely on unpredictable spot checks (e.g., by hidden roadside cameras or roaming police vehicles) to catch potentially cheating drivers. In this paper we identify large-scale driver collusion as a threat to the necessary unpredictability of these spot checks. Most directly, the VPriv and PrETP audit protocols both reveal to drivers the locations of spot-check cameras \textemdash{} information that colluding drivers can then use to avoid paying road fees. We describe Milo, a new privacy-preserving toll collection system based on PrETP, whose audit protocol does not have this information leak, even when drivers misbehave and collude. We then evaluate the additional cost of Milo and find that, when compared to na\"ive methods to protect against cheating drivers, Milo offers a significantly more cost-effective approach.},
  file = {D\:\\GDrive\\zotero\\Meiklejohn et al\\meiklejohn_et_al_2011_the_phantom_tollbooth.pdf},
  language = {en}
}

@techreport{melnikDremelInteractiveAnalysis2150,
  title = {Dremel: {{Interactive Analysis}} of {{Web}}-{{Scale Datasets}}},
  author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo},
  year = {2150},
  abstract = {Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggrega-tion queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.},
  file = {D\:\\GDrive\\zotero\\Melnik et al\\melnik_et_al_2150_dremel.pdf},
  keywords = {distributed systems}
}

@article{menendezPracticalFormalTechniques2018,
  title = {Practical {{Formal Techniques}} and {{Tools}} for {{Developing Llvm}} ' {{S Peephole}}},
  author = {Menendez, David},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Menendez\\menendez_2018_practical_formal_techniques_and_tools_for_developing_llvm_’_s_peephole.pdf}
}

@article{mengMLlibMachineLearning,
  title = {{{MLlib}}: {{Machine Learning}} in {{Apache Spark}}},
  author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
  pages = {7},
  abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.},
  file = {D\:\\GDrive\\zotero\\Meng et al\\meng_et_al_mllib.pdf},
  language = {en}
}

@techreport{meredithHowSuccessfulData2005,
  title = {How {{Successful Is Data Structure Analysis}} in {{Isolating}} and {{Analyzing Linked Data Structures}}?},
  author = {Meredith, Patrick and Pankaj, Balpreet and Sahoo, Swarup and Lattner, Chris and Adve, Vikram},
  year = {2005},
  abstract = {This report describes a set of experiments to evaluate qualitatively the effectiveness of Data Structure Analysis (DSA) in identifying properties of a program's data structures. We manually inspected several benchmarks to identify linked data structures and their properties, and compared these against the results produced by DSA. The properties we considered are those that were the primary goals of DSA: distinguishing different kinds of data structures, distinct instances of a particular kind, type information for objects within an LDS, and information about the lifetime of such objects (particularly, those local to a function rather than global). We define a set of metrics for the DS graphs computed by DSA that we use to summarize our results concisely for each benchmark. The results of the study are summarized in the last section.},
  file = {D\:\\GDrive\\zotero\\Meredith\\meredith_2005_how_successful_is_data_structure_analysis_in_isolating_and_analyzing_linked.pdf}
}

@techreport{merkleSecureCommunicationsInsecure,
  title = {Secure {{Communications Over Insecure Channels}}},
  author = {Merkle, Ralph C},
  abstract = {According to traditional conceptions of cryptographic security, it is necessary to transmit a key, by secret means, before encrypted messages can be sent securely. This paper shows that it is possible to select a key over open communications channels in such a fashion that communications security can be maintained. A method is described which forces any enemy to expend an amount of work which increases as the square of the work required of the two communicants to select the key. The method provides a logically new kind of protection against the passive eavesdropper. It suggests that further research on this topic will be highly rewarding, both in a theoretical and a practical sense.},
  file = {D\:\\GDrive\\zotero\\Merkle\\merkle_secure_communications_over_insecure_channels.pdf}
}

@article{MetaCircularAbstractInterpretation,
  title = {Meta-{{Circular Abstract Interpretation}} in {{Prolog}}}
}

@book{metzGeniusMakersMavericks2021,
  title = {Genius Makers: The Mavericks Who Brought {{A}}.{{I}}. to {{Google}}, {{Facebook}}, and the World},
  shorttitle = {Genius Makers},
  author = {Metz, Cade},
  year = {2021},
  publisher = {{Dutton, an imprint of Penguin Random House LLC}},
  address = {{New York}},
  abstract = {"New York Times Silicon Valley beat reporter Cade Metz has an insider's perspective on the greatest tech story of our time--a story that no one else has been in a position to tell"--},
  isbn = {978-1-5247-4267-6},
  keywords = {Artificial intelligence,Biography,Computer scientists,History,Industrial applications History,Intelligent personal assistants (Computer software),Machine learning,Research History},
  lccn = {TA347.A78 M48 2021}
}

@techreport{meyerTodayWasGood2019,
  title = {Today Was a {{Good Day}}: {{The Daily Life}} of {{Software Developers}}},
  author = {Meyer, Andr{\'e} N and Barr, Earl T and Bird, Christian and Zimmermann, Thomas},
  year = {2019},
  abstract = {What is a good workday for a software developer? What is a typical workday? We seek to answer these two questions to learn how to make good days typical. Concretely, answering these questions will help to optimize development processes and select tools that increase job satisfaction and productivity. Our work adds to a large body of research on how software developers spend their time. We report the results from 5971 responses of professional developers at Microsoft, who reflected about what made their workdays good and typical, and self-reported about how they spent their time on various activities at work. We developed conceptual frameworks to help define and characterize developer workdays from two new perspectives: good and typical. Our analysis confirms some findings in previous work, including the fact that developers actually spend little time on development and developers' aversion for meetings and interruptions. It also discovered new findings, such as that only 1.7\% of survey responses mentioned emails as a reason for a bad workday, and that meetings and interruptions are only unproductive during development phases; during phases of planning, specification and release, they are common and constructive. One key finding is the importance of agency, developers' control over their workday and whether it goes as planned or is disrupted by external factors. We present actionable recommendations for researchers and managers to prioritize process and tool improvements that make good workdays typical. For instance, in light of our finding on the importance of agency, we recommend that, where possible, managers empower developers to choose their tools and tasks.},
  file = {D\:\\GDrive\\zotero\\Meyer et al\\meyer_et_al_2019_today_was_a_good_day2.pdf},
  keywords = {Good Workdays,Index Terms-Software Developer Workdays,Job Satisfaction,Productivity,Quantified Workplace,Typical Workdays}
}

@techreport{michaelSimpleFastPractical,
  title = {Simple, {{Fast}}, and {{Practical Non}}-{{Blocking}} and {{Blocking Concurrent Queue Algorithms}}},
  author = {Michael, Maged M and Scott, Michael L},
  abstract = {Drawing ideas from previous authors, we present a new non-blocking concurrent queue algorithm and a new two-lock queue algorithm in which one enqueue and one de-queue can proceed concurrently. Both algorithms are simple , fast, and practical; we were surprised not to find them in the literature. Experiments on a 12-node SGI Challenge multiprocessor indicate that the new non-blocking queue consistently outperforms the best known alternatives; it is the clear algorithm of choice for machines that provide a universal atomic primitive (e.g. compare and swap or load linked/store conditional). The two-lock concurrent queue outperforms a single lock when several processes are competing simultaneously for access; it appears to be the algorithm of choice for busy queues on machines with non-universal atomic primitives (e.g. test and set). Since much of the motivation for non-blocking algorithms is rooted in their immunity to large, unpredictable delays in process execution, we report experimental results both for systems with dedicated processors and for systems with several processes multiprogrammed on each processor.},
  keywords = {compare and swap,concurrent queue,lock-free,multiprogramming,non-blocking}
}

@techreport{microsoftAESCBCElephantDiffuser2006,
  title = {{{AES}}-{{CBC}} + {{Elephant}} Diffuser {{A Disk Encryption Algorithm}} for {{Windows Vista}}},
  author = {Microsoft, Niels Ferguson},
  year = {2006},
  abstract = {The Bitlocker Drive Encryption feature of Windows Vista poses an interesting set of security and performance requirements on the encryption algorithm used for the disk data. We discuss why no existing cipher satisfies the requirements of this application and document our solution which consists of using AES in CBC mode with a dedicated diffuser to improve the security against manipulation attacks.},
  file = {D\:\\GDrive\\zotero\\Microsoft\\microsoft_2006_aes-cbc_+_elephant_diffuser_a_disk_encryption_algorithm_for_windows_vista.pdf}
}

@article{miersZerocoinAnonymousDistributed2013,
  title = {Zerocoin: {{Anonymous}} Distributed e-Cash from Bitcoin},
  author = {Miers, Ian and Garman, Christina and Green, Matthew and Rubin, Aviel D.},
  year = {2013},
  pages = {397--411},
  publisher = {{IEEE}},
  issn = {10816011},
  doi = {10.1109/SP.2013.34},
  abstract = {Bitcoin is the first e-cash system to see widespread adoption. While Bitcoin offers the potential for new types of financial interaction, it has significant limitations regarding privacy. Specifically, because the Bitcoin transaction log is completely public, users' privacy is protected only through the use of pseudonyms. In this paper we propose Zerocoin, a cryptographic extension to Bitcoin that augments the protocol to allow for fully anonymous currency transactions. Our system uses standard cryptographic assumptions and does not introduce new trusted parties or otherwise change the security model of Bitcoin. We detail Zerocoin's cryptographic construction, its integration into Bitcoin, and examine its performance both in terms of computation and impact on the Bitcoin protocol. \textcopyright{} 2013 IEEE.},
  isbn = {9780769549774},
  journal = {Proceedings - IEEE Symposium on Security and Privacy}
}

@article{mineaInterproceduralBoundsChecker2009,
  title = {Interprocedural Bounds Checker for {{C}} Programs Using Symbolic Constraints and Slicing},
  author = {Minea, Marius},
  year = {2009},
  abstract = {A tool for finding out-of-bounds memory access bugs in C programs in time comparable to compilation time.},
  file = {D\:\\GDrive\\zotero\\Minea\\minea_2009_interprocedural_bounds_checker_for_c_programs_using_symbolic_constraints_and.pdf}
}

@article{mineaVerificareaAcceselorMemorie2009,
  title = {Verificarea Acceselor La Memorie \^In Programe {{C}} Folosind Constr\^angeri Simbolice s , i Slicing},
  author = {Minea, Marius},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Minea\\minea_2009_verificarea_acceselor_la_memorie_în_programe_c_folosind_constrângeri_simbolice.pdf}
}

@techreport{misraTheoryProgrammingPractice2006,
  title = {Theory in {{Programming Practice}}},
  author = {Misra, Jayadev},
  year = {2006},
  file = {D\:\\GDrive\\zotero\\Misra\\misra_2006_theory_in_programming_practice.pdf}
}

@article{mitchellModelingAnalysisAttacks2016,
  title = {Modeling and {{Analysis}} of {{Attacks}} and {{Counter Defense Mechanisms}} for {{Cyber Physical Systems}}},
  author = {Mitchell, Robert and Chen, Ing Ray},
  year = {2016},
  volume = {65},
  pages = {350--358},
  publisher = {{IEEE}},
  issn = {00189529},
  doi = {10.1109/TR.2015.2406860},
  abstract = {In this paper, we develop an analytical model based on stochastic Petri nets to capture the dynamics between adversary behavior and defense for cyber physical systems. We consider several types of failures including attrition failure, pervasion failure, and exfiltration failure which can happen to a cyber physical system. Using a modernized electrical grid as an example, we illustrate the parameterization process. Our results reveal optimal design conditions, including the intrusion detection interval, and the redundancy level, under which the modernized electrical grid's mean time to failure is maximized. Further, there exists a design tradeoff between exfiltration failure, attrition failure, and pervasion failure when using redundancy to improve the overall system reliability.},
  file = {D\:\\GDrive\\zotero\\Mitchell\\mitchell_2016_modeling_and_analysis_of_attacks_and_counter_defense_mechanisms_for_cyber.pdf},
  journal = {IEEE Transactions on Reliability},
  keywords = {Cyber physical systems,intrusion detection,mean time to failure,modeling and analysis,redundancy engineering},
  number = {1}
}

@misc{MITLCSTM394Pdf,
  title = {{{MIT}}-{{LCS}}-{{TM}}-394.Pdf},
  file = {D\:\\GDrive\\zotero\\undefined\\mit-lcs-tm-394.pdf}
}

@book{mittelbachLaTeXCompanion2004,
  title = {The {{LaTeX}} Companion},
  author = {Mittelbach, Frank and Goossens, Michel and Braams, Johannes and Rowley, Chris},
  year = {2004},
  edition = {2nd ed},
  publisher = {{Addison-Wesley}},
  address = {{Boston}},
  file = {D\:\\GDrive\\zotero\\Mittelbach et al\\mittelbach_et_al_2004_the_latex_companion.pdf},
  isbn = {978-0-201-36299-2},
  keywords = {Computerized typesetting,LaTeX (Computer file)},
  language = {en},
  lccn = {Z253.4.L38 G66 2004},
  series = {Addison-{{Wesley}} Series on Tools and Techniques for Computer Typesetting}
}

@book{mobisys11conferencecommitteeAnalyzingInterApplicationCommunication2011,
  title = {Analyzing {{Inter}}-{{Application Communication}} in {{Android}}},
  author = {{Mobisys 11 Conference Committee}},
  year = {2011},
  publisher = {{Association for Computing Machinery}},
  file = {D\:\\GDrive\\zotero\\Mobisys 11 Conference Committee\\mobisys_11_conference_committee_2011_analyzing_inter-application_communication_in_android.pdf},
  isbn = {978-1-4503-0643-0}
}

@techreport{mockapetrisDevelopmentDomainName,
  title = {Development of the {{Domain Name System}}*},
  author = {Mockapetris, Paul V and Dunlap, Kevin J},
  abstract = {The Domain Name System (DNS) provides name service for the DARPA Internet. It is one of the largest name services in operation today, serves a highly diverse community of hosts, users, and networks , and uses a unique combination of hierarchies , caching, and datagram access. This paper examines the ideas behind the initial design of the DNS in 1983, discusses the evolution of these ideas into the current implementations and usages , notes conspicuous surprises, successes and shortcomings, and attempts to predict its future evolution .},
  file = {D\:\\GDrive\\zotero\\Mockapetris\\mockapetris_development_of_the_domain_name_system.pdf}
}

@techreport{moghaddamWatchingYouWatch2019,
  title = {Watching {{You Watch}}: {{The Tracking Ecosystem}} of {{Over}}-the-{{Top TV Streaming Devices}}},
  author = {Moghaddam, Hooman Mohajeri and Acar, Gunes and Burgess, Ben and Mathur, Arunesh and Huang, Danny Yuxing and Feamster, Nick and Felten, Edward W and Mittal, Prateek and Narayanan, Arvind},
  year = {2019},
  abstract = {The number of Internet-connected TV devices has grown significantly in recent years, especially Over-the-Top ("OTT") streaming devices, such as Roku TV and Amazon Fire TV. OTT devices offer an alternative to multi-channel television subscription services, and are often monetized through behavioral advertising. To shed light on the privacy practices of such platforms, we developed a system that can automatically download OTT apps (also known as channels), and interact with them while intercepting the network traffic and performing best-effort TLS interception. We used this smart crawler to visit more than 2,000 channels on two popular OTT platforms, namely Roku and Amazon Fire TV. Our results show that tracking is pervasive on both OTT platforms, with traffic to known trackers present on 69\% of Roku channels and 89\% of Amazon Fire TV channels. We also discover widespread practice of collecting and transmitting unique identifiers, such as device IDs, serial numbers, WiFi MAC addresses and SSIDs, at times over un-encrypted connections. Finally, we show that the countermeasures available on these devices, such as limiting ad tracking options and adblocking, are practically ineffective. Based on our findings, we make recommendations for researchers, regulators, policy makers, and platform/app developers.},
  file = {D\:\\GDrive\\zotero\\Moghaddam\\moghaddam_2019_watching_you_watch.pdf}
}

@techreport{mooreInferringInternetDenialofService,
  title = {Inferring {{Internet Denial}}-of-{{Service Activity}}},
  author = {Moore, David and Voelker, Geoffrey M and Savage, Stefan},
  abstract = {In this paper, we seek to answer a simple question: "How prevalent are denial-of-service attacks in the Internet to-day?". Our motivation is to understand quantitatively the nature of the current threat as well as to enable longer-term analyses of trends and recurring patterns of attacks. We present a new technique, called "backscatter anal-ysis", that provides an estimate of worldwide denial-of-service activity. We use this approach on three week-long datasets to assess the number, duration and focus of attacks , and to characterize their behavior. During this period , we observe more than 12,000 attacks against more than 5,000 distinct targets, ranging from well known e-commerce companies such as Amazon and Hotmail to small foreign ISPs and dial-up connections. We believe that our work is the only publically available data quantifying denial-of-service activity in the Internet.},
  file = {D\:\\GDrive\\zotero\\Moore\\moore_inferring_internet_denial-of-service_activity.pdf}
}

@article{mooreShouldCryptocurrenciesBe2016,
  title = {Should Cryptocurrencies Be Included in the Portfolio of International Reserves Held by Central Banks?},
  author = {Moore, Winston and Stephen, Jeremy},
  year = {2016},
  volume = {4},
  issn = {23322039},
  doi = {10.1080/23322039.2016.1147119},
  abstract = {In most countries, the central bank is required to hold reserve assets as a means of providing credibility for the value of the fiat currency. These assets can be in the form of gold, foreign exchange or some other internationally recognised reserve asset and are held to permit the country to engage in international transactions. Within recent years, cryptocurrencies have been increasingly utilised for international transactions, and it is possible that the use of these cryptocurrencies might expand in the future. This paper therefore examines the potential role of digital currency balances as part of the portfolio of external assets held by a central bank. Using the case of Barbados, the paper also provides a simulation of the effect holding some proportion of their asset-base would have had on the stability of the foreign reserves as well as the return on the portfolio of assets.},
  file = {D\:\\GDrive\\zotero\\Moore\\moore_2016_should_cryptocurrencies_be_included_in_the_portfolio_of_international_reserves.pdf},
  journal = {Cogent Economics and Finance},
  keywords = {Cryptocurrencies,Fixed exchange rate,International reserves},
  number = {1}
}

@techreport{mooreSpreadSapphireSlammer,
  title = {The {{Spread}} of the {{Sapphire}}/{{Slammer Worm By}} (in Alphabetical Order)},
  author = {Moore, David and Paxson, Vern and Savage, Stefan and Shannon, Colleen and Staniford, Stuart and Weaver, Nicholas},
  file = {D\:\\GDrive\\zotero\\Moore\\moore_the_spread_of_the_sapphire-slammer_worm_by_(in_alphabetical_order).pdf}
}

@techreport{moorsCriticalReviewEndtoend2002,
  title = {A Critical Review of "{{End}}-to-End Arguments in System Design"},
  author = {Moors, Tim},
  year = {2002},
  abstract = {The end-to-end arguments raised by Saltzer, Reed and Clark in the early 1980s are amongst the most influential of all communication protocol design guides. However, they have recently been challenged by the advent of firewalls, caches, active networks, NAT, multicasting and network QOS. This paper reviews the end-to-end arguments, highlighting their subtleties, and provides additional arguments for and against end-to-end implementations. It shows the importance of trust as a criterion for deciding whether to implement a function locally or end-to-end, and how end-to-end implementations can help robustness, scalability, ease of deployment, and the provision of appropriate service. It focuses on the performance implications of end-to-end or localized functionality, and argues against end-to-end congestion control of the form used by TCP. .},
  file = {D\:\\GDrive\\zotero\\Moors\\moors_2002_a_critical_review_of_end-to-end_arguments_in_system_design.pdf}
}

@article{morabitoDigitalCurrencies2017,
  title = {Digital {{Currencies}}},
  author = {Morabito, Vincenzo and Morabito, Vincenzo},
  year = {2017},
  pages = {81--100},
  doi = {10.1007/978-3-319-48478-5_5},
  abstract = {Digital currencies, and especially those which have an embedded decentralised transfer mechanism based on the use of a distributed ledger, are an innovation that could have a range of impacts on various aspects of financial markets and the wider economy. These could include potential disruption to business models and systems, as well as facilitating new economic interactions and linkages. Currently, such schemes are not widely used or accepted, and they face a series of challenges that could limit their future growth. However, some digital currency schemes have demonstrated that their underlying technology could feasibly be used for peer-to-peer transactions in the absence of a trusted third party. Such technology may have potential to improve some aspects of the efficiency of payment services and financial market infrastructures (FMIs) in general. In particular, these improvements might arise in circumstances where intermediation through a central party is not currently cost-effective. This report considers the possible implications of interest to central banks arising from these innovations.},
  isbn = {9789291973842},
  journal = {Business Innovation Through Blockchain},
  number = {November}
}

@techreport{MoreSquaringMultiplying,
  title = {More {{On Squaring}} and {{Multiplying Large Integers}}},
  abstract = {899 vzsub(r,w,a,b) Cout = vzaddwco(r,w,a,b) More On Squaring and Multiplying Large Integers Dan Zuras r[O,w-1] = a[O,w-1]-b[O,w-1] r[O,w-11 + Cout = a[O,w-1] + b[O,w-1] Abstract-Methods of squaring and multiplying large integers are discussed. The obvious O(n") methods turn out to be best for become better as the numbers get bigger. New methods that and O(nlogO/'ogb) M O(n1.366) are presented. In actual experiments , all of these methods turn out to be faster than FFT multiplies for numbers that can be quite large ({$>$} 37,000,000 bits). Squaring seems to be fundamentally faster than multiplying but it is shown that Tmultiply 5 2Tssuare + O(n). Index Terms-Integer multiply, Karatsuba and Ofman, Twm-Cook, Schihhage and Strassen, FFT multiply. small numbers. Existing O(nIog 3/10g ") N O(ni.6*5) methods O(n'Og 6 / U 3) M O(n1.465), O(n'"g 7 / b 4) M O(n1.404), vzshMadd(r,w ,a,b,M) vzshMsub(r,w,a,b,M) t = vzge(b,w,a)},
  file = {D\:\\GDrive\\zotero\\undefined\\more_on_squaring_and_multiplying_large_integers.pdf},
  isbn = {00189340/94\$04.0}
}

@article{morgenthalerUsingStaticAnalysis2008,
  title = {Using {{Static Analysis}} to {{Find Bugs}}},
  author = {Morgenthaler, J David and Penix, John and Ayewah, Nathaniel},
  year = {2008},
  abstract = {Static analysis examines code in the absence of input data and without running the code. It can detect potential security violations (SQL injection), runtime errors (dereferencing a null pointer) and logical inconsistencies (a conditional test that can't possibly be true). Although a rich body of literature exists on algorithms and analytical frameworks used by such tools, reports describing experiences in industry are much harder to come by. The authors describe FindBugs, an open source static-analysis tool for Java, and experiences using it in production settings. FindBugs evaluates what kinds of defects can be effectively detected with relatively simple techniques and helps developers understand how to incorporate such tools into software development.},
  file = {D\:\\GDrive\\zotero\\Morgenthaler et al\\morgenthaler_et_al_2008_using_static_analysis_to_find_bugs.pdf}
}

@article{moriniBlockchainHypeReal2016,
  title = {From '{{Blockchain Hype}}' to a {{Real Business Case}} for {{Financial Markets}}},
  author = {Morini, Massimo},
  year = {2016},
  doi = {10.2139/ssrn.2760184},
  abstract = {Introduction: Blockchain Hype vs Blockchain Seclusion?  There has been a lot of noise in the press about the great potential uses for financial markets of Bitcoin-related technology, that could be extracted from the Bitcoin world and applied to existing markets to increase efficiency dramatically. Later, there has been a lot of noise about the fact that there is no actual use but all boils down to a generic enthusiasm called Blockchain Hype, and Bitcoin is the only reality where such technology can be fruitfully used.   This paper shows that there are real business cases for improving financial markets based on the lesson learnt from cryptocurrencies, but, differently from what the hype-enthusiasts say, they are not application of a technology to the existing business model of financial markets. They are reforms of the business model itself. What needs to be exported from the world of cryptocurrencies are aspects of the market organization, inspiration for a different accounting and legal system, and some aspects of the technology. These can be a huge contribution towards more robust, efficient and stable markets, but the process cannot be immediate and effortless, and can only be achieved within a market-wide strategic view.   One crucial misunderstanding here is the idea that Blockchain Technology can be exported to financial markets as they are to make them more efficient. This is meaningless; Blockchain technology was created to change some trust-based business processes to make them less reliant on trust; without structural changes in this direction the best of Blockchain technology is lost and just the inefficiencies are left. This misunderstanding is the perfect partner of the idea that Blockchain technology cannot be used outside the Bitcoin world. This is equally meaningless; Bitcoin was created to attempt a level of independence from trust sufficient to allow players to be anonymous and do without any legal protection; other business solutions based on a level of trust intermediate between Bitcoin and traditional finance can use similar technology and yet be very different from Bitcoins. But we must ready to use the concept of trust in a totally different way, as a way to analyze the different parts of a business process and the reason for its current inefficiencies and risks.  In the next we develop these concepts first in a parallel analysis of cryptocurrencies and financial markets. Then we focus on a specific business case regarding the collateralization of financial derivatives, that we describe bottom-up including quantifiable benefits in reducing costs, capital and risk. It is an example where the use of cryptocurrency technology is not more important than the business ideas developed in the analysis of cryptocurrencies; yet it was unconceivable before examples of distributed ledgers, smart contracts and oracles were visible in marketplaces. In fact, it was first presented in Morini and Sams (2015), in an introduction of the Blockchain innovation for the derivatives world.},
  file = {D\:\\GDrive\\zotero\\Morini\\morini_2016_from_'blockchain_hype'_to_a_real_business_case_for_financial_markets.pdf},
  journal = {SSRN Electronic Journal},
  number = {August 2015}
}

@techreport{morrisClickModularRouter1999,
  title = {The {{Click}} Modular Router},
  author = {Morris, Robert and Kohler, Eddie and Jannotti, John and Kaashoek, M Frans},
  year = {1999},
  volume = {34},
  pages = {217--231},
  abstract = {Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. Complete configurations are built by connecting elements into a graph; packets flow along the graph's edges. Several features make individual elements more powerful and complex configurations easier to write, including pull processing, which models packet flow driven by transmitting interfaces, and flow-based router context, which helps an element locate other interesting elements. We demonstrate several working configurations, including an IP router and an Ethernet bridge. These configurations are modular-the IP router has 16 elements on the forwarding path-and easy to extend by adding additional elements, which we demonstrate with augmented configurations. On commodity PC hardware running Linux, the Click IP router can forward 64-byte packets at 73,000 packets per second, just 10\% slower than Linux alone.},
  file = {D\:\\GDrive\\zotero\\Morris\\morris_1999_the_click_modular_router.pdf},
  journal = {Operating Systems Review},
  number = {5}
}

@article{morrisPasswordSecurityCase1979,
  title = {Password {{Security}}: {{A Case History}}},
  author = {Morris, Robert and Thompson, Ken},
  year = {1979},
  volume = {22},
  pages = {594--597},
  issn = {15577317},
  doi = {10.1145/359168.359172},
  abstract = {This paper describes the history of the design of the password security scheme on a remotely accessed time-sharing system. The present design was the result of countering observed attempts to penetrate the system. The result is a compromise between extreme security and ease of use. \textcopyright{} 1979, ACM. All rights reserved.},
  journal = {Communications of the ACM},
  keywords = {computer security,operating systems,passwords},
  number = {11}
}

@techreport{moseleyOutTarPit2006,
  title = {Out of the {{Tar Pit}}},
  author = {Moseley, Ben and Marks, Peter},
  year = {2006},
  abstract = {Complexity is the single major difficulty in the successful development of large-scale software systems. Following Brooks we distinguish accidental from essential difficulty, but disagree with his premise that most complexity remaining in contemporary systems is essential. We identify common causes of complexity and discuss general approaches which can be taken to eliminate them where they are accidental in nature. To make things more concrete we then give an outline for a potential complexity-minimizing approach based on functional programming and Codd's relational model of data.},
  file = {D\:\\GDrive\\zotero\\Moseley\\moseley_2006_out_of_the_tar_pit.pdf}
}

@techreport{moserAnonymityBitcoinTransactionsa,
  title = {Anonymity of {{Bitcoin Transactions An Analysis}} of {{Mixing Services}}},
  author = {M{\"o}ser, Malte},
  abstract = {Bitcoin, a distributed, cryptographic, digital currency, gained a lot of media attention for being an anonymous e-cash system. But as all transactions in the network are stored publicly in the blockchain, allowing anyone to inspect and analyze them, the system does not provide real anonymity but pseudonymity. There have already been studies showing the possibility to deanonymize bitcoin users based on the transaction graph and publicly available data. Furthermore, users could be tracked by bitcoin exchanges or shops, where they have to provide personal information that can then be linked to their bitcoin addresses. Special bitcoin mixing services claim to obfuscate the origin of transactions and thereby increase the anonymity of its users. In this paper we evaluate three of these services-Bitcoin Fog, BitLaun-dry, and the Send Shared functionality of Blockchain.info-by analyzing the transaction graph. While Bitcoin Fog and Blockchain.info successfully mix our transaction, we are able to find a direct relation between the input and output transactions in the graph of BitLaundry.},
  file = {D\:\\GDrive\\zotero\\Möser\\möser_anonymity_of_bitcoin_transactions_an_analysis_of_mixing_services.pdf},
  keywords = {anonymity,bitcoin,blockchain,laundry,mix,pseudonymity,shared wallet,transaction}
}

@techreport{moserAnonymousAloneMeasuring,
  title = {Anonymous {{Alone}}? {{Measuring Bitcoin}}'s {{Second}}-{{Generation Anonymization Techniques}}},
  author = {M{\"o}ser, Malte and B{\"o}hme, Rainer},
  abstract = {This paper contributes a systematic account of transaction anonymization techniques that do not require trust in a single entity and support the existing cryptographic currency Bitcoin. It surveys and compares four known techniques, proposes tailored metrics to identify the use of each technique (but not necessarily its users), and presents longitudinal measurements indicating adoption trends and teething troubles. There is a trade-off between the choice of users' preferred protection mechanisms and the risk that pertaining transactions can be singled out, which hurts privacy due to smaller anonymity sets unless a critical mass adopts the mechanism.},
  file = {D\:\\GDrive\\zotero\\Möser\\möser_anonymous_alone.pdf}
}

@article{moserLimitsStaticAnalysis2007,
  title = {Limits of Static Analysis for Malware Detection},
  author = {Moser, Andreas and Kruegel, Christopher and Kirda, Engin},
  year = {2007},
  pages = {421--430},
  issn = {10639527},
  doi = {10.1109/ACSAC.2007.21},
  abstract = {Malicious code is an increasingly important problem that threatens the security of computer systems. The traditional line of defense against malware is composed of malware detectors such as virus and spyware scanners. Unfortunately, both researchers and malware authors have demonstrated that these scanners, which use pattern matching to identify malware, can be easily evaded by simple code transformations. To address this shortcoming, more powerful malware detectors have been proposed. These tools rely on semantic signatures and employ static analysis techniques such as model checking and theorem proving to perform detection. While it has been shown that these systems are highly effective in identifying current malware, it is less clear how successful they would be against adversaries that take into account the novel detection mechanisms. The goal of this paper is to explore the limits of static analysis for the detection of malicious code. To this end, we present a binary obfuscation scheme that relies on the idea of opaque constants, which are primitives that allow us to load a constant into a register such that an analysis tool cannot determine its value. Based on opaque constants, we build obfuscation transformations that obscure program control flow, disguise access to local and global variables, and interrupt tracking of values held in processor registers. Using our proposed obfuscation approach, we were able to show that advanced semantics-based malware detectors can be evaded. Moreover, our opaque constant primitive can be applied in a way such that is provably hard to analyze for any static code analyzer. This demonstrates that static analysis techniques alone might no longer be sufficient to identify malware. \textcopyright{} 2007 IEEE.},
  file = {D\:\\GDrive\\zotero\\Moser\\moser_2007_limits_of_static_analysis_for_malware_detection.pdf},
  isbn = {0769530605},
  journal = {Proceedings - Annual Computer Security Applications Conference, ACSAC}
}

@inproceedings{moweryAreAESX862012,
  title = {Are {{AES}} X86 Cache Timing Attacks Still Feasible?},
  booktitle = {Proceedings of the 2012 {{ACM Workshop}} on {{Cloud}} Computing Security Workshop - {{CCSW}} '12},
  author = {Mowery, Keaton and Keelveedhi, Sriram and Shacham, Hovav},
  year = {2012},
  pages = {19},
  publisher = {{ACM Press}},
  address = {{Raleigh, North Carolina, USA}},
  doi = {10.1145/2381913.2381917},
  abstract = {We argue that five recent software and hardware developments \textemdash{} the AES-NI instructions, multicore processors with per-core caches, complex modern software, sophisticated prefetchers, and physically tagged caches \textemdash{} combine to make it substantially more difficult to mount data-cache side-channel attacks on AES than previously realized. We propose ways in which some of the challenges posed by these developments might be overcome. We also consider scenarios where sidechannel attacks are attractive, and whether our proposed workarounds might be applicable to these scenarios.},
  file = {D\:\\GDrive\\zotero\\Mowery et al\\mowery_et_al_2012_are_aes_x86_cache_timing_attacks_still_feasible.pdf},
  isbn = {978-1-4503-1665-1},
  language = {en}
}

@article{moweryFingerprintingInformationJavaScript2011,
  title = {Fingerprinting {{Information}} in {{JavaScript Implementations}}},
  author = {Mowery, Keaton and Bogenreif, Dillon and Yilek, Scott and Shacham, Hovav},
  year = {2011},
  pages = {11},
  abstract = {To date, many attempts have been made to fingerprint users on the web. These fingerprints allow browsing sessions to be linked together and possibly even tied to a user's identity. They can be used constructively by sites to supplement traditional means of user authentication such as passwords; and they can be used destructively to counter attempts to stay anonymous online.},
  file = {D\:\\GDrive\\zotero\\Mowery et al\\mowery_et_al_2011_fingerprinting_information_in_javascript_implementations.pdf},
  language = {en}
}

@article{moweryPixelPerfectFingerprinting2012,
  title = {Pixel {{Perfect}}: {{Fingerprinting Canvas}} in {{HTML5}}},
  author = {Mowery, Keaton and Shacham, Hovav},
  year = {2012},
  pages = {12},
  abstract = {Tying the browser more closely to operating system functionality and system hardware means that websites have more access to these resources, and that browser behavior varies depending on the behavior of these resources.},
  file = {D\:\\GDrive\\zotero\\Mowery_Shacham\\mowery_shacham_2012_pixel_perfect.pdf},
  language = {en}
}

@inproceedings{moweryWelcomeEntropicsBootTime2013,
  title = {Welcome to the {{Entropics}}: {{Boot}}-{{Time Entropy}} in {{Embedded Devices}}},
  shorttitle = {Welcome to the {{Entropics}}},
  booktitle = {2013 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Mowery, K. and Wei, M. and Kohlbrenner, D. and Shacham, H. and Swanson, S.},
  year = {2013},
  month = may,
  pages = {589--603},
  publisher = {{IEEE}},
  address = {{Berkeley, CA}},
  doi = {10.1109/SP.2013.46},
  abstract = {We present three techniques for extracting entropy during boot on embedded devices. Our first technique times the execution of code blocks early in the Linux kernel boot process. It is simple to implement and has a negligible runtime overhead, but, on many of the devices we test, gathers hundreds of bits of entropy.},
  file = {D\:\\GDrive\\zotero\\Mowery et al\\mowery_et_al_2013_welcome_to_the_entropics.pdf},
  isbn = {978-0-7695-4977-4 978-1-4673-6166-8},
  language = {en}
}

@misc{MyAdviceMy,
  title = {My {{Advice To My Graduate Students}}},
  journal = {My Advice To My Graduate Students}
}

@misc{Nabeelqu,
  title = {Nabeelqu},
  abstract = {Persona},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\9HR3JMYU\\understanding.html},
  howpublished = {http://nabeelqu.co},
  language = {en}
}

@techreport{nakamotoBitcoinPeertoPeerElectronic,
  title = {Bitcoin: {{A Peer}}-to-{{Peer Electronic Cash System}}},
  author = {Nakamoto, Satoshi},
  abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
  file = {D\:\\GDrive\\zotero\\Nakamoto\\nakamoto_bitcoin.pdf},
  keywords = {ss}
}

@inproceedings{narayananLightweightKernelIsolation2020,
  title = {Lightweight Kernel Isolation with Virtualization and {{VM}} Functions},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}/{{SIGOPS International Conference}} on {{Virtual Execution Environments}}},
  author = {Narayanan, Vikram and Huang, Yongzhe and Tan, Gang and Jaeger, Trent and Burtsev, Anton},
  year = {2020},
  month = mar,
  pages = {157--171},
  publisher = {{ACM}},
  address = {{Lausanne Switzerland}},
  doi = {10.1145/3381052.3381328},
  abstract = {Commodity operating systems execute core kernel subsystems in a single address space along with hundreds of dynamically loaded extensions and device drivers. Lack of isolation within the kernel implies that a vulnerability in any of the kernel subsystems or device drivers opens a way to mount a successful attack on the entire kernel.},
  file = {D\:\\GDrive\\zotero\\Narayanan et al\\narayanan_et_al_2020_lightweight_kernel_isolation_with_virtualization_and_vm_functions.pdf},
  isbn = {978-1-4503-7554-2},
  language = {en}
}

@article{narayananLXDsIsolationKernel2019,
  title = {{{LXDs}}: {{Towards Isolation}} of {{Kernel Subsystems}}},
  author = {Narayanan, Vikram and Balasubramanian, Abhiram and Jacobsen, Charlie},
  year = {2019},
  pages = {16},
  abstract = {Modern operating systems are monolithic. Today, however, lack of isolation is one of the main factors undermining security of the kernel. Inherent complexity of the kernel code and rapid development pace combined with the use of unsafe, low-level programming language results in a steady stream of errors. Even after decades of efforts to make commodity kernels more secure, i.e., development of numerous static and dynamic approaches aimed to prevent exploitation of most common errors, several hundreds of serious kernel vulnerabilities are reported every year. Unfortunately, in a monolithic kernel a single exploitable vulnerability potentially provides an attacker with access to the entire kernel.},
  file = {D\:\\GDrive\\zotero\\Narayanan et al\\narayanan_et_al_2019_lxds.pdf},
  language = {en}
}

@inproceedings{narayananPipeDreamGeneralizedPipeline2019,
  title = {{{PipeDream}}: Generalized Pipeline Parallelism for {{DNN}} Training},
  shorttitle = {{{PipeDream}}},
  booktitle = {Proceedings of the 27th {{ACM Symposium}} on {{Operating Systems Principles}}},
  author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
  year = {2019},
  month = oct,
  pages = {1--15},
  publisher = {{ACM}},
  address = {{Huntsville Ontario Canada}},
  doi = {10.1145/3341301.3359646},
  abstract = {DNN training is extremely time-consuming, necessitating efficient multi-accelerator parallelization. Current approaches to parallelizing training primarily use intra-batch parallelization, where a single iteration of training is split over the available workers, but suffer from diminishing returns at higher worker counts. We present PipeDream, a system that adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible. Unlike traditional pipelining, DNN training is bi-directional, where a forward pass through the computation graph is followed by a backward pass that uses state and intermediate data computed during the forward pass. Na\"ive pipelining can thus result in mismatches in state versions used in the forward and backward passes, or excessive pipeline flushes and lower hardware efficiency. To address these challenges, PipeDream versions model parameters for numerically correct gradient computations, and schedules forward and backward passes of different minibatches concurrently on different workers with minimal pipeline stalls. PipeDream also automatically partitions DNN layers among workers to balance work and minimize communication. Extensive experimentation with a range of DNN tasks, models, and hardware configurations shows that PipeDream trains models to high accuracy up to 5.3\texttimes{} faster than commonly used intra-batch parallelism techniques.},
  file = {D\:\\GDrive\\zotero\\Narayanan et al\\narayanan_et_al_2019_pipedream.pdf},
  isbn = {978-1-4503-6873-5},
  language = {en}
}

@article{narayananRedLeafIsolationCommunication2020,
  title = {{{RedLeaf}}: {{Isolation}} and {{Communication}} in a {{Safe Operating System}}},
  author = {Narayanan, Vikram and Huang, Tianjiao and Detweiler, David and Appel, Dan and Li, Zhaofeng and Zellweger, Gerd and Burtsev, Anton},
  year = {2020},
  pages = {19},
  abstract = {RedLeaf is a new operating system developed from scratch in Rust to explore the impact of language safety on operating system organization. In contrast to commodity systems, RedLeaf does not rely on hardware address spaces for isolation and instead uses only type and memory safety of the Rust language. Departure from costly hardware isolation mechanisms allows us to explore the design space of systems that embrace lightweight fine-grained isolation. We develop a new abstraction of a lightweight language-based isolation domain that provides a unit of information hiding and fault isolation. Domains can be dynamically loaded and cleanly terminated, i.e., errors in one domain do not affect the execution of other domains. Building on RedLeaf isolation mechanisms, we demonstrate the possibility to implement end-to-end zero-copy, fault isolation, and transparent recovery of device drivers. To evaluate the practicality of RedLeaf abstractions, we implement Rv6, a POSIX-subset operating system as a collection of RedLeaf domains. Finally, to demonstrate that Rust and fine-grained isolation are practical\textemdash we develop efficient versions of a 10Gbps Intel ixgbe network and NVMe solid-state disk device drivers that match the performance of the fastest DPDK and SPDK equivalents.},
  file = {D\:\\GDrive\\zotero\\Narayanan et al\\narayanan_et_al_2020_redleaf.pdf},
  language = {en}
}

@inproceedings{narayananRedLeafOperatingSystem2019,
  title = {{{RedLeaf}}: {{Towards An Operating System}} for {{Safe}} and {{Verified Firmware}}},
  shorttitle = {{{RedLeaf}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Hot Topics}} in {{Operating Systems}}},
  author = {Narayanan, Vikram and Baranowski, Marek S. and Ryzhyk, Leonid and Rakamari{\'c}, Zvonimir and Burtsev, Anton},
  year = {2019},
  month = may,
  pages = {37--44},
  publisher = {{ACM}},
  address = {{Bertinoro Italy}},
  doi = {10.1145/3317550.3321449},
  abstract = {RedLeaf is a new operating system being developed from scratch to utilize formal verification for implementing provably secure firmware. RedLeaf is developed in a safe language, Rust, and relies on automated reasoning using satisfiability modulo theories (SMT) solvers for formal verification. RedLeaf builds on two premises: (1) Rust's linear type system enables practical language safety even for systems with tightest performance and resource budgets (e.g., firmware), and (2) a combination of SMT-based reasoning and pointer discipline enforced by linear types provides a unique way to automate and simplify verification effort scaling it to the size of a small OS kernel.},
  file = {D\:\\GDrive\\zotero\\Narayanan et al\\narayanan_et_al_2019_redleaf.pdf},
  isbn = {978-1-4503-6727-1},
  language = {en}
}

@article{narayanGobiWebAssemblyPractical2019,
  title = {Gobi: {{WebAssembly}} as a {{Practical Path}} to {{Library Sandboxing}}},
  author = {Narayan, Shravan and Garfinkel, Tal and Lerner, Sorin and Stefan, Deian and Shacham, Hovav and Austin, UT},
  year = {2019},
  pages = {7},
  abstract = {Software based fault isolation (SFI) is a powerful approach to reducing the impact of security vulnerabilities in large and critical C/C++ applications like Firefox and Apache. Unfortunately, practical SFI tools have not been broadly available.},
  file = {D\:\\GDrive\\zotero\\Narayan et al\\narayan_et_al_2019_gobi.pdf},
  language = {en}
}

@article{narayanRetrofittingFineGrain2020,
  title = {Retrofitting {{Fine Grain Isolation}} in the {{Firefox Renderer}} ({{Extended Version}})},
  author = {Narayan, Shravan and Disselkoen, Craig and Garfinkel, Tal and Froyd, Nathan and Rahm, Eric and Lerner, Sorin and Shacham, Hovav and Stefan, Deian},
  year = {2020},
  month = mar,
  abstract = {Firefox and other major browsers rely on dozens of third-party libraries to render audio, video, images, and other content. These libraries are a frequent source of vulnerabilities. To mitigate this threat, we are migrating Firefox to an architecture that isolates these libraries in lightweight sandboxes, dramatically reducing the impact of a compromise. Retrofitting isolation can be labor-intensive, very prone to security bugs, and requires critical attention to performance. To help, we developed RLBox, a framework that minimizes the burden of converting Firefox to securely and efficiently use untrusted code. To enable this, RLBox employs static information flow enforcement, and lightweight dynamic checks, expressed directly in the C++ type system. RLBox supports efficient sandboxing through either software-based-fault isolation or multi-core process isolation. Performance overheads are modest and transient, and have only minor impact on page latency. We demonstrate this by sandboxing performance-sensitive image decoding libraries ( libjpeg and libpng ), video decoding libraries ( libtheora and libvpx ), the libvorbis audio decoding library, and the zlib decompression library. RLBox, using a WebAssembly sandbox, has been integrated into production Firefox to sandbox the libGraphite font shaping library.},
  archiveprefix = {arXiv},
  eprint = {2003.00572},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Narayan et al\\narayan_et_al_2020_retrofitting_fine_grain_isolation_in_the_firefox_renderer_(extended_version).pdf;C\:\\Users\\Admin\\Zotero\\storage\\RNLB36R2\\2003.html},
  journal = {arXiv:2003.00572 [cs]},
  keywords = {Computer Science - Cryptography and Security,D.4.6},
  primaryclass = {cs}
}

@inproceedings{naylorBalancingAccountabilityPrivacy2015,
  title = {Balancing Accountability and Privacy in the Network},
  booktitle = {Computer {{Communication Review}}},
  author = {Naylor, David and Mukerjee, Matthew K. and Steenkiste, Peter},
  year = {2015},
  month = feb,
  volume = {44},
  pages = {75--86},
  publisher = {{Association for Computing Machinery}},
  issn = {19435819},
  doi = {10.1145/2619239.2626306},
  abstract = {Though most would agree that accountability and privacy are both valuable, today's Internet provides little support for either. Previous efforts have explored ways to offer stronger guarantees for one of the two, typically at the expense of the other; indeed, at first glance accountability and privacy appear mutually exclusive. At the center of the tussle is the source address: in an accountable Internet, source addresses undeniably link packets and senders so hosts can be punished for bad behavior. In a privacy-preserving Internet, source addresses are hidden as much as possible. In this paper, we argue that a balance is possible. We introduce the Accountable and Private Internet Protocol (APIP), which splits source addresses into two separate fields --- an accountability address and a return address --- and introduces independent mechanisms for managing each. Accountability addresses, rather than pointing to hosts, point to accountability delegates, which agree to vouch for packets on their clients' behalves, taking appropriate action when misbehavior is reported. With accountability handled by delegates, senders are now free to mask their return addresses; we discuss a few techniques for doing so.},
  file = {D\:\\GDrive\\zotero\\Naylor\\naylor_2015_balancing_accountability_and_privacy_in_the_network.pdf},
  keywords = {Accountability,Privacy,Source address}
}

@techreport{neculaCCuredTypeSafeRetrofitting2002,
  title = {{{CCured}}: {{Type}}-{{Safe Retrofitting}} of {{Legacy Code}}},
  author = {Necula, George C and Mcpeak, Scott and Weimer, Westley},
  year = {2002},
  pages = {128--139},
  abstract = {In this paper we propose a scheme that combines type inference and run-time checking to make existing C programs type safe. We describe the CCured type system, which extends that of C by separating pointer types according to their usage. This type system allows both pointers whose usage can be verified statically to be type safe, and pointers whose safety must be checked at run time. We prove a type soundness result and then we present a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs. Our experience with the CCured system shows that the inference is very effective for many C programs, as it is able to infer that most or all of the pointers are statically verifiable to be type safe. The remaining pointers are instru-mented with efficient run-time checks to ensure that they are used safely. The resulting performance loss due to run-time checks is 0-150\%, which is several times better than comparable approaches that use only dynamic checking. Using CCured we have discovered programming bugs in established C programs such as several SPECINT95 benchmarks.},
  file = {D\:\\GDrive\\zotero\\Necula\\necula_2002_ccured.pdf}
}

@article{nethercoteValgrindFrameworkHeavyweight2007,
  title = {Valgrind: {{A Framework}} for {{Heavyweight Dynamic Binary Instrumentation}}},
  author = {Nethercote, Nicholas and Seward, Julian},
  year = {2007},
  pages = {12},
  abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values\textemdash a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
  file = {D\:\\GDrive\\zotero\\Nethercote_Seward\\nethercote_seward_2007_valgrind.pdf},
  language = {en}
}

@article{neustifterEfficientProfilingLLVM2010,
  title = {Efficient {{Profiling}} in the {{LLVM Compiler Infrastructure}}},
  author = {Neustifter, Andreas},
  year = {2010},
  volume = {2010},
  abstract = {In computer science profiling is the process of determining the execution fre- quencies of parts of a program. This can be done by instrumenting the program code with counters that are incremented when a part of the program is exe- cuted or by sampling the program counter at certain time intervals. From this data it is possible to calculate exact (in the case of counters) or relative (in the case of sampling) execution frequencies of all parts of the program. Currently the LLVM Compiler Infrastructure supports the profiling of pro- grams by placing counters in the code and reading the resulting profiling data during consecutive compilations. But these counters are placed with a na{\" } \i ve and inefficient algorithm that uses more counters than necessary. Also the recorded profiling information is not used in the compiler during optimisation or in the code generating backend when recompiling the program. This work tries to improve the existing profiling support in LLVM in several ways. First, the number of counters placed in the code is decreased as presented by Ball and Larus [19]. Counters are placed only at the leaves of each functions control flow graph (CFG), which gives an incomplete profile after the program execution. This incomplete profile can be completed by propagating the values of the leaves back into the tree. Secondly, the profiling information is made available to the code generating backend. The CFG modifications and instruction selection passes are modified where necessary to preserve the profiling information so that backend passes and code generation can benefit from it. For example the register allocator is one such backend pass that could benefit since the spilling decisions are based on the execution frequency information. Thirdly, a compile time estimator to predict execution frequencies when no profiling information is available is implemented and evaluated as proposed by Wu et.al. in [71]. This estimator is based on statistical data which is combined in order to give more accurate branch predictions as compared to methods where only a single heuristic is used for prediction. The efficiency of the implemented counter placing algorithm is evaluated by measuring profiling overhead for the na{\" } \i ve and for the improved counter place- ment. The improvements from having the profiling information in the code generating backend is measured by the program performance for code which was compiled without and with profiling information as well as for code that was compiled using the compile time estimator.},
  file = {D\:\\GDrive\\zotero\\Neustifter\\neustifter_2010_efficient_profiling_in_the_llvm_compiler_infrastructure.pdf}
}

@techreport{neustifterEfficientProfilingLLVM2010a,
  title = {Efficient {{Profiling}} in the {{LLVM Compiler Infrastructure DIPLOMARBEIT}} Zur {{Erlangung}} Des Akademischen {{Grades}}},
  author = {Neustifter, Andreas and Betreuer, Betreuung and {Univ-Prof Dipl-Ing Andreas Krall Wien}, ao},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Neustifter et al\\neustifter_et_al_2010_efficient_profiling_in_the_llvm_compiler_infrastructure_diplomarbeit_zur.pdf}
}

@article{newcombeHowAmazonWeb2015,
  title = {How Amazon Web Services Uses Formal Methods},
  author = {Newcombe, Chris and Rath, Tim and Zhang, Fan and Munteanu, Bogdan and Brooker, Marc and Deardeuff, Michael},
  year = {2015},
  volume = {58},
  pages = {66--73},
  issn = {15577317},
  doi = {10.1145/2699417},
  file = {D\:\\GDrive\\zotero\\Newcombe\\newcombe_2015_how_amazon_web_services_uses_formal_methods.pdf},
  journal = {Communications of the ACM},
  number = {4}
}

@article{ngGlobalRegionalNational2014,
  title = {Global, Regional, and National Prevalence of Overweight and Obesity in Children and Adults during 1980-2013: A Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2013 {{Institute}} for {{Health Metrics}} and {{Evaluation}} (},
  author = {Ng, M and Fleming, T BS and Robinson, M BA and Thomson, B BA and Graetz, N BS and Margono, C BS and Mullany BA, E C and Biryukov, S BS and Achoki, T and Dandona, L and Flaxman, A and Forouzanfar, M H and Mokdad, A H and Naghavi, M and Nelson MLIS, E L and Tobias, M and Vos, T and L Murray, C J and Gakidou, E and {of Medicine}, School and Ng, Marie and Fleming, Tom and Robinson, Margaret and Thomson, Blake and Graetz, Nicholas and Margono, Christopher and Mullany, Erin C and Biryukov, Stan and Abbafati, Cristiana and Ferede Abera, Semaw and Abraham, Jerry P and {E Abu-Rmeileh}, Niveen M and Achoki, Tom and AlBuhairan, Fadia S and Alemu, Zewdie A and Alfonso, Rafael and Ali, Mohammed K and Ali, Raghib and Alvis Guzman, Nelson and Ammar, Walid and Anwari, Palwasha and Banerjee, Amitava and Barquera, Simon and Basu, Sanjay and Bennett, Derrick A and {qar Bhutta}, Zulfi and Blore, Jed and Cabral, Norberto and Campos Nonato, Ismael and Chang, Jung-Chen and Chowdhury, Rajiv and Courville, Karen J and Criqui, Michael H and Cundiff, David K and Dabhadkar, Kaustubh C and Dandona, Lalit and Davis, Adrian and Dayama, Anand and Dharmaratne, Samath D and Ding, Eric L and Durrani, Adnan M and Esteghamati, Alireza and Farzadfar, Farshad and J Fay, Derek F and Feigin, Valery L and Flaxman, Abraham and Forouzanfar, Mohammad H and Goto, Atsushi and Green, Mark A and Gupta, Rajeev and {Hafezi-Nejad}, Nima and Hankey, Graeme J and Harewood, Heather C and Havmoeller, Rasmus and Hay, Simon and Hernandez, Lucia and Husseini, Abdullatif and Idrisov, Bulat T and Ikeda, Nayu and Islami, Farhad and Jahangir, Eiman and Jassal, Simerjot K and Ha Jee, Sun and {Jeff reys}, Mona and Jonas, Jost B and Kabagambe, Edmond K and Eldin Ali Hassan Khalifa, Shams and Pascal Kengne, Andre and Saleh Khader, Yousef and Khang, Young-Ho and Kim, Daniel and Kimokoti, Ruth W and Kinge, Jonas M and Kokubo, Yoshihiro and Kosen, Soewarta and Kwan, Gene and Lai, Taavi and Leinsalu, Mall and Li, Yichong and Liang, Xiaofeng and Liu, Shiwei and Logroscino, Giancarlo and Lotufo, Paulo A and Lu, Yuan and Ma, Jixiang and Kwaku Mainoo, Nana and Mensah, George A and Merriman, Tony R and Mokdad, Ali H and Moschandreas, Joanna and Naghavi, Mohsen and Naheed, Aliya and Nand, Devina and Venkat Narayan, K M and Leigh Nelson, Erica and Neuhouser, Marian L and Imran Nisar, Muhammad and Ohkubo, Takayoshi and Oti, Samuel O and Pedroza, Andrea and Prabhakaran, Dorairaj and Roy, Nobhojit and Sampson, Uchechukwu and Seo, Hyeyoung and Sepanlou, Sadaf G and Shibuya, Kenji and Shiri, Rahman and Shiue, Ivy and Singh, Gitanjali M and Singh, Jasvinder A and Skirbekk, Vegard and C Stapelberg, Nicolas J and Sturua, Lela and Sykes, Bryan L and Tobias, Martin and Tran, Bach X and Trasande, Leonardo and Toyoshima, Hideaki and {van de Vijver}, Steven and Vasankari, Tommi J and Lennert Veerman, J and {Velasquez-Melendez}, Gustavo and Victorovich Vlassov, Vasiliy and Emil Vollset, Stein and Vos, Theo and Wang, Claire and Wang, XiaoRong and Weiderpass, Elisabete and Werdecker, Andrea and Wright, Jonathan L and Claire Yang, Y and Yatsuya, Hiroshi and Yoon, Jihyun and Yoon, Seok-Jun and Zhao, Yong and Zhou, Maigeng and Zhu, Shankuan and Lopez, Alan D and L Murray, Christopher J and Gakidou, Emmanuela},
  year = {2014},
  volume = {384},
  doi = {10.1016/S0140-6736(14)60460-8},
  abstract = {Background In 2010, overweight and obesity were estimated to cause 3{$\cdot$}4 million deaths, 3{$\cdot$}9\% of years of life lost, and 3{$\cdot$}8\% of disability-adjusted life-years (DALYs) worldwide. The rise in obesity has led to widespread calls for regular monitoring of changes in overweight and obesity prevalence in all populations. Comparable, up-to-date information about levels and trends is essential to quantify population health eff ects and to prompt decision makers to prioritise action. We estimate the global, regional, and national prevalence of overweight and obesity in children and adults during 1980-2013.},
  journal = {www.thelancet.com}
}

@article{nguyenSurveyConsensusAlgorithms2018,
  title = {A {{Survey}} about {{Consensus Algorithms Used}} in {{Blockchain}}},
  author = {Nguyen, Giang-Truong and Kim, Kyungbaek},
  year = {2018},
  doi = {10.3745/JIPS.01.0024},
  abstract = {Thanks to its potential in many applications, Blockchain has recently been nominated as one of the technologies exciting intense attention. Blockchain has solved the problem of changing the original low-trust centralized ledger held by a single third-party, to a high-trust decentralized form held by different entities, or in other words, verifying nodes. The key contribution of the work of Blockchain is the consensus algorithm, which decides how agreement is made to append a new block between all nodes in the verifying network. Blockchain algorithms can be categorized into two main groups. The first group is proof-based consensus, which requires the nodes joining the verifying network to show that they are more qualified than the others to do the appending work. The second group is voting-based consensus, which requires nodes in the network to exchange their results of verifying a new block or transaction, before making the final decision. In this paper, we present a review of the Blockchain consensus algorithms that have been researched and that are being applied in some well-known applications at this time.},
  file = {D\:\\GDrive\\zotero\\Nguyen\\nguyen_2018_a_survey_about_consensus_algorithms_used_in_blockchain.pdf},
  keywords = {Blockchain,Consensus Algorithm}
}

@article{nguyenYourCacheHas2019,
  title = {Your Cache Has Fallen: {{Cache}}-Poisoned Denial-of-Service Attack},
  author = {Nguyen, Hoai Viet and Iacono, Luigi Lo and Federrath, Hannes},
  year = {2019},
  pages = {1915--1930},
  issn = {15437221},
  doi = {10.1145/3319535.3354215},
  abstract = {Web caching enables the reuse of HTTP responses with the aim to reduce the number of requests that reach the origin server, the volume of network traffic resulting from resource requests, and the user-perceived latency of resource access. For these reasons, a cache is a key component in modern distributed systems as it enables applications to scale at large. In addition to optimizing performance metrics, caches promote additional protection against Denial of Service (DoS) attacks. In this paper we introduce and analyze a new class of web cache poisoning attacks. By provoking an error on the origin server that is not detected by the intermediate caching system, the cache gets poisoned with the server-generated error page and instrumented to serve this useless content instead of the intended one, rendering the victim service unavailable. In an extensive study of fifteen web caching solutions we analyzed the negative impact of the Cache-Poisoned DoS (CPDoS) attack-as we coined it. We show the practical relevance by identifying one proxy cache product and five CDN services that are vulnerable to CPDoS. Amongst them are prominent solutions that in turn cache high-value websites. The consequences are severe as one simple request is sufficient to paralyze a victim website within a large geographical region. The awareness of the newly introduced CPDoS attack is highly valuable for researchers for obtaining a comprehensive understanding of causes and countermeasures as well as practitioners for implementing robust and secure distributed systems.},
  file = {D\:\\GDrive\\zotero\\Nguyen\\nguyen_2019_your_cache_has_fallen.pdf},
  isbn = {9781450367479},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Cache Poisoning,Denial of Service,HTTP,Web Caching}
}

@article{NicholsJacobsonModern,
  title = {Nichols and {{Jacobson}} - {{A}} Modern {{AQM}} Is Just One Piece of the Solution To},
  file = {D\:\\GDrive\\zotero\\undefined\\nichols_and_jacobson_-_a_modern_aqm_is_just_one_piece_of_the_solution_to.pdf}
}

@techreport{nightingaleFlatDatacenterStorage,
  title = {Flat {{Datacenter Storage}}},
  author = {Nightingale, Edmund B and Elson, Jeremy and Fan, Jinliang and Hofmann, Owen and Howell, Jon and Suzue, Yutaka},
  abstract = {Flat Datacenter Storage (FDS) is a high-performance, fault-tolerant, large-scale, locality-oblivious blob store. Using a novel combination of full bisection bandwidth networks, data and metadata striping, and flow control, FDS multiplexes an application's large-scale I/O across the available throughput and latency budget of every disk in a cluster. FDS therefore makes many optimizations around data locality unnecessary. Disks also communicate with each other at their full bandwidth, making recovery from disk failures extremely fast. FDS is designed for datacenter scale, fully distributing metadata operations that might otherwise become a bottleneck. FDS applications achieve single-process read and write performance of more than 2 GB/s. We measure recovery of 92 GB data lost to disk failure in 6.2 s and recovery from a total machine failure with 655 GB of data in 33.7 s. Application performance is also high: we describe our FDS-based sort application which set the 2012 world record for disk-to-disk sorting.},
  file = {D\:\\GDrive\\zotero\\Nightingale\\nightingale_flat_datacenter_storage.pdf}
}

@article{nishtalaScalingMemcacheFacebook2013,
  title = {Scaling Memcache at Facebook},
  author = {Nishtala, Rajesh and Fugal, Hans and Grimm, Steven and Kwiatkowski, Marc and Lee, Herman and Li, Harry C. and McElroy, Ryan and Paleczny, Mike and Peek, Daniel and Saab, Paul and Stafford, David and Tung, Tony and Venkataramani, Venkateshwaran},
  year = {2013},
  pages = {385--398},
  abstract = {Memcached is a well known, simple, in-memory caching solution. This paper describes how Facebook leverages memcached as a building block to construct and scale a distributed key-value store that supports the world's largest social network. Our system handles billions of requests per second and holds trillions of items to deliver a rich experience for over a billion users around the world.},
  file = {D\:\\GDrive\\zotero\\Nishtala\\nishtala_2013_scaling_memcache_at_facebook.pdf},
  isbn = {9781931971003},
  journal = {Proceedings of the 10th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2013}
}

@techreport{noghabiSamzaStatefulScalable2150,
  title = {Samza: {{Stateful Scalable Stream Processing}} at {{LinkedIn}}},
  author = {Noghabi, Shadi A and Paramasivam, Kartik and Pan, Yi and Ramesh, Navina and Bringhurst, Jon and Gupta, Indranil and Campbell, Roy H},
  year = {2150},
  abstract = {Distributed stream processing systems need to support state-ful processing, recover quickly from failures to resume such processing, and reprocess an entire data stream quickly. We present Apache Samza, a distributed system for stateful and fault-tolerant stream processing. Samza utilizes a partitioned local state along with a low-overhead background changelog mechanism, allowing it to scale to massive state sizes (hundreds of TB) per application. Recovery from failures is sped up by rescheduling based on Host Affinity. In addition to processing infinite streams of events, Samza supports processing a finite dataset as a stream, from either a streaming source (e.g., Kafka), a database snapshot (e.g., Databus), or a file system (e.g. HDFS), without having to change the application code (unlike the popular Lambda-based architectures which necessitate maintenance of separate code bases for batch and stream path processing). Samza is currently in use at LinkedIn by hundreds of production applications with more than 10, 000 containers. Samza is an open-source Apache project adopted by many top-tier companies (e.g., LinkedIn, Uber, Netflix, TripAdvi-sor, etc.). Our experiments show that Samza: a) handles state efficiently, improving latency and throughput by more than 100\texttimes{} compared to using a remote storage; b) provides recovery time independent of state size; c) scales performance linearly with number of containers; and d) supports reprocessing of the data stream quickly and with minimal interference on real-time traffic.},
  file = {D\:\\GDrive\\zotero\\Noghabi\\noghabi_2150_samza.pdf}
}

@article{noormanSancusLowcostTrustworthy2013,
  title = {Sancus: {{Low}}-Cost {{Trustworthy Extensible Networked Devices}} with a {{Zero}}-Software {{Trusted Computing Base}}},
  author = {Noorman, Job and Agten, Pieter and Daniels, Wilfried and Strackx, Raoul and Van Herrewege, Anthony and Huygens, Christophe and Preneel, Bart and Verbauwhede, Ingrid and Piessens, Frank and Leuven, K U},
  year = {2013},
  abstract = {In this paper we propose Sancus, a security architecture for networked embedded devices. Sancus supports exten-sibility in the form of remote (even third-party) software installation on devices while maintaining strong security guarantees. More specifically, Sancus can remotely attest to a software provider that a specific software module is running uncompromised, and can authenticate messages from software modules to software providers. Software modules can securely maintain local state, and can securely interact with other software modules that they choose to trust. The most distinguishing feature of San-cus is that it achieves these security guarantees without trusting any infrastructural software on the device. The Trusted Computing Base (TCB) on the device is only the hardware. Moreover, the hardware cost of Sancus is low. We describe the design of Sancus, and develop and evaluate a prototype FPGA implementation of a Sancus-enabled device. The prototype extends an MSP430 processor with hardware support for the memory access control and cryptographic functionality required to run San-cus. We also develop a C compiler that targets our device and that can compile standard C modules to Sancus protected software modules.},
  file = {D\:\\GDrive\\zotero\\Noorman et al\\noorman_et_al_2013_sancus.pdf}
}

@techreport{nordstromBewareBGPAttacks,
  title = {Beware of {{BGP Attacks}}},
  author = {Nordstr{\"o}m, Ola and Nordstr{\"o}m, Nordstr{\textasciidieresis} and Dovrolis, Constantinos},
  abstract = {This note attempts to raise awareness within the network research community about the security of the interdomain routing infrastructure. We identify several attack objectives and mechanisms, assuming that one or more BGP routers have been compromised. Then, we review the existing and proposed countermeasures, showing that they are either generally ineffective (route filtering), or probably too heavyweight to deploy (S-BGP). We also review several recent proposals, and conclude by arguing that a significant research effort is urgently needed in the area of routing security .},
  file = {D\:\\GDrive\\zotero\\Nordström\\nordström_beware_of_bgp_attacks.pdf}
}

@techreport{nowlanCommunicatedLackPriori,
  title = {Communicated by {{The Lack}} of {{A Priori Distinctions Between Learning Algorithms}}},
  author = {Nowlan, Steven and Wolpert, David H},
  abstract = {This is the first of two papers that use off-training set (OTS) error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which fhere are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are "as many" targets (or priors over targets) for which A has lower expected OTS error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is "anti-cross-validation'' (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your OTS error is small. Other implications for "membership queries" algorithms and "punting" algorithms are also discussed. "Even after the observation of the frequent conjunction of objects , we have no reason to draw any inference concerning any object beyond those of which we have had experience." David Hume, in A Treatise of Human Nature, Book I, part 3, Section 12.},
  file = {D\:\\GDrive\\zotero\\Nowlan\\nowlan_communicated_by_the_lack_of_a_priori_distinctions_between_learning_algorithms.pdf}
}

@techreport{noyesBitAVFastAntiMalware,
  title = {{{BitAV}}: {{Fast Anti}}-{{Malware}} by {{Distributed Blockchain Consensus}} and {{Feedforward Scanning}}},
  author = {Noyes, Charles},
  abstract = {Synopsis-In the age of information, of the Internet, the protection of our most vital infrastructure becomes increasingly important. Moores law continues to prove accurate, with the number of transistors on standard integrated circuits doubling about every two years, but virus scanning applications have not innovated on the same level and development has stagnated. Thus, the attack surfaces become larger and the targets more lucrative, while the defensive mechanisms are failing to improve at a comparable rate. I present the design and implementation of a novel anti-malware environment called BitAV. BitAV allows for the decentralization of the update and maintenance mechanisms of the software, traditionally performed by a central host, and uses a staggered scanning mechanism in order to improve performance. The peer-to-peer network maintenance mechanism lowered the average update propagation speed by 500\% and is far less susceptible to targeted denial-of-service attacks. The feedforward scanning mechanism significantly improved end-to-end performance of the mal-ware matching system, to a degree of an average 14\texttimes{} increase, by decomposing the file matching process into efficient queries that operate in verifiably constant (O(1)) time.}
}

@techreport{noyesBlockchainMultipartyComputation,
  title = {Blockchain {{Multiparty Computation Markets}} at {{Scale}}},
  author = {Noyes, Charles},
  volume = {0},
  abstract = {We explore ways of allowing for the offloading of computationally rigorous tasks from devices with slow logical processors onto a network of anonymous peer-processors. Recent advances in secret sharing schemes, decentralized consensus mechanisms, and multiparty computation (MPC) protocols are combined to create a P2P MPC market. Unlike other computational "clouds", ours is able to generically compute any arithmetic circuit, providing a viable platform for processing on the semantic web. Finally, we show that such a system works in a hostile environment, that it scales well, and that it adapts very easily to any future advances in the complexity theoretic cryptography used. Specifically, we show that the feasibility of our system can only improve, and is historically guaranteed to do so.},
  file = {D\:\\GDrive\\zotero\\Noyes\\noyes_blockchain_multiparty_computation_markets_at_scale.pdf},
  journal = {IEEE TRANSACTIONS ON XXXXX},
  number = {0}
}

@article{nunesDIALEDDataIntegrity2021,
  title = {{{DIALED}}: {{Data Integrity Attestation}} for {{Low}}-End {{Embedded Devices}}},
  shorttitle = {{{DIALED}}},
  author = {Nunes, Ivan De Oliveira and Jakkamsetti, Sashidhar and Tsudik, Gene},
  year = {2021},
  month = mar,
  abstract = {Verifying integrity of software execution in low-end micro-controller units (MCUs) is a well-known open problem. The central challenge is how to securely detect software exploits with minimal overhead, since these MCUs are designed for low cost, low energy and small size. Some recent work yielded inexpensive hardware/software co-designs for remotely verifying code and execution integrity. In particular, a means of detecting unauthorized code modifications and control-flow attacks were proposed, referred to as Remote Attestation (RA) and Control-Flow Attestation (CFA), respectively. Despite this progress, detection of data-only attacks remains elusive. Such attacks exploit software vulnerabilities to corrupt intermediate computation results stored in data memory, changing neither the program code nor its control flow. Motivated by lack of any current techniques (for low-end MCUs) that detect these attacks, in this paper we propose, implement and evaluate DIALED, the first Data-Flow Attestation (DFA) technique applicable to the most resource-constrained embedded devices (e.g., TI MSP430). DIALED works in tandem with a companion CFA scheme to detect all (currently known) types of runtime software exploits at fairly low cost.},
  archiveprefix = {arXiv},
  eprint = {2103.12928},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Nunes et al\\nunes_et_al_2021_dialed.pdf;C\:\\Users\\Admin\\Zotero\\storage\\SPDCJD9V\\2103.html},
  journal = {arXiv:2103.12928 [cs]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Hardware Architecture},
  primaryclass = {cs}
}

@article{nunesTinyCFAMinimalisticApproach2020,
  title = {Tiny-{{CFA}}: {{A Minimalistic Approach}} for {{Control}}-{{Flow Attestation Using Verified Proofs}} of {{Execution}}},
  shorttitle = {Tiny-{{CFA}}},
  author = {Nunes, Ivan De Oliveira and Jakkamsetti, Sashidhar and Tsudik, Gene},
  year = {2020},
  month = dec,
  abstract = {The design of tiny trust anchors has received significant attention over the past decade, to secure low-end MCU-s that cannot afford expensive security mechanisms. In particular, hardware/software (hybrid) co-designs offer low hardware cost, while retaining similar security guarantees as (more expensive) hardware-based techniques. Hybrid trust anchors support security services, such as remote attestation, proofs of software update/erasure/reset, proofs of remote software execution, in resource-constrained MCU-s, e.g., MSP430 and AVR AtMega32. Despite these advances, detection of control-flow attacks in low-end MCU-s remains a challenge, since hardware requirements of the cheapest related architectures are often more expensive than the MCU-s themselves. In this work, we tackle this challenge by designing Tiny-CFA - a control-flow attestation (CFA) technique with a single hardware requirement - the ability to generate proofs of remote software execution (PoX). In turn, PoX can be implemented very efficiently and securely in low-end MCU-s. Consequently, our design achieves the lowest hardware overhead of any CFA architecture (i.e., two orders of magnitude cheaper), while relying on a formally verified PoX architecture as its sole hardware requirement. With respect to runtime overhead, Tiny-CFA also achieves better performance than prior CFA techniques based on code instrumentation. We implement and evaluate Tiny-CFA, analyze its security, and demonstrate its practicality using real-world publicly available applications.},
  archiveprefix = {arXiv},
  eprint = {2011.07400},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Nunes et al\\nunes_et_al_2020_tiny-cfa.pdf;C\:\\Users\\Admin\\Zotero\\storage\\E45VC4EC\\2011.html},
  journal = {arXiv:2011.07400 [cs]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Hardware Architecture},
  primaryclass = {cs}
}

@article{nunesTOCTOUProblemRemote2021,
  title = {On the {{TOCTOU Problem}} in {{Remote Attestation}}},
  author = {Nunes, Ivan De Oliveira and Jakkamsetti, Sashidhar and Rattanavipanon, Norrathep and Tsudik, Gene},
  year = {2021},
  month = apr,
  abstract = {We propose Remote Attestation with TOCTOU Avoidance (RATA): a provably secure approach to address the RA TOCTOU problem. With RATA, even malware that erases itself before execution of the next RA, can not hide its ephemeral presence. RATA targets hybrid RA architectures (implemented as Hardware/Software co-designs), which are aimed at low-end embedded devices. We present two alternative techniques - RATAa and RATAb - suitable for devices with and without real-time clocks, respectively. Each is shown to be secure and accompanied by a publicly available and formally verified implementation. Our evaluation demonstrates low hardware overhead of both techniques. Compared with current RA architectures - that offer no TOCTOU protection - RATA incurs no extra runtime overhead. In fact, RATA substantially reduces computational costs of RA execution.},
  archiveprefix = {arXiv},
  eprint = {2005.03873},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Nunes et al\\nunes_et_al_2021_on_the_toctou_problem_in_remote_attestation.pdf;C\:\\Users\\Admin\\Zotero\\storage\\8EHBGY3E\\2005.html},
  journal = {arXiv:2005.03873 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@article{nunesVRASEDVerifiedHardware2019,
  title = {{{VRASED}}: {{A Verified Hardware}}/{{Software Co}}-{{Design}} for {{Remote Attestation}}},
  author = {Nunes, Ivan De Oliveira and Eldefrawy, Karim and Rattanavipanon, Norrathep},
  year = {2019},
  pages = {19},
  abstract = {Remote Attestation (RA) is a distinct security service that allows a trusted verifier (V rf) to measure the software state of an untrusted remote prover (P rv). If correctly implemented, RA allows V rf to remotely detect if P rv is in an illegal or compromised state. Although several RA approaches have been explored (including hardware-based, software-based, and hybrid) and many concrete methods have been proposed, comparatively little attention has been devoted to formal verification. In particular, thus far, no RA designs and no implementations have been formally verified with respect to claimed security properties.},
  file = {D\:\\GDrive\\zotero\\Nunes et al\\nunes_et_al_2019_vrased.pdf},
  language = {en}
}

@article{nygrenAkamaiNetworkPlatform2010,
  title = {The {{Akamai}} Network: {{A}} Platform for High-Performance {{Internet}} Applications},
  author = {Nygren, Erik and Sitaraman, Ramesh K. and Sun, Jennifer},
  year = {2010},
  volume = {44},
  pages = {2--19},
  issn = {01635980},
  doi = {10.1145/1842733.1842736},
  abstract = {Comprising more than 61,000 servers located across nearly 1,000 networks in 70 countries worldwide, the Akamai platform delivers hundreds of billions of Internet interactions daily, helping thousands of enterprises boost the performance and reliability of their Internet applications. In this paper, we give an overview of the components and capabilities of this large-scale distributed computing platform, and offer some insight into its architecture, design principles, operation, and management.},
  file = {D\:\\GDrive\\zotero\\Nygren\\nygren_2010_the_akamai_network.pdf},
  journal = {Operating Systems Review (ACM)},
  keywords = {Akamai,Application acceleration,CDN,Content delivery,DNS,HTTP,Overlay networks,Quality of service,Streaming media},
  number = {3}
}

@techreport{ocallahanLightweightUserSpaceRecord,
  title = {Lightweight {{User}}-{{Space Record And Replay}}},
  author = {O'callahan, Robert and Jones, Chris and Froyd, Nathan and Huey, Kyle and Noll, Albert and Partush, Nimrod},
  abstract = {The ability to record and replay program executions with low overhead enables many applications, such as reverse-execution debugging, debugging of hard-to-reproduce test failures, and "black box" forensic analysis of failures in deployed systems. Existing record-and-replay approaches rely on recording an entire virtual machine (which is heavyweight), modifying the OS kernel (which adds deployment and maintenance costs), or pervasive code in-strumentation (which imposes significant performance and complexity overhead). We investigated whether it is possible to build a practical record-and-replay system avoiding all these issues. The answer turns out to be yes-if the CPU and operating system meet certain non-obvious constraints. Fortunately modern Intel CPUs, Linux kernels and user-space frameworks meet these constraints , although this has only become true recently. With some novel optimizations, our system RR records and replays real-world workloads with low overhead with an entirely user-space implementation running on stock hardware and operating systems. RR forms the basis of an open-source reverse-execution debugger seeing significant use in practice. We present the design and implementation of RR, describe its performance on a variety of workloads, and identify constraints on hardware and operating system design required to support our approach.},
  file = {D\:\\GDrive\\zotero\\O'callahan\\o'callahan_lightweight_user-space_record_and_replay.pdf}
}

@article{oconnorBlake3SpecOne,
  title = {Blake3 {{Spec}}: One Function, Fast Everywhere},
  author = {O'Connor, Jack and Aumasson, Jean-Philippe and Neves, Samuel and {Wilcox-O'Hearn}, Zooko},
  pages = {31},
  file = {D\:\\GDrive\\zotero\\O’Connor et al\\o’connor_et_al_blake3_spec.pdf},
  language = {en}
}

@article{ohmeProbabilisticAnalysisEfficiency2015,
  title = {A {{Probabilistic Analysis}} of the {{Efficiency}} of {{Automated Software Testing}}},
  author = {Ohme, Marcel B {\textasciidieresis} and Paul, Soumya},
  year = {2015},
  pages = {1},
  doi = {10.1109/TSE.2015.2487274},
  abstract = {We study the relative efficiencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and define sampling strategies for random (R) and systematic (S 0) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of confidence x in a program's correctness and (ii) discovering a maximal number of errors within a given time bound\textasciicircum nbound\textasciicircum{} bound\textasciicircum n. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S 0 on the average. Moreover for (i), this bound depends asymptotically only on x. We show that the efficiency of R can be fitted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S 0 when S 0 is expected to discover more errors per unit time. In our experiments we find that H performs similarly or better than the most efficient of both and that S 0 may need to be significantly faster than our bounds suggest to retain efficiency over R.},
  file = {D\:\\GDrive\\zotero\\Ohme\\ohme_2015_a_probabilistic_analysis_of_the_efficiency_of_automated_software_testing.pdf},
  journal = {TRANSACTIONS ON SOFTWARE ENGINEERING}
}

@techreport{okasakiPurelyFunctionalData1996,
  title = {Purely {{Functional Data Structures}}},
  author = {Okasaki, Chris and Harper, Robert and Sleator, Daniel and Tarjan, Robert},
  year = {1996},
  file = {D\:\\GDrive\\zotero\\Okasaki\\okasaki_1996_purely_functional_data_structures.pdf}
}

@article{omohundroCryptocurrenciesSmartContracts2014,
  title = {Cryptocurrencies, Smart Contracts, and Artificial Intelligence},
  author = {Omohundro, Steve},
  year = {2014},
  volume = {1},
  pages = {19--21},
  issn = {2372-3483},
  doi = {10.1145/2685328.2685334},
  abstract = {Recent developments in "cryptocurrencies" and "smart contracts" are creating new opportunities for applying AI techniques. These economic technologies would benefit from greater real world knowledge and reasoning as they become integrated with everyday commerce. Cryptocurrencies and smart contracts may also provide an infrastructure for ensuring that AI systems follow specified legal and safety regulations as they become more integrated into human society.},
  journal = {AI Matters},
  number = {2}
}

@inproceedings{onarliogluGFreeDefeatingReturnoriented2010,
  title = {G-{{Free}}: Defeating Return-Oriented Programming through Gadget-Less Binaries},
  shorttitle = {G-{{Free}}},
  booktitle = {Proceedings of the 26th {{Annual Computer Security Applications Conference}} on - {{ACSAC}} '10},
  author = {Onarlioglu, Kaan and Bilge, Leyla and Lanzi, Andrea and Balzarotti, Davide and Kirda, Engin},
  year = {2010},
  pages = {49},
  publisher = {{ACM Press}},
  address = {{Austin, Texas}},
  doi = {10.1145/1920261.1920269},
  abstract = {Despite the numerous prevention and protection mechanisms that have been introduced into modern operating systems, the exploitation of memory corruption vulnerabilities still represents a serious threat to the security of software systems and networks. A recent exploitation technique, called Return-Oriented Programming (ROP), has lately attracted a considerable attention from academia. Past research on the topic has mostly focused on refining the original attack technique, or on proposing partial solutions that target only particular variants of the attack.},
  file = {D\:\\GDrive\\zotero\\Onarlioglu et al\\onarlioglu_et_al_2010_g-free.pdf},
  isbn = {978-1-4503-0133-6},
  language = {en}
}

@techreport{oneilLogStructuredMergeTreeLSMTree,
  title = {The {{Log}}-{{Structured Merge}}-{{Tree}} ({{LSM}}-{{Tree}})},
  author = {O'neil, Patrick and Cheng, Edward and Gawlick, Dieter and O'neil, Elizabeth},
  abstract = {High-performance transaction system applications typically insert rows in a History table to provide an activity trace; at the same time the transaction system generates log records for purposes of system recovery. Both types of generated information can benefit from efficient indexing. An example in a well-known setting is the TPC-A benchmark application, modified to support efficient queries on the History for account activity for specific accounts. This requires an index by account-id on the fast-growing History table. Unfortunately, standard disk-based index structures such as the B-tree will effectively double the I/O cost of the transaction to maintain an index such as this in real time, increasing the total system cost up to fifty percent. Clearly a method for maintaining a real-time index at low cost is desirable. The Log-Structured Merge-tree (LSM-tree) is a disk-based data structure designed to provide low-cost indexing for a file experiencing a high rate of record inserts (and deletes) over an extended period. The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort. During this process all index values are continuously accessible to retrievals (aside from very short locking periods), either through the memory component or one of the disk components. The algorithm has greatly reduced disk arm movements compared to a traditional access methods such as B-trees, and will improve cost-performance in domains where disk arm costs for inserts with traditional access methods overwhelm storage media costs. The LSM-tree approach also generalizes to operations other than insert and delete. However, indexed finds requiring immediate response will lose I/O efficiency in some cases, so the LSM-tree is most useful in applications where index inserts are more common than finds that retrieve the entries. This seems to be a common property for History tables and log files, for example. The conclusions of Section 6 compare the hybrid use of memory and disk components in the LSM-tree access method with the commonly understood advantage of the hybrid method to buffer disk pages in memory.},
  file = {D\:\\GDrive\\zotero\\O'neil\\o'neil_the_log-structured_merge-tree_(lsm-tree).pdf}
}

@article{oneSmashingStackFun1996,
  title = {Smashing the {{Stack}} for {{Fun}} and Profit},
  author = {One, Aleph},
  year = {1996},
  pages = {1--32},
  file = {D\:\\GDrive\\zotero\\One\\one_smashing_the_stack_for_fun.pdf}
}

@phdthesis{ongaroCONSENSUSBRIDGINGTHEORY2014,
  title = {{{CONSENSUS}}: {{BRIDGING THEORY AND PRACTICE A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY}}},
  author = {Ongaro, Diego},
  year = {2014},
  file = {D\:\\GDrive\\zotero\\Ongaro\\ongaro_2014_consensus.pdf}
}

@techreport{ongaroSearchUnderstandableConsensus,
  title = {In {{Search}} of an {{Understandable Consensus Algorithm}} ({{Extended Version}})},
  author = {Ongaro, Diego and Ousterhout, John},
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandabil-ity, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.}
}

@techreport{ongaroSearchUnderstandableConsensus2013,
  title = {In {{Search}} of an {{Understandable Consensus Algorithm}}},
  author = {Ongaro, Diego and Ousterhout, John},
  year = {2013},
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandabil-ity, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
  file = {D\:\\GDrive\\zotero\\Ongaro\\ongaro_2013_in_search_of_an_understandable_consensus_algorithm.pdf}
}

@article{ongtangSemanticallyRichApplicationcentric2012,
  title = {Semantically Rich Application-Centric Security in {{Android}}},
  author = {Ongtang, M. and McLaughlin, S. and Enck, W and McDaniel, P.},
  year = {2012},
  volume = {5},
  pages = {422--437},
  issn = {09739769},
  doi = {10.1002/sec},
  abstract = {To protect user privacy and data security in cloud computing, a secure k-nearest neighbor computation-enhanced scheme on encrypted database has been proposed by Wong, Cheung, Kao and Mamoulis. The scheme is proven resistant to the known-plaintext attack. We show that contrary to claims, the enhanced asymmetric scalar-product-preserving encryption cannot resist known-plaintext attack by directly solving a secret key from a set of known plaintext\textendash ciphertext pairs.},
  file = {D\:\\GDrive\\zotero\\Ongtang\\ongtang_2012_semantically_rich_application-centric_security_in_android.pdf},
  isbn = {1111010110111},
  journal = {Security and Communication Networks},
  keywords = {Authentication,Privacy,Protocol design and analysis,RFID,Security},
  number = {August 2011}
}

@article{Operatorsoninhomogeneoustimeseries,
  title = {Operators-on-Inhomogeneous-Time-Series},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\7279CLA7\\operators-on-inhomogeneous-time-series.pdf}
}

@article{orenSpySandboxPractical2015,
  title = {The Spy in the Sandbox: {{Practical}} Cache Attacks in {{JavaScript}} and Their Implications},
  author = {Oren, Yossef and Kemerlis, Vasileios P. and Sethumadhavan, Simha and Keromytis, Angelos D.},
  year = {2015},
  volume = {2015-Octob},
  pages = {1406--1418},
  issn = {15437221},
  doi = {10.1145/2810103.2813708},
  abstract = {We present a micro-architectural side-channel attack that runs entirely in the browser. In contrast to previous work in this genre, our attack does not require the attacker to install software on the victim's machine; to facilitate the attack, the victim needs only to browse to an untrusted webpage that contains attacker-controlled content. This makes our attack model highly scalable, and extremely relevant and practical to today'sWeb, as most desktop browsers currently used to access the Internet are affected by such side channel threats. Our attack, which is an extension to the last-level cache attacks of Liu et al. [14], allows a remote adversary to recover information belonging to other processes, users, and even virtual machines running on the same physical host with the victim web browser. We describe the fundamentals behind our attack, and evaluate its performance characteristics. In addition, we show how it can be used to compromise user privacy in a common setting, letting an attacker spy after a victim that uses private browsing. Defending against this side channel is possible, but the required countermeasures can exact an impractical cost on benign uses of the browser.},
  file = {D\:\\GDrive\\zotero\\Oren\\oren_2015_the_spy_in_the_sandbox.pdf},
  isbn = {9781450338325},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Cache-timing attacks,Covert channel,JavaScript-based cache attacks,Side-channel attacks,User tracking}
}

@techreport{orsoEffectsPointersData2001,
  title = {Effects of {{Pointers}} on {{Data Dependences}}},
  author = {Orso, Alessandro and Sinha, Saurabh and Harrold, Mary Jean},
  year = {2001},
  abstract = {This paper presents a technique for computing and classifying data dependences that takes into account the complexities introduced by specific language constructs, such as pointers, arrays, and structures. The classification is finer-grained than previously proposed classifications. Moreover, unlike previous work, the paper presents empirical results that illustrate the distribution of data dependences for a set of C subjects. The paper also presents a potential application for the proposed classification-program slicing-and a technique that computes slices based on data-dependence types. This technique facilitates the use of slicing for program understanding because a user can either augment a slice incrementally, by incorporating data dependences based on their relevance, or focus on specific kinds of dependences. Finally, the paper presents a case study that shows how the incremental addition of data dependences allows for growing the size of the slices in steps.},
  file = {D\:\\GDrive\\zotero\\Orso et al\\orso_et_al_effects_of_pointers_on_data_dependences.pdf}
}

@techreport{orsoIncrementalSlicingBased2001,
  title = {Incremental {{Slicing Based}} on {{Data}}-{{Dependences Types}}},
  author = {Orso, Alessandro and Sinha, Saurabh and Harrold, Mary Jean},
  year = {2001},
  institution = {{IEEE Copyright}},
  abstract = {Program slicing is useful for assisting with many software-maintenance tasks. The presence and frequent usage of pointers in languages such as C causes complex data dependences. To function effectively on such programs , slicing techniques must account for pointer-induced data dependences. Existing slicing techniques do not distinguish data dependences based on their types. This paper presents a new slicing technique, in which slices are computed based on types of data dependences. This new slicing technique offers several benefits and can be exploited in different ways, such as identifying subtle data dependences for debugging, computing reduced-size slices quickly for complex programs, and performing incremental slicing. This paper describes an algorithm for incremental slicing that increases the scope of a slice in steps, by incorporating different types of data dependences at each step. The paper also presents empirical results to illustrate the performance of the technique in practice. The results illustrate that in-cremental slices can be significantly smaller than complete slices. Finally, the paper presents a case study that explores the usefulness of incremental slicing for debugging.},
  file = {D\:\\GDrive\\zotero\\Orso et al\\orso_et_al_incremental_slicing_based_on_data-dependences_types.pdf}
}

@article{ortizUnderstandingSecurityAndroid2010,
  title = {Understanding Security on {{Android}}},
  author = {Ortiz, C Enrique},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Ortiz\\ortiz_2010_understanding_security_on_android.pdf},
  journal = {IBM - Develorper Works}
}

@book{osterweiluclaQuantifyingOperationalStatus2008,
  title = {Quantifying the {{Operational Status}} of the {{DNSSEC Deployment}}},
  author = {Osterweil UCLA, Eric and Ryan UCLA, Michael and Massey, Dan and Zhang UCLA, Lixia},
  year = {2008},
  abstract = {This paper examines the deployment of the DNS Security Extensions (DNSSEC), which adds cryptographic protection to DNS, one of the core components in the Internet infrastructure. We analyze the data collected from the initial DNSSEC deployment which started over 2 years ago, and identify three critical metrics to gauge the deployment: availability, verifiability, and validity. Our results provide the first comprehensive look at DNSSEC's deployment and reveal a number of challenges that were not anticipated in the design but have become evident in the deployment. First, obstacles such as middle-boxes (firewalls, NATs, etc.) that exist in today's Internet infrastructure have proven to be problematic and have resulted in unforeseen availability problems. Second, the public-key delegation system of DNSSEC has not evolved as it was hoped and it currently leaves over 97\% of DNSSEC zones isolated and unverifi-able, unless some external key authentication mechanism is added. Furthermore, our results show that cryptographic verification is not equivalent to validation; a piece of verified data can still contain the wrong value. Finally, our results demonstrate the essential role of monitoring and measurement in the DNSSEC deployment. We believe that the observations and lessons from the DNSSEC deployment can provide insights into measuring future Internet-scale cryptographic systems.},
  file = {D\:\\GDrive\\zotero\\Osterweil UCLA\\osterweil_ucla_2008_quantifying_the_operational_status_of_the_dnssec_deployment.pdf},
  isbn = {978-1-60558-334-1},
  keywords = {C20 [Computer Systems Organization]: Computer-Communication Networks-Security and protection,C23 [Computer Systems Organization]: Computer-Com-munication Networks-Network Monitoring,C4 [Computer Systems Organization]: Performance of Systems General Terms Management; Measurement; Security Keywords DNSSEC; Measurement; Internet-Scale Security; Metrics}
}

@article{osvikCacheAttacksCountermeasures2006,
  title = {Cache Attacks and Counter-Measures: {{The}} Case of {{AES}}},
  author = {Osvik, Dag Arne and Shamir, Adi and Tromer, Eran},
  year = {2006},
  volume = {3960 LNCS},
  pages = {1--20},
  issn = {03029743},
  doi = {10.1007/11605805_1},
  abstract = {We describe several software side-channel attacks based on inter-process leakage through the state of the CPU's memory cache. This leakage reveals memory access patterns, which can be used for cryptanalysis of cryptographic primitives that employ data-dependent table lookups, The attacks allow an unprivileged process to attack other processes running in parallel on the same processor, despite partitioning methods such as memory protection, sandboxing and virtualization. Some of our methods require only the ability to trigger services that perform encryption or MAC using the unknown key, such as encrypted disk partitions or secure network links. Moreover, we demonstrate an extremely strong type of attack, which requires knowledge of neither the specific plaintexts nor ciphertexts, and works by merely monitoring the effect of the cryptographic process on the cache, We discuss in detail several such attacks on AES, and experimentally demonstrate their applicability to real systems, such as OpenSSL and Linux's dm-crypt encrypted partitions (in the latter case, the full key can be recovered after just 800 writes to the partition, taking 65 milliseconds). Finally, we describe several countermeasures for mitigating such attacks. \textcopyright{} Springer-Verlag Berlin Heidelberg 2006.},
  file = {D\:\\GDrive\\zotero\\Osvik\\osvik_2006_cache_attacks_and_counter-measures.pdf},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {AES,Cache,Cryptanalysis,Memory access,Side-channel attack}
}

@techreport{ottensteinProgramDependenceGraph1984,
  title = {The {{Program Dependence Graph}} in a {{Software Development Environment}}},
  author = {Ottenstein, Karl J and Ottenstein, Linda M},
  year = {1984},
  abstract = {The internal program representation chosen for a software development environment plays a critical role in the nature of that enviromnent. A form should facilitate implementation and contribute to the responsiveness of the environment to the user. The program depe\textasciitilde dgnce\_\textasciitilde raph (PDG) may he a suitable internal form. It allows programs to be sliced in linear time for debugging and for use by language-directed editors. The slices obtained are more accurate than those obtained with existing methods because I/O is accounted for correctly and irrelevant statements on multi-statement lines are not displayed. The PDG may be interpreted in a data driven fashion or may have highly optimized (including vectorized) code produced from it. It is amenable to incremental data flow analysis, improving response time to the user in an interactive environment and facilitating debugging through data flow anomaly detection. It may also offer a good basis for software complexity metrics, adding to the completeness of an environment based on it.},
  file = {D\:\\GDrive\\zotero\\Ottenstein_Ottenstein\\ottenstein_ottenstein_the_program_dependence_graph_in_a_software_development_environment.pdf},
  keywords = {code optimization,control flow,D22,D25,D26,D28,D34 Additional Key Words and Phrases: internal program representation,data flow,debugging,DI2,interpreter,program slice,software complexity metrics}
}

@inproceedings{ousterhoutSparrowDistributedLow2013,
  title = {Sparrow: Distributed, Low Latency Scheduling},
  shorttitle = {Sparrow},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth ACM Symposium}} on {{Operating Systems Principles}}},
  author = {Ousterhout, Kay and Wendell, Patrick and Zaharia, Matei and Stoica, Ion},
  year = {2013},
  month = nov,
  pages = {69--84},
  publisher = {{ACM}},
  address = {{Farminton Pennsylvania}},
  doi = {10.1145/2517349.2522716},
  abstract = {Large-scale data analytics frameworks are shifting towards shorter task durations and larger degrees of parallelism to provide low latency. Scheduling highly parallel jobs that complete in hundreds of milliseconds poses a major challenge for task schedulers, which will need to schedule millions of tasks per second on appropriate machines while offering millisecond-level latency and high availability. We demonstrate that a decentralized, randomized sampling approach provides near-optimal performance while avoiding the throughput and availability limitations of a centralized design. We implement and deploy our scheduler, Sparrow, on a 110-machine cluster and demonstrate that Sparrow performs within 12\% of an ideal scheduler.},
  file = {D\:\\GDrive\\zotero\\Ousterhout et al\\ousterhout_et_al_2013_sparrow.pdf},
  isbn = {978-1-4503-2388-8},
  language = {en}
}

@article{P56intanagonwiwat,
  title = {P56-Intanagonwiwat},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CIMZ9CVX\\p56-intanagonwiwat.pdf}
}

@inproceedings{pachecoRandoopFeedbackdirectedRandom2007,
  title = {Randoop: {{Feedback}}-Directed Random Testing for {{Java}}},
  booktitle = {Proceedings of the {{Conference}} on {{Object}}-{{Oriented Programming Systems}}, {{Languages}}, and {{Applications}}, {{OOPSLA}}},
  author = {Pacheco, Carlos and Ernst, Michael D},
  year = {2007},
  volume = {2},
  pages = {815--816},
  doi = {10.1145/1297846.1297902},
  abstract = {RANDOOP FOR JAVA generates unit tests for Java code using feedback-directed random test generation. Below we describe RANDOOP'S input, output, and test generation algorithm. We also give an overview of RANDOOP'S annotation-based interface for specifying configuration parameters that affect RANDOOP'S behavior and output.},
  file = {D\:\\GDrive\\zotero\\Pacheco\\pacheco_2007_randoop.pdf},
  isbn = {978-1-59593-865-7},
  keywords = {Java,Random testing}
}

@article{padlewskiDevirtualizationLLVM2017,
  title = {Devirtualization in {{LLVM}}},
  author = {Padlewski, Piotr},
  year = {2017},
  doi = {10.1145/3135932.3135947},
  abstract = {Devirtualization is an optimization changing indirect (vir-tual) calls to direct calls. It improves performance by allowing extra inlining and removal of redundant loads. This paper presents a novel way of handling C++ devirtualization in LLVM by unifying virtual table loads across calls using different SSA values to represent different dynamic types.},
  file = {D\:\\GDrive\\zotero\\Padlewski\\padlewski_2017_devirtualization_in_llvm.pdf},
  isbn = {9781450355148},
  keywords = {CCS Concepts • Software and its engineering → Source code generation,Keywords devirtualization; llvm; C++; virtual function; in-direct call,Polymorphism}
}

@techreport{PageRankCitationRanking1998,
  title = {The {{PageRank Citation Ranking}}: {{Bringing Order}} to the {{Web}}},
  year = {1998},
  abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, eeectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to eeciently compute PageRank for large numbers of pages. And, we show h o w to apply PageRank to search and to user navigation.},
  file = {D\:\\GDrive\\zotero\\undefined\\1998_the_pagerank_citation_ranking.pdf}
}

@article{pailoorAutomatedPolicySynthesis2020,
  title = {Automated Policy Synthesis for System Call Sandboxing},
  author = {Pailoor, Shankara and Wang, Xinyu and Shacham, Hovav and Dillig, Isil},
  year = {2020},
  month = nov,
  volume = {4},
  pages = {1--26},
  issn = {2475-1421},
  doi = {10.1145/3428203},
  abstract = {SHANKARA PAILOOR, University of Texas at Austin, USA XINYU WANG, University of Michigan, USA HOVAV SHACHAM, University of Texas at Austin, USA ISIL DILLIG, University of Texas at Austin, USA System call whitelisting is a powerful sandboxing approach that can significantly reduce the capabilities of an attacker if an application is compromised. Given a policy that specifies which system calls can be invoked with what arguments, a sandboxing framework terminates any execution that violates the policy. While this mechanism greatly reduces the attack surface of a system, manually constructing these policies is time-consuming and error-prone. As a result, many applications \DH including those that take untrusted user input\DH{} opt not to use a system call sandbox. Motivated by this problem, we propose a technique for automatically constructing system call whitelisting policies for a given application and policy DSL. Our method combines static code analysis and program synthesis to construct sound and precise policies that never erroneously terminate the application, while restricting the program's system call usage as much as possible. We have implemented our approach in a tool called Abhaya and experimentally evaluate it 674 Linux and OpenBSD applications by automatically synthesizing Seccomp-bpf and Pledge policies. Our experimental results indicate that Abhaya can efficiently generate useful and precise sandboxes for real-world applications. CCS Concepts: {$\cdot$} Security and privacy \textrightarrow{} Trust frameworks.},
  file = {D\:\\GDrive\\zotero\\Pailoor et al\\pailoor_et_al_2020_automated_policy_synthesis_for_system_call_sandboxing.pdf},
  journal = {Proceedings of the ACM on Programming Languages},
  language = {en},
  number = {OOPSLA}
}

@article{pakinLATEXSymbolList2005,
  title = {{{LATEX Symbol}} List},
  author = {Pakin, Scott},
  year = {2005},
  volume = {225},
  pages = {1--58},
  issn = {15327752},
  doi = {10.1080/00223891.2010.497396},
  abstract = {The interaction of FK-506 with K(V)1.3, stably expressed in Chinese hamster ovary cells, was investigated with the whole cell patch-clamp technique. FK-506 inhibited K(V)1.3 in a reversible, concentration-dependent manner with an IC(50) of 5.6 microM. Rapamycin, another immunosuppressant, produced effects that were similar to those of FK-506 (IC(50) = 6.7 microM). Other calcineurin inhibitors (cypermethrin or calcineurin autoinhibitory peptide) alone had no effect on the amplitude or kinetics of K(V)1.3. In addition, the inhibitory action of FK-506 continued, even after the inhibition of calcineurin activity. The inhibition produced by FK-506 was voltage dependent, increasing in the voltage range for channel activation. At potentials positive to 0 mV (where maximal conductance is reached), however, no voltage-dependent inhibition was found. FK-506 exhibited a strong use-dependent inhibition of K(V)1.3. FK-506 shifted the steady-state inactivation curves of K(V)1.3 in the hyperpolarizing direction in a concentration-dependent manner. The apparent dissociation constant for FK-506 to inhibit K(V)1.3 in the inactivated state was estimated from the concentration-dependent shift in the steady-state inactivation curve and was calculated to be 0.37 microM. Moreover, the rate of recovery from inactivation of K(V)1.3 was decreased. In inside-out patches, FK-506 not only reduced the current amplitude but also accelerated the rate of inactivation during depolarization. FK-506 also inhibited K(V)1.5 and K(V)4.3 in a concentration-dependent manner with IC(50) of 4.6 and 53.9 microM, respectively. The present results indicate that FK-506 inhibits K(V)1.3 directly and that this effect is not mediated via the inhibition of the phosphatase activity of calcineurin},
  journal = {Network},
  number = {September},
  pmid = {17166943}
}

@techreport{paksulaPersistingObjectsRedis,
  title = {Persisting {{Objects}} in {{Redis Key}}-{{Value Database}}},
  author = {Paksula, Matti},
  abstract = {In this paper an approach to persist objects in Redis key-value database is provided. Pattern focuses on the consistency of objects under race and failure conditions. Proposed pattern is useful for cloud storage implementations that require speed and scalability. This approach can optionally be implemented with schemaless data model. Implementation can be done with any language, although the examples are presented with Ruby. Short overview on cloud architecture design is given as setting the context for using this persistence pattern.},
  file = {D\:\\GDrive\\zotero\\Paksula\\paksula_persisting_objects_in_redis_key-value_database.pdf}
}

@article{palkarWeldCommonRuntime,
  title = {Weld: {{A Common Runtime}} for {{High Performance Data Analytics}}},
  author = {Palkar, Shoumik and Thomas, James J and Shanbhag, Anil and Narayanan, Deepak and Pirk, Holger and Schwarzkopf, Malte and Amarasinghe, Saman and Zaharia, Matei},
  pages = {8},
  abstract = {Modern analytics applications combine multiple functions from different libraries and frameworks to build increasingly complex workflows. Even though each function may achieve high performance in isolation, the performance of the combined workflow is often an order of magnitude below hardware limits due to extensive data movement across the functions. To address this problem, we propose Weld, a runtime for data-intensive applications that optimizes across disjoint libraries and functions. Weld uses a common intermediate representation to capture the structure of diverse dataparallel workloads, including SQL, machine learning and graph analytics. It then performs key data movement optimizations and generates efficient parallel code for the whole workflow. Weld can be integrated incrementally into existing frameworks like TensorFlow, Apache Spark, NumPy and Pandas without changing their user-facing APIs. We show that Weld can speed up these frameworks, as well as applications that combine them, by up to 30\texttimes.},
  file = {D\:\\GDrive\\zotero\\Palkar et al\\palkar_et_al_weld.pdf},
  language = {en}
}

@techreport{pangZanzibarGoogleConsistent,
  title = {Zanzibar: {{Google}}'s {{Consistent}}, {{Global Authorization System}}},
  author = {Pang, Ruoming and C{\'a}ceres, Ram{\'o}n and Burrows, Mike and Chen, Zhifeng and Dave, Pratik and Germer, Nathan and Golynski, Alexander and Graney, Kevin and Kang, Nina and Kissner, Lea and Korn, Jeffrey L and Parmar, Abhishek and Richards, Christopher D and Wang, Mengzhi and Google, Llc ;},
  abstract = {Determining whether online users are authorized to access digital objects is central to preserving privacy. This paper presents the design, implementation, and deployment of Zanzibar, a global system for storing and evaluating access control lists. Zanzibar provides a uniform data model and configuration language for expressing a wide range of access control policies from hundreds of client services at Google, including Calendar, Cloud, Drive, Maps, Photos, and YouTube. Its authorization decisions respect causal ordering of user actions and thus provide external consistency amid changes to access control lists and object contents. Zanzibar scales to trillions of access control lists and millions of authorization requests per second to support services used by billions of people. It has maintained 95th-percentile la-tency of less than 10 milliseconds and availability of greater than 99.999\% over 3 years of production use.},
  file = {D\:\\GDrive\\zotero\\Pang\\pang_zanzibar.pdf}
}

@article{paolinoTKVMTrustedArchitecture2015,
  title = {T-{{KVM}}: {{A Trusted}} Architecture for {{KVM ARM}} v7 and v8 {{Virtual Machines Securing Virtual Machines}} by Means of {{KVM}}, {{TrustZone}}, {{TEE}} and {{SELinux}}},
  author = {Paolino, Michele and Rigo, Alvise and Spyridakis, Alexander and Fanguede, Jeremy and Lalov, Petar and Raho, Daniel},
  year = {2015},
  pages = {39--45},
  issn = {2308-4294},
  abstract = {The first market release of Advanced RISC Machines (ARM) v8 System on Chips (SoCs) has created big expectations from smart devices, servers and network equipment vendors, who see compelling advantages in integrating them into their systems. As a consequence software stack deployments for ARMv8 platforms translate market requirements to support OpenStack, Network Functions Virtualization (NFV), Mobile Edge Computing (MEC), In-Vehicle Infotainment (IVI) automotive functions. At the same time, ARMv8 will empower Internet of Things (IoT), Cyber Physical Systems (CPS) and user convergence devices. In this context, virtualization is a key feature to enable the cloud delivery model, to implement multitenancy, to isolate different execution environments and to improve hardware/software standardization and consolidation. Since guaranteeing a strict ownership of both the data and the code executed in Virtual Machines (VMs), which belong to governments, companies, telecom operators and private users, counts more than ever, the security of the hypervisor and its guests has become dramatically important. In this paper, Trusted Kernel-based Virtual Machine (T-KVM), a novel security architecture for the KVM-on-ARM hypervisor, is proposed to satisfy the current market trend. T-KVM integrates software and hardware components to secure guest Operating Systems (OSes) and enable Trusted Computing in ARM virtual machines. The proposed architecture combines four isolation layers: ARM Virtualization and Security Extensions (also known as ARM VE and TrustZone), GlobalPlatform Trusted Execution Environment (TEE) APIs and SELinux Mandatory Access Control (MAC) security policy. The T-KVM architecture can be implemented on platforms based on ARM v7 and v8 architectures, without requiring additional custom hardware extensions, since, starting from Cortex-A15 (ARM v7 architecture) released in 2012, both the ARM VE and TrustZone are made available. In this paper the T-KVM architecture is described in details, as well as its key implementation challenges and system security considerations. Lastly, a performance evaluation of the proposed solution is presented.},
  isbn = {9781612083889},
  journal = {Cloud Computing 2015: the Sixth International Conference on Cloud Computing, Grids, and Virtualization},
  keywords = {arm,arm virtualization,armv8 trusted com-,kvm security,kvm trustzone,puting,selinux,tee,trusted kvm},
  number = {c}
}

@article{papamarcosIPOCPROC1984,
  title = {{{IP OC I PROC I Roc I}}},
  author = {Papamarcos, Mark S and Patel, Janak H},
  year = {1984},
  pages = {348--354},
  file = {D\:\\GDrive\\zotero\\Papamarcos\\papamarcos_1984_ip_oc_i_proc_i_roc_i.pdf},
  journal = {System}
}

@article{PaperFloatingPoint,
  title = {Paper - Floating Point}
}

@techreport{paperPublicPrivateBlockchains2015,
  title = {Public versus {{Private Blockchains Part}} 2: {{Permissionless Blockchains}}},
  author = {Paper, White},
  year = {2015},
  abstract = {Blockchain-based solutions are one of the major areas of research for financial institutions and in other applications across the globe. There is currently an ongoing debate whether the existing blockchain-based systems (such as Bitcoin and other cryptocurrencies) can be utilized as is in proprietary contexts, and whether their openness and censorship resistance are fitting properties in this case. We provide arguments for the use of permissionless blockchains and open blockchain protocols in creating ledgers and registries, devoting particular attention to the Bitcoin blockchain as the most commercially successful and secure permissionless blockchain. We study potential applications of permissionless chains in proprietary environments, such as colored coins, peer-to-peer payment channels and transaction processing by known validators.},
  file = {D\:\\GDrive\\zotero\\Paper\\paper_2015_public_versus_private_blockchains_part_2.pdf}
}

@misc{ParadoxChoice,
  title = {Paradox of {{Choice}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\43H747P4\\paradox-of-choice.html},
  journal = {Azeria-Labs},
  language = {en-US}
}

@book{parijaThesisWritingMaster2018,
  title = {Thesis {{Writing}} for {{Master}}'s and {{Ph}}.{{D}}. {{Program}}},
  editor = {Parija, Subhash Chandra and Kate, Vikram},
  year = {2018},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-0890-1},
  file = {D\:\\GDrive\\zotero\\Parija_Kate\\parija_kate_2018_thesis_writing_for_master's_and_ph.pdf},
  isbn = {9789811308895 9789811308901},
  language = {en}
}

@article{parkCarFastAchievingHigher2012,
  title = {{{CarFast}}: {{Achieving}} Higher Statement Coverage Faster},
  author = {Park, Sangmin and Hossain, B. M.Mainul and Hussain, Ishtiaque and Csallner, Christoph and Grechanik, Mark and Taneja, Kunal and Fu, Chen and Xie, Qing},
  year = {2012},
  pages = {1--11},
  doi = {10.1145/2393596.2393636},
  abstract = {Test coverage is an important metric of software quality, since it indicates thoroughness of testing. In industry, test coverage is often measured as statement coverage. A fundamental problem of software testing is how to achieve higher statement coverage faster, and it is a difficult problem since it requires testers to cleverly find input data that can steer execution sooner toward sections of application code that contain more statements. We created a novel fully automatic approach for aChieving higher stAtement coveRage FASTer (CarFast), which we implemented and evaluated on twelve generated Java applications whose sizes range from 300 LOC to one million LOC. We compared CarFast with several popular test case generation techniques, including pure random, adaptive random, and Directed Automated Random Testing (DART). Our results indicate with strong statistical significance that when execution time is measured in terms of the number of runs of the application on different input test data, CarFast outperforms the evaluated competitive approaches on most subject applications. \textcopyright{} 2012 ACM.},
  file = {D\:\\GDrive\\zotero\\Park\\park_2012_carfast.pdf},
  isbn = {9781450316149},
  journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012},
  keywords = {experimentation,statement coverage,testing}
}

@book{partridgeInnovationsInternetworking1988,
  title = {Innovations in Internetworking},
  author = {Partridge, Craig},
  year = {1988},
  publisher = {{Artech House}},
  file = {D\:\\GDrive\\zotero\\Partridge\\partridge_1988_innovations_in_internetworking.pdf},
  isbn = {0-89006-337-0}
}

@techreport{pattabiramanAUTOMATEDDERIVATIONAPPLICATIONAWARE2001,
  title = {{{AUTOMATED DERIVATION OF APPLICATION}}-{{AWARE ERROR AND ATTACK DETECTORS}}},
  author = {Pattabiraman, Karthik},
  year = {2001},
  file = {D\:\\GDrive\\zotero\\Pattabiraman\\pattabiraman_2001_automated_derivation_of_application-aware_error_and_attack_detectors.pdf}
}

@techreport{paulWhySecurityDefects2021,
  title = {Why {{Security Defects Go Unnoticed}} during {{Code Reviews}}? {{A Case}}-{{Control Study}} of the {{Chromium OS Project}}},
  author = {Paul, Rajshakhar and Kamal Turzo, Asif and Bosu, Amiangshu},
  year = {2021},
  abstract = {Peer code review has been found to be effective in identifying security vulnerabilities. However, despite practicing mandatory code reviews, many Open Source Software (OSS) projects still encountering a large number of post-release security vulnerabilities, as some security defects remain undetected during code reviews. Therefore, a project manager may wonder if there was any weakness or inconsistency during a code review that missed a security vulnerability. Knowledge about the types of security defects that are escaping and the characteristics of code review that failed to identify security defects can help practitioners to improve the effectiveness of code reviews in identifying security defects. Therefore, this research aims to identify the factors that differentiate code reviews that successfully identified security defects from those that missed such defects.. With this goal, we conduct a case-control study of Chromium OS project. Using multi-stage semi-automated approaches, we build a dataset of 516 code reviews that successfully identified security defects and 374 code reviews where security defects escaped. The results of our empirical study suggest that there is a significant difference between types of security defects that are identified and escaped during code reviews. We also build a logistic regression model to fit our dataset. The results of our model analysis suggest that certain factors play a significant role in distinguishing code reviews that successfully identify security defects from code reviews where security defects escape.},
  file = {D\:\\GDrive\\zotero\\Paul et al\\paul_et_al_2021_why_security_defects_go_unnoticed_during_code_reviews.pdf},
  keywords = {code review,discussion,Index Terms-security,venerability}
}

@techreport{paxsonBroSystemDetecting,
  title = {Bro: {{A System}} for {{Detecting Network Intruders}} in {{Real}}-{{Time}}},
  author = {Paxson, Vern},
  abstract = {We describe Bro, a stand-alone system for detecting network intruders in real-time by passively monitoring a network link over which the intruder's traffic transits. We give an overview of the system's design, which emphasizes high-speed (FDDI-rate) monitoring, real-time notification, clear separation between mechanism and policy, and extensibility. To achieve these ends, Bro is divided into an "event engine" that reduces a kernel-filtered network traffic stream into a series of higher-level events, and a "policy script interpreter" that interprets event handlers written in a specialized language used to express a site's security policy. Event handlers can update state information, synthesize new events, record information to disk, and generate real-time notifications via syslog. We also discuss a number of attacks that attempt to subvert passive monitoring systems and defenses against these, and give particulars of how Bro analyzes the four applications integrated into it so far: Finger, FTP, Portmapper and Telnet. The system is publicly available in source code form.},
  file = {D\:\\GDrive\\zotero\\Paxson\\paxson_bro.pdf}
}

@techreport{paxsonEndtoEndInternetPacket1997,
  title = {End-to-{{End Internet Packet Dynamics}}},
  author = {Paxson, Vern},
  year = {1997},
  abstract = {We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20,000 TCP bulk transfers between 35 Internet sites. Because we traced each 100 Kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-to-end behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We characterize the prevalence of unusual network events such as out-of-order delivery and packet corruption; discuss a robust receiver-based algorithm for estimating "bottleneck bandwidth" that addresses deficiencies discovered in techniques based on "packet pair"; investigate patterns of packet loss, finding that loss events are not well-modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales.},
  file = {D\:\\GDrive\\zotero\\Paxson\\paxson_1997_end-to-end_internet_packet_dynamics.pdf}
}

@techreport{paxsonStrategiesSoundInternet2004,
  title = {Strategies for {{Sound Internet Measurement}}},
  author = {Paxson, Vern},
  year = {2004},
  abstract = {Conducting an Internet measurement study in a sound fashion can be much more difficult than it might first appear. We present a number of strategies drawn from experiences for avoiding or overcoming some of the pitfalls. In particular, we discuss dealing with errors and inaccuracies; the importance of associating meta-data with measurements; the technique of calibrating measurements by examining outliers and testing for consistencies; difficulties that arise with large-scale measurements; the utility of developing a discipline for reliably reproducing analysis results; and issues with making datasets publicly available. We conclude with thoughts on the sorts of tools and community practices that can assist researchers with conducting sound measurement studies.},
  file = {D\:\\GDrive\\zotero\\Paxson\\paxson_2004_strategies_for_sound_internet_measurement.pdf},
  keywords = {C25 [Local and Wide-Area Networks]: Internet General Terms: Measurement,Calibration,Datasets,Experimentation Keywords: Internet Measurement,Meta-data,Reproducibility}
}

@incollection{payerFineGrainedControlFlowIntegrity2015,
  title = {Fine-{{Grained Control}}-{{Flow Integrity Through Binary Hardening}}},
  booktitle = {Detection of {{Intrusions}} and {{Malware}}, and {{Vulnerability Assessment}}},
  author = {Payer, Mathias and Barresi, Antonio and Gross, Thomas R.},
  editor = {Almgren, Magnus and Gulisano, Vincenzo and Maggi, Federico},
  year = {2015},
  volume = {9148},
  pages = {144--164},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-20550-2_8},
  abstract = {Applications written in low-level languages without type or memory safety are prone to memory corruption. Attackers gain code execution capabilities through memory corruption despite all currently deployed defenses. Control-Flow Integrity (CFI) is a promising security property that restricts indirect control-flow transfers to a static set of well-known locations.},
  file = {D\:\\GDrive\\zotero\\Payer et al\\payer_et_al_2015_fine-grained_control-flow_integrity_through_binary_hardening.pdf},
  isbn = {978-3-319-20549-6 978-3-319-20550-2},
  language = {en}
}

@techreport{peaseReachingAgreementPresence1980,
  title = {Reaching {{Agreement}} in the {{Presence}} of {{Faults}}},
  author = {Pease, M and Lamport, L},
  year = {1980},
  abstract = {The problem addressed here concerns a set of isolated processors, some unknown subset of which may be faulty, that communicate only by means of two-party messages. Each nonfaulty processor has a private value of reformation that must be communicated to each other nonfanlty processor. Nonfaulty processors always communicate honestly, whereas faulty processors may lie The problem is to devise an algorithm in which processors communicate their own values and relay values received from others that allows each nonfaulty processor to refer a value for each other processor The value referred for a nonfanlty processor must be that processor's private value, and the value inferred for a faulty one must be consistent wRh the corresponding value inferred by each other nonfanlty processor It is shown that the problem is solvable for, and only for, n {$>\_$} 3m + 1, where m IS the number of faulty processors and n is the total number. It is also shown that if faulty processors can refuse to pass on reformation but cannot falsely relay information, the problem is solvable for arbitrary n \_{$>$} m \_{$>$} 0. This weaker assumption can be approxunated m practice using cryptographic methods},
  file = {D\:\\GDrive\\zotero\\Pease\\pease_1980_reaching_agreement_in_the_presence_of_faults.pdf},
  keywords = {439,529,539,622,agreement,AND eHRASES,authentication,consistency,distributed executive,fault avoidance,fault tolerance,synchronization,voting CR CATEGORIES: 381}
}

@techreport{pengLargescaleIncrementalProcessing,
  title = {Large-Scale {{Incremental Processing Using Distributed Transactions}} and {{Notifications}}},
  author = {Peng, Daniel and Dabek, Frank},
  abstract = {Updating an index of the web as documents are crawled requires continuously transforming a large repository of existing documents as new documents arrive. This task is one example of a class of data processing tasks that transform a large repository of data via small, independent mutations. These tasks lie in a gap between the capabilities of existing infrastructure. Databases do not meet the storage or throughput requirements of these tasks: Google's indexing system stores tens of petabytes of data and processes billions of updates per day on thousands of machines. MapReduce and other batch-processing systems cannot process small updates individually as they rely on creating large batches for efficiency. We have built Percolator, a system for incrementally processing updates to a large data set, and deployed it to create the Google web search index. By replacing a batch-based indexing system with an indexing system based on incremental processing using Percolator, we process the same number of documents per day, while reducing the average age of documents in Google search results by 50\%.},
  file = {D\:\\GDrive\\zotero\\Peng\\peng_large-scale_incremental_processing_using_distributed_transactions_and.pdf}
}

@article{pengWhatHappensYou2019,
  title = {What Happens after You Leak Your Password: {{Understanding}} Credential Sharing on Phishing Sites},
  author = {Peng, Peng and Xu, Chao and Quinn, Luke and Hu, Hang and Viswanath, Bimal and Wang, Gang},
  year = {2019},
  pages = {181--192},
  doi = {10.1145/3321705.3329818},
  abstract = {Phishing has been a big concern due to its active roles in recent data breaches and state-sponsored attacks. While existing works have extensively analyzed phishing websites and their operations, there is still a limited understanding of the information sharing flows throughout the end-to-end phishing process. In this paper, we perform an empirical measurement on the transmission and sharing of stolen login credentials. Over 5 months, our measurement covers more than 179,000 phishing URLs (47,000 live phishing sites). First, we build a measurement tool to feed fake credentials to live phishing sites. The goal is to monitor how the credential information is shared with the phishing server and potentially third-party collectors on the client side. Second, we obtain phishing kits from a subset of phishing sites to analyze how credentials are sent to attackers and third-parties on the server side. Third, we set up honey accounts to monitor the post-phishing exploitation activities from attackers. Our study reveals the key mechanisms for information sharing during phishing, particularly with third-parties. We also discuss the implications of our results for phishing defenses.},
  file = {D\:\\GDrive\\zotero\\Peng\\peng_2019_what_happens_after_you_leak_your_password.pdf},
  isbn = {9781450367523},
  journal = {AsiaCCS 2019 - Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security},
  keywords = {Honey Account,Measurement,Phishing}
}

@book{pequegnatHowWriteSuccessful2011,
  title = {How to {{Write}} a {{Successful Research Grant Application}}},
  editor = {Pequegnat, Willo and Stover, Ellen and Boyce, Cheryl Anne},
  year = {2011},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4419-1454-5},
  file = {D\:\\GDrive\\zotero\\Pequegnat et al\\pequegnat_et_al_2011_how_to_write_a_successful_research_grant_application.pdf},
  isbn = {978-1-4419-1453-8 978-1-4419-1454-5},
  language = {en}
}

@article{percivalCacheMissingFun2005,
  title = {Cache Missing for Fun and Profit},
  author = {Percival, C},
  year = {2005},
  pages = {1--13},
  abstract = {Abstract. Simultaneous multithreading \^a put simply, the sharing of the execution resources of a superscalar processor between multiple execution threads \^a has recently become widespread via its introduction (under the name \^aHyper-Threading\^a) into Intel Pentium 4 processors. In this implementation, for reasons of efficiency and economy of processor area, the sharing of processor resources between threads extends beyond the execution units; of particular concern is that the threads share access to the memory caches. We demonstrate that this shared access to memory caches provides not only an easily used high bandwidth covert channel between threads, but also permits a malicious thread (operating, in theory, with limited privileges) to monitor the execution of another thread, allowing in many cases for theft of cryptographic keys. Finally, we provide some suggestions to processor designers, operating system vendors, and the authors of cryptographic software, of how this attack could be mitigated or eliminated entirely. 1.},
  file = {D\:\\GDrive\\zotero\\Percival\\percival_2005_cache_missing_for_fun_and_profit.pdf},
  journal = {BSDCan 2005},
  keywords = {and phrases,caching,side channels,simultaneous multithreading}
}

@article{perezMexicaComputerModel2001,
  title = {Mexica: {{A}} Computer Model of a Cognitive Account of Creative Writing},
  author = {P{\'E}rez, Rafael P{\'E}rez {\'Y}. and Sharples, Mike},
  year = {2001},
  month = apr,
  volume = {13},
  pages = {119--139},
  issn = {13623079},
  doi = {10.1080/09528130010029820},
  abstract = {MEXICA is a computer model that produces frameworks for short stories based on the engagement-reflection cognitive account of writing. During engagement MEXICA generates material guided by content and rhetorical constraints, avoiding the use of explicit goals or story-structure information. During reflection the system breaks impasses, evaluates the novelty and interestingness of the story in progress and verifies that coherence requirements are satisfied. In this way, MEXICA complements and extends those models of computerised story-telling based on traditional problem-solving techniques where explicit goals drive the generation of stories. This paper describes the engagement-reflection account of writing, the general characteristics of MEXICA and reports an evaluation of the program. \textcopyright{} 2001 Taylor  \&  Francis Group, LLC.},
  file = {D\:\\GDrive\\zotero\\PÉrez\\pérez_2001_mexica.pdf},
  journal = {Journal of Experimental and Theoretical Artificial Intelligence},
  keywords = {Computerised storyteller,Creativity,Engagement,Reflection},
  number = {2}
}

@article{petersUnderstandingModernBanking2015,
  title = {Understanding {{Modern Banking Ledgers Through Blockchain Technologies}}: {{Future}} of {{Transaction Processing}} and {{Smart Contracts}} on the {{Internet}} of {{Money}}},
  author = {Peters, Gareth William and Panayi, Efstathios},
  year = {2015},
  pages = {1--33},
  doi = {10.2139/ssrn.2692487},
  abstract = {In this chapter we provide an overview of the concept of blockchain technology and its potential to disrupt the world of banking through facilitating global money remittance, smart contracts, automated banking ledgers and digital assets. In this regard, we first provide a brief overview of the core aspects of this technology, as well as the second-generation contract-based developments. From there we discuss key issues that must be considered in developing such ledger based technologies in a banking context.},
  file = {D\:\\GDrive\\zotero\\Peters\\peters_2015_understanding_modern_banking_ledgers_through_blockchain_technologies.pdf},
  journal = {SSRN Electronic Journal}
}

@book{petrovDatabaseInternalsDeep2019,
  title = {Database Internals: A Deep Dive into How Distributed Data Systems Work},
  shorttitle = {Database Internals},
  author = {Petrov, Alex},
  year = {2019},
  edition = {First edition},
  publisher = {{O'Reilly Media}},
  address = {{Sebastopol, CA}},
  abstract = {"When it comes to choosing, using, and maintaining a database, understanding its internals is essential. But with so many distributed databases and tools available today, it's often difficult to learn what each one offers and how they differ. With this practical guide, Alex Petrov guides developers through the concepts behind modern database and storage engine internals. Throughout the book, you'll explore relevant material gleaned from numerous books, papers, blog posts, and the source code of several open source databases. You'll discover that the most significant distinctions among many modern databases reside in subsystems that determine how storage is organized and how data is distributed."--},
  annotation = {OCLC: on1091370549},
  file = {D\:\\GDrive\\zotero\\Petrov\\petrov_2019_database_internals.pdf},
  isbn = {978-1-4920-4034-7},
  keywords = {Database management,Database selection,Distributed databases,File organization (Computer science)},
  lccn = {QA76.9.D3 P49518 2019}
}

@techreport{peytonjonesjohnhughesjohnlaunchburyHowGiveGood,
  title = {How to Give a Good Research Talk},
  author = {Peyton Jones John Hughes John Launchbury, Simon L},
  abstract = {Giving a good research talk is not easy. We try to identify some things which we have found helpful, in the hope that they may be useful to you. This paper appears in SIGPLAN Notices 28(11) (Nov 1993).},
  annotation = {https://www.youtube.com/watch?v=sT\_-owjKIbA\&ab\_channel=MicrosoftResearch},
  file = {D\:\\GDrive\\zotero\\Peyton Jones John Hughes John Launchbury\\peyton_jones_john_hughes_john_launchbury_how_to_give_a_good_research_talk.pdf},
  keywords = {research}
}

@book{pfaffDesignImplementationOpen,
  title = {The {{Design}} and {{Implementation}} of {{Open vSwitch The Design}} and {{Implementation}} of {{Open vSwitch}}},
  author = {Pfaff, Ben and Pettit, Justin and Koponen, Teemu and Jackson, Ethan and Zhou, Andy and Rajahalme, Jarno and Gross, Jesse and Wang, Alex and Stringer, Joe and Shelar, Pravin and Jackson, Ethan J and Stringer, Jonathan and Amidon, Keith and Casado, Mart{\'i}n and Networks, Awake},
  abstract = {We describe the design and implementation of Open vSwitch, a multi-layer, open source virtual switch for all major hypervisor platforms. Open vSwitch was designed de novo for networking in virtual environments, resulting in major design departures from traditional software switching architectures. We detail the advanced flow classification and caching techniques that Open vSwitch uses to optimize its operations and conserve hypervisor resources. We evaluate Open vSwitch performance, drawing from our deployment experiences over the past seven years of using and improving Open vSwitch.},
  file = {D\:\\GDrive\\zotero\\Pfaff\\pfaff_the_design_and_implementation_of_open_vswitch_the_design_and_implementation_of.pdf;D\:\\GDrive\\zotero\\Pfaff\\pfaff_the_design_and_implementation_of_open_vswitch_the_design_and_implementation_of2.pdf},
  isbn = {978-1-931971-21-8}
}

@techreport{pfaffExtendingNetworkingVirtualization,
  title = {Extending {{Networking}} into the {{Virtualization Layer}}},
  author = {Pfaff, Ben and Pettit, Justin and Koponen, Teemu and Amidon, Keith and Casado, Martin and Shenker, Scott},
  abstract = {The move to virtualization has created a new network access layer residing on hosts that connects the various VMs. Virtualized deployment environments impose requirements on networking for which traditional models are not well suited. They also provide advantages to the networking layer (such as software flexibility and well-defined end host events) that are not present in physical networks. To date, this new virtualization network layer has been largely built around standard Ethernet switching, but this technology neither satisfies these new requirements nor leverages the available advantages. We present Open vSwitch, a network switch specifically built for virtual environments. Open vSwitch differs from traditional approaches in that it exports an external interface for fine-grained control of configuration state and forwarding behavior. We describe how Open vSwitch can be used to tackle problems such as isolation in joint-tenant environments, mobility across subnets, and distributing configuration and visibility across hosts.},
  file = {D\:\\GDrive\\zotero\\Pfaff\\pfaff_extending_networking_into_the_virtualization_layer.pdf}
}

@techreport{picardAffectiveComputing,
  title = {Affective {{Computing}}},
  author = {Picard, R W},
  abstract = {Computers are beginning to acquire the ability to express and recognize affect, and may soon be given the ability to "have emotions." The essential role of emotion in both human cognition and perception, as demonstrated by recent neurological studies, indicates that affective computers should not only provide better performance in assisting humans, but also might enhance computers' abilities to make decisions. This paper presents and discusses key issues in "affective computing," computing that relates to, arises from, or influences emotions. Models are suggested for computer recognition of human emotion, and new applications are presented for computer-assisted learning, perceptual information retrieval, arts and entertainment, and human health and interaction. Affective computing, coupled with new wear-able computers, will also provide the ability to gather new data necessary for advances in emotion and cog-nition theory. 1 Fear, Emotion, and Science Nothing in life is to be feared. It is only to be understood .-Marie Curie Emotions have a stigma in science; they are believed to be inherently non-scientific. Scientific principles are derived from rational thought, logical arguments, testable hypotheses, and repeatable experiments. There is room alongside science for "non-interfering" emotions such as those involved in curiosity, frustration, and the pleasure of discovery. In fact, much scientific research has been prompted by fear. Nonetheless, the role of emotions is marginalized at best. Why bring "emotion" or "affect" into any of the deliberate tools of science? Moreover, shouldn't it be completely avoided when considering properties to design into computers? After all, computers control significant parts of our lives-the phone system, the stock market, nuclear power plants, jet landings, and more. Who wants a computer to be able to "feel angry" at them? To feel contempt for any living thing? In this essay I will submit for discussion a set of ideas on what I call "affective computing," computing that relates to, arises from, or influences emotions. This will need some further clarification which I shall attempt below. I should say up front that I am not proposing the pursuit of computerized cingulotomies 1 or even into the business of building "emotional computers". 1 The making of small wounds in the ridge of the limbic system known as the cingulate gyrus, a surgical procedure to aid severely depressed patients. Nor will I propose answers to the difficult and intriguing questions , "what are emotions?" "what causes them?" and "why do we have them?" 2 Instead, by a variety of short scenarios, I will define important issues in affective computing. I will suggest models for affect recognition, and present my ideas for new applications of affective computing to computer-assisted learning, perceptual information retrieval, arts and entertainment, and human health and interaction. I also describe how advances in affective computing, especially combined with wearable computers, can help advance emotion and cognition theory. First, let us begin with a brief scenario.},
  file = {D\:\\GDrive\\zotero\\Picard\\picard_affective_computing.pdf}
}

@article{pickhardtSecurityPrivacyLightning2021,
  title = {Security and {{Privacy}} of {{Lightning Network Payments}} with {{Uncertain Channel Balances}}},
  author = {Pickhardt, Rene and Tikhomirov, Sergei and Biryukov, Alex and Nowostawski, Mariusz},
  year = {2021},
  month = mar,
  abstract = {The Lightning Network (LN) is a prominent payment channel network aimed at addressing Bitcoin's scalability issues. Due to the privacy of channel balances, senders cannot reliably choose sufficiently liquid payment paths and resort to a trial-and-error approach, trying multiple paths until one succeeds. This leaks private information and decreases payment reliability, which harms the user experience. This work focuses on the reliability and privacy of LN payments. We create a probabilistic model of the payment process in the LN, accounting for the uncertainty of the channel balances. This enables us to express payment success probabilities for a given payment amount and a path. Applying negative Bernoulli trials for single- and multi-part payments allows us to compute the expected number of payment attempts for a given amount, sender, and receiver. As a consequence, we analytically derive the optimal number of parts into which one should split a payment to minimize the expected number of attempts. This methodology allows us to define service level objectives and quantify how much private information leaks to the sender as a side effect of payment attempts. We propose an optimized path selection algorithm that does not require a protocol upgrade. Namely, we suggest that nodes prioritize paths that are most likely to succeed while making payment attempts. A simulation based on the real-world LN topology shows that this method reduces the average number of payment attempts by 20\% compared to a baseline algorithm similar to the ones used in practice. This improvement will increase to 48\% if the LN protocol is upgraded to implement the channel rebalancing proposal described in BOLT14.},
  archiveprefix = {arXiv},
  eprint = {2103.08576},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Pickhardt et al\\pickhardt_et_al_2021_security_and_privacy_of_lightning_network_payments_with_uncertain_channel.pdf;C\:\\Users\\Admin\\Zotero\\storage\\5LBF66TM\\2103.html},
  journal = {arXiv:2103.08576 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@phdthesis{pignottiEfficientActionScriptJustInTime2008,
  title = {An Efficient {{ActionScript}} 3.0 {{Just}}-{{In}}-{{Time}} Compiler Implementation},
  author = {Pignotti, Alessandro and Lettieri, Dott Giuseppe},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Pignotti\\pignotti_2008_an_efficient_actionscript_3.pdf}
}

@article{pintoDemystifyingArmTrustZone2019,
  title = {Demystifying {{Arm TrustZone}}: {{A Comprehensive Survey}}},
  shorttitle = {Demystifying {{Arm TrustZone}}},
  author = {Pinto, Sandro and Santos, Nuno},
  year = {2019},
  month = feb,
  volume = {51},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3291047},
  abstract = {The world is undergoing an unprecedented technological transformation, evolving into a state where ubiquitous Internet-enabled ``things'' will be able to generate and share large amounts of security- and privacy-sensitive data. To cope with the security threats that are thus foreseeable, system designers can find in Arm TrustZone hardware technology a most valuable resource. TrustZone is a System-on-Chip and CPU system-wide security solution, available on today's Arm application processors and present in the new generation Arm microcontrollers, which are expected to dominate the market of smart ``things.'' Although this technology has remained relatively underground since its inception in 2004, over the past years, numerous initiatives have significantly advanced the state of the art involving Arm TrustZone. Motivated by this revival of interest, this paper presents an in-depth study of TrustZone technology. We provide a comprehensive survey of relevant work from academia and industry, presenting existing systems into two main areas, namely, Trusted Execution Environments and hardware-assisted virtualization. Furthermore, we analyze the most relevant weaknesses of existing systems and propose new research directions within the realm of tiniest devices and the Internet of Things, which we believe to have potential to yield high-impact contributions in the future.},
  file = {D\:\\GDrive\\zotero\\Pinto_Santos\\pinto_santos_2019_demystifying_arm_trustzone.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {6}
}

@article{pinzariIntroductionNXTechnology2003,
  title = {Introduction to {{NX}} Technology},
  author = {Pinzari, Gian Filippo},
  year = {2003},
  pages = {1--7},
  file = {D\:\\GDrive\\zotero\\Pinzari\\pinzari_2003_introduction_to_nx_technology.pdf}
}

@techreport{pnueliTEMPORALLOGICPROGRAMS,
  title = {{{THE TEMPORAL LOGIC OF PROGRAMS}}*},
  author = {Pnueli, Amir},
  abstract = {A unified approach to program verification is suggested, which applies to both sequential and parallel programs. The main proof method suggested is that of temporal reasoning in which the time dependence of events is the basic concept. Two forma 1 systems are presented for providing a basis for temporal reasoning. One forms a formalization of the method of intermittent assertions, while the other is an adaptation of the tense logic system K b , and is particularly suitable for reasoning about concurrent programs.},
  file = {D\:\\GDrive\\zotero\\Pnueli\\pnueli_the_temporal_logic_of_programs.pdf}
}

@article{poeplauSymbolicExecutionSYMCC2020,
  title = {Symbolic Execution with {{SYMCC}}: {{Don}}'t Interpret, Compile!},
  author = {Poeplau, Sebastian and Francillon, Aur{\'e}lien},
  year = {2020},
  pages = {19},
  abstract = {A major impediment to practical symbolic execution is speed, especially when compared to near-native speed solutions like fuzz testing. We propose a compilation-based approach to symbolic execution that performs better than state-of-the-art implementations by orders of magnitude. We present SYMCC, an LLVM-based C and C++ compiler that builds concolic execution right into the binary. It can be used by software developers as a drop-in replacement for clang and clang++, and we show how to add support for other languages with little effort. In comparison with KLEE, SYMCC is faster by up to three orders of magnitude and an average factor of 12. It also outperforms QSYM, a system that recently showed great performance improvements over other implementations, by up to two orders of magnitude and an average factor of 10. Using it on real-world software, we found that our approach consistently achieves higher coverage, and we discovered two vulnerabilities in the heavily tested OpenJPEG project, which have been confirmed by the project maintainers and assigned CVE identifiers.},
  file = {D\:\\GDrive\\zotero\\Poeplau_Francillon\\poeplau_francillon_2020_symbolic_execution_with_symcc.pdf},
  language = {en}
}

@inproceedings{polychronakisROPPayloadDetection2011,
  title = {{{ROP}} Payload Detection Using Speculative Code Execution},
  booktitle = {2011 6th {{International Conference}} on {{Malicious}} and {{Unwanted Software}}},
  author = {Polychronakis, Michalis and Keromytis, Angelos D.},
  year = {2011},
  month = oct,
  pages = {58--65},
  doi = {10.1109/MALWARE.2011.6112327},
  abstract = {The prevalence of code injection attacks has led to the wide adoption of exploit mitigations based on nonexecutable memory pages. In turn, attackers are increasingly relying on return-oriented programming (ROP) to bypass these protections. At the same time, existing detection techniques based on shellcode identification are oblivious to this new breed of exploits, since attack vectors may not contain binary code anymore. In this paper, we present a detection method for the identification of ROP payloads in arbitrary data such as network traffic or process memory buffers. Our technique speculatively drives the execution of code that already exists in the address space of a targeted process according to the scanned input data, and identifies the execution of valid ROP code at runtime. Our experimental evaluation demonstrates that our prototype implementation can detect a broad range of ROP exploits against Windows applications without false positives, while it can be easily integrated into existing defenses based on shell-code detection.},
  file = {D\:\\GDrive\\zotero\\Polychronakis_Keromytis\\polychronakis_keromytis_2011_rop_payload_detection_using_speculative_code_execution.pdf},
  keywords = {Detectors,Payloads,Programming,Prototypes,Runtime,Software,Vectors}
}

@article{poonBitcoinLightningNetwork2016,
  title = {The {{Bitcoin Lightning Network}}: {{Scalable Off}}-{{Chain Instant Payments}}},
  author = {Poon, Joseph and Dryja, Thaddeus},
  year = {2016},
  volume = {18},
  pages = {205--208},
  issn = {15325962},
  doi = {10.3758/BF03205969},
  abstract = {The bitcoin protocol can encompass the global financial transac-tion volume in all electronic payment systems today, without a single custodial third party holding funds or requiring participants to have anything more than a computer using a broadband connection. A decentralized system is proposed whereby transactions are sent over a network of micropayment channels (a.k.a. payment channels or transaction channels) whose transfer of value occurs off-blockchain. If Bitcoin transactions can be signed with a new sighash type that addresses malleability, these transfers may occur between untrusted parties along the transfer route by contracts which, in the event of un-cooperative or hostile participants, are enforceable via broadcast over the bitcoin blockchain in the event of uncooperative or hostile partici-pants, through a series of decrementing timelocks.},
  file = {D\:\\GDrive\\zotero\\Poon_Dryja\\poon_dryja_2016_the_bitcoin_lightning_network.pdf},
  journal = {Perception \& Psychophysics},
  number = {3}
}

@article{poonenPracticalSuggestionsMathematical,
  title = {Practical Suggestions for Mathematical Writing},
  author = {Poonen, Bjorn},
  pages = {5},
  file = {D\:\\GDrive\\zotero\\Poonen\\poonen_practical_suggestions_for_mathematical_writing.pdf},
  language = {en}
}

@phdthesis{praherChangeFrameworkBased2007,
  title = {A {{Change Framework}} Based on the {{Low Level Virtual Machine Compiler Infrastructure}}},
  author = {Praher, Jakob and {Dipl-Ing hc Hanspeter M{\"o}ssenb{\"o}ck}, oUniv-Prof},
  year = {2007},
  abstract = {I Abstract When developing or deploying large applications, one would like to have more insights into what an application is doing at runtime. Frequently it is required to change defective parts of an application as fast as possible. For instance one may wish to replace a certain function call in a program with another function call whenever a specified condition holds. This master thesis aims at building the change framework, a system for dynamic program instrumentation and analysis. This research builds atop of the Low Level Virtual Machine (LLVM) for representing C/C++ applications in an intermediate form. The change framework consists of two parts, the application under analysis, and a monitor process. The application under analysis is a C/C++ application compiled to LLVM bytecodes. The monitor process communicates with the application process and is able to dynamically instrument and analyze the application process using a domain specific language. This change language has powerful constructs for defining and conditionally applying application changes. An important overall goal of this system is to ease the analysis as well as alteration of low level system software at run-time.},
  file = {D\:\\GDrive\\zotero\\Praher\\praher_2007_a_change_framework_based_on_the_low_level_virtual_machine_compiler.pdf;D\:\\GDrive\\zotero\\Praher\\praher_2007_a_change_framework_based_on_the_low_level_virtual_machine_compiler2.pdf;D\:\\GDrive\\zotero\\Praher\\praher_2007_a_change_framework_based_on_the_low_level_virtual_machine_compiler3.pdf}
}

@techreport{praitheeshanSecurityAnalysisMethods,
  title = {Security {{Analysis Methods}} on {{Ethereum Smart Contract Vulnerabilities}}-{{A Survey}}},
  author = {Praitheeshan, Purathani and Pan, Lei and Yu, Jiangshan and Liu, Joseph and Doss, Robin},
  abstract = {Smart contracts are software programs featuring both traditional applications and distributed data storage on blockchains. Ethereum is a prominent blockchain platform with the support of smart contracts. The smart contracts act as autonomous agents in critical decentralized applications and hold a significant amount of cryptocurrency to perform trusted transactions and agreements. Millions of dollars as part of the assets held by the smart contracts were stolen or frozen through the notorious attacks just between 2016 and 2018, such as the DAO attack, Parity Multi-Sig Wallet attack, and the integer underflow/overflow attacks. These attacks were caused by a combination of technical flaws in designing and implementing software codes. However, many more vulnerabilities of less severity are to be discovered because of the scripting natures of the Solidity language and the non-updateable feature of blockchains. Hence, we surveyed 16 security vulnerabilities in smart contract programs, and some vulnerabilities do not have a proper solution. This survey aims to identify the key vulnerabilities in smart contracts on Ethereum in the perspectives of their internal mechanisms and software security vulnerabilities. By correlating 16 Ethereum vulnerabilities and 19 software security issues, we predict that many attacks are yet to be exploited. And we have explored many software tools to detect the security vulnerabilities of smart contracts in terms of static analysis, dynamic analysis, and formal verification. This survey presents the security problems in smart contracts together with the available analysis tools and the detection methods. We also investigated the limitations of the tools or analysis methods with respect to the identified security vulnerabilities of the smart contracts.},
  file = {D\:\\GDrive\\zotero\\Praitheeshan\\praitheeshan_security_analysis_methods_on_ethereum_smart_contract_vulnerabilities-a_survey.pdf},
  keywords = {Formal Verification,Index Terms-Ethereum,Security Analysis Tools,Smart Contracts,Vulnerability De-tection}
}

@article{prandiniReturnOrientedProgramming2012,
  title = {Return-{{Oriented Programming}}},
  author = {Prandini, Marco and Ramilli, Marco},
  year = {2012},
  month = nov,
  volume = {10},
  pages = {84--87},
  issn = {1558-4046},
  doi = {10.1109/MSP.2012.152},
  abstract = {Attackers able to compromise the memory of a target machine can change its behavior and usually gain complete control over it. Despite the ingenious prevention and protection mechanisms that have been implemented in modern operating systems, memory corruption attacks still account for a big share of the security breaches afflicting software systems. This article describes a growing attack trend that uses return-oriented programming (ROP) techniques to bypass the most common memory protection systems.},
  file = {D\:\\GDrive\\zotero\\Prandini_Ramilli\\prandini_ramilli_2012_return-oriented_programming.pdf},
  journal = {IEEE Security Privacy},
  keywords = {attack,buffer overflows,Buffer overflows,Computer crime,Computer security,operating systems,Operating systems,Programming,return-oriented programming,ROP},
  number = {6}
}

@techreport{priceSolarisZonesOperating,
  title = {Solaris {{Zones}}: {{Operating System Support}} for {{Consolidating Commercial Workloads}}},
  author = {Price, Daniel and Tucker, Andrew},
  abstract = {Server consolidation, which allows multiple workloads to run on the same system, has become increasingly important as a way to improve the utilization of computing resources and reduce costs. Consolidation is common in mainframe environments, where technology to support running multiple workloads and even multiple operating systems on the same hardware has been evolving since the late 1960's. This technology is now becoming an important differentiator in the UNIX and Linux server market as well, both at the low end (virtual web hosting) and high end (traditional data center server consolidation). This paper introduces Solaris Zones (zones), a fully realized solution for server consolidation projects in a commercial UNIX operating system. By creating virtualized application execution environments within a single instance of the operating system, the facility strikes a unique balance between competing requirements. On the one hand, a system with multiple workloads needs to run those workloads in isolation, to ensure that applications can neither observe data from other applications nor affect their operation. It must also prevent applications from over-consuming system resources. On the other hand, the system as a whole has to be flexible, manageable, and observable, in order to reduce administrative costs and increase efficiency. By focusing on the support of multiple application environments rather than multiple operating system instances, zones meets isolation requirements without sacrificing manageability.},
  file = {D\:\\GDrive\\zotero\\Price\\price_solaris_zones.pdf}
}

@misc{PrinciplesEffectiveResearch,
  title = {Principles of {{Effective Research}} | {{Michael Nielsen}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\WHAB54UE\\principles-of-effective-research.html},
  language = {en-US}
}

@article{priscillaPracticalSecurityAssessment,
  title = {Practical {{Security Assessment}} in {{Allen Bradley PLCs}}},
  author = {Priscilla, Benjamin Mary}
}

@phdthesis{priscillaPracticalSecurityAssessmenta,
  title = {Practical {{Security Assessment}} in {{Allen Bradley PLCs}}},
  author = {Priscilla, Benjamin Mary},
  file = {D\:\\GDrive\\zotero\\Priscilla\\priscilla_practical_security_assessment_in_allen_bradley_plcs.pdf},
  language = {en}
}

@article{ProtectionOperatingSystems1976,
  title = {Protection in  {{Operating Systems}}},
  year = {1976},
  file = {D\:\\GDrive\\zotero\\undefined\\1976_protection_in_operating_systems.pdf}
}

@techreport{provosPreventingPrivilegeEscalation,
  title = {Preventing {{Privilege Escalation}}},
  author = {Provos, Niels and Friedl GeNUA Peter Honeyman, Markus},
  abstract = {Many operating system services require special privilege to execute their tasks. A programming error in a privileged service opens the door to system compromise in the form of unauthorized acquisition of privileges. In the worst case, a remote attacker may obtain superuser privileges. In this paper, we discuss the methodology and design of privilege separation, a generic approach that lets parts of an application run with different levels of privilege. Programming errors occurring in the un-privileged parts can no longer be abused to gain unauthorized privileges. Privilege separation is orthogonal to capability systems or application confinement and enhances the security of such systems even further. Privilege separation is especially useful for system services that authenticate users. These services execute privileged operations depending on internal state not known to an application confinement mechanism. As a concrete example, the concept of privilege separation has been implemented in OpenSSH. However, privilege separation is equally useful for other authenticating services. We illustrate how separation of privileges reduces the amount of OpenSSH code that is executed with special privilege. Privilege separation prevents known security vulnerabilities in prior OpenSSH versions including some that were unknown at the time of its implementation.},
  file = {D\:\\GDrive\\zotero\\Provos\\provos_preventing_privilege_escalation.pdf}
}

@article{purdyHighSecurityLogin1974,
  title = {A {{High Security Log}}-in {{Procedure}}},
  author = {Purdy, George B.},
  year = {1974},
  volume = {17},
  pages = {442--445},
  issn = {15577317},
  doi = {10.1145/361082.361089},
  abstract = {The protection of time sharing systems from unauthorized users is often achieved by the use of passwords. By using one-way ciphers to code the passwords, the risks involved with storing the passwords in the computer can be avoided. We discuss the selection of a suitable one-way cipher and suggest that for this purpose polynomials over a prime modulus are superior to one-way ciphers derived from Shannon codes. \textcopyright{} 1974, ACM. All rights reserved.},
  file = {D\:\\GDrive\\zotero\\Purdy\\purdy_1974_a_high_security_log-in_procedure.pdf;D\:\\GDrive\\zotero\\Purdy\\purdy_1974_a_high_security_log-in_procedure2.pdf},
  journal = {Communications of the ACM},
  keywords = {cryptography,operating systems,security,time sharing systems},
  number = {8}
}

@techreport{qianSimilarityEuclideanCosine2004,
  title = {Similarity between {{Euclidean}} and Cosine Angle Distance for Nearest Neighbor Queries},
  author = {Qian, Gang and Sural, Shamik and Gu, Yuelong},
  year = {2004},
  abstract = {Understanding the relationship among different distance measures is helpful in choosing a proper one for a particular application. In this paper, we compare two commonly used distance measures in vector models, namely, Euclidean distance (EUD) and cosine angle distance (CAD), for nearest neighbor (NN) queries in high dimensional data spaces. Using theoretical analysis and experimental results, we show that the retrieval results based on EUD are similar to those based on CAD when dimension is high. We have applied CAD for content based image retrieval (CBIR). Retrieval results show that CAD works no worse than EUD, which is a commonly used distance measure for CBIR, while providing other advantages, such as naturally normalized distance.},
  file = {D\:\\GDrive\\zotero\\Qian\\qian_2004_similarity_between_euclidean_and_cosine_angle_distance_for_nearest_neighbor.pdf},
  keywords = {Content based image retrieval,Cosine angle distance,Euclidean distance,Inter-feature normalization,Vector model}
}

@article{quanHotspotSymbolicExecution2016,
  title = {Hotspot Symbolic Execution of Floating-Point Programs},
  author = {Quan, Minghui},
  year = {2016},
  volume = {13-18-Nove},
  pages = {1112--1114},
  doi = {10.1145/2950290.2983966},
  abstract = {This paper presents hotspot symbolic execution (HSE) to scale the symbolic execution of oating-point programs. The essential idea of HSE is to (1) explore the paths of some functions (called hotspot functions) in priority, and (2) di-vide the paths of a hotspot function into different equiva-lence classes, and explore as fewer path as possible inside the function while ensuring the coverage of all the classes. We have implemented HSE on KLEE and carried out extensive experiments on all 5528 functions in GNU Scientific Library (GSL). The experimental results demonstrate the effective-ness and efficiency of HSE. Compared with the baseline, HSE detects {$>$}12 times of exceptions in 30 minutes.},
  file = {D\:\\GDrive\\zotero\\Quan\\quan_2016_hotspot_symbolic_execution_of_floating-point_programs.pdf},
  isbn = {9781450342186},
  journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
  keywords = {Floating point,Hotspo,Symbolic Execution}
}

@techreport{quesnelleLinkabilityZcashTransactions,
  title = {On the Linkability of {{Zcash}} Transactions},
  author = {Quesnelle, Jeffrey},
  abstract = {Zcash is a fork of Bitcoin with optional anonymity features. While transparent transactions are fully linkable, shielded transactions use zero-knowledge proofs to obscure the parties and amounts of the transactions. First, we observe various metrics regarding the usage of shielded addresses. Moreover, we show that most coins sent to shielded addresses are later sent back to transparent addresses. We then search for round-trip transactions, where the same, or nearly the same number of coins are sent from a transparent address, to a shielded address, and back again to a transparent address. We argue that such behavior exhibits high linkability, especially when they occur nearby temporally. Using this heuristic our analysis matched 31.5\% of all coins sent to shielded addresses.},
  file = {D\:\\GDrive\\zotero\\Quesnelle\\quesnelle_on_the_linkability_of_zcash_transactions.pdf}
}

@article{racordonFuelCompilerFramework2021,
  title = {Fuel: {{A Compiler Framework}} for {{Safe Memory Management}}},
  shorttitle = {Fuel},
  author = {Racordon, Dimitri and Coet, Aur{\'e}lien and Buchs, Didier},
  year = {2021},
  month = jun,
  abstract = {Flow-sensitive type systems offer an elegant way to ensure memory-safety in programming languages. Unfortunately, their adoption in new or existing languages is often hindered by a painful effort to implement or integrate them into compilers. This paper presents early results in our effort to alleviate this task. We introduce Fuel, a type capability-based library that can be plugged onto a compiler toolchain to check for memory-safety properties. Fuel builds upon well-established ideas in the domain of capability-based system, and adds a mechanism leveraging dynamic checks to recover capabilities where static reasoning is either too difficult or impossible. This approach allows the analysis to potentially cover situations where a typical type system might not be expressive enough to statically reason about memory safety.},
  archiveprefix = {arXiv},
  eprint = {2106.12434},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Racordon et al\\racordon_et_al_2021_fuel.pdf;C\:\\Users\\Admin\\Zotero\\storage\\MR3G84PS\\2106.html},
  journal = {arXiv:2106.12434 [cs]},
  keywords = {Computer Science - Programming Languages,D.2.4},
  primaryclass = {cs}
}

@article{raffHowReadUnderstand,
  title = {How to Read and Understand a Scientific Article},
  author = {Raff, Jennifer},
  pages = {3},
  file = {D\:\\GDrive\\zotero\\Raff\\raff_how_to_read_and_understand_a_scientific_article.pdf},
  language = {en}
}

@misc{raffHowReadUnderstand2013,
  title = {How to Read and Understand a Scientific Paper: A Guide for Non-Scientists},
  shorttitle = {How to Read and Understand a Scientific Paper},
  author = {Raff, Jennifer},
  year = {2013},
  month = aug,
  abstract = {Update (1/3/18) I've been overwhelmed with requests for the shorter guide, and the email address below no longer works. So I've uploaded a copy of the guide for anyone to download and s\ldots},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ZEFH45WP\\how-to-read-and-understand-a-scientific-paper-2.html},
  journal = {Violent metaphors},
  language = {en}
}

@article{ragheshFrameworkAutomaticOpenMP2011,
  title = {A {{Framework}} for {{Automatic OpenMP Code Generation}}},
  author = {Raghesh, Aloor},
  year = {2011},
  abstract = {It is always a tedious task to manually analyze and detect parallelism in programs. When we deal with autoparallelism the task becomes more complex. Frameworks such as OpenMP is available through which we can manually annotate the code to realize parallelism and take the advantage of underlying multi-core architecture. But the programmer's life becomes simple when this is done automatically. In this reportwe present a framework for autoparallelism through Polly, a project to enable polyhedral optimizations in LLVM and the work done towards automatically generating OpenMP library calls for relevant parts of the code. Various powerful polyhedral techniques exist to optimize computation intensive programs effectively. Applying these techniques on any non-trivial program is still surprisingly difficult and often not as effective as expected. Most polyhedral tools are limited to a specific programming language. Even for this language, relevant code needs to match specific syntax that rarely appears in existing code. It is therefore hard or even impossible to process existing programs automatically. In addition, most tools target C or OpenCL code,which prevents effective communication with compiler internal optimizers. As a result target architecture specific optimizations are either little effective or not approached at all. Polly automatically detects and transforms relevant program parts in a language-independent and syntactically transparent way. Therefore, it supports programs written in most common programming languages and constructs like C++ iterators, goto based loops and pointer arithmetic. Internally it provides a state-of-the-art polyhedral library with full support for Z-polyhedra, advanced data dependency analysis and support for external optimizers. Through LLVM, machine code for CPUs and GPU accelerators, C source code and even hardware descriptions can be targeted.},
  file = {D\:\\GDrive\\zotero\\Raghesh\\raghesh_2011_a_framework_for_automatic_openmp_code_generation.pdf},
  keywords = {Autoparallelism,Loop Transformation,OpenMP,Polyhedral Model,Vectorization},
  number = {April}
}

@article{rahamanProgramAnalysisCryptographic2017,
  title = {Program {{Analysis}} of {{Cryptographic Implementations}} for {{Security}}},
  author = {Rahaman, Sazzadur and Yao, Danfeng},
  year = {2017},
  pages = {61--68},
  doi = {10.1109/SecDev.2017.23},
  abstract = {Cryptographic implementation errors in popular open source libraries (e.g., OpenSSL, GnuTLS, BotanTLS, etc.) and the misuses of cryptographic primitives (e.g., as in Juniper Network) have been the major source of vulnerabilities in the wild. These serious problems prompt the need for new compile-time security checking. Such security enforcements demand the study of various cryptographic properties and their mapping into enforceable program analysis rules. We refer to this new security approach as cryptographic program analysis (CPA). In this paper, we show how cryptographic program analysis can be performed effectively andits security applications. Specifically, we systematically investigate different threat categories on various cryptographicimplementations and their usages. Then, we derive varioussecurity rules, which are enforceable by program analysistools during code compilation. We also demonstrate the capabilities of static taint analysis to enforce most of these security rules and provide a prototype implementation. We point out promising future research and development directions in this new area of cryptographic program analysis.},
  file = {D\:\\GDrive\\zotero\\Rahaman\\rahaman_2017_program_analysis_of_cryptographic_implementations_for_security.pdf},
  isbn = {9781538634677},
  journal = {Proceedings - 2017 IEEE Cybersecurity Development Conference, SecDev 2017},
  keywords = {Cryptographic Program Analysis,Cryptography,Program Analysis,Security}
}

@techreport{raikwarSoKUsedCryptography2019,
  title = {{{SoK}} of {{Used Cryptography}} in {{Blockchain}}},
  author = {Raikwar, Mayank and Gligoroski, Danilo and Kralevska, Katina},
  year = {2019},
  abstract = {The underlying fundaments of blockchain are cryptography and cryptographic concepts that provide reliable and secure decentralized solutions. Although many recent papers study the use-cases of blockchain in different industrial areas, such as finance, health care, legal relations, IoT, information security, and consensus building systems, only few studies scrutinize the cryptographic concepts used in blockchain. To the best of our knowledge, there is no Systematization of Knowledge (SoK) that gives a complete picture of the existing cryptographic concepts which have been deployed or have the potential to be deployed in blockchain. In this paper, we thoroughly review and systematize all cryptographic concepts which are already used in blockchain. Additionally, we give a list of cryptographic concepts which have not yet been applied but have big potentials to improve the current blockchain solutions. We also include possible instantiations of these cryptographic concepts in the blockchain domain. Last but not least, we explicitly postulate 21 challenging problems that cryptographers interested in blockchain can work on.},
  file = {D\:\\GDrive\\zotero\\Raikwar et al\\raikwar_et_al_2019_sok_of_used_cryptography_in_blockchain.pdf;C\:\\Users\\Admin\\Zotero\\storage\\AEUWSGUV\\735.html},
  keywords = {access control,accumulator,Blockchain,consensus,cryptographic protocols,encryption,hash function,signature,zero-knowledge proofs},
  number = {735}
}

@article{rak-amnouykitPythonTypesWild2020,
  title = {Python 3 Types in the Wild: {{A}} Tale of Two Type Systems},
  author = {{Rak-Amnouykit}, Ingkarat and McCrevan, Daniel and Milanova, Ana and Hirzel, Martin and Dolby, Julian},
  year = {2020},
  pages = {57--70},
  doi = {10.1145/3426422.3426981},
  abstract = {Python 3 is a highly dynamic language, but it has introduced a syntax for expressing types with PEP484. This paper explores how developers use these type annotations, the type system semantics provided by type checking and inference tools, and the performance of these tools. We evaluate the types and tools on a corpus of public GitHub repositories. We review MyPy and PyType, two canonical static type checking and inference tools, and their distinct approaches to type analysis. We then address three research questions: (i) How often and in what ways do developers use Python 3 types? (ii) Which type errors do developers make? (iii) How do type errors from different tools compare? Surprisingly, when developers use static types, the code rarely type-checks with either of the tools. MyPy and PyType exhibit false positives, due to their static nature, but also flag many useful errors in our corpus. Lastly, MyPy and PyType embody two distinct type systems, flagging different errors in many cases. Understanding the usage of Python types can help guide tool-builders and researchers. Understanding the performance of popular tools can help increase the adoption of static types and tools by practitioners, ultimately leading to more correct and more robust Python code.},
  file = {D\:\\GDrive\\zotero\\Rak-Amnouykit\\rak-amnouykit_2020_python_3_types_in_the_wild.pdf},
  isbn = {9781450381758},
  journal = {DLS 2020 - Proceedings of the 16th ACM SIGPLAN International Symposium on Dynamic Languages - Co-located with SPLASH 2020},
  keywords = {Python,type checking,type inference}
}

@techreport{rakamaricAutomaticInferenceFramea,
  title = {Automatic {{Inference}} of {{Frame Axioms Using Static Analysis}} *},
  author = {Rakamari{\textasciiacute}c, Zvonimir Rakamari{\textasciiacute}c and Hu, Alan J},
  abstract = {Many approaches to software verification are currently semi-automatic: a human must provide key logical insights-e.g., loop invariants, class invariants, and frame axioms that limit the scope of changes that must be analyzed. This paper describes a technique for automatically inferring frame axioms of procedures and loops using static analysis. The technique builds on a pointer analysis that generates limited information about all data structures in the heap. Our technique uses that information to over-approximate a potentially unbounded set of memory locations modified by each procedure/loop; this over-approximation is a candidate frame axiom. We have tested this approach on the buffer-overflow benchmarks from ASE 2007. With manually provided specifications and invariants/axioms, our tool could verify/falsify 226 of the 289 benchmarks. With our automatically inferred frame axioms, the tool could verify/falsify 203 of the 289, demonstrating the effectiveness of our approach.},
  file = {D\:\\GDrive\\zotero\\Rakamari´c\\rakamari´c_automatic_inference_of_frame_axioms_using_static_analysis.pdf}
}

@techreport{ramasubramanianBeehiveLookupPerformance,
  title = {Beehive: {{O}}(1) {{Lookup Performance}} for {{Power}}-{{Law Query Distributions}} in {{Peer}}-to-{{Peer Overlays}}},
  author = {Ramasubramanian, Venugopalan and Sirer, Emin G{\"u}n},
  abstract = {Structured peer-to-peer hash tables provide decentralization , self-organization, failure-resilience, and good worst-case lookup performance for applications, but suffer from high latencies (O(logN)) in the average case. Such high latencies prohibit them from being used in many relevant, demanding applications such as DNS. In this paper, we present a proactive replication framework that can provide constant lookup performance for common Zipf-like query distributions. This framework is based around a closed-form optimal solution that achieves O(1) lookup performance with low storage requirements , bandwidth overhead and network load. Simulations show that this replication framework can realistically achieve good latencies, outperform passive caching, and adapt efficiently to sudden changes in object popularity, also known as flash crowds. This framework provides a feasible substrate for high-performance, low-latency applications, such as peer-to-peer domain name service.},
  file = {D\:\\GDrive\\zotero\\Ramasubramanian\\ramasubramanian_beehive.pdf}
}

@article{ramseyTeachingHowDesign2014,
  title = {On {{Teaching How}} to {{Design Programs Observations}} from a {{Newcomer}}},
  author = {Ramsey, Norman},
  year = {2014},
  doi = {10.1145/2628136.2628137Reprinted},
  abstract = {This paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's six-step design process as an eight-step design process ending in a new "review and refactor" step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples , and other parts of the design process. It connects the process to interactive "world programs." It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.},
  file = {D\:\\GDrive\\zotero\\Ramsey\\ramsey_2014_on_teaching_how_to_design_programs_observations_from_a_newcomer.pdf},
  isbn = {9781450328739},
  keywords = {D11 [Applicative (Func-tional) Programming],How to Design Programs,K32 [Computer and Information Science Education]: Computer Science Education Keywords Introductory programming course,Program by Design,Racket,Reflective practice}
}

@article{rappsSelectingSoftwareTest1985,
  title = {Selecting {{Software Test Data Using Data Flow Information}}},
  author = {Rapps, Sandra and Weyuker, Elaine J.},
  year = {1985},
  volume = {SE-11},
  pages = {367--375},
  issn = {00985589},
  doi = {10.1109/TSE.1985.232226},
  abstract = {This paper defines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria, which examine only the control flow of a program, are inadequate. Our procedure associates with each point in a program at which a variable is defined, those points at which the value is used. Several test data selection criteria, differing in the type and number of these associations, are defined and compared. \textcopyright{} 1985 IEEE.},
  file = {D\:\\GDrive\\zotero\\Rapps\\rapps_1985_selecting_software_test_data_using_data_flow_information.pdf},
  journal = {IEEE Transactions on Software Engineering},
  keywords = {Data flow,program testing,test data selection},
  number = {4}
}

@techreport{rashmiSolutionNetworkChallenges,
  title = {A {{Solution}} to the {{Network Challenges}} of {{Data Recovery}} in {{Erasure}}-Coded {{Distributed Storage Systems}}: {{A Study}} on the {{Facebook Warehouse Cluster}}},
  author = {Rashmi, K V and Shah, Nihar B and Gu Facebook Hairong Kuang, Dikang and Borthakur Facebook Kannan Ramchandran, Dhruba},
  abstract = {Erasure codes, such as Reed-Solomon (RS) codes, are being increasingly employed in data centers to combat the cost of reliably storing large amounts of data. Although these codes provide optimal storage efficiency, they require significantly high network and disk usage during recovery of missing data. In this paper, we first present a study on the impact of recovery operations of erasure-coded data on the data-center network, based on measurements from Face-book's warehouse cluster in production. To the best of our knowledge, this is the first study of its kind available in the literature. Our study reveals that recovery of RS-coded data results in a significant increase in network traffic, more than a hundred terabytes per day, in a cluster storing multiple petabytes of RS-coded data. To address this issue, we present a new storage code using our recently proposed Piggybacking framework, that reduces the network and disk usage during recovery by 30\% in theory, while also being storage optimal and supporting arbitrary design parameters. The implementation of the proposed code in the Hadoop Distributed File System (HDFS) is underway. We use the measurements from the warehouse cluster to show that the proposed code would lead to a reduction of close to fifty terabytes of cross-rack traffic per day.},
  file = {D\:\\GDrive\\zotero\\Rashmi\\rashmi_a_solution_to_the_network_challenges_of_data_recovery_in_erasure-coded.pdf}
}

@techreport{ratnasamyCapturingComplexityNetworked,
  title = {Capturing {{Complexity}} in {{Networked Systems Design}}: {{The Case}} for {{Improved Metrics}}},
  author = {Ratnasamy, Sylvia},
  abstract = {The systems and networking community lays great store by "clean", "elegant" system designs. Yet, our notion of what these terms mean often relies more on intuition and qualitative discussion than rigorous quantitative metrics. This paper questions whether we can do better and takes a first stab at quantifying this notion of complexity with regard to the algorithmic component of a networked system design. While the success of our particular attempt is unclear, we believe identifying such metrics would be valuable not only in improving our own design and analysis method-ologies but also to better articulate our design aesthetic to other communities that design for Internet contexts (e.g., algorithms, formal distributed systems, graph theory).},
  file = {D\:\\GDrive\\zotero\\Ratnasamy\\ratnasamy_capturing_complexity_in_networked_systems_design.pdf}
}

@book{ravitchAutomaticGenerationLibrary,
  title = {Automatic {{Generation}} of {{Library Bindings Using Static Analysis}} *},
  author = {Ravitch, Tristan and Jackson, Steve and Aderhold, Eric and Liblit, Ben},
  abstract = {High-level languages are growing in popularity. However, decades of C software development have produced large libraries of fast, time-tested, meritorious code that are impractical to recreate from scratch. Cross-language bindings can expose low-level C code to high-level languages. Unfortunately, writing bindings by hand is tedious and error-prone, while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect. We present an improved binding-generation strategy based on static analysis of unannotated library source code. We characterize three high-level idioms that are not uniquely expressible in C's low-level type system: array parameters, resource managers, and multiple return values. We describe a suite of interprocedural analyses that recover this high-level information, and we show how the results can be used in a binding generator for the Python programming language. In experiments with four large C libraries, we find that our approach avoids the mistakes characteristic of handwritten bindings while offering a level of Python integration unmatched by prior automated approaches. Among the thousands of functions in the public interfaces of these libraries, roughly 40\% exhibit the behaviors detected by our static analyses.},
  file = {D\:\\GDrive\\zotero\\Ravitch\\ravitch_automatic_generation_of_library_bindings_using_static_analysis.pdf},
  isbn = {978-1-60558-392-1},
  keywords = {bindings,dataflow analysis,FFI,foreign function interfaces,libraries,modular static program analysis,multi-language code reuse}
}

@techreport{rayPracticalTechniquesPerformance2005,
  title = {Practical {{Techniques}} for {{Performance Estimation}} of {{Processors}}},
  author = {Ray, Abhijit and Srikanthan, Thambipillai and Jigang, Wu},
  year = {2005},
  abstract = {Performance estimation of processor is important to select the right processor for an application. Poorly chosen processors can either under perform very badly or over perform but with high cost. Most previous work on performance estimation are based on generating the development tools, i.e., compilers, assemblers etc from a processor description file and then additionally generating an instruction set simulator to get the performance. In this work we present a simpler strategy for performance estimation. We propose an estimation technique based on the intermediate format of an application. The estimation process does not require the generation of all the development tools as in the prevalent methods. As a result our method is not only cheaper but also faster.},
  file = {D\:\\GDrive\\zotero\\Ray\\ray_2005_practical_techniques_for_performance_estimation_of_processors.pdf}
}

@misc{ReadingTakingNotes,
  title = {Reading and {{Taking Notes}} on {{Scholarly Journal Articles}}},
  file = {D\:\\GDrive\\zotero\\_\\reading_and_taking_notes_on_scholarly_journal_articles.pdf},
  keywords = {done}
}

@techreport{reedAnonymousConnectionsOnion,
  title = {Anonymous {{Connections}} and {{Onion Routing}}},
  author = {Reed, Michael G and Syverson, Paul F and Goldschlag, David M},
  abstract = {Onion Routing is an infrastructure for private communication over a public network. It provides anonymous connections that are strongly resistant to both eavesdropping and traac analysis. Onion routing's anonymous connections are bidirectional and near real-time, and can be used anywhere a socket connection can be used. Any identifying information must be in the data stream carried over an anonymous connection. An onion is a data structure that is treated as the destination address by onion routers; thus, it is used to establish an anonymous connection. Onions themselves appear diierently to each onion router as well as to network observers. The same goes for data carried over the connections they establish. Proxy aware applications , such as web browsing and e-mail, require n o modiication to use onion routing, and do so through a series of proxies. A p r ototype onion routing network is running between our lab and other sites. This paper describes anonymous connections and their implementation using onion routing. This paper also describes several application proxies for onion routing, as well as conngurations of onion routing networks.},
  file = {D\:\\GDrive\\zotero\\Reed et al\\reed_et_al_anonymous_connections_and_onion_routing.pdf}
}

@techreport{ReflectionsTrustingTrust,
  title = {Reflections on {{Trusting Trust}}},
  file = {D\:\\GDrive\\zotero\\undefined\\reflections_on_trusting_trust.pdf}
}

@phdthesis{rehmeINTERNALREPRESENTATIONADAPTIVE2009,
  title = {{{AN INTERNAL REPRESENTATION FOR ADAPTIVE ONLINE PARALLELIZATION}}},
  author = {Rehme, Koy D},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Rehme\\rehme_2009_an_internal_representation_for_adaptive_online_parallelization.pdf},
  keywords = {parallelism internal representation task graph dyn,parallelism internal representation task graph dynamic parallel computing compiler runtime optimization multicore manycore diversity adaptation ADOPAR}
}

@techreport{REPRESENTINGGAMEDIALOGUE,
  title = {{{REPRESENTING GAME DIALOGUE AS EXPRESSIONS IN FIRST}}-{{ORDER LOGIC}}},
  abstract = {Despite advancements in graphics, physics, and artificial intelligence, modern video games are still lacking in believable dialogue generation. The more complex and interactive stories in modern games may allow the player to experience different paths in dialogue trees, but such trees are still required to be manually created by authors. Recently, there has been research on methods of creating emergent believable behaviour, but these are lacking true dialogue construction capabilities. Because the mapping of natural language to meaningful computational representations (logical forms) is a difficult problem, an important first step may be to develop a means of representing in-game dialogue as logical expressions. This thesis introduces and describes a system for representing dialogue as first-order logic predicates, demonstrates its equivalence with current dialogue authoring techniques, and shows how this representation is more dynamic and flexible.},
  keywords = {Believable Characters,Game Development,Game Dialogue,Logic Program-ming ii}
}

@article{rhodinPTXCodeGenerator2010,
  title = {A {{PTX Code}} Generator in {{LLVM}}},
  author = {Rhodin, Helge and Saarlandes, Universit{\"a}t and I, Naturwissenschaftlich-technische Fakult{\"a}t},
  year = {2010},
  file = {D\:\\GDrive\\zotero\\Rhodin\\rhodin_ptx_code_generator_in_llvm.pdf}
}

@article{richarteFourDifferentTricks2002,
  title = {Four Different Tricks to Bypass Stackshield and Stackguard Protection},
  author = {Richarte, Gerardo},
  year = {2002},
  pages = {1--26},
  abstract = {Stack shielding technologies have been developed to protect programs against exploitation of stack based buffer overflows. Among different types of protections, we can separate two major groups. Those that modify the environment where applications are executed, for example PaX now integrated into the OpenWall project[4], and those that alter the way programs are compiled. We will focus on the last group, specially in StackGuard, StackShield, and Microsoft's new stack smashing protection. Techniques that exploit stack based buffer overflows on protected pro- grams and environment have been presented in the past in [3], [2] and [16]. Here we'll describe how the studied protections work, and then we'll present four more tricks to bypass stack smashing protections, some of which are extentions of older techniques, and some we think are novel.},
  file = {D\:\\GDrive\\zotero\\Richarte\\richarte_2002_four_different_tricks_to_bypass_stackshield_and_stackguard_protection.pdf},
  journal = {World Wide Web, http://www1. corest.com/files/ \ldots}
}

@techreport{rickSuperBriefSuper2011,
  title = {A {{Super Brief}}, {{Yet Super Awesome}}, {{LaTeX Cheat Sheet}}},
  author = {Rick, Richard ( and Freedman, ) G},
  year = {2011},
  file = {D\:\\GDrive\\zotero\\Rick\\rick_2011_a_super_brief,_yet_super_awesome,_latex_cheat_sheet.pdf}
}

@book{riegelAutomaticDataPartitioning2008,
  title = {Automatic {{Data Partitioning}} in {{Software Transactional Memories}}},
  author = {Riegel, Torvald and Fetzer, Christof and Felber, Pascal},
  year = {2008},
  abstract = {We investigate to which extent data partitioning can help improve the performance of software transactional memory (STM). Our main idea is that the access patterns of the various data structures of an application might be sufficiently different so that it would be beneficial to tune the behavior of the STM for individual data partitions. We evaluate our approach using standard transactional memory benchmarks. We show that these applications contain partitions with different characteristics and, despite the runtime overhead introduced by partition tracking and dynamic tuning, that partitioning provides significant performance improvements .},
  file = {D\:\\GDrive\\zotero\\Riegel\\riegel_2008_automatic_data_partitioning_in_software_transactional_memories.pdf},
  isbn = {978-1-59593-973-9},
  keywords = {Concurrent Program-ming General Terms Algorithms,Performance}
}

@techreport{riegelMakingObjectBasedSTM,
  title = {Making {{Object}}-{{Based STM Practical}} in {{Unmanaged Environments}} *},
  author = {Riegel, Torvald and Becker De Brum, Diogo},
  abstract = {Current transactifying compilers for unmanaged environments (e.g., systems software written in C/C++) target only word-based software transactional memories (STMs) because the compiler cannot easily infer whether it is safe to transform a transactional access to a certain memory location in an object-based way. To use object-based STMs in these environments, programmers must use explicit calls to the STM or use a restricted language dialect, both of which are not practical. In this paper, we show how an existing pointer analysis can be used to let a transactifying compiler for C/C++ use object-based accesses whenever this is possible and safe, while falling back to word-based accesses otherwise. Programmers do not need to provide any annotations and do not have to use a restricted language. Our evaluation also shows that an object-based STM can be significantly faster than a word-based STM with an otherwise identical design and implementation, even if the parameters of the latter have been tuned.},
  file = {D\:\\GDrive\\zotero\\Riegel\\riegel_making_object-based_stm_practical_in_unmanaged_environments.pdf}
}

@techreport{rigoPyPyApproachVirtual,
  title = {{{PyPy}}'s {{Approach}} to {{Virtual Machine Construction}}},
  author = {Rigo, Armin and Pedroni, Samuele},
  abstract = {The PyPy project seeks to prove both on a research and a practical level the feasibility of constructing a virtual machine (VM) for a dynamic language in a dynamic language-in this case, Python. The aim is to translate (i.e. compile) the VM to arbitrary target environments, ranging in level from C/Posix to Smalltalk/Squeak via Java and CLI/.NET, while still being of reasonable efficiency within these environments. A key tool to achieve this goal is the systematic reuse of the Python language as a system programming language at various levels of our architecture and translation process. For each level, we design a corresponding type system and apply a generic type inference engine-for example, the garbage collector is written in a style that manipulates simulated pointer and address objects, and when translated to C these operations become C-level pointer and address instructions.},
  file = {D\:\\GDrive\\zotero\\Rigo\\rigo_pypy's_approach_to_virtual_machine_construction.pdf},
  isbn = {159593491X},
  keywords = {D34 [Programming Lan-guages]: Processors-code generation,Experimentation,interpreters,Metacircularity,Performance Keywords Virtual Machine,Python,Re-targettable Code Generation,run-time envi-ronments; F32 [Logics and Meanings of Programs]: Semantics of Programming Languages-program analysis General Terms Languages,Type Inference}
}

@article{ristenpart56CloudsecPdf2009,
  title = {56 {{Cloudsec}}.{{Pdf}}},
  author = {Ristenpart, Thomas and Tromer, Eran and Savage, Stefan},
  year = {2009},
  abstract = {Third-party cloud computing represents the promise of out- sourcing as applied to computation. Services, such as Mi- crosoft's Azure and Amazon's EC2, allow users to instanti- ate virtual machines (VMs) on demand and thus purchase precisely the capacity they require when they require it. In turn, the use of virtualization allows third-party cloud providers to maximize the utilization of their sunk capital costs by multiplexing many customer VMs across a shared physical infrastructure. However, in this paper, we show that this approach can also introduce new vulnerabilities. Using the Amazon EC2 service as a case study, we show that it is possible to map the internal cloud infrastructure, iden- tify where a particular target VMis likely to reside, and then instantiate new VMs until one is placed co-resident with the target. We explore how such placement can then be used to mount cross-VMside-channel attacks to extract information from a target VM on the same machine.},
  file = {D\:\\GDrive\\zotero\\Ristenpart\\ristenpart_2009_56_cloudsec.pdf},
  isbn = {9781605583525},
  journal = {Ccs},
  keywords = {cloud computing,side channels,virtual machine security}
}

@article{ristenpartCarefulCompositionLimitations2011,
  title = {Careful with {{Composition}}: {{Limitations}} of {{Indifferentiability}} and {{Universal Composability}}},
  author = {Ristenpart, Thomas and Shacham, Hovav and Shrimpton, Thomas},
  year = {2011},
  pages = {30},
  abstract = {We exhibit a hash-based storage auditing scheme which is provably secure in the random-oracle model (ROM), but easily broken when one instead uses typical indifferentiable hash constructions. This contradicts the widely accepted belief that the indifferentiability composition theorem applies to any cryptosystem. We characterize the uncovered limitation of the indifferentiability framework by showing that the formalizations used thus far implicitly exclude security notions captured by experiments that have multiple, disjoint adversarial stages. Examples include deterministic public-key encryption (PKE), password-based cryptography, hash function nonmalleability, key-dependent message security, and more. We formalize a stronger notion, reset indifferentiability, that enables an indifferentiabilitystyle composition theorem covering such multi-stage security notions, but then show that practical hash constructions cannot be reset indifferentiable. We discuss how these limitations also affect the universal composability framework. We finish by showing the chosen-distribution attack security (which requires a multi-stage game) of some important public-key encryption schemes built using a hash construction paradigm introduced by Dodis, Ristenpart, and Shrimpton.},
  file = {D\:\\GDrive\\zotero\\Ristenpart et al\\ristenpart_et_al_2011_careful_with_composition.pdf},
  language = {en}
}

@article{ristenpartHeyYouGet2009,
  title = {Hey, {{You}}, {{Get Off}} of {{My Cloud}}: {{Exploring Information Leakage}} in {{Third}}-{{Party Compute Clouds}}},
  author = {Ristenpart, Thomas and Tromer, Eran and Shacham, Hovav and Savage, Stefan},
  year = {2009},
  abstract = {Third-party cloud computing represents the promise of out-sourcing as applied to computation. Services, such as Mi-crosoft's Azure and Amazon's EC2, allow users to instantiate virtual machines (VMs) on demand and thus purchase precisely the capacity they require when they require it. In turn, the use of virtualization allows third-party cloud providers to maximize the utilization of their sunk capital costs by multiplexing many customer VMs across a shared physical infrastructure. However, in this paper, we show that this approach can also introduce new vulnerabilities. Using the Amazon EC2 service as a case study, we show that it is possible to map the internal cloud infrastructure, identify where a particular target VM is likely to reside, and then instantiate new VMs until one is placed co-resident with the target. We explore how such placement can then be used to mount cross-VM side-channel attacks to extract information from a target VM on the same machine.},
  file = {D\:\\GDrive\\zotero\\Ristenpart\\ristenpart_2009_hey,_you,_get_off_of_my_cloud.pdf;D\:\\GDrive\\zotero\\Ristenpart\\ristenpart_2009_hey,_you,_get_off_of_my_cloud2.pdf},
  keywords = {Cloud computing,Side channels,side-channel,ss,Virtual machine security}
}

@inproceedings{ristenpartHeyYouGet2009a,
  title = {Hey, You, Get off of My Cloud: Exploring Information Leakage in Third-Party Compute Clouds},
  shorttitle = {Hey, You, Get off of My Cloud},
  booktitle = {Proceedings of the 16th {{ACM}} Conference on {{Computer}} and Communications Security - {{CCS}} '09},
  author = {Ristenpart, Thomas and Tromer, Eran and Shacham, Hovav and Savage, Stefan},
  year = {2009},
  pages = {199},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1653662.1653687},
  abstract = {Third-party cloud computing represents the promise of outsourcing as applied to computation. Services, such as Microsoft's Azure and Amazon's EC2, allow users to instantiate virtual machines (VMs) on demand and thus purchase precisely the capacity they require when they require it. In turn, the use of virtualization allows third-party cloud providers to maximize the utilization of their sunk capital costs by multiplexing many customer VMs across a shared physical infrastructure. However, in this paper, we show that this approach can also introduce new vulnerabilities. Using the Amazon EC2 service as a case study, we show that it is possible to map the internal cloud infrastructure, identify where a particular target VM is likely to reside, and then instantiate new VMs until one is placed co-resident with the target. We explore how such placement can then be used to mount cross-VM side-channel attacks to extract information from a target VM on the same machine.},
  file = {D\:\\GDrive\\zotero\\Ristenpart et al\\ristenpart_et_al_2009_hey,_you,_get_off_of_my_cloud.pdf},
  isbn = {978-1-60558-894-0},
  language = {en}
}

@techreport{ritchieDevelopmentLanguage1993,
  title = {The {{Development}} of the {{C Language}}},
  author = {Ritchie, Dennis M},
  year = {1993},
  abstract = {The C programming language was devised in the early 1970s as a system implementation language for the nascent Unix operating system. Derived from the typeless language BCPL, it evolved a type structure; created on a tiny machine as a tool to improve a meager programming environment, it has become one of the dominant languages of today. This paper studies its evolution. Introduction This paper is about the development of the C programming language, the influences on it, and the conditions under which it was created. For the sake of brevity, I omit full descriptions of C itself, its parent B [Johnson 73] and its grandparent BCPL [Richards 79], and instead concentrate on characteristic elements of each language and how they evolved.},
  file = {D\:\\GDrive\\zotero\\Ritchie\\ritchie_1993_the_development_of_the_c_language.pdf}
}

@techreport{rivestMethodObtainingDigital,
  title = {A {{Method}} for {{Obtaining Digital Signatures}} and {{Public}}-{{Key Cryptosystems}}},
  author = {Rivest, R L and Shamir, A and Adleman, L},
  abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: 1. Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key. 2. A message can be "signed" using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed en-cryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in "electronic mail" and "electronic funds transfer" systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret prime numbers p and q. Decryption is similar; only a different, secret, power d is used, where e {$\cdot$} d {$\equiv$} 1 (mod (p - 1) {$\cdot$} (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n. General permission to make fair use in teaching or research of all or part of this material is granted to individual readers and to nonprofit libraries acting for them provided that ACM's copyright notice is given and that reference is made to the publication, to its date of issue, and to the fact that reprinting privileges were granted by permission of the Association for Computing Machinery. To otherwise reprint a figure, table, other substantial excerpt, or the entire work requires specific permission as does republication, or systematic or multiple reproduction.},
  file = {D\:\\GDrive\\zotero\\Rivest\\rivest_a_method_for_obtaining_digital_signatures_and_public-key_cryptosystems.pdf},
  keywords = {315,350,381,525 *,and Phrases: digital signatures,authentication,cryptography CR Categories: 212,electronic funds transfer,electronic mail,factorization,message-passing,pri-vacy,prime number,public-key cryptosystems,security}
}

@techreport{rivestPayWordMicroMintTwo2001,
  title = {{{PayWord}} and {{MicroMint}}: {{Two}} Simple Micropayment Schemes},
  author = {Rivest, Ronald L and Shamir, Adi},
  year = {2001},
  file = {D\:\\GDrive\\zotero\\Rivest\\rivest_2001_payword_and_micromint.pdf}
}

@techreport{robertsonOkapiTREC3,
  title = {Okapi at {{TREC}}\{3\vphantom\}},
  author = {Robertson, S E and Walker, S and Jones, S and {Hancock-Beaulieu}, M M},
  file = {D\:\\GDrive\\zotero\\Robertson\\robertson_okapi_at_trec 3.pdf}
}

@article{robinsonUnderstandingAndroidSecurity2015,
  title = {Understanding Android Security},
  author = {Robinson, Gregor and Weir, George R.S.},
  year = {2015},
  volume = {534},
  pages = {189--199},
  issn = {18650929},
  doi = {10.1007/978-3-319-23276-8_17},
  abstract = {This paper details a survey of Android users in an attempt to shed light on how users perceive the risks associated with app permissions and in-built adware. A series of questions was presented in a Web survey, with results suggesting interesting differences between males and females in installation behaviour and attitudes toward security.},
  file = {D\:\\GDrive\\zotero\\Robinson\\robinson_2015_understanding_android_security.pdf},
  isbn = {9783319232751},
  journal = {Communications in Computer and Information Science},
  keywords = {Android OS - user awareness,Mobile security}
}

@article{rocchettoAttackerModelsProfiles2016,
  title = {On Attacker Models and Profiles for Cyber-Physical Systems},
  author = {Rocchetto, Marco and Tippenhauer, Nils Ole},
  year = {2016},
  volume = {9879 LNCS},
  pages = {427--449},
  issn = {16113349},
  doi = {10.1007/978-3-319-45741-3_22},
  abstract = {Attacker models are a fundamental part of research on security of any system. For different application scenarios, suitable attacker models have to be chosen to allow comprehensive coverage of possible attacks. We consider Cyber-Physical Systems (CPS), that typically consist of networked embedded systems which are used to sense, actuate, and control physical processes. The physical layer aspects of such systems add novel attack vectors and opportunities for defenses, that require extended models of attackers' capabilities. We develop a taxonomy to classify and compare attacker models in related work. We show that, so far, there are no commonly used attacker models for such CPS. In addition, concepts of what information belongs in an attacker model are widely different among the community. To address that problem, we develop a framework to classify attacker models and use it to review related work on CPS Security. Using our framework, we propose a set of attacker profiles and show that those profiles capture most types of attackers described in the related work. Our framework provides a more formal and standardized definition of attacker model for CPS, enabling the use of well-defined and uniform attacker models in the future.},
  file = {D\:\\GDrive\\zotero\\Rocchetto\\rocchetto_2016_on_attacker_models_and_profiles_for_cyber-physical_systems.pdf},
  isbn = {9783319457406},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{rocchettoCPDYExtendingDolevyao2016,
  title = {{{CPDY}}: {{Extending}} the Dolev-Yao Attacker with Physical-Layer Interactions},
  author = {Rocchetto, Marco and Tippenhauer, Nils Ole},
  year = {2016},
  volume = {10009 LNCS},
  pages = {175--192},
  issn = {16113349},
  doi = {10.1007/978-3-319-47846-3_12},
  abstract = {We propose extensions to the Dolev-Yao attacker model to make it suitable for arguments about security of Cyber-Physical Systems. The Dolev-Yao attacker model uses a set of rules to define potential actions by an attacker with respect to messages (i.e. information) exchanged between parties during a protocol execution. As the traditional Dolev-Yao model considers only information (exchanged over a channel controlled by the attacker), the model cannot directly be used to argue about the security of cyber-physical systems where physicallayer interactions are possible. Our Dolev-Yao extension, called Cyber- Physical Dolev-Yao (CPDY), allows additional orthogonal interaction channels between the parties. In particular, such orthogonal channels can be used to model physical-layer mechanical, chemical, or electrical interactions between components. In addition, we discuss the inclusion of physical properties such as location or distance in the rule set. We present an example set of additional rules for the Dolev-Yao attacker, using those we are able to formally discover physical attacks that previously could only be found by empirical methods or detailed physical process models.},
  file = {D\:\\GDrive\\zotero\\Rocchetto\\rocchetto_2016_cpdy.pdf},
  isbn = {9783319478456},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{rocchettoFormalSecurityAnalysis2017,
  title = {Towards Formal Security Analysis of Industrial Control Systems},
  author = {Rocchetto, Marco and Tippenhauer, Nils Ole},
  year = {2017},
  pages = {114--126},
  doi = {10.1145/3052973.3053024},
  abstract = {We discuss the use of formal modeling to discover potential attacks on Cyber-Physical systems, in particular Industrial Control Systems. We propose a general approach to achieve that goal considering physical-layer interactions, time and state discretization of the physical process and logic, and the use of suitable attacker profiles. We then apply the approach to model a real-world water treatment testbed using ASLan++ and analyze the resulting transition system using CL-AtSe, identifying four attack classes. To show that the attacks identified by our formal assessment represent valid attacks, we compare them against practical attacks on the same system found independently by six teams from industry and academia. We find that 7 out of the 8 practical attacks were also identified by our formal assessment. We discuss limitations resulting from our chosen level of abstraction, and a number of modeling shortcuts to reduce the runtime of the analysis.},
  file = {D\:\\GDrive\\zotero\\Rocchetto\\rocchetto_2017_towards_formal_security_analysis_of_industrial_control_systems.pdf},
  isbn = {9781450349444},
  journal = {ASIA CCS 2017 - Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security}
}

@article{roemerReturnorientedProgrammingSystems2012,
  title = {Return-Oriented Programming: {{Systems}}, Languages, and Applications},
  author = {Roemer, Ryan and Buchanan, Erik and Shacham, Hovav and Savage, Stefan},
  year = {2012},
  volume = {15},
  doi = {10.1145/2133375.2133377},
  abstract = {We introduce return-oriented programming, a technique by which an attacker can induce arbitrary behavior in a program whose control flow he has diverted, without injecting any code. A return-oriented program chains together short instruction sequences already present in a program's address space, each of which ends in a "return" instruction. Return-oriented programming defeats the W{$\oplus$}X protections recently deployed by Microsoft, Intel, and AMD; in this context, it can be seen as a generalization of traditional return-into-libc attacks. But the threat is more general. Return-oriented programming is readily exploitable on multiple architectures and systems. It also bypasses an entire category of security measures-those that seek to prevent malicious computation by preventing the execution of malicious code. To demonstrate the wide applicability of return-oriented programming, we construct a Turing-complete set of building blocks called gadgets using the standard C libraries of two very different architectures: Linux/x86 and Solaris/SPARC. To demonstrate the power of return-oriented programming, we present a high-level, general-purpose language for describing return-oriented exploits and a compiler that translates it to gadgets.},
  file = {D\:\\GDrive\\zotero\\Roemer et al\\roemer_et_al_2012_return-oriented_programming.pdf},
  journal = {ACM Trans. Inf. Syst. Secur},
  keywords = {Algorithms Additional Key Words and Phrases: Return-oriented programming,attacks,control flow integrity,D46 [Operating Systems]: Security and Protection General Terms: Security,memory safety,NX,return-into-libc,RISC,SPARC,W-xor-X,x86},
  number = {2}
}

@techreport{rogawayMoralCharacterCryptographic2015,
  title = {The {{Moral Character}} of {{Cryptographic Work}}},
  author = {Rogaway, Phillip},
  year = {2015},
  abstract = {Cryptography rearranges power: it configures who can do what, from what. This makes cryptography an inherently political tool, and it confers on the field an intrinsically moral dimension. The Snowden revelations motivate a reassessment of the political and moral positioning of cryptography. They lead one to ask if our inability to effectively address mass surveillance constitutes a failure of our field. I believe that it does. I call for a community-wide effort to develop more effective means to resist mass surveillance. I plead for a reinvention of our disciplinary culture to attend not only to puzzles and math, but, also, to the societal implications of our work. Preamble. Most academic cryptographers seem to think that our field is a fun, deep, and politically neutral game-a set of puzzles involving communicating parties and notional adversaries. This vision of who we are animates a field whose work is intellectually impressive and rapidly produced, but also quite inbred and divorced from real-world concerns. Is this what cryptography should be like? Is it how we should expend the bulk of our intellectual capital? For me, these questions came to a head with the Snowden disclosures of 2013. If cryptography's most basic aim is to enable secure communications, how could it not be a colossal failure of our field when ordinary people lack even a modicum of communication privacy when interacting electronically? Yet I soon realized that most cryptographers didn't see it this way. Most seemed to feel that the disclosures didn't even implicate us cryptographers.},
  file = {D\:\\GDrive\\zotero\\Rogaway\\rogaway_2015_the_moral_character_of_cryptographic_work.pdf},
  keywords = {Snowden · social responsibility}
}

@article{rongASAPPrioritizingAttention2017,
  title = {{{ASAP}}: {{Prioritizing Attention}} via {{Time Series Smoothing}}},
  author = {Rong, Kexin and Bailis, Peter},
  year = {2017},
  month = mar,
  doi = {10.14778/3137628.3137645},
  abstract = {Time series visualization of streaming telemetry (i.e., charting of key metrics such as server load over time) is increasingly prevalent in modern data platforms and applications. However, many existing systems simply plot the raw data streams as they arrive, often obscuring large-scale trends due to small-scale noise. We propose an alternative: to better prioritize end users' attention, smooth time series visualizations as much as possible to remove noise, while retaining large-scale structure to highlight significant deviations. We develop a new analytics operator called ASAP that automatically smooths streaming time series by adaptively optimizing the trade-off between noise reduction (i.e., variance) and trend retention (i.e., kurtosis). We introduce metrics to quantitatively assess the quality of smoothed plots and provide an efficient search strategy for optimizing these metrics that combines techniques from stream processing, user interface design, and signal processing via autocorrelation-based pruning, pixel-aware preaggregation, and on-demand refresh. We demonstrate that ASAP can improve users' accuracy in identifying long-term deviations in time series by up to 38.4\% while reducing response times by up to 44.3\%. Moreover, ASAP delivers these results several orders of magnitude faster than alternative search strategies.},
  file = {D\:\\GDrive\\zotero\\Rong\\rong_2017_asap.pdf}
}

@techreport{roscoeWritingReviewsSystems2007,
  title = {Writing Reviews for Systems Conferences},
  author = {Roscoe, Timothy and Z{\"u}rich, Eth},
  year = {2007},
  file = {D\:\\GDrive\\zotero\\Roscoe\\roscoe_2007_writing_reviews_for_systems_conferences.pdf}
}

@inproceedings{rousseauGraphofwordTWIDFNew2013,
  title = {Graph-of-Word and {{TW}}-{{IDF}}: {{New}} Approach to {{Ad Hoc IR}}},
  booktitle = {International {{Conference}} on {{Information}} and {{Knowledge Management}}, {{Proceedings}}},
  author = {Rousseau, Fran{\c c}ois and Vazirgiannis, Michalis},
  year = {2013},
  pages = {59--68},
  doi = {10.1145/2505515.2505671},
  abstract = {In this paper, we introduce novel document representation (graph-of-word) and retrieval model (TW-IDF) for ad hoc IR. Questioning the term independence assumption behind the traditional bag-of-word model, we propose a different representation of a document that captures the relationships between the terms using an unweighted directed graph of terms. From this graph, we extract at indexing time meaningful term weights (TW) that replace traditional term frequencies (TF) and from which we define a novel scoring function, namely TW-IDF, by analogy with TF-IDF. This approach leads to a retrieval model that consistently and significantly outperforms BM25 and in some cases its extension BM25+ on various standard TREC datasets. In particular, experiments show that counting the number of different contexts in which a term occurs inside a document is more effective and relevant to search than considering an overall concave term frequency in the context of ad hoc IR. Copyright 2013 ACM.},
  file = {D\:\\GDrive\\zotero\\Rousseau\\rousseau_2013_graph-of-word_and_tw-idf.pdf},
  isbn = {978-1-4503-2263-8},
  keywords = {Graph representation of document,Graph-based term weighting,Graph-of-word,IR theory,Scoring functions,TW-IDF}
}

@techreport{rouxHowReadMathematics2003,
  title = {How to Read Mathematics},
  author = {Roux, Alet},
  year = {2003},
  file = {D\:\\GDrive\\zotero\\Roux\\roux_2003_how_to_read_mathematics.pdf}
}

@techreport{rowstronPastryScalableDecentralized,
  title = {Pastry: {{Scalable}}, Decentralized Object Location and Routing for Large-Scale Peer-to-Peer Systems},
  author = {Rowstron, Antony and Druschel, Peter},
  abstract = {This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing substrate for wide-area peer-to-peer applications. Pastry performs application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a variety of peer-to-peer applications, including global data storage, data sharing, group communication and naming. Each node in the Pastry network has a unique identifier (nodeId). When presented with a message and a key, a Pastry node efficiently routes the message to the node with a nodeId that is numerically closest to the key, among all currently live Pastry nodes. Each Pastry node keeps track of its immediate neighbors in the nodeId space, and notifies applications of new node arrivals, node failures and recoveries. Pastry takes into account network locality; it seeks to minimize the distance messages travel, according to a to scalar proximity metric like the number of IP routing hops. Pastry is completely decentralized, scalable, and self-organizing; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on an emulated network of up to 100,000 nodes confirm Pastry's scalability and efficiency, its ability to self-organize and adapt to node failures, and its good network locality properties.},
  file = {D\:\\GDrive\\zotero\\Rowstron\\rowstron_pastry.pdf}
}

@techreport{rubirabrancoEvaluatingEffectsCache,
  title = {Evaluating Effects of Cache Memory Compression on Embedded Systems 53},
  author = {Rubira Branco, Rodrigo and Briglia, Anderson and Bezerra, Allan and Moiseichuk, Leonid and Gupta, Nitin},
  file = {D\:\\GDrive\\zotero\\Rubira Branco\\rubira_branco_evaluating_effects_of_cache_memory_compression_on_embedded_systems_53.pdf}
}

@techreport{ruddSurveyStealthMalware2016,
  title = {A {{Survey}} of {{Stealth Malware Attacks}}, {{Mitigation Measures}}, and {{Steps Toward Autonomous Open World Solutions}}},
  author = {Rudd, Ethan M and Rozsa, Andras and G{\"u}nther, Manuel and Boult, Terrance E},
  year = {2016},
  abstract = {As our professional, social, and financial existences become increasingly digitized and as our government, healthcare, and military infrastructures rely more on computer technologies, they present larger and more lucrative targets for malware. Stealth malware in particular poses an increased threat because it is specifically designed to evade detection mechanisms, spreading dormant, in the wild for extended periods of time, gathering sensitive information or positioning itself for a high-impact zero-day attack. Policing the growing attack surface requires the development of efficient anti-malware solutions with improved generalization to detect novel types of malware and resolve these occurrences with as little burden on human experts as possible. In this paper, we survey malicious stealth technologies as well as existing solutions for detecting and categorizing these countermeasures autonomously. While machine learning offers promising potential for increasingly autonomous solutions with improved generalization to new malware types, both at the network level and at the host level, our findings suggest that several flawed assumptions inherent to most recognition algorithms prevent a direct mapping between the stealth malware recognition problem and a machine learning solution. The most notable of these flawed assumptions is the closed world assumption: that no sample belonging to a class outside of a static training set will appear at query time. We present a formalized adaptive open world framework for stealth malware recognition and relate it mathematically to research from other machine learning domains.},
  file = {D\:\\GDrive\\zotero\\Rudd et al\\rudd_et_al_a_survey_of_stealth_malware_attacks,_mitigation_measures,_and_steps_toward.pdf},
  keywords = {Anomaly Detection,Extreme Value Theory,Index Terms-Stealth,Intrusion Detection,Machine Learning,Malware,Novelty Detection,Open Set,Outlier Detection,Recognition,Rootkits}
}

@techreport{ruderOverviewGradientDescent,
  title = {An Overview of Gradient Descent Optimization Algorithms *},
  author = {Ruder, Sebastian},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  file = {D\:\\GDrive\\zotero\\Ruder\\ruder_an_overview_of_gradient_descent_optimization_algorithms.pdf}
}

@techreport{ruffingCoinShufflePracticalDecentralized,
  title = {{{CoinShuffle}}: {{Practical Decentralized Coin Mixing}} for {{Bitcoin}}},
  author = {Ruffing, Tim and {Moreno-Sanchez}, Pedro and Kate, Aniket},
  abstract = {The decentralized currency network Bitcoin is emerging as a potential new way of performing financial transactions across the globe. Its use of pseudonyms towards protecting users' privacy has been an attractive feature to many of its adopters. Nevertheless, due to the inherent public nature of the Bitcoin transaction ledger, users' privacy is severely restricted to linkable anonymity, and a few Bitcoin transaction deanonymization attacks have been reported thus far. In this paper we propose CoinShuffle, a completely decentralized Bitcoin mixing protocol that allows users to utilize Bitcoin in a truly anonymous manner. CoinShuffle is inspired by the accountable anonymous group communication protocol Dissent and enjoys several advantages over its predecessor Bitcoin mixing protocols. It does not require any (trusted, accountable or untrusted) third party and it is perfectly compatible with the current Bitcoin system. CoinShuffle introduces only a small communication overhead for its users, while completely avoiding additional anonymization fees and minimizing the computation and communication overhead for the rest of the Bitcoin system.},
  file = {D\:\\GDrive\\zotero\\Ruffing\\ruffing_coinshuffle.pdf}
}

@article{ruijtersFaultTreeAnalysis2015,
  title = {Fault Tree Analysis: {{A}} Survey of the State-of-the-Art in Modeling, Analysis and Tools},
  author = {Ruijters, Enno and Stoelinga, Mari{\"e}lle},
  year = {2015},
  volume = {15},
  pages = {29--62},
  issn = {15740137},
  doi = {10.1016/j.cosrev.2015.03.001},
  abstract = {Fault tree analysis (FTA) is a very prominent method to analyze the risks related to safety and economically critical assets, like power plants, airplanes, data centers and web shops. FTA methods comprise of a wide variety of modeling and analysis techniques, supported by a wide range of software tools. This paper surveys over 150 papers on fault tree analysis, providing an in-depth overview of the state-of-the-art in FTA. Concretely, we review standard fault trees, as well as extensions such as dynamic FT, repairable FT, and extended FT. For these models, we review both qualitative analysis methods, like cut sets and common cause failures, and quantitative techniques, including a wide variety of stochastic methods to compute failure probabilities. Numerous examples illustrate the various approaches, and tables present a quick overview of results.},
  file = {D\:\\GDrive\\zotero\\Ruijters\\ruijters_2015_fault_tree_analysis.pdf},
  journal = {Computer Science Review},
  keywords = {Dependability evaluation,Dynamic Fault Trees,Fault trees,Graphical models,Reliability,Risk analysis}
}

@techreport{rushbyBellPadulaSecurity,
  title = {The {{Bell}} and {{La Padula Security Model}}},
  author = {Rushby, John},
  abstract = {A precise description is given of the Bell and La Padula security model using modern notation. The development faithfully follows that of the original presentation [1, 2]. The paper is intended to provide a basis for more exact, formal, and scientific discussion of the model than has been the case heretofore.},
  file = {D\:\\GDrive\\zotero\\Rushby\\rushby_the_bell_and_la_padula_security_model.pdf}
}

@book{ryankoesNearOptimalInstructionSelection2008,
  title = {Near-{{Optimal Instruction Selection}} on {{DAGs}}},
  author = {Ryan Koes, David and Copen Goldstein, Seth},
  year = {2008},
  abstract = {Instruction selection is a key component of code generation. High quality instruction selection is of particular importance in the embedded space where complex instruction sets are common and code size is a prime concern. Although instruction selection on tree expressions is a well understood and easily solved problem, instruction selection on directed acyclic graphs is NP-complete. In this paper we present NOLTIS, a near-optimal, linear time instruction selection algorithm for DAG expressions. NOLTIS is easy to implement , fast, and effective with a demonstrated average code size improvement of 5.1\% compared to the traditional tree decomposition and tiling approach.},
  file = {D\:\\GDrive\\zotero\\Ryan Koes_Copen Goldstein\\ryan_koes_copen_goldstein_2008_near-optimal_instruction_selection_on_dags.pdf},
  isbn = {978-1-59593-978-4},
  keywords = {Compilers,D34 [Programming Languages]: Processors-Code generation,Optimization General Terms Algorithm,Performance Keywords Instruction Selection}
}

@techreport{sadeghiPropertybasedAttestationComputing2004,
  title = {Property-Based {{Attestation}} for {{Computing Platforms}}: {{Caring}} about Properties, Not Mechanisms},
  author = {Sadeghi, Ahmad-Reza and St{\"u}ble, Christian and St{\"u}ble, St{\textasciidieresis}},
  year = {2004},
  abstract = {Over the past years, the computing industry has started various initiatives announced to increase computer security by means of new hardware architectures. The most notable effort is the Trusted Computing Group (TCG) and the Next-Generation Secure Computing Base (NGSCB). This technology offers useful new functionalities as the possibility to verify the integrity of a platform (attestation) or binding quantities on a specific platform (sealing). In this paper, we point out the deficiencies of the attestation and sealing functionalities proposed by the existing specification of the TCG: we show that these mechanisms can be misused to discriminate certain platforms, i.e., their operating systems and consequently the corresponding vendors. A particular problem in this context is that of managing the multitude of possible configurations. Moreover, we highlight other shortcomings related to the attestation, namely system updates and backup. Clearly, the consequences caused by these problems lead to an unsatisfactory situation both for the private and business branch, and to an unbalanced market when such platforms are in wide use. To overcome these problems generally, we propose a completely new approach: the attestation of a platform should not depend on the specific software or/and hardware (config-uration) as it is today's practice but only on the "properties" that the platform offers. Thus, a property-based attestation should only verify whether these properties are sufficient to fulfill certain (security) requirements of the party who asks for attestation. We propose and discuss a variety of solutions based on the existing Trusted Computing (TC) functional-ity. We also demonstrate, how a property-based attestation protocol can be realized based on the existing TC hardware such as a Trusted Platform Module (TPM).},
  file = {D\:\\GDrive\\zotero\\Sadeghi\\sadeghi_property-based_attestation_for_computing_platforms.pdf}
}

@book{sadeghiShortPaperLightweight2011,
  title = {Short {{Paper}}: {{Lightweight Remote Attestation}} Using {{Physical Functions}}},
  author = {Sadeghi, Ahmad-Reza and Schulz, Steffen and Wachsmann, Christian},
  year = {2011},
  abstract = {Remote attestation is a mechanism to securely and verifiably obtain information about the state of a remote computing platform. However, resource-constrained embedded devices cannot afford the required trusted hardware components, while software attestation is generally vulnerable to network and collusion attacks. In this paper, we present a lightweight remote attestation scheme that links software attestation to remotely identifiable hardware by means of Physically Unclonable Functions (PUFs). In contrast to existing software attestation schemes, our solution (1) resists collusion attacks, (2) allows the at-testation of remote platforms, and (3) enables the detection of hardware attacks due to the tamper-evidence of PUFs.},
  file = {D\:\\GDrive\\zotero\\Sadeghi\\sadeghi_2011_short_paper.pdf},
  isbn = {978-1-4503-0692-8},
  keywords = {Embedded Devices,inva-sive software (eg,K65 [Security and Protection]: Physical security,Physically Unclonable Functions (PUFs),Security Keywords Remote Attestation,Software-based Attestation,Trojan horses) General Terms Design,viruses,worms}
}

@techreport{sadowskiTricorderBuildingProgram2015,
  title = {Tricorder: {{Building}} a {{Program Analysis Ecosystem}}},
  author = {Sadowski, Caitlin and Van Gogh, Jeffrey and Jaspan, Ciera and S{\"o}derberg, Emma and Winter, Collin},
  year = {2015},
  address = {{2015}},
  abstract = {Static analysis tools help developers find bugs, improve code readability, and ensure consistent style across a project. However, these tools can be difficult to smoothly integrate with each other and into the developer workflow, particularly when scaling to large codebases. We present TRICORDER, a program analysis platform aimed at building a data-driven ecosystem around program analysis. We present a set of guiding principles for our program analysis tools and a scalable architecture for an analysis platform implementing these principles. We include an empirical, in-situ evaluation of the tool as it is used by developers across Google that shows the usefulness and impact of the platform.},
  file = {D\:\\GDrive\\zotero\\Sadowski et al\\sadowski_et_al_2015_tricorder.pdf},
  keywords = {Index Terms-program analysis,static analysis}
}

@inproceedings{sahaApacheTezUnifying2015,
  title = {Apache Tez: {{A}} Unifying Framework for Modeling and Building Data Processing Applications},
  booktitle = {Proceedings of the {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Saha, Bikas and Shah, Hitesh and Seth, Siddharth and Vijayaraghavan, Gopal and Murthy, Arun and Curino, Carlo},
  year = {2015},
  month = may,
  volume = {2015-May},
  pages = {1357--1369},
  publisher = {{Association for Computing Machinery}},
  issn = {07308078},
  doi = {10.1145/2723372.2742790},
  abstract = {The broad success of Hadoop has led to a fast-evolving and di-verse ecosystem of application engines that are building upon the YARN resource management layer. The open-source implemen-tation of MapReduce is being slowly replaced by a collection of engines dedicated to specific verticals. This has led to growing fragmentation and repeated efforts\textemdash with each new vertical engine re-implementing fundamental features (e.g. fault-tolerance, secu-rity, stragglers mitigation, etc.) from scratch. In this paper, we introduce Apache Tez, an open-source frame-work designed to build data-flow driven processing runtimes. Tez provides a scaffolding and library components that can be used to quickly build scalable and efficient data-flow centric engines. Cen-tral to our design is fostering component re-use, without hindering customizability of the performance-critical data plane. This is in fact the key differentiator with respect to the previous generation of systems (e.g. Dryad, MapReduce) and even emerging ones (e.g. Spark), that provided an d mandated a fixed data plane implemen-tation. Furthermore, Tez provides native support to build runtime optimizations, such as dynamic partition pruning for Hive. Tez is deployed at Yahoo!, Microsoft Azure, LinkedIn and nu-merous Hortonworks customer sites, and a growing number of en-gines are being integrated with it. This confirms our intuition that most of the popular vertical engines can leverage a core set of building blocks. We complement qualitative accounts of real-world adoption with quantitative experimental evidence that Tez-based implementations of Hive, Pig, Spark, and Cascading on YARN outperform their original YARN implementation on popular bench-marks (TPC-DS, TPC-H) and production workloads.},
  file = {D\:\\GDrive\\zotero\\Saha\\saha_2015_apache_tez.pdf},
  isbn = {978-1-4503-2758-9}
}

@techreport{saltzerENDTOENDARGUMENTSSYSTEM1981,
  title = {{{END}}-{{TO}}-{{END ARGUMENTS IN SYSTEM DESIGN}}},
  author = {Saltzer, J H and Reed, D P and Clark, D D},
  year = {1981},
  abstract = {This paper presents a design principle that helps guide placement of functions among the modules of a distributed computer system. The principle, called the end-to-end argument, suggests that functions placed at low levels of a system may be redundant or of little value when compared with the cost of providing them at that low level. Examples discussed in the paper include bit error recovery, security using encryption, duplicate message suppression, recovery from system crashes, and delivery acknowledgement. Low level mechanisms to support these functions are justified only as performance enhancements.},
  file = {D\:\\GDrive\\zotero\\Saltzer\\saltzer_1981_end-to-end_arguments_in_system_design.pdf},
  isbn = {0890063370}
}

@article{sanchezBibliographicalReviewCyber2019,
  title = {Bibliographical Review on Cyber Attacks from a Control Oriented Perspective},
  author = {S{\'a}nchez, Helem S. and Rotondo, Damiano and Escobet, Teresa and Puig, Vicen{\c c} and Quevedo, Joseba},
  year = {2019},
  volume = {48},
  pages = {103--128},
  issn = {13675788},
  doi = {10.1016/j.arcontrol.2019.08.002},
  abstract = {This paper presents a bibliographical review of definitions, classifications and applications concerning cyber attacks in networked control systems (NCSs) and cyber-physical systems (CPSs). This review tackles the topic from a control-oriented perspective, which is complementary to information or communication ones. After motivating the importance of developing new methods for attack detection and secure control, this review presents security objectives, attack modeling, and a characterization of considered attacks and threats presenting the detection mechanisms and remedial actions. In order to show the properties of each attack, as well as to provide some deeper insight into possible defense mechanisms, examples available in the literature are discussed. Finally, open research issues and paths are presented.},
  file = {D\:\\GDrive\\zotero\\Sánchez\\sánchez_2019_bibliographical_review_on_cyber_attacks_from_a_control_oriented_perspective.pdf},
  journal = {Annual Reviews in Control},
  keywords = {Attack detection,Cyber attacks,Cyber-physical systems,Networked control systems,Secure control}
}

@techreport{sandhuLatticeBasedAccessControl1993,
  title = {Lattice-{{Based Access Control Models}} 1},
  author = {Sandhu, Ravi S},
  year = {1993},
  volume = {26},
  pages = {9--19},
  abstract = {The objective of this article is to give a tutorial on lattice-based access control models for computer security. The paper begins with a review of Denning's axioms for information ow policies, which provide a theoretical foundation for these models. The structure of security labels in the military and government sectors, and the resulting lattice is discussed. This is followed by a review of the Bell-LaPadula model, which enforces information ow policies by means of its simple-security and ?-properties. It is noted that information ow through covert channels is beyond the scope of such access controls. Variations of the Bell-LaPadula model are considered. The paper next discusses the Biba integrity model, examining its relationship to the Bell-LaPadula model. The paper then reviews the Chinese Wall policy, which arises in a segment of the commercial sector. It is shown how this policy can be enforced in a lattice framework.},
  file = {D\:\\GDrive\\zotero\\Sandhu\\sandhu_1993_lattice-based_access_control_models_1.pdf},
  journal = {IEEE Computer},
  keywords = {access control,conndentiality,integrity,lattice model,security},
  number = {11}
}

@article{sankaranarayananProgramAnalysisUsing2007,
  title = {Program Analysis Using Symbolic Ranges},
  author = {Sankaranarayanan, Sriram and Ivan{\v c}i{\'c}, Franjo and Gupta, Aarti},
  year = {2007},
  volume = {4634 LNCS},
  pages = {366--383},
  issn = {03029743},
  doi = {10.1007/978-3-540-74061-2_23},
  abstract = {Interval analysis seeks static lower and upper bounds on the values of program variables. These bounds are useful, especially for inferring invariants to prove buffer overflow checks. In practice, however, intervals by themselves are often inadequate as invariants due to the lack of relational information among program variables. In this paper, we present a technique for deriving symbolic bounds on variable values. We study a restricted class of polyhedra whose constraints are stratified with respect to some variable ordering provided by the user, or chosen heuristically. We define a notion of normalization for such constraints and demonstrate polynomial time domain operations on the resulting domain of symbolic range constraints. The abstract domain is intended to complement widely used domains such as intervals and octagons for use in buffer overflow analysis. Finally, we study the impact of our analysis on commercial software using an overflow analyzer for the C language. \textcopyright{} Springer-Verlag Berlin Heidelberg 2007.},
  isbn = {9783540740605},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{sankarSurveyConsensusProtocols2017,
  title = {Survey of Consensus Protocols on Blockchain Applications},
  author = {Sankar, Lakshmi Siva and Sindhu, M. and Sethumadhavan, M.},
  year = {2017},
  doi = {10.1109/ICACCS.2017.8014672},
  abstract = {Blockchain is a distributed, transparent, immutable ledger. Consensus protocol forms the core of blockchain. They decide how a blockchain works. With the advent of new possibilities in blockchain technology, researchers are keen to find a well-optimized Byzantine fault tolerant consensus protocol. Creating a global consensus protocol or tailoring a cross-platform plug and play software application for implementation of various consensus protocols are ideas of huge interest. Stellar Consensus Protocol (SCP) is considered to be a global consensus protocol and promises to be Byzantine Fault Tolerant (BFT) by bringing with it the concept of quorum slices and federated byzantine fault tolerance. This consensus's working and its comparison with other protocols that were earlier proposed are analyzed here. Also, hyperledger an open-source project by Linux Foundation which includes implementing the concept of practical byzantine fault tolerance and also a platform where various other consensus protocols and blockchain applications can be deployed in a plug and play manner is also being discussed here. This paper focuses on analyzing these consensus protocols already proposed and their feasibility and efficiency in meeting the characteristics they propose to provide.},
  file = {D\:\\GDrive\\zotero\\Sankar\\sankar_2017_survey_of_consensus_protocols_on_blockchain_applications.pdf},
  isbn = {9781509045594},
  journal = {2017 4th International Conference on Advanced Computing and Communication Systems, ICACCS 2017},
  keywords = {Blockchain,Byzantine Fault Tolerance,Consensus Protocol,Quorum Slice}
}

@techreport{sarwateRisklimitingAuditsNonplurality2011,
  title = {Risk-Limiting {{Audits}} for {{Nonplurality Elections}}:},
  shorttitle = {Risk-Limiting {{Audits}} for {{Nonplurality Elections}}},
  author = {Sarwate, Anand and Checkoway, Stephen and Shacham, Hovav},
  year = {2011},
  month = jun,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA547054},
  abstract = {Post-election audits are an important method for verifying the outcome of an election. Recent work on risk-limiting, post-election audits has focused almost exclusively on plurality elections. Several organization and municipalities use nonplurality methods such as range voting, the Borda count, and instant-runoff voting (IRV). We believe that it is crucial to develop effective methods of performing risk-limiting, post-election audits for these methods. We define a general notion of the margin of victory and develop risk-limiting auditing procedures for these nonplurality methods. For positional or scored systems, we show how to adapt methods from plurality auditing. For IRV, the situation is markedly different. We provide a risk-limiting method for auditing the candidate elimination order. We provide a more efficient audit for the elections in which the margin of the IRV election can be efficiently calculated or bounded. We provide efficiently computable upper and lower bounds on the margin and, where possible, compare them to the exact margins for a large number of real elections.},
  file = {D\:\\GDrive\\zotero\\Sarwate et al\\sarwate_et_al_2011_risk-limiting_audits_for_nonplurality_elections.pdf},
  language = {en}
}

@techreport{savageEndtoEndEffectsInternet,
  title = {The {{End}}-to-{{End Effects}} of {{Internet Path Selection}}},
  author = {Savage, Stefan and Collins, Andy and Hoffman, Eric and Snell, John and Anderson, Thomas},
  abstract = {The path taken by a packet traveling across the Internet depends on a large number of factors, including routing protocols and per-network routing policies. The impact of these factors on the end-to-end performance experienced by users is poorly understood. In this paper, we conduct a measurement-based study comparing the performance seen using the "default" path taken in the Internet with the potential performance available using some alternate path. Our study uses five distinct datasets containing measurements of "path quality", such as round-trip time, loss rate, and bandwidth, taken between pairs of geographically diverse Internet hosts. We construct the set of potential altcmate paths by composing these measurements to form new synthetic paths. We find that in 30-80\% of the cases, there is an alternate path with significantly superior quality. We argue that the overall result is robust and we explore two hypotheses for explaining it.},
  file = {D\:\\GDrive\\zotero\\Savage\\savage_the_end-to-end_effects_of_internet_path_selection.pdf}
}

@techreport{savageTCPCongestionControl,
  title = {{{TCP Congestion Control}} with a {{Misbehaving Receiver}}},
  author = {Savage, Stefan and Cardwell, Neal and Wetherall, David and Anderson, Tom},
  abstract = {In this paper, we explore the operation of TCP congestion control when the receiver can misbehave, as might occur with a greedy Web client. We first demonstrate that there are simple attacks that allow a misbehaving receiver to drive a standard TCP sender arbitrarily fast, without losing end-to-end reliability. These attacks are widely applicable because they stem from the sender behavior specified in RFC 2581 rather than implementation bugs. We then show that it is possible to modify TCP to eliminate this undesirable behavior entirely, without requiring assumptions of any kind about receiver behavior. This is a strong result: with our solution a receiver can only reduce the data transfer rate by misbehaving, thereby eliminating the incentive to do so.},
  file = {D\:\\GDrive\\zotero\\Savage\\savage_tcp_congestion_control_with_a_misbehaving_receiver.pdf}
}

@techreport{schaferOptimalChainRule2006,
  title = {Optimal {{Chain Rule Placement}} for {{Instruction Selection Based}} on {{SSA Graphs}} *},
  author = {Sch{\"a}fer, Stefan and Scholz, Bernhard},
  year = {2006},
  pages = {2007},
  abstract = {Instruction selection is a compiler optimisation that translates the intermediate representation of a program into a lower intermediate representation or an assembler program. We use the SSA form as an intermediate representation for instruction selection. Patterns are used for translation and are expressed as production rules in a graph grammar. The instruction selector seeks for a syntax derivation with minimal costs optimising execution time, code size, or a combination of both. Production rules are either base rules which match nodes in the SSA graph or chain rules which convert results of operations. We present a new algorithm for placing chain rules in a control flow graph. This new algorithm places chain rules optimally for an arbitrary cost metric. Experiments with the MiBench and SPEC2000 benchmark suites show that our proposed algorithm is feasible and always yields better results than simple strategies currently in use. We reduce the costs for placing chain rules by 25\% for the MiBench suite and by 11\% for the SPEC2000 suite.},
  file = {D\:\\GDrive\\zotero\\Schäfer\\schäfer_2006_optimal_chain_rule_placement_for_instruction_selection_based_on_ssa_graphs.pdf}
}

@article{schechterPopularityEverythingNew2010,
  title = {Popularity Is Everything {{A}} New Approach to Protecting Passwords from Statistical-Guessing Attacks},
  author = {Schechter, Stuart and Herley, Cormac and Mitzenmacher, Michael},
  year = {2010},
  pages = {1--6},
  abstract = {We propose to strengthen user-selected passwords against statistical-guessing attacks by allowing users of Internet- scale systems to choose any password they want\textemdash so long as it's not already too popular with other users. We create an oracle to identify undesirably popular passwords using an existing data structure known as a count-min sketch, which we populate with existing users' passwords and update with each new user pass- word. Unlike most applications of probabilistic data structures, which seek to achieve only a maximum ac- ceptable rate false-positives, we set a minimum accept- able false-positive rate to confound attackers who might query the oracle or even obtain a copy of it.},
  file = {D\:\\GDrive\\zotero\\Schechter\\schechter_2010_popularity_is_everything_a_new_approach_to_protecting_passwords_from.pdf},
  journal = {USENIX: Hot Topics on Security}
}

@article{schieCompilingHaskellLLVM2008,
  title = {Compiling {{Haskell}} to {{LLVM}}},
  author = {Schie, John Van},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Schie\\schie_2008_compiling_haskell_to_llvm.pdf}
}

@techreport{scholzUserInputDependenceAnalysis2008,
  title = {User-{{Input Dependence Analysis}} via {{Graph Reachability}}},
  author = {Scholz, Bernhard and Zhang, Chenyi and Cifuentes, Cristina},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Scholz\\scholz_2008_user-input_dependence_analysis_via_graph_reachability.pdf}
}

@article{schubertKnowYourAnalysis2019,
  title = {Know Your Analysis: {{How}} Instrumentation {{AIDS}} Understanding Static Analysis},
  author = {Schubert, Philipp Dominik and Leer, Richard and Hermann, Ben and Bodden, Eric},
  year = {2019},
  pages = {8--13},
  doi = {10.1145/3315568.3329965},
  abstract = {The development of a high-quality data-flow analysis - -one that is precise and scalable - -is a challenging task. A concrete client analysis not only requires data-flow but, in addition, type-hierarchy, points-to, and call-graph information, all of which need to be obtained by wisely chosen and correctly parameterized algorithms. Therefore, many static analysis frameworks have been developed that provide analysis writers with generic data-flow solvers as well as those additional pieces of information. Such frameworks ease the development of an analysis by requiring only a description of the data-flow problem to be solved and a set of framework parameters. Yet, analysis writers often struggle when an analysis does not behave as expected on real-world code. It is usually not apparent what causes a failure due to the complex interplay of the several algorithms and the client analysis code within such frameworks. In this work, we present some of the insights we gained by instrumenting the LLVM-based static analysis framework PhASAR for C/C++ code and show the broad area of applications at which flexible instrumentation supports analysis and framework developers. We present five cases in which instrumentation gave us valuable insights to debug and improve both, the concrete analyses and the underlying PhASAR framework.},
  file = {D\:\\GDrive\\zotero\\Schubert\\schubert_2019_know_your_analysis.pdf},
  isbn = {9781450367202},
  journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  keywords = {C/C++,Framework,Instrumentation,Static analysis}
}

@book{schubertPhASARInterproceduralStatic2019,
  title = {{{PhASAR}}: {{An Inter}}-Procedural {{Static Analysis Framework}} for {{C}}/{{C}}++},
  author = {Schubert, Philipp Dominik and Hermann, Ben and Bodden, Eric},
  year = {2019},
  volume = {11428 LNCS},
  publisher = {{Springer International Publishing}},
  issn = {16113349},
  doi = {10.1007/978-3-030-17465-1_22},
  abstract = {Static program analysis is used to automatically determine program properties, or to detect bugs or security vulnerabilities in programs. It can be used as a stand-alone tool or to aid compiler optimization as an intermediary step. Developing precise, inter-procedural static analyses, however, is a challenging task, due to the algorithmic complexity, implementation effort, and the threat of state explosion which leads to unsatisfactory performance. Software written in C and C++ is notoriously hard to analyze because of the deliberately unsafe type system, unrestricted use of pointers, and (for C++) virtual dispatch. In this work, we describe the design and implementation of the LLVM-based static analysis framework PhASAR for C/C++ code. PhASAR allows data-flow problems to be solved in a fully automated manner. It provides class hierarchy, call-graph, points-to, and data-flow information, hence requiring analysis developers only to specify a definition of the data-flow problem. PhASAR thus hides the complexity of static analysis behind a high-level API, making static program analysis more accessible and easy to use. PhASAR is available as an open-source project. We evaluate PhASAR's scalability during whole-program analysis. Analyzing 12 real-world programs using a taint analysis written in PhASAR, we found PhASAR's abstractions and their implementations to provide a whole-program analysis that scales well to real-world programs. Furthermore, we peek into the details of analysis runs, discuss our experience in developing static analyses for C/C++, and present possible future improvements. Data or code related to this paper is available at: [34].},
  file = {D\:\\GDrive\\zotero\\Schubert\\schubert_2019_phasar.pdf},
  isbn = {978-3-030-17464-4},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {C/C++,Inter-procedural static analysis,LLVM}
}

@inproceedings{schusterCounterfeitObjectorientedProgramming2015,
  title = {Counterfeit {{Object}}-Oriented {{Programming}}: {{On}} the {{Difficulty}} of {{Preventing Code Reuse Attacks}} in {{C}}++ {{Applications}}},
  shorttitle = {Counterfeit {{Object}}-Oriented {{Programming}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Schuster, Felix and Tendyck, Thomas and Liebchen, Christopher and Davi, Lucas and Sadeghi, Ahmad-Reza and Holz, Thorsten},
  year = {2015},
  month = may,
  pages = {745--762},
  publisher = {{IEEE}},
  address = {{San Jose, CA}},
  doi = {10.1109/SP.2015.51},
  abstract = {Code reuse attacks such as return-oriented programming (ROP) have become prevalent techniques to exploit memory corruption vulnerabilities in software programs. A variety of corresponding defenses has been proposed, of which some have already been successfully bypassed\textemdash and the arms race continues. In this paper, we perform a systematic assessment of recently proposed CFI solutions and other defenses against code reuse attacks in the context of C++. We demonstrate that many of these defenses that do not consider object-oriented C++ semantics precisely can be generically bypassed in practice. Our novel attack technique, denoted as counterfeit object-oriented programming (COOP), induces malicious program behavior by only invoking chains of existing C++ virtual functions in a program through corresponding existing call sites. COOP is Turing complete in realistic attack scenarios and we show its viability by developing sophisticated, real-world exploits for Internet Explorer 10 on Windows and Firefox 36 on Linux. Moreover, we show that even recently proposed defenses (CPS, T-VIP, vfGuard, and VTint) that specifically target C++ are vulnerable to COOP. We observe that constructing defenses resilient to COOP that do not require access to source code seems to be challenging. We believe that our investigation and results are helpful contributions to the design and implementation of future defenses against controlflow hijacking attacks.},
  file = {D\:\\GDrive\\zotero\\Schuster et al\\schuster_et_al_2015_counterfeit_object-oriented_programming.pdf},
  isbn = {978-1-4673-6949-7},
  language = {en}
}

@techreport{schwartzExploitHardeningMade2011,
  title = {Q: {{Exploit Hardening Made Easy}}},
  author = {Schwartz, Edward J and Avgerinos, Thanassis and Brumley, David},
  year = {2011},
  abstract = {Prior work has shown that return oriented programming (ROP) can be used to bypass W{$\oplus$}X, a software defense that stops shellcode, by reusing instructions from large libraries such as libc. Modern operating systems have since enabled address randomization (ASLR), which ran-domizes the location of libc, making these techniques unusable in practice. However, modern ASLR implementations leave smaller amounts of executable code unran-domized and it has been unclear whether an attacker can use these small code fragments to construct payloads in the general case. In this paper, we show defenses as currently deployed can be bypassed with new techniques for automatically creating ROP payloads from small amounts of unran-domized code. We propose using semantic program verification techniques for identifying the functionality of gadgets, and design a ROP compiler that is resistant to missing gadget types. To demonstrate our techniques, we build Q, an end-to-end system that automatically generates ROP payloads for a given binary. Q can produce payloads for 80\% of Linux /usr/bin programs larger than 20KB. We also show that Q can automatically perform exploit hardening: given an exploit that crashes with defenses on, Q outputs an exploit that bypasses both W{$\oplus$}X and ASLR. We show that Q can harden nine real-world Linux and Windows exploits, enabling an attacker to automatically bypass defenses as deployed by industry for those programs.},
  file = {D\:\\GDrive\\zotero\\Schwartz et al\\schwartz_et_al_2011_q.pdf}
}

@techreport{schwartzRippleProtocolConsensus,
  title = {The {{Ripple Protocol Consensus Algorithm}}},
  author = {Schwartz, David and Youngs, Noah and Britto, Arthur},
  abstract = {While several consensus algorithms exist for the Byzantine Generals Problem, specifically as it pertains to distributed payment systems, many suffer from high latency induced by the requirement that all nodes within the network communicate synchronously. In this work, we present a novel consensus algorithm that circumvents this requirement by utilizing collectively-trusted subnetworks within the larger network. We show that the "trust" required of these subnetworks is in fact minimal and can be further reduced with principled choice of the member nodes. In addition, we show that minimal connectivity is required to maintain agreement throughout the whole network. The result is a low-latency consensus algorithm which still maintains robustness in the face of Byzantine failures. We present this algorithm in its embodiment in the Ripple Protocol.},
  file = {D\:\\GDrive\\zotero\\Schwartz\\schwartz_the_ripple_protocol_consensus_algorithm.pdf}
}

@article{schwarzFantasticTimersWhere2017,
  title = {Fantastic Timers and Where to Find Them: {{High}}-Resolution Microarchitectural Attacks in Javascript},
  author = {Schwarz, Michael and Maurice, Cl{\'e}mentine and Gruss, Daniel and Mangard, Stefan},
  year = {2017},
  volume = {10322 LNCS},
  pages = {247--267},
  issn = {16113349},
  doi = {10.1007/978-3-319-70972-7_13},
  abstract = {Research showed that microarchitectural attacks like cache attacks can be performed through websites using JavaScript. These timing attacks allow an adversary to spy on users secrets such as their keystrokes, leveraging fine-grained timers. However, the W3C and browser vendors responded to this significant threat by eliminating fine-grained timers from JavaScript. This renders previous high-resolution microarchitectural attacks non-applicable. We demonstrate the inefficacy of this mitigation by finding and evaluating a wide range of new sources of timing information. We develop measurement methods that exceed the resolution of official timing sources by 3 to 4 orders of magnitude on all major browsers, and even more on Tor browser. Our timing measurements do not only re-enable previous attacks to their full extent but also allow implementing new attacks. We demonstrate a new DRAM-based covert channel between a website and an unprivileged app in a virtual machine without network hardware. Our results emphasize that quick-fix mitigations can establish a dangerous false sense of security.},
  file = {D\:\\GDrive\\zotero\\Schwarz\\schwarz_2017_fantastic_timers_and_where_to_find_them.pdf},
  isbn = {9783319709710},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{schwarzKeyDrownEliminatingSoftwareBased2018,
  title = {{{KeyDrown}} : {{Eliminating Software}}-{{Based Keystroke Timing Side}}-{{Channel Attacks To}} Cite This Version : {{HAL Id}} : Hal-01872534 {{KeyDrown}} : {{Eliminating Software}}-{{Based Keystroke Timing Side}}-{{Channel Attacks}}},
  author = {Schwarz, Michael and Lipp, Moritz and Gruss, Daniel and Weiser, Samuel and Spreitzer, Raphael and Mangard, Stefan and Schwarz, Michael and Lipp, Moritz and Gruss, Daniel and Weiser, Samuel and Maurice, Cl{\'e}mentine and Schwarz, Michael and Lipp, Moritz and Gruss, Daniel and Weiser, Samuel},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Schwarz\\schwarz_2018_keydrown.pdf}
}

@article{schwarzMalwareGuardExtension2017,
  title = {Malware Guard Extension: {{Using SGX}} to Conceal Cache Attacks},
  author = {Schwarz, Michael and Weiser, Samuel and Gruss, Daniel and Maurice, Cl{\'e}mentine and Mangard, Stefan},
  year = {2017},
  volume = {10327 LNCS},
  pages = {3--24},
  issn = {16113349},
  doi = {10.1007/978-3-319-60876-1_1},
  abstract = {In modern computer systems, user processes are isolated from each other by the operating system and the hardware. Additionally, in a cloud scenario it is crucial that the hypervisor isolates tenants from other tenants that are co-located on the same physical machine. However, the hypervisor does not protect tenants against the cloud provider and thus the supplied operating system and hardware. Intel SGX provides a mechanism that addresses this scenario. It aims at protecting user-level software from attacks from other processes, the operating system, and even physical attackers. In this paper, we demonstrate fine-grained software-based sidechannel attacks from a malicious SGX enclave targeting co-located enclaves. Our attack is the first malware running on real SGX hardware, abusing SGX protection features to conceal itself. Furthermore, we demonstrate our attack both in a native environment and across multiple Docker containers. We perform a Prime+Probe cache side-channel attack on a co-located SGX enclave running an up-to-date RSA implementation that uses a constant-time multiplication primitive. The attack works although in SGX enclaves there are no timers, no large pages, no physical addresses, and no shared memory. In a semi-synchronous attack, we extract 96\% of an RSA private key from a single trace. We extract the full RSA private key in an automated attack from 11 traces.},
  file = {D\:\\GDrive\\zotero\\Schwarz\\schwarz_2017_malware_guard_extension.pdf},
  isbn = {9783319608754},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
}

@article{schwittmannDomainImpersonationFeasible2019,
  title = {Domain Impersonation Is Feasible: {{A}} Study of {{CA}} Domain Validation Vulnerabilities},
  author = {Schwittmann, Lorenz and Wander, Matthaus and Weis, Torben},
  year = {2019},
  pages = {544--559},
  doi = {10.1109/EuroSP.2019.00046},
  abstract = {Web security relies on the assumption that certificate authorities (CAs) issue certificates to rightful domain owners only. However, we show that CAs expose vulnerabilities which allow an attacker to obtain certificates from major CAs for domains he does not own. We present a measurement method that allows us to check CAs for a list of technical weaknesses during their domain validation procedures. Our results show that all tested CAs are vulnerable in one or even multiple ways, because they rely on a combination of insecure protocols like DNS and HTTP and do not implement existing secure alternatives like DNSSEC and TLS. We have validated our methodology experimentally and disclosed these vulnerabilities to CAs. Based upon our findings we provide recommendations to domain owners and CAs to close this fundamental weakness in web security.},
  file = {D\:\\GDrive\\zotero\\Schwittmann\\schwittmann_2019_domain_impersonation_is_feasible.pdf},
  isbn = {9781728111476},
  journal = {Proceedings - 4th IEEE European Symposium on Security and Privacy, EURO S and P 2019},
  keywords = {Certificate Authority,DANE,DNSSEC,Domain Validation,Initial Validation,Public Key Infrastructure}
}

@techreport{searsStasisFlexibleTransactional,
  title = {Stasis: {{Flexible Transactional Storage}}},
  author = {Sears, Russell and Brewer, Eric},
  abstract = {An increasing range of applications requires robust support for atomic, durable and concurrent transactions. Databases provide the default solution, but force applications to interact via SQL and to forfeit control over data layout and access mechanisms. We argue there is a gap between DBMSs and file systems that limits designers of data-oriented applications. Stasis is a storage framework that incorporates ideas from traditional write-ahead logging algorithms and file systems. It provides applications with flexible control over data structures, data layout, robustness, and performance. Stasis enables the development of unforeseen variants on transactional storage by generalizing write-ahead logging algorithms. Our partial implementation of these ideas already provides specialized (and cleaner) semantics to applications. We evaluate the performance of a traditional trans-actional storage system based on Stasis, and show that it performs favorably relative to existing systems. We present examples that make use of custom access methods , modified buffer manager semantics, direct log file manipulation, and LSN-free pages. These examples facilitate sophisticated performance optimizations such as zero-copy I/O. These extensions are composable, easy to implement and significantly improve performance.},
  file = {D\:\\GDrive\\zotero\\Sears\\sears_stasis.pdf}
}

@phdthesis{SecureVirtualArchitecture2014,
  title = {Secure {{Virtual Architecture}}: {{Security For Commodity Software Systems}}},
  year = {2014},
  file = {D\:\\GDrive\\zotero\\_\\2014_secure_virtual_architecture.pdf}
}

@article{SecurityPracticesDevOps2016,
  title = {Security {{Practices}} in {{DevOps}}},
  year = {2016},
  doi = {10.1145/2898375.2898383},
  file = {D\:\\GDrive\\zotero\\undefined\\security_practices_in_devops.pdf},
  isbn = {9781450342773}
}

@article{seiler-hwangDonSeeWhy2019,
  title = {``{{I}} Don't See Why i Would Ever Want to Use It'': {{Analyzing}} the Usability of Popular Smartphone Password Managers},
  author = {{Seiler-Hwang}, Sunyoung and Almenares, Florina and {Arias-Cabarcos}, Patricia and {D{\'i}az-S{\'a}nchez}, Daniel and Mar{\'i}n, Andr{\'e}s and Becker, Christian},
  year = {2019},
  pages = {1937--1953},
  issn = {15437221},
  doi = {10.1145/3319535.3354192},
  abstract = {Passwords are an often unavoidable authentication mechanism, despite the availability of additional alternative means. In the case of smartphones, usability problems are aggravated because interaction happens through small screens and multilayer keyboards. While password managers (PMs) can improve this situation and contribute to hardening security, their adoption is far from widespread. To understand the underlying reasons, we conducted the first empirical usability study of mobile PMs, covering both quantitative and qualitative evaluations. Our findings show that popular PMs are barely acceptable according to the standard System Usability Scale, and that there are three key areas for improvement: integration with external applications, security, and user guidance and interaction. We build on the collected evidence to suggest recommendations that can fill this gap.},
  file = {D\:\\GDrive\\zotero\\Seiler-Hwang\\seiler-hwang_2019_“i_don't_see_why_i_would_ever_want_to_use_it”.pdf},
  isbn = {9781450367479},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Authentication,Password managers,Usable security,User study}
}

@techreport{sekarDesignImplementationConsolidated,
  title = {Design and {{Implementation}} of a {{Consolidated Middlebox Architecture}}},
  author = {Sekar, Vyas and Egi, Norbert and Ratnasamy, Sylvia and Reiter, Michael K and Shi, Guangyu},
  abstract = {Network deployments handle changing application, workload, and policy requirements via the deployment of specialized network appliances or "middleboxes". Today , however, middlebox platforms are expensive and closed systems, with little or no hooks for extensibil-ity. Furthermore, they are acquired from independent vendors and deployed as standalone devices with little cohesiveness in how the ensemble of middleboxes is managed. As network requirements continue to grow in both scale and variety, this bottom-up approach puts middlebox deployments on a trajectory of growing device sprawl with corresponding escalation in capital and management costs. To address this challenge, we present CoMb, a new architecture for middlebox deployments that systematically explores opportunities for consolidation, both at the level of building individual middleboxes and in managing a network of middleboxes. This paper addresses key resource management and implementation challenges that arise in exploiting the benefits of consolidation in middlebox deployments. Using a prototype implementation in Click, we show that CoMb reduces the network provisioning cost 1.8-2.5\texttimes{} and reduces the load imbalance in a network by 2-25\texttimes.},
  file = {D\:\\GDrive\\zotero\\Sekar\\sekar_design_and_implementation_of_a_consolidated_middlebox_architecture.pdf}
}

@techreport{sekarFrameworkInternetForensic,
  title = {Toward a {{Framework}} for {{Internet Forensic Analysis}}},
  author = {Sekar, Vyas and Xie, Yinglian and Maltz, David A and Reiter, Michael K and Zhang, Hui},
  abstract = {The world of network security is an arms race where attackers constantly change the signatures of their attacks to avoid detection. Aiding the white-hats in this race is one fundamental invariant across all network attacks (present and future): for the attack to progress there must be communication among attacker, the associated set of compromised hosts and the victim(s), and this communication is visible to the network. We argue that the Internet architecture should be extended to include auditing mechanisms that enable the forensic analysis of network data, with a goal of identifying the true originator of each attack-even if the attacker recruits innocent hosts as zombies or stepping stones to propagate the attack. In this paper we outline an approach to the problem of Attacker Identification and Attack Reconstruction , describe the challenges involved, and explain our efforts that show the promise of this approach.},
  file = {D\:\\GDrive\\zotero\\Sekar\\sekar_toward_a_framework_for_internet_forensic_analysis.pdf}
}

@article{seoHowWeGet2014,
  title = {How We Get There: {{A}} Context-Guided Search Strategy in Concolic Testing},
  author = {Seo, Hyunmin and Kim, Sunghun},
  year = {2014},
  volume = {16-21-Nove},
  pages = {413--424},
  doi = {10.1145/2635868.2635872},
  abstract = {One of the biggest challenges in concolic testing, an automatic test generation technique, is its huge search space. Concolic testing generates next inputs by selecting branches from previous execution paths. However, a large number of candidate branches makes a simple exhaustive search infeasible, which often leads to poor test coverage. Several search strategies have been proposed to explore high-priority branches only. Each strategy applies different criteria to the branch selection process but most do not consider context, how we got to the branch, in the selection process. In this paper, we introduce a context-guided search (CGS) strategy. CGS looks at preceding branches in execution paths and selects a branch in a new context for the next input. We evaluate CGS with two publicly available concolic testing tools, CREST and CarFast, on six C subjects and six Java subjects. The experimental results show that CGS achieves the highest coverage of all twelve subjects and reaches a target coverage with a much smaller number of iterations on most subjects than other strategies.},
  file = {D\:\\GDrive\\zotero\\Seo\\seo_2014_how_we_get_there.pdf},
  isbn = {9781450330565},
  journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
  keywords = {Concolic testing,Search strategies,Symbolic execution}
}

@techreport{seshadriPioneerVerifyingCode2005,
  title = {Pioneer: {{Verifying Code Integrity}} and {{Enforcing Untampered Code Execution}} on {{Legacy Systems}} *},
  author = {Seshadri, Arvind and Luk, Mark and Shi, Elaine and Cylab, Cmu / and Leendert, Adrian Perrig and Doorn, Van and Khosla, Pradeep},
  year = {2005},
  abstract = {We propose a primitive, called Pioneer, as a first step towards verifiable code execution on untrusted legacy hosts. Pioneer does not require any hardware support such as secure co-processors or CPU-architecture extensions. We implement Pioneer on an Intel Pentium IV Xeon processor. Pioneer can be used as a basic building block to build security systems. We demonstrate this by building a kernel rootkit detector.},
  file = {D\:\\GDrive\\zotero\\Seshadri et al\\seshadri_et_al_2005_pioneer.pdf},
  keywords = {Dynamic Root of Trust,Operating Sys-tems,Rootkit Detection,Security and Protection,Self-check-summing Code,Software,Software-based Code At-testation,Verification General Terms: Security Keywords: Verifiable Code Execution}
}

@techreport{seshadriSWATTSoftWarebasedATTestation2004,
  title = {{{SWATT}}: {{SoftWare}}-Based {{ATTestation}} for {{Embedded Devices}} *},
  author = {Seshadri, Arvind and Perrig, Adrian and Van Doorn, Leendert and Khosla, Pradeep},
  year = {2004},
  abstract = {We expect a future where we are surrounded by embedded devices, ranging from Java-enabled cell phones to sensor networks and smart appliances. An adversary can compromise our privacy and safety by maliciously modifying the memory contents of these embedded devices. In this paper, we propose a SoftWare-based ATTestation technique (SWATT) to verify the memory contents of embedded devices and establish the absence of malicious changes to the memory contents. SWATT does not need physical access to the device's memory, yet provides memory content at-testation similar to TCG or NGSCB without requiring secure hardware. SWATT can detect any change in memory contents with high probability, thus detecting viruses, unexpected configuration settings, and Trojan Horses. To circumvent SWATT, we expect that an attacker needs to change the hardware to hide memory content changes. We present an implementation of SWATT in off-the-shelf sensor network devices, which enables us to verify the contents of the program memory even while the sensor node is running.},
  file = {D\:\\GDrive\\zotero\\Seshadri\\seshadri_swatt.pdf}
}

@article{shabtaiGoogleAndroidComprehensive2009,
  title = {Google Android: {{A}} Comprehensive Security Assessment},
  author = {Shabtai, A. and Fledel, Y. and Kanonov, U. and Elovici, Y.Dolev, S. and Glezer, C.},
  year = {2009},
  pages = {35--44},
  issn = {2072-8743},
  file = {D\:\\GDrive\\zotero\\Shabtai\\shabtai_2009_google_android.pdf},
  journal = {T-Comm - \cyrchar\CYRT\cyrchar\cyre\cyrchar\cyrl\cyrchar\cyre\cyrchar\cyrk\cyrchar\cyro\cyrchar\cyrm\cyrchar\cyrm\cyrchar\cyru\cyrchar\cyrn\cyrchar\cyri\cyrchar\cyrk\cyrchar\cyra\cyrchar\cyrc\cyrchar\cyri\cyrchar\cyri{} \cyrchar\cyri{} \cyrchar\CYRT\cyrchar\cyrr\cyrchar\cyra\cyrchar\cyrn\cyrchar\cyrs\cyrchar\cyrp\cyrchar\cyro\cyrchar\cyrr\cyrchar\cyrt},
  number = {\cyrchar\CYRI\cyrchar\CYRT\cyrchar\CYRS}
}

@article{shachamClientsideCachingTLS2004,
  title = {Client-Side Caching for {{TLS}}},
  author = {Shacham, Hovav and Boneh, Dan and Rescorla, Eric},
  year = {2004},
  month = nov,
  volume = {7},
  pages = {553--575},
  issn = {1094-9224, 1557-7406},
  doi = {10.1145/1042031.1042034},
  abstract = {We propose two new mechanisms for caching handshake information on TLS clients. The ``fast-track'' mechanism provides a client side cache of a server's public parameters and negotiated parameters in the course of an initial, enabling handshake. These parameters need not be resent on subsequent handshakes. Fast-track reduces both network traffic and the number of round trips, and requires no additional server state. These savings are most useful in highlatency environments such as wireless networks. The second mechanism, ``client-side session caching,'' allows the server to store an encrypted version of the session information on a client, allowing a server to maintain a much larger number of active sessions in a given memory footprint. Our design is fully backward-compatible with TLS: extended clients can interoperate with servers unaware of our extensions and vice versa. We have implemented our fast-track proposal to demonstrate the resulting efficiency improvements.},
  file = {D\:\\GDrive\\zotero\\Shacham et al\\shacham_et_al_2004_client-side_caching_for_tls.pdf},
  journal = {ACM Transactions on Information and System Security},
  language = {en},
  number = {4}
}

@article{shachamCompactProofsRetrievability2008,
  title = {Compact {{Proofs}} of {{Retrievability}}},
  author = {Shacham, Hovav and Waters, Brent},
  year = {2008},
  pages = {38},
  abstract = {In a proof-of-retrievability system, a data storage center must prove to a verifier that he is actually storing all of a client's data. The central challenge is to build systems that are both efficient and provably secure \textemdash{} that is, it should be possible to extract the client's data from any prover that passes a verification check. In this paper, we give the first proof-of-retrievability schemes with full proofs of security against arbitrary adversaries in the strongest model, that of Juels and Kaliski.},
  file = {D\:\\GDrive\\zotero\\Shacham_Waters\\shacham_waters_2008_compact_proofs_of_retrievability.pdf},
  language = {en}
}

@techreport{shachamEffectivenessAddressSpaceRandomization2004,
  title = {On the {{Effectiveness}} of {{Address}}-{{Space Randomization}}},
  author = {Shacham, Hovav and Page, Matthew and Pfaff, Ben and Goh, Eu-Jin and Modadugu, Nagendra and Boneh, Dan},
  year = {2004},
  abstract = {Address-space randomization is a technique used to fortify systems against buffer overflow attacks. The idea is to introduce artificial diversity by randomizing the memory location of certain system components. This mechanism is available for both Linux (via PaX ASLR) and OpenBSD. We study the effectiveness of address-space randomization and find that its utility on 32-bit architectures is limited by the number of bits available for address randomization. In particular, we demonstrate a derandomization attack that will convert any standard buffer-overflow exploit into an exploit that works against systems protected by address-space randomization. The resulting exploit is as effective as the original, albeit somewhat slower: on average 216 seconds to compromise Apache running on a Linux PaX ASLR system. The attack does not require running code on the stack. We also explore various ways of strengthening address-space randomization and point out weaknesses in each. Surprisingly , increasing the frequency of re-randomizations adds at most 1 bit of security. Furthermore, compile-time ran-domization appears to be more effective than runtime ran-domization. We conclude that, on 32-bit architectures, the only benefit of PaX-like address-space randomization is a small slowdown in worm propagation speed. The cost of randomization is extra complexity in system support.},
  file = {D\:\\GDrive\\zotero\\Shacham\\shacham_2004_on_the_effectiveness_of_address-space_randomization.pdf},
  keywords = {automated attacks,D46 [Operating Systems]: Security and Protection General Terms Security,diversity,Measurement Keywords Address-space randomization}
}

@inproceedings{shachamEffectivenessAddressspaceRandomization2004,
  title = {On the Effectiveness of Address-Space Randomization},
  booktitle = {Proceedings of the 11th {{ACM}} Conference on {{Computer}} and Communications Security  - {{CCS}} '04},
  author = {Shacham, Hovav and Page, Matthew and Pfaff, Ben and Goh, Eu-Jin and Modadugu, Nagendra and Boneh, Dan},
  year = {2004},
  pages = {298},
  publisher = {{ACM Press}},
  address = {{Washington DC, USA}},
  doi = {10.1145/1030083.1030124},
  abstract = {Address-space randomization is a technique used to fortify systems against buffer overflow attacks. The idea is to introduce artificial diversity by randomizing the memory location of certain system components. This mechanism is available for both Linux (via PaX ASLR) and OpenBSD. We study the effectiveness of address-space randomization and find that its utility on 32-bit architectures is limited by the number of bits available for address randomization. In particular, we demonstrate a derandomization attack that will convert any standard buffer-overflow exploit into an exploit that works against systems protected by address-space randomization. The resulting exploit is as effective as the original, albeit somewhat slower: on average 216 seconds to compromise Apache running on a Linux PaX ASLR system. The attack does not require running code on the stack.},
  file = {D\:\\GDrive\\zotero\\Shacham et al\\shacham_et_al_2004_on_the_effectiveness_of_address-space_randomization.pdf},
  isbn = {978-1-58113-961-7},
  language = {en}
}

@incollection{shachamEfficientRingSignatures2007,
  title = {Efficient {{Ring Signatures Without Random Oracles}}},
  booktitle = {Public {{Key Cryptography}} \textendash{} {{PKC}} 2007},
  author = {Shacham, Hovav and Waters, Brent},
  editor = {Okamoto, Tatsuaki and Wang, Xiaoyun},
  year = {2007},
  volume = {4450},
  pages = {166--180},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-71677-8_12},
  abstract = {We describe the first efficient ring signature scheme secure, without random oracles, based on standard assumptions. Our ring signatures are based in bilinear groups. For l members of a ring our signatures consist of 2l + 2 group elements and require 2l + 3 pairings to verify. We prove our scheme secure in the strongest security model proposed by Bender, Katz, and Morselli: namely, we show our scheme to be anonymous against full key exposure and unforgeable with respect to insider corruption. A shortcoming of our approach is that all the users' keys must be defined in the same group.},
  file = {D\:\\GDrive\\zotero\\Shacham_Waters\\shacham_waters_2007_efficient_ring_signatures_without_random_oracles.pdf},
  isbn = {978-3-540-71676-1 978-3-540-71677-8},
  language = {en}
}

@techreport{shachamGeometryInnocentFlesh2007,
  title = {The {{Geometry}} of {{Innocent Flesh}} on the {{Bone}}: {{Return}}-into-Libc without {{Function Calls}} (on the X86)},
  author = {Shacham, Hovav},
  year = {2007},
  institution = {{ACM Press}},
  abstract = {We present new techniques that allow a return-into-libc attack to be mounted on x86 exe-cutables that calls no functions at all. Our attack combines a large number of short instruction sequences to build gadgets that allow arbitrary computation. We show how to discover such instruction sequences by means of static analysis. We make use, in an essential way, of the properties of the x86 instruction set.},
  file = {D\:\\GDrive\\zotero\\Shacham\\shacham_2007_the_geometry_of_innocent_flesh_on_the_bone.pdf;D\:\\GDrive\\zotero\\Shacham\\shacham_2007_the_geometry_of_innocent_flesh_on_the_bone2.pdf;D\:\\GDrive\\zotero\\Shacham\\shacham_2007_the_geometry_of_innocent_flesh_on_the_bone3.pdf},
  keywords = {Instruction set,Return-into-libc,Turing completeness}
}

@article{shachamGeometryInnocentFlesh2007a,
  title = {The {{Geometry}} of {{Innocent Flesh}} on the {{Bone}}: {{Return}}-into-Libc without {{Function Calls}} (on the X86)},
  author = {Shacham, Hovav},
  year = {2007},
  pages = {30},
  abstract = {We present new techniques that allow a return-into-libc attack to be mounted on x86 executables that calls no functions at all. Our attack combines a large number of short instruction sequences to build gadgets that allow arbitrary computation. We show how to discover such instruction sequences by means of static analysis. We make use, in an essential way, of the properties of the x86 instruction set.},
  file = {D\:\\GDrive\\zotero\\Shacham\\shacham_2007_the_geometry_of_innocent_flesh_on_the_bone4.pdf},
  language = {en}
}

@incollection{shachamImprovingSSLHandshake2001,
  title = {Improving {{SSL Handshake Performance}} via {{Batching}}},
  booktitle = {Topics in {{Cryptology}} \textemdash{} {{CT}}-{{RSA}} 2001},
  author = {Shacham, Hovav and Boneh, Dan},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Naccache, David},
  year = {2001},
  volume = {2020},
  pages = {28--43},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45353-9_3},
  abstract = {We present an algorithmic approach for speeding up SSL's performance on a web server. Our approach improves the performance of SSL's handshake protocol by up to a factor of 2.5 for 1024-bit RSA keys. It is designed for heavily-loaded web servers handling many concurrent SSL sessions. We improve the server's performance by batching the SSL handshake protocol. That is, we show that b SSL handshakes can be done faster as a batch than doing the b handshakes separately one after the other. Experiments show that taking b = 4 leads to optimal results, namely a speedup of a factor of 2.5. Our starting point is a technique due to Fiat for batching RSA decryptions. We improve the performance of batch RSA and describe an architecture for using it in an SSL web server. We give experimental results for all the proposed techniques.},
  file = {D\:\\GDrive\\zotero\\Shacham_Boneh\\shacham_boneh_2001_improving_ssl_handshake_performance_via_batching.pdf},
  isbn = {978-3-540-41898-6 978-3-540-45353-6},
  language = {en}
}

@phdthesis{shachamNEWPARADIGMSSIGNATURE2005,
  title = {{{NEW PARADIGMS IN SIGNATURE SCHEMES}}},
  author = {Shacham, Hovav},
  year = {2005},
  file = {D\:\\GDrive\\zotero\\Shacham\\shacham_2005_new_paradigms_in_signature_schemes.pdf},
  language = {en}
}

@article{shachamShortUniqueSignatures2018,
  title = {Short Unique Signatures from {{RSA}} with a Tight Security Reduction (in the Random Oracle Model)},
  author = {Shacham, Hovav},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Shacham\\shacham_2018_short_unique_signatures_from_rsa_with_a_tight_security_reduction_(in_the_random.pdf}
}

@techreport{shahanirbanmitradhruvmataniAlgorithmImplementingLFU2010,
  title = {An {{O}}(1) Algorithm for Implementing the {{LFU}} Cache Eviction Scheme},
  author = {Shah Anirban Mitra Dhruv Matani, Ketan},
  year = {2010},
  abstract = {Cache eviction algorithms are used widely in operating systems, databases and other systems that use caches to speed up execution by caching data that is used by the application. There are many policies such as MRU (Most Recently Used), MFU (Most Frequently Used), LRU (Least Recently Used) and LFU (Least Frequently Used) which each have their advantages and drawbacks and are hence used in specific scenarios. By far, the most widely used algorithm is LRU, both for its O(1) speed of operation as well as its close resemblance to the kind of behaviour that is expected by most applications. The LFU algorithm also has behaviour desirable by many real world workloads. However, in many places, the LRU algorithm is is preferred over the LFU algorithm because of its lower run time complexity of O(1) versus O(log n). We present here an LFU cache eviction algorithm that has a runtime complexity of O(1) for all of its operations, which include insertion, access and deletion(eviction).},
  file = {D\:\\GDrive\\zotero\\Shah Anirban Mitra Dhruv Matani\\shah_anirban_mitra_dhruv_matani_2010_an_o(1)_algorithm_for_implementing_the_lfu_cache_eviction_scheme.pdf}
}

@techreport{shahKeyboardsCovertChannels,
  title = {Keyboards and {{Covert Channels}}},
  author = {Shah, Gaurav and Molina, Andres and Blaze, Matt},
  abstract = {This paper introduces JitterBugs, a class of inline interception mechanisms that covertly transmit data by perturbing the timing of input events likely to affect externally observable network traffic. JitterBugs positioned at input devices deep within the trusted environment (e.g., hidden in cables or connectors) can leak sensitive data without compromising the host or its software. In particular , we show a practical Keyboard JitterBug that solves the data exfiltration problem for keystroke loggers by leaking captured passwords through small variations in the precise times at which keyboard events are delivered to the host. Whenever an interactive communication application (such as SSH, Telnet, instant messaging, etc) is running, a receiver monitoring the host's network traffic can recover the leaked data, even when the session or link is encrypted. Our experiments suggest that simple Keyboard JitterBugs can be a practical technique for capturing and exfiltrating typed secrets under conventional OSes and interactive network applications, even when the receiver is many hops away on the Internet.},
  file = {D\:\\GDrive\\zotero\\Shah\\shah_keyboards_and_covert_channels.pdf}
}

@techreport{shannonCommunicationTheorySecrecy,
  title = {Communication {{Theory}} of {{Secrecy Systems}}},
  author = {Shannon, C E},
  file = {D\:\\GDrive\\zotero\\Shannon\\shannon_communication_theory_of_secrecy_systems.pdf}
}

@techreport{shannonMathematicalTheoryCommunication,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  volume = {27},
  pages = {623--656},
  file = {D\:\\GDrive\\zotero\\Shannon\\shannon_a_mathematical_theory_of_communication.pdf},
  journal = {The Bell System Technical Journal}
}

@article{shapiroComprehensiveStudyConvergent2011,
  title = {A Comprehensive Study of {{Convergent}} and {{Commutative Replicated Data Types}}},
  author = {Shapiro, Marc and Baquero, Carlos and Zawirski, Marek},
  year = {2011},
  issn = {0249-6399},
  file = {D\:\\GDrive\\zotero\\Shapiro\\shapiro_2011_a_comprehensive_study_of_convergent_and_commutative_replicated_data_types.pdf}
}

@techreport{sharifImpedingMalwareAnalysis,
  title = {Impeding {{Malware Analysis Using Conditional Code Obfuscation}}},
  author = {Sharif, Monirul and Lanzi, Andrea and Giffin, Jonathon and Lee, Wenke},
  abstract = {Malware programs that incorporate trigger-based behavior initiate malicious activities based on conditions satisfied only by specific inputs. State-of-the-art malware an-alyzers discover code guarded by triggers via multiple path exploration, symbolic execution, or forced conditional execution , all without knowing the trigger inputs. We present a malware obfuscation technique that automatically conceals specific trigger-based behavior from these malware analyzers. Our technique automatically transforms a program by encrypting code that is conditionally dependent on an input value with a key derived from the input and then removing the key from the program. We have implemented a compiler-level tool that takes a malware source program and automatically generates an obfuscated binary. Experiments on various existing malware samples show that our tool can hide a significant portion of trigger based code. We provide insight into the strengths, weaknesses, and possible ways to strengthen current analysis approaches in order to defeat this malware obfuscation technique.},
  file = {D\:\\GDrive\\zotero\\Sharif et al\\sharif_et_al_impeding_malware_analysis_using_conditional_code_obfuscation.pdf}
}

@techreport{sharmaAnalysisCredentialStealing,
  title = {Analysis of {{Credential Stealing Attacks}} in an {{Open Networked Environment}}},
  author = {Sharma, A and Kalbarczyk, Z and Iyer, R and Barlow, J},
  abstract = {This paper analyses the forensic data on credential stealing incidents over a period of 5 years across 5000 machines monitored at the National Center for Supercomputing Applications at the University of Illinois. The analysis conducted is the first attempt in an open operational environment (i) to evaluate the intricacies of carrying out SSH-based credential stealing attacks, (ii) to highlight and quantify key characteristics of such attacks, and (iii) to provide the system level characterization of such incidents in terms of distribution of alerts and incident consequences.},
  file = {D\:\\GDrive\\zotero\\Sharma\\sharma_analysis_of_credential_stealing_attacks_in_an_open_networked_environment.pdf},
  keywords = {Credential stealing,Incident analysis,Intrusion detection}
}

@article{sharplesBlockchainKudosDistributed2016,
  title = {The {{Blockchain}} and {{Kudos}}: {{A Distributed System}} for {{Educational Record}}, {{Reputation}} and {{Reward Book Section The Blockchain}} and {{Kudos}}: {{A Distributed System}} for {{Educational Record}}, {{Reputation}} and {{Reward}}},
  author = {Sharples, Mike and Domingue, John},
  year = {2016},
  pages = {490--496},
  publisher = {{Springer}},
  doi = {10.1007/978-3-319-45153-4},
  abstract = {The 'blockchain' is the core mechanism for the Bitcoin digital payment system. It embraces a set of interrelated technologies: the blockchain itself as a distributed record of digital events, the distributed consensus method to agree whether a new block is legitimate, automated smart contracts, and the data structure associated with each block. We propose a permanent distributed record of intellectual effort and associated reputational reward, based on the blockchain that instantiates and democratises educational reputation beyond the academic community. We are undertaking initial trials of a private blockchain or storing educational records, drawing also on our previous research into reputation management for educational systems.},
  file = {D\:\\GDrive\\zotero\\Sharples\\sharples_2016_the_blockchain_and_kudos.pdf},
  journal = {Lecture Notes in Computer Science. Switzerland},
  keywords = {Blockchain ·,Records of achievement,Reputation management ·,Self-determined learning · e-portfolios ·}
}

@techreport{sheehyBitcaskLogStructuredHash2010,
  title = {Bitcask {{A Log}}-{{Structured Hash Table}} for {{Fast Key}}/{{Value Data}} with Inspiration from {{Eric Brewer}}},
  author = {Sheehy, Justin and Smith, David and Technologies, Basho},
  year = {2010},
  abstract = {The origin of Bitcask is tied to the history of the Riak distributed database. In a Riak key/value cluster, each node uses pluggable local storage; nearly anything k/v-shaped can be used as the per-host storage engine. This pluggability allowed progress on Riak to be parallelized such that storage engines could be improved and tested without impact on the rest of the codebase. Many such local key/value stores already exist, including but not limited to Berkeley DB, Tokyo Cabinet, and Innostore. There are many goals we sought when evaluating such storage engines, including: \textbullet{} low latency per item read or written \textbullet{} high throughput, especially when writing an incoming stream of random items \textbullet{} ability to handle datasets much larger than RAM w/o degradation \textbullet{} crash friendliness, both in terms of fast recovery and not losing data \textbullet{} ease of backup and restore \textbullet{} a relatively simple, understandable (and thus supportable) code structure and data format \textbullet{} predictable behavior under heavy access load or large volume \textbullet{} a license that allowed for easy default use in Riak Achieving some of these is easy. Achieving them all is less so. None of the local key/value storage systems available (including but not limited to those written by the authors) were ideal with regard to all of the above goals. We were discussing this issue with Eric Brewer when he had a key insight about hash table log merging: that doing so could potentially be made as fast or faster than LSM-trees. This led us to explore some of the techniques used in the log-structured file systems first developed in the 1980s and 1990s in a new light. That exploration led to the development of Bitcask, a storage system that meets all of the above goals very well. While Bitcask was originally developed with a goal of being used under Riak, it was built to be generic and can serve as a local key/value store for other applications as well.},
  file = {D\:\\GDrive\\zotero\\Sheehy\\sheehy_2010_bitcask_a_log-structured_hash_table_for_fast_key-value_data_with_inspiration.pdf}
}

@article{Shenkerjsac95,
  title = {Shenker-Jsac95},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\QDFKIAYP\\shenker-jsac95.pdf}
}

@techreport{sherwoodCanProductionNetwork,
  title = {Can the {{Production Network Be}} the {{Testbed}}?},
  author = {Sherwood, Rob and Gibb, Glen and Yap, Kok-Kiong and Appenzeller, Guido and Casado, Martin and Mckeown, Nick and Parulkar, Guru},
  abstract = {A persistent problem in computer network research is validation. When deciding how to evaluate a new feature or bug fix, a researcher or operator must trade-off realism (in terms of scale, actual user traffic, real equipment) and cost (larger scale costs more money, real user traffic likely requires downtime, and real equipment requires vendor adoption which can take years). Building a realistic testbed is hard because "real" networking takes place on closed, commercial switches and routers with special purpose hardware. But if we build our testbed from software switches, they run several orders of magnitude slower. Even if we build a realistic network testbed, it is hard to scale, because it is special purpose and is in addition to the regular network. It needs its own location , support and dedicated links. For a testbed to have global reach takes investment beyond the reach of most researchers. In this paper, we describe a way to build a testbed that is embedded in-and thus grows with-the network. The technique-embodied in our first prototype, FlowVisor-slices the network hardware by placing a layer between the control plane and the data plane. We demonstrate that FlowVisor slices our own production network, with legacy protocols running in their own protected slice, alongside experiments created by researchers. The basic idea is that if unmodified hardware supports some basic primitives (in our prototype, Open-Flow, but others are possible), then a worldwide testbed can ride on the coat-tails of deployments, at no extra expense. Further, we evaluate the performance impact and describe how FlowVisor is deployed at seven other campuses as part of a wider evaluation platform.},
  file = {D\:\\GDrive\\zotero\\Sherwood\\sherwood_can_the_production_network_be_the_testbed.pdf}
}

@inproceedings{shinAVANTGUARDScalableVigilant2013,
  title = {{{AVANT}}-{{GUARD}}: {{Scalable}} and Vigilant Switch Flow Management in Software-Defined Networks},
  booktitle = {Proceedings of the {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Shin, Seungwon and Yegneswaran, Vinod and Porras, Phillip and Gu, Guofei},
  year = {2013},
  pages = {413--424},
  issn = {15437221},
  doi = {10.1145/2508859.2516684},
  abstract = {Among the leading reference implementations of the Software De- fined Networking (SDN) paradigm is the OpenFlow framework, which decouples the control plane into a centralized application. In this paper, we consider two aspects of OpenFlow that pose secu- rity challenges, and we propose two solutions that could address these concerns. The first challenge is the inherent communication bottleneck that arises between the data plane and the control plane, which an adversary could exploit by mounting a control plane sat- uration attack that disrupts network operations. Indeed, even well- mined adversarial models, such as scanning or denial-of-service (DoS) activity, can produce more potent impacts on OpenFlow net- works than traditional networks. To address this challenge, we in- troduce an extension to the OpenFlow data plane called connec- tion migration, which dramatically reduces the amount of data- to-control-plane interactions that arise during such attacks. The second challenge is that of enabling the control plane to expedite both detection of, and responses to, the changing flow dynamics within the data plane. For this, we introduce actuating triggers over the data plane's existing statistics collection services. These triggers are inserted by control layer applications to both register for asynchronous call backs, and insert conditional flow rules that are only activated when a trigger condition is detected within the data plane's statistics module. We present AVANT-GUARD, an im- plementation of our two data plane extensions, evaluate the perfor- mance impact, and examine its use for developing more scalable and resilient SDN security services.},
  file = {D\:\\GDrive\\zotero\\Shin\\shin_2013_avant-guard.pdf},
  isbn = {978-1-4503-2477-9},
  keywords = {control plane saturation attack,openflow,security and resilience,software-defined network (sdn)}
}

@techreport{shorPolynomialTimeAlgorithmsPrime,
  title = {Polynomial-{{Time Algorithms}} for {{Prime Factorization}} and {{Discrete Logarithms}} on a {{Quantum Computer}} *},
  author = {Shor, Peter W},
  pages = {124--134},
  institution = {{IEEE Computer Society Press}},
  abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.},
  file = {D\:\\GDrive\\zotero\\Shor\\shor_polynomial-time_algorithms_for_prime_factorization_and_discrete_logarithms_on_a.pdf},
  journal = {\textdagger{} AT\&T Research},
  keywords = {03D10,11Y05,68Q10,algorithmic number theory,Church's thesis,discrete logarithms,foundations of quantum mechanics,Fourier transforms AMS subject classifications: 81P10,prime factorization,quantum computers,spin systems}
}

@misc{ShouldReadPapers,
  title = {Should {{I Read Papers}}?},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\JR4XERFG\\should-i-read-papers.html},
  howpublished = {https://michaelrbernste.in/2014/10/21/should-i-read-papers.html}
}

@inproceedings{shoumikScalableMicroserviceBased2017,
  title = {Scalable Micro-Service Based Approach to {{FHIR}} Server with Golang and {{No}}-{{SQL}}},
  booktitle = {2017 20th {{International Conference}} of {{Computer}} and {{Information Technology}} ({{ICCIT}})},
  author = {Shoumik, Fahim Shariar and Talukder, Md. Ibna Masum Millat and Jami, Ahmed Imtiaz and Protik, Neeaz Wahed and Hoque, Md. Moinul},
  year = {2017},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ICCITECHN.2017.8281846},
  abstract = {Fast Health Interoperability Resource (FHIR) for Electronic Health Record (EHR) not only opens a new era for health-care systems to exchange data between themselves but also allows other various systems following the same framework to communicate between them. FHIR is engaging and evolving rapidly and it is creating the need of massive data storage, retrieval and transfer requirements. For a server that implements this framework, has to be agile to cope with the growing and massive data handling. E-health-care data also seem to grow proportional to time. So, an ideal FHIR Server has to be scalable to fit this demand over time. Several different tools could help store, anonymize, analyze and extract data because there are no single tools to get all those things done. In this work, we propose a scalable, agile, and reliable Micro-service based Architecture for FHIR server that is highly available with the help of Document Oriented Database and helps to store health-care data. The proposed architecture is fast responsive. It gives our system the flexibility to use different tools that implement FHIR Framework interface in such way that the whole system could be made vertically scalable and the system can perform better in terms of time complexity compared to a Monolithic based implementation of the framework. Experimental results show that the proposed model can be highly used as an alternative to currently available FHIR system implementation.},
  file = {D\:\\GDrive\\zotero\\Shoumik et al\\shoumik_et_al_2017_scalable_micro-service_based_approach_to_fhir_server_with_golang_and_no-sql.pdf},
  keywords = {Benchmark testing,Business,Databases,FHIR,Health-Care system,Interoperability,Logic gates,micro-service,REST API,scalability,Scalability,Servers,Tools}
}

@techreport{showcaseAllYouEver2010,
  title = {All {{You Ever Wanted}} to {{Know About Dynamic Taint Analysis}} and {{Forward Symbolic Execution}} (but Might Have Been Afraid to Ask)},
  author = {Showcase, Research and Cmu, @ and Schwartz, Edward J and Avgerinos, Thanassis and Brumley, David},
  year = {2010},
  pages = {317--331},
  abstract = {Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are twofold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context.},
  file = {D\:\\GDrive\\zotero\\Showcase\\showcase_2010_all_you_ever_wanted_to_know_about_dynamic_taint_analysis_and_forward_symbolic.pdf}
}

@techreport{showcaseCriteriaBeUsed,
  title = {On the Criteria to Be Used in Decomposing Systems into Modules},
  author = {Showcase, Research and Lorge Parnas, David},
  file = {D\:\\GDrive\\zotero\\Showcase\\showcase_on_the_criteria_to_be_used_in_decomposing_systems_into_modules.pdf}
}

@techreport{shuklaLIGHTWEIGHTCROSSPROCEDURETRACING,
  title = {{{LIGHTWEIGHT}}, {{CROSS}}-{{PROCEDURE TRACING FOR RUNTIME OPTIMIZATION}}},
  author = {Shukla, Anand},
  file = {D\:\\GDrive\\zotero\\Shukla\\shukla_lightweight,_cross-procedure_tracing_for_runtime_optimization.pdf}
}

@techreport{shuteF1DistributedSQL2150,
  title = {F1: {{A Distributed SQL Database That Scales}}},
  author = {Shute, Jeff and Vingralek, Radek and Samwel, Bart and Handy, Ben and Whipkey, Chad and Rollins, Eric and Oancea, Mircea and Littlefield, Kyle and Menestrina, David and Ellner, Stephan and Cieslewicz, John and Rae, Ian and Stancescu, Traian and Apte, Himani},
  year = {2150},
  abstract = {F1 is a distributed relational database system built at Google to support the AdWords business. F1 is a hybrid database that combines high availability, the scalability of NoSQL systems like Bigtable, and the consistency and us-ability of traditional SQL databases. F1 is built on Spanner , which provides synchronous cross-datacenter replica-tion and strong consistency. Synchronous replication implies higher commit latency, but we mitigate that latency by using a hierarchical schema model with structured data types and through smart application design. F1 also includes a fully functional distributed SQL query engine and automatic change tracking and publishing.},
  file = {D\:\\GDrive\\zotero\\Shute\\shute_2150_f1.pdf},
  keywords = {distributed systems}
}

@phdthesis{shyanQuantifyingModelingPrivacy,
  title = {Quantifying and {{Modeling Privacy Risks}} in {{Smart City Systems}}},
  author = {Shyan, Phang Boon},
  file = {D\:\\GDrive\\zotero\\Shyan\\shyan_quantifying_and_modeling_privacy_risks_in_smart_city_systems.pdf},
  language = {en}
}

@article{sibyEncryptedDNSPrivacy2020,
  title = {Encrypted {{DNS}} --{$>$} {{Privacy}}? {{A Traffic Analysis Perspective}}},
  author = {Siby, Sandra and Juarez, Marc and Diaz, Claudia and {Vallina-Rodriguez}, Narseo and Troncoso, Carmela},
  year = {2020},
  doi = {10.14722/ndss.2020.24301},
  abstract = {Virtually every connection to an Internet service is preceded by a DNS lookup which is performed without any traffic-level protection, thus enabling manipulation, redirection, surveillance, and censorship. To address these issues, large organizations such as Google and Cloudflare are deploying recently standardized protocols that encrypt DNS traffic between end users and recursive resolvers such as DNS-over-TLS (DoT) and DNS-over-HTTPS (DoH). In this paper, we examine whether encrypting DNS traffic can protect users from traffic analysis-based monitoring and censoring. We propose a novel feature set to perform the attacks, as those used to attack HTTPS or Tor traffic are not suitable for DNS' characteristics. We show that traffic analysis enables the identification of domains with high accuracy in closed and open world settings, using 124 times less data than attacks on HTTPS flows. We find that factors such as location, resolver, platform, or client do mitigate the attacks performance but they are far from completely stopping them. Our results indicate that DNS-based censorship is still possible on encrypted DNS traffic. In fact, we demonstrate that the standardized padding schemes are not effective. Yet, Tor -- which does not effectively mitigate traffic analysis attacks on web traffic -- is a good defense against DoH traffic analysis.},
  file = {D\:\\GDrive\\zotero\\Siby\\siby_2020_encrypted_dns_--_privacy.pdf},
  number = {September}
}

@techreport{sigelmanGoogleTechnicalReport2010,
  title = {Google {{Technical Report}} Dapper {{Dapper}}, a {{Large}}-{{Scale Distributed Systems Tracing Infrastructure}}},
  author = {Sigelman, Benjamin H and Barroso, Andr{\'e} and Burrows, Mike and Stephenson, Pat and Plakal, Manoj and Beaver, Donald and Jaspan, Saul and Shanbhag, Chandan},
  year = {2010},
  abstract = {Modern Internet services are often implemented as complex , large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facilities. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment. Here we introduce the design of Dapper, Google's production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries. The main goal of this paper is to report on our experience building, deploying and using the system for over two years, since Dapper's foremost measure of success has been its usefulness to developer and operations teams. Dapper began as a self-contained tracing tool but evolved into a monitoring platform which has enabled the creation of many different tools, some of which were not anticipated by its designers. We describe a few of the analysis tools that have been built using Dapper, share statistics about its usage within Google, present some example use cases, and discuss lessons learned so far.},
  file = {D\:\\GDrive\\zotero\\Sigelman\\sigelman_2010_google_technical_report_dapper_dapper,_a_large-scale_distributed_systems.pdf}
}

@article{sikeridisPostQuantumAuthenticationTLS2020,
  title = {Post-{{Quantum Authentication}} in {{TLS}} 1.3: {{A Performance Study}}},
  author = {Sikeridis, Dimitrios and Kampanakis, Panos and Devetsikiotis, Michael},
  year = {2020},
  doi = {10.14722/ndss.2020.24203},
  abstract = {The potential development of large-scale quantum computers is raising concerns among IT and security research professionals due to their ability to solve (elliptic curve) discrete logarithm and integer factorization problems in polynomial time. All currently used public key algorithms would be deemed insecure in a post-quantum (PQ) setting. In response, the National Institute of Standards and Technology (NIST) has initiated a process to standardize quantum-resistant crypto algorithms, focusing primarily on their security guarantees. Since PQ algorithms present significant differences over classical ones, their overall evaluation should not be performed out-of-context. This work presents a detailed performance evaluation of the NIST signature algorithm candidates and investigates the imposed latency on TLS 1.3 connection establishment under realistic network conditions. In addition, we investigate their impact on TLS session throughput and analyze the trade-off between lengthy PQ signatures and computationally heavy PQ cryptographic operations. Our results demonstrate that the adoption of at least two PQ signature algorithms would be viable with little additional overhead over current signature algorithms. Also, we argue that many NIST PQ candidates can effectively be used for less time-sensitive applications, and provide an in-depth discussion on the integration of PQ authentication in encrypted tunneling protocols, along with the related challenges, improvements, and alternatives. Finally, we propose and evaluate the combination of different PQ signature algorithms across the same certificate chain in TLS. Results show a reduction of the TLS handshake time and a significant increase of a server's TLS tunnel connection rate over using a single PQ signature scheme.},
  file = {D\:\\GDrive\\zotero\\Sikeridis\\sikeridis_2020_post-quantum_authentication_in_tls_1.pdf},
  isbn = {1891562614},
  number = {February}
}

@article{silverMasteringChessShogi2017,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  pages = {1--19},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  file = {D\:\\GDrive\\zotero\\Silver\\silver_2017_mastering_chess_and_shogi_by_self-play_with_a_general_reinforcement_learning.pdf}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  volume = {529},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  file = {D\:\\GDrive\\zotero\\Silver\\silver_2016_mastering_the_game_of_go_with_deep_neural_networks_and_tree_search.pdf},
  journal = {Nature},
  number = {7587},
  pmid = {26819042}
}

@article{simonWhatYouGet2018,
  title = {What {{You Get}} Is {{What You C}}: {{Controlling Side Effects}} in {{Mainstream C Compilers}}},
  author = {Simon, Laurent and Chisnall, David and Anderson, Ross},
  year = {2018},
  pages = {1--15},
  doi = {10.1109/EuroSP.2018.00009},
  abstract = {Security engineers have been fighting with C compilers for years. A careful programmer would test for null pointer dereferencing or division by zero; but the compiler would fail to understand, and optimize the test away. Modern compilers now have dedicated options to mitigate this. But when a programmer tries to control side effects of code, such as to make a cryptographic algorithm execute in constant time, the problem remains. Programmers devise complex tricks to obscure their intentions, but compiler writers find ever smarter ways to optimize code. A compiler upgrade can suddenly and without warning open a timing channel in previously secure code. This arms race is pointless and has to stop. We argue that we must stop fighting the compiler, and instead make it our ally. As a starting point, we analyze the ways in which compiler optimization breaks implicit properties of crypto code; and add guarantees for two of these properties in Clang/LLVM. Our work explores what is actually involved in controlling side effects on modern CPUs with a standard toolchain. Similar techniques can and should be applied to other security properties; achieving intentions by compiler commands or annotations makes them explicit, so we can reason about them. It is already understood that explicitness is essential for cryptographic protocol security and for compiler performance; it is essential for language security too. We therefore argue that this should be only the first step in a sustained engineering effort.},
  file = {D\:\\GDrive\\zotero\\Simon\\simon_2018_what_you_get_is_what_you_c.pdf},
  isbn = {9781538642276},
  journal = {Proceedings - 3rd IEEE European Symposium on Security and Privacy, EURO S and P 2018},
  keywords = {C,C abstract machine,Clang,compiler optimizations,compilers,constant-time,cryptography,erasing,LLVM,side channels,side effects,stack,zeroing}
}

@techreport{singhalFrameworkAutomatedGeneration,
  title = {A {{Framework}} for {{Automated Generation}} of {{Questions Across Formal Domains}}},
  author = {Singhal, Rahul and Henz, Martin and Goyal, Shubham},
  abstract = {In this work, questions are tasks posed to secondary students to help them understand a subject, or to help educators assess their level of competency in it. Automated question generation is important today as content providers in education try to scale their efforts. In particular , MOOCs need a continuous supply of new questions in order to offer educational content to thousands of students, and to provide a fair assessment process. This paper innovates in three ways; (1) we describe question generation across domains, and, previous efforts as instances of a general framework; (2) we establish first-order logic as a suitable formal tool to describe question scenarios, questions and answers; and (3) we generalize a published question generation method based on logic programming and theorem proving to work across domains. We apply this approach to three domains in high school education-geometry, algebra and mechanics (physics)-and report initial results.},
  file = {D\:\\GDrive\\zotero\\Singhal\\singhal_a_framework_for_automated_generation_of_questions_across_formal_domains.pdf},
  keywords = {automated deduction,axiomatic approach,Constraint Handling Rules (CHR),First-order logic,formal domains,pattern matching}
}

@article{singhJupiterRising2015,
  title = {Jupiter {{Rising}}},
  author = {Singh, Arjun and Ong, Joon and Agarwal, Amit and Anderson, Glen and Armistead, Ashby and Bannon, Roy and Boving, Seb and Desai, Gaurav and Felderman, Bob and Germano, Paulie and Kanagala, Anand and Provost, Jeff and Simmons, Jason and Tanda, Eiichi and Wanderer, Jim and H{\"o}lzle, Urs and Stuart, Stephen and Vahdat, Amin},
  year = {2015},
  volume = {45},
  pages = {183--197},
  issn = {0146-4833},
  doi = {10.1145/2829988.2787508},
  abstract = {We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet, scaling in capacity by 100x over ten years to more than 1Pbps of bisection bandwidth.},
  isbn = {9781450335423},
  journal = {ACM SIGCOMM Computer Communication Review},
  keywords = {centralized control and management,clos topology,datacenter networks,merchant silicon},
  number = {4}
}

@inproceedings{singlaInternetSpeedLight2014,
  title = {The Internet at the Speed of Light},
  booktitle = {Proceedings of the 13th {{ACM Workshop}} on {{Hot Topics}} in {{Networks}}, {{HotNets}} 2014},
  author = {Singla, Ankit and Chandrasekaran, Balakrishnan and Brighten Godfrey, P. and Maggs, Bruce},
  year = {2014},
  month = oct,
  publisher = {{Association for Computing Machinery, Inc}},
  doi = {10.1145/2670518.2673876},
  abstract = {For many Internet services, reducing latency improves the user experience and increases revenue for the service provider. While in principle latencies could nearly match the speed of light, we find that infrastructural inefficiencies and proto- col overheads cause today's Internet to be much slower than this bound: typically by more than one, and often, by more than two orders of magnitude. Bridging this large gap would not only add value to today's Internet applications, but could also open the door to exciting new applications. Thus, we propose a grand challenge for the networking research com- munity: a speed-of-light Internet. To inform this research agenda, we investigate the causes of latency inflation in the Internet across the network stack. We also discuss a few broad avenues for latency improvement.},
  file = {D\:\\GDrive\\zotero\\Singla\\singla_2014_the_internet_at_the_speed_of_light.pdf},
  isbn = {978-1-4503-3256-9}
}

@techreport{singlaSpeedLightInternet,
  title = {Towards a {{Speed}} of {{Light Internet}}},
  author = {Singla, Ankit and Chandrasekaran, Balakrishnan and Brighten Godfrey, P and Maggs, Bruce},
  abstract = {In principle, a network can transfer data at nearly the speed of light. Today's Internet, however, is much slower: our measurements show that latencies are typically more than one, and often more than two orders of magnitude larger than the lower bound implied by the speed of light. Closing this gap would not only add value to today's Internet applications, but might also open the door to exciting new applications. Thus, we propose a grand challenge for the networking research community: building a speed-of-light Internet. Towards addressing this goal, we begin by investigating the causes of latency inflation in the Internet across the network stack. Our analysis reveals that while protocol overheads, which have dominated the community's attention , are indeed important, infrastructural inefficiencies are a significant and under-explored problem. Thus, we propose a radical, yet surprisingly low-cost approach to mitigating latency inflation at the lowest layers and building a nearly speed-of-light Internet infrastructure.},
  file = {D\:\\GDrive\\zotero\\Singla\\singla_towards_a_speed_of_light_internet.pdf}
}

@techreport{sirerEludingCarnivoresFile2004,
  title = {Eluding {{Carnivores}}: {{File Sharing}} with {{Strong Anonymity}}},
  author = {Sirer, Emin G{\"u}n and Goel, Sharad and Robson, Mark and Do{\textasciicaron}gan Engin, Do{\textasciicaron}gan},
  year = {2004},
  abstract = {Anonymity is increasingly important for networked applications amidst concerns over censorship and privacy. This paper outlines the design of Herbi-voreFS, a scalable and efficient file sharing system that provides strong anonymity. HerbivoreFS provides computational guarantees that even adversaries able to monitor all network traffic cannot deduce the identity of a sender or receiver beyond an anonymiz-ing clique of k peers. HerbivoreFS achieves scala-bility by partitioning the global network into smaller anonymizing cliques. Measurements on PlanetLab indicate that the system achieves high anonymous bandwidth when deployed on the Internet.},
  file = {D\:\\GDrive\\zotero\\Sirer\\sirer_2004_eluding_carnivores.pdf}
}

@book{sivaramanNoSilverBullet2013,
  title = {No {{Silver Bullet}}: {{Extending SDN}} to the {{Data Plane}}},
  author = {Sivaraman, Anirudh and Winstein, Keith and Subramanian, Suvinay and Balakrishnan, Hari},
  year = {2013},
  abstract = {The data plane is in a continuous state of flux. Every few months, researchers publish the design of a new high-performance queueing or scheduling scheme that runs inside the network fabric. Many such schemes have been queen for a day, only to be surpassed soon after as methods-or evaluation metrics-evolve. The lesson, in our view: there will never be a conclusive victor to govern queue management and scheduling inside network hardware. We provide quantitative evidence by demonstrating bidirectional cyclic preferences among three popular contemporary AQM and scheduling configurations. We argue that the way forward requires carefully extending Software-Defined Networking to control the fast-path scheduling and queueing behavior of a switch. To this end, we propose adding a small FPGA to switches. We have synthesized , placed, and routed hardware implementations of CoDel and RED. These schemes require only a few thousand FPGA "slices" to run at 10 Gbps or more-a minuscule fraction of current low-end FPGAs-demonstrating the feasibility and economy of our approach.},
  file = {D\:\\GDrive\\zotero\\Sivaraman\\sivaraman_2013_no_silver_bullet.pdf},
  isbn = {978-1-4503-2596-7}
}

@misc{SlimBinaries1997,
  title = {Slim {{Binaries}}},
  year = {1997},
  abstract = {The traditional path to software portability among various hardware platforms takes a new turn with the use of slim binaries. The power of computers has increased dramatically over the past 20 years. Not only has the performance of processors risen continuously from one generation to the next and from architecture to architecture, but the interval between these performance steps has also been shrinking steadily. Unfortunately, computer users often cannot take immediate advantage of these improvements, as they are working with software optimized for earlier processor generations.},
  file = {D\:\\GDrive\\zotero\\_\\1997_slim_binaries.pdf;C\:\\Users\\Admin\\Zotero\\storage\\PPH5EAHI\\download.html}
}

@techreport{SlimcoinPeertoPeerCryptoCurrency2014,
  title = {Slimcoin {{A Peer}}-to-{{Peer Crypto}}-{{Currency}} with {{Proof}}-of-{{Burn}} "{{Mining}} without {{Powerful Hardware}}"},
  year = {2014},
  abstract = {Slimcoin is a peer-to-peer cryptocurrency derived from PPCoin's and Bitcoin's designs. Proof-of-burn joins with proof-of-stake and proof-of-work to provide block generation and network security. Proof-of-work is used as a mean for generating the initial money supply. As time passes and as the network accumulates a sufficient supply of coins, proof-of-work mining will become less necessary. Therefore, the network will rely more on proof-of-burn and proof-of-stake, the more energy efficient alternatives. Proof-of-burn is based on the idea of burning coins and generating subsequent burn hashes through a method exclusive to burn transactions. The combination of proof-of-burn, proof-of-stake, and proof-of-work strengthens the blockchain's security.},
  file = {D\:\\GDrive\\zotero\\undefined\\2014_slimcoin_a_peer-to-peer_crypto-currency_with_proof-of-burn_mining_without.pdf}
}

@phdthesis{slipetskyyrostyslavSecurityIssuesOpenStack,
  title = {Security {{Issues}} in {{OpenStack}}},
  author = {Slipetskyy, Rostyslav},
  file = {D\:\\GDrive\\zotero\\Slipetskyy, Rostyslav\\slipetskyy,_rostyslav_security_issues_in_openstack.pdf}
}

@article{smithLetRevokeScalable2020,
  title = {Let's {{Revoke}}: {{Scalable Global Certificate Revocation}}},
  author = {Smith, Trevor and Dickinson, Luke and Seamons, Kent},
  year = {2020},
  doi = {10.14722/ndss.2020.24084},
  abstract = {Current revocation strategies have numerous issues that prevent their widespread adoption and use, including scalability, privacy, and new infrastructure requirements. Consequently, revocation is often ignored, leaving clients vulnerable to man-in-the-middle attacks. This paper presents Let's Revoke, a scalable global revocation strategy that addresses the concerns of current revocation checking. Let's Revoke introduces a new unique identifier to each certificate that serves as an index to a dynamically-sized bit vector containing revocation status information. The bit vector approach enables significantly more efficient revocation checking for both clients and certificate authorities. We compare Let's Revoke to existing revocation schemes and show that it requires less storage and network bandwidth than other systems, including those that cover only a fraction of the global certificate space. We further demonstrate through simulations that Let's Revoke scales linearly up to ten billion certificates, even during mass revocation events.},
  file = {D\:\\GDrive\\zotero\\Smith\\smith_2020_let's_revoke.pdf},
  isbn = {1891562614}
}

@article{smithWithdrawingBGPReRouting2020,
  title = {Withdrawing the {{BGP Re}}-{{Routing Curtain}}: {{Understanding}} the {{Security Impact}} of {{BGP Poisoning}} through {{Real}}-{{World Measurements}}},
  author = {Smith, Jared M. and Birkeland, Kyle and McDaniel, Tyler and Schuchard, Max},
  year = {2020},
  doi = {10.14722/ndss.2020.24240},
  abstract = {The security of the Internet's routing infrastructure has underpinned much of the past two decades of distributed systems security research. However, the converse is increasingly true. Routing and path decisions are now important for the security properties of systems built on top of the Internet. In particular, BGP poisoning leverages the de facto routing protocol between Autonomous Systems (ASes) to maneuver the return paths of upstream networks onto previously unusable, new paths. These new paths can be used to avoid congestion, censors, geo-political boundaries, or any feature of the topology which can be expressed at an AS-level. Given the increase in BGP poisoning usage as a security primitive, we set out to evaluate poisoning feasibility in practice beyond simulation. To that end, using an Internet-scale measurement infrastructure, we capture and analyze over 1,400 instances of BGP poisoning across thousands of ASes as a mechanism to maneuver return paths of traffic. We analyze in detail the performance of steering paths, the graph-theoretic aspects of available paths, and re-evaluate simulated systems with this data. We find that the real-world evidence does not completely support the findings from simulated systems published in the literature. We also analyze filtering of BGP poisoning across types of ASes and ISP working groups. We explore the connectivity concerns when poisoning by reproducing a decade old experiment to uncover the current state of an Internet triple the size. We build predictive models for understanding an ASes' vulnerability to poisoning. Finally, an exhaustive measurement of an upper bound on the maximum path length of the Internet is presented, detailing how security research should react to ASes leveraging poisoned long paths. In total, our results and analysis expose the real-world impact of BGP poisoning on past and future security research.},
  file = {D\:\\GDrive\\zotero\\Smith\\smith_2020_withdrawing_the_bgp_re-routing_curtain.pdf},
  isbn = {1891562614},
  number = {February}
}

@article{snowJustInTimeCodeReuse2013,
  title = {Just-{{In}}-{{Time Code Reuse}}: {{On}} the {{Effectiveness}} of {{Fine}}-{{Grained Address Space Layout Randomization}}},
  author = {Snow, Kevin Z and Monrose, Fabian and Davi, Lucas and Dmitrienko, Alexandra and Liebchen, Christopher and Sadeghi, Ahmad-Reza},
  year = {2013},
  doi = {10.1109/SP.2013.45},
  abstract = {Fine-grained address space layout randomization (ASLR) has recently been proposed as a method of efficiently mitigating runtime attacks. In this paper, we introduce the design and implementation of a framework based on a novel attack strategy, dubbed just-in-time code reuse, that undermines the benefits of fine-grained ASLR. Specifically, we derail the assumptions embodied in fine-grained ASLR by exploiting the ability to repeatedly abuse a memory disclosure to map an application's memory layout on-the-fly, dynamically discover API functions and gadgets, and JIT-compile a target program using those gadgets-all within a script environment at the time an exploit is launched. We demonstrate the power of our framework by using it in conjunction with a real-world exploit against Internet Explorer, and also provide extensive evaluations that demonstrate the practicality of just-in-time code reuse attacks. Our findings suggest that fine-grained ASLR may not be as promising as first thought.},
  file = {D\:\\GDrive\\zotero\\Snow\\snow_2013_just-in-time_code_reuse.pdf}
}

@techreport{sompolinskySecureHighRateTransaction,
  title = {Secure {{High}}-{{Rate Transaction Processing}} in {{Bitcoin}} (Full Version)},
  author = {Sompolinsky, Yonatan and Zohar, Aviv},
  abstract = {Bitcoin is a disruptive new crypto-currency based on a decentralized open-source protocol which has been gradually gaining momentum. Perhaps the most important question that will affect Bitcoin's success, is whether or not it will be able to scale to support the high volume of transactions required from a global currency system. We investigate the implications of having a higher transaction throughput on Bitcoin's security against double-spend attacks. We show that at high throughput, substantially weaker attackers are able to reverse payments they have made, even well after they were considered accepted by recipients. We address this security concern through the GHOST rule, a modification to the way Bitcoin nodes construct and reorganize the block chain, Bitcoin's core distributed data-structure. GHOST has been adopted and a variant of it has been implemented as part of the Ethereum project, a second generation distributed applications platform.},
  file = {D\:\\GDrive\\zotero\\Sompolinsky\\sompolinsky_secure_high-rate_transaction_processing_in_bitcoin_(full_version).pdf}
}

@techreport{sonCovertTimingChannel,
  title = {Covert {{Timing Channel Analysis}} of {{Rate Monotonic Real}}-{{Time Scheduling Algorithm}} in {{MLS}} Systems},
  author = {Son, Joon and {Alves-Foss}, Jim},
  abstract = {The modern digital battlesphere requires the development and deployment of multi-level secure computing systems and networks. A portion of these systems will necessarily be operating under real-time processing constraints. High assurance systems processing national security information must be analyzed for possible information leakages, including covert channels. In this paper we provide a mathematical framework for examining the impact the rate-monotonic real-time scheduling algorithm has on covert timing channels. We prove that in some system configurations , it will not be possible to completely close the covert channel due to the rate-monotonic timing constraints. In addition, we propose a simple method to formulate a security metric to compare covert channels in terms of the relative amount of possible information leakage.},
  file = {D\:\\GDrive\\zotero\\Son\\son_covert_timing_channel_analysis_of_rate_monotonic_real-time_scheduling_algorithm.pdf}
}

@article{songSoKSanitizingSecurity2018,
  title = {{{SoK}}: {{Sanitizing}} for {{Security}}},
  shorttitle = {{{SoK}}},
  author = {Song, Dokyung and Lettner, Julian and Rajasekaran, Prabhu and Na, Yeoul and Volckaert, Stijn and Larsen, Per and Franz, Michael},
  year = {2018},
  month = jun,
  abstract = {The C and C++ programming languages are notoriously insecure yet remain indispensable. Developers therefore resort to a multi-pronged approach to find security issues before adversaries. These include manual, static, and dynamic program analysis. Dynamic bug finding tools --- henceforth "sanitizers" --- can find bugs that elude other types of analysis because they observe the actual execution of a program, and can therefore directly observe incorrect program behavior as it happens. A vast number of sanitizers have been prototyped by academics and refined by practitioners. We provide a systematic overview of sanitizers with an emphasis on their role in finding security issues. Specifically, we taxonomize the available tools and the security vulnerabilities they cover, describe their performance and compatibility properties, and highlight various trade-offs.},
  archiveprefix = {arXiv},
  eprint = {1806.04355},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Song et al\\song_et_al_2018_sok.pdf;C\:\\Users\\Admin\\Zotero\\storage\\MPYN39DQ\\1806.html},
  journal = {arXiv:1806.04355 [cs]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages},
  primaryclass = {cs}
}

@techreport{songTimingAnalysisKeystrokes,
  title = {Timing {{Analysis}} of {{Keystrokes}} and {{Timing Attacks}} on {{SSH}}},
  author = {Song, Dawn Xiaodong and Wagner, David and Tian, Xuqing},
  abstract = {SSH is designed to provide a secure channel between two hosts. Despite the encryption and authentication mechanisms it uses, SSH has two weakness: First, the transmitted packets are padded only to an eight-byte boundary (if a block cipher is in use), which reveals the approximate size of the original data. Second, in interactive mode, every individual keystroke that a user types is sent to the remote machine in a separate IP packet immediately after the key is pressed, which leaks the inter-keystroke timing information of users' typing. In this paper, we show how these seemingly minor weaknesses result in serious security risks. First we show that even very simple statistical techniques suffice to reveal sensitive information such as the length of users' passwords or even root passwords. More importantly, we further show that by using more advanced statistical techniques on timing information collected from the network, the eavesdropper can learn significant information about what users type in SSH sessions. In particular, we perform a statistical study of users' typing patterns and show that these patterns reveal information about the keys typed. By developing a Hidden Markov Model and our key sequence prediction algorithm, we can predict key sequences from the inter-keystroke timings. We further develop an attacker system , Herbivore , which tries to learn users' passwords by monitoring SSH sessions. By collecting timing information on the network, Herbivore can speed up exhaustive search for passwords by a factor of 50. We also propose some countermeasures. In general our results apply not only to SSH, but also to a general class of protocols for encrypting interactive traffic. We show that timing leaks open a new set of security risks, and hence caution must be taken when designing this type of protocol. \textexclamdown},
  file = {D\:\\GDrive\\zotero\\Song\\song_timing_analysis_of_keystrokes_and_timing_attacks_on_ssh.pdf},
  keywords = {ss}
}

@article{songweiConcolicTestingWoHuoYongsitaShiZhuangbesunoHuiGuitesuto2015,
  title = {Concolic {{Testing}} を活用した実装ベースの回帰テスト\textasciitilde{} 人手によるテストケース設計の全廃\textasciitilde},
  author = {松尾, 谷徹 and 増田, 聡 and 湯本, 剛 and 植月, 啓次 and 津田, 和彦},
  year = {2015},
  volume = {2014},
  pages = {47--56},
  file = {D\:\\MEGA\\zotero\\松尾\\松尾_2015_concolic_testing_を活用した実装ベースの回帰テスト~_人手によるテストケース設計の全廃~.pdf},
  isbn = {9781595938824},
  journal = {ソフトウェア・シンポジウム 2015in 和歌山 SEA: ソフトウェア技術者協会},
  keywords = {checking,concolic testing,data structure testing,explicit path model-,random testing,testing c pro-,unit testing},
  number = {1}
}

@article{sorinPrimerMemoryConsistency2011,
  title = {A Primer on Memory Consistency and Cache Coherence},
  author = {Sorin, Daniel J. and Hill, Mark D. and Wood, David A.},
  year = {2011},
  volume = {16},
  pages = {1--212},
  issn = {19353235},
  doi = {10.2200/S00346ED1V01Y201104CAC016},
  abstract = {Many modern computer systems and most multicore chips (chip multiprocessors) support shared memory in hardware. In a shared memory system, each of the processor cores may read and write to a single shared address space. For a shared memory machine, the memory consistency model defines the architecturally visible behavior of its memory system. Consistency definitions provide rules about loads and stores (or memory reads and writes) and how they act upon memory. As part of supporting a memory consistency model, many machines also provide cache coherence protocols that ensure that multiple cached copies of data are kept up-to-date. The goal of this primer is to provide readers with a basic understanding of consistency and coherence. This understanding includes both the issues that must be solved as well as a variety of solutions. We present both highlevel concepts as well as specific, concrete examples from real-world systems. Copyright \textcopyright{} 2010 by Morgan \& Claypool.},
  file = {D\:\\GDrive\\zotero\\Sorin\\sorin_2011_a_primer_on_memory_consistency_and_cache_coherence.pdf},
  isbn = {9781608455645},
  journal = {Synthesis Lectures on Computer Architecture},
  keywords = {cache coherence,computer architecture,memory consistency,memory systems,multicore processor,multiprocessor,shared memory},
  number = {May 2014}
}

@article{Sovran2011Transactional,
  title = {Sovran et al. - 2011 - {{Transactional}} Storage for Geo-Replicated Systems},
  file = {D\:\\GDrive\\zotero\\undefined\\sovran_et_al.pdf}
}

@techreport{spacefoxDNSSpoofingTechniques,
  title = {{{DNS Spoofing}} Techniques},
  author = {Spacefox, By},
  file = {D\:\\GDrive\\zotero\\Spacefox\\spacefox_dns_spoofing_techniques.pdf},
  isbn = {212.153.32.65}
}

@article{Spanningtree,
  title = {Spanning-Tree},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\A522D6TR\\spanning-tree.pdf}
}

@techreport{spencerFlaskSecurityArchitecture,
  title = {The {{Flask Security Architecture}}: {{System Support}} for {{Diverse Security Policies}}},
  author = {Spencer, Ray and Smalley, Stephen and Loscocco, Peter and Hibler, Mike and Andersen, David and Lepreau, Jay},
  abstract = {Operating systems must be flexible in their support for security policies, providing sufficient mechanisms for supporting the wide variety of real-world security policies. Such flexibility requires controlling the propagation of access rights, enforcing fine-grained access rights and supporting the revocation of previously granted access rights. Previous systems are lacking in at least one of these areas. In this paper we present an operating system security architecture that solves these problems. Control over propagation is provided by ensuring that the security policy is consulted for every security decision. This control is achieved without significant performance degradation through the use of a security decision caching mechanism that ensures a consistent view of policy decisions. Both fine-grained access rights and revocation support are provided by mechanisms that are directly integrated into the service-providing components of the system. The architecture is described through its prototype implementation in the Flask microkernel-based operating system, and the policy flexibility of the prototype is evaluated. We present initial evidence that the architecture's impact on both performance and code complexity is modest. Moreover, our architecture is applicable to many other types of operating systems and environments.},
  file = {D\:\\GDrive\\zotero\\Spencer\\spencer_the_flask_security_architecture.pdf}
}

@article{spinellisWhyComputingStudents2021,
  title = {Why Computing Students Should Contribute to Open Source Software Projects},
  author = {Spinellis, Diomidis},
  year = {2021},
  month = jul,
  volume = {64},
  pages = {36--38},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3437254},
  abstract = {Acquiring developer-prized practical skills, knowledge, and experiences.},
  file = {D\:\\GDrive\\zotero\\Spinellis\\spinellis_2021_why_computing_students_should_contribute_to_open_source_software_projects.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {7}
}

@techreport{spofforInternetWormProgram1988,
  title = {The {{Internet Worm Program}} : {{An Analysis The Internet Worm Program}} : {{An Analysi}} s},
  author = {Spoffor, Eugene H and Spafford, Eugene H},
  year = {1988},
  abstract = {On the evening of 2 November 1988, someone infected the Internet with a worm program. That program exploited flaws in utility programs in system s based on BSD-derived versions of UMx. The flaws allowed the program t o break into those machines and copy itself, thus infecting those systems. Thi s program eventually spread to thousands of machines, and disrupted norma l activities and Internet connectivity for many days. This report gives a detailed description of the components of the wor m program-data and functions. It is based on study of two completely independent reverse-compilations of the worm and a version disassembled to VAX assembly language. Almost no source code is given in the paper because o f current concerns about the state of the "immune system" of Internet hosts, bu t the description should be detailed enough to allow the reader to understand th e behavior of the program. The paper contains a review of the security flaws exploited by the worm program, and gives some recommendations on how to eliminate or mitigat e their future use. The report also includes an analysis of the coding style and methods used by the author(s) of the worm, and draws some conclusions abou t his abilities and intent .},
  file = {D\:\\GDrive\\zotero\\Spoffor\\spoffor_1988_the_internet_worm_program.pdf}
}

@techreport{sprankelTechnicalBasisDigital2013,
  title = {Technical {{Basis}} of {{Digital Currencies}}},
  author = {Sprankel, Simon},
  year = {2013},
  file = {D\:\\GDrive\\zotero\\Sprankel\\sprankel_2013_technical_basis_of_digital_currencies.pdf}
}

@article{SSABackTranslationFaster2011,
  title = {{{SSA Back}}-{{Translation}} : {{Faster Results}} with {{Edge Splitting}} and {{Post Optimization}}},
  year = {2011},
  file = {D\:\\GDrive\\zotero\\undefined\\ssa_back-translation.pdf},
  journal = {Technology}
}

@techreport{SSLTLSProtocol,
  title = {{{SSL}}/{{TLS Protocol Enablement}} for {{Key Recovery}}},
  abstract = {The TLS handshake is how cryptographic attributes are negotiated for a TLS session and connections for a TLS session. There are a series of TLS handshake messages that are exchanged during this handshake.A brief explanation of the messages pertinent to KR follows: 1. Client Hello-this is the message which initiates a TLS handshake for a connection. It contains a list of cipher specs that the client can support.A cipher spec defines the encryption algorithm with the session key size and Message Authentication Code (MAC) algorithm. Also contained in the client hello is a session identifier. This is used when attempting to reuse cryptographic attributes negotiated during a previous connection. If a new session is desired the session identifier would not be sent in this message. 2. Server Hello-the server compares the list of cipher specs sent by the client with the list that the server supports and chooses a single cipher spec to be used for the connection. The server tells the client by placing selected cipher in the Server Hello message. If the server 'remembered' the session identifier contained in the Client Hello the server would return it back to the client in the Server Hello message. 3. Certificate Request-optionally sent by the serv-er to request that a client send a certificate to be used for client authentication by the server. 4. Certificate-contains the certificate used for peer authentication.This is optionally sent by the client when a server requests it for client authen-tication. For the server a Certificate handshake message is always sent when a new TLS session is being created. 5. Client Key Exchange-sent by the client when creating a new TLS session. The content of this message is the pre-master secret encrypted with the public key from the server's certificate. This message is not sent when a TLS session is being resumed. 6. Server Key Exchange-used for sending a temporary public key to the client to be used to encrypt the premaster secret. This is used when the server's public keysize exceeds 512 bit and an export cipher spec is negotiated for the TLS connection. 7. Certificate Verify-used during the verification process of the client's certificate.This is optionally sent when client authentication is requested by the server. 8. Client/Server Finished-this is the first message that uses the newly negotiated cipher specification. It is the last handshake message sent by both the client and the server. It contains an encrypted hash of all of the handshake messages. The purpose of this message is not only to test the selected cipher spec but to also guarantee that none of the handshake messages have been tampered with. Both the client and server create the hash of the handshake messages and compare to the peer hash of the handshake messages. 3.2 Scenario 1: Sending the KRB Within the TLS Handshake This method of sending KR data to a client requires modification to the TLS handshake message flow. This document covers modification to TLS using both RSA and Diffie-Hellman (DH) key exchange methods. Currently there are 9 RSA and 17 DH cipher specs. Of these only 9 can be exported outside of the United States due to export laws. It may also be desirable for weaker cipher specs to make use of KR.These new cipher specs are defined by a new family of cipher specs similar to the existing RSA and DH with the addition of KR data. If the client and server negotiated one of the KR cipher specs a KRB would be sent within the TLS handshake. The KRB would be sent via a new TLS record with a content type for KR data. The new record type would have the value of 0xE9. The KRB record would be sent before the server/client would send the Finished message. A KRB record would contain the bulk read/write keys and the TLS session identifier.},
  file = {D\:\\GDrive\\zotero\\undefined\\ssl-tls_protocol_enablement_for_key_recovery.pdf}
}

@article{staatsParallelSymbolicExecution2010,
  title = {Parallel Symbolic Execution for Structural Test Generation},
  author = {Staats, Matt and P{\v a}s{\v a}reanu, Corina},
  year = {2010},
  pages = {183--193},
  doi = {10.1145/1831708.1831732},
  abstract = {Symbolic execution is a popular technique for automatically generating test cases achieving high structural coverage. Symbolic execution suffers from scalability issues since the number of symbolic paths that need to be explored is very large (or even infinite) for most realistic programs. To address this problem, we propose a technique, Simple Static Partitioning, for parallelizing symbolic execution. The technique uses a set of pre-conditions to partition the symbolic execution tree, allowing us to effectively distribute symbolic execution and decrease the time needed to explore the symbolic execution tree. The proposed technique requires little communication between parallel instances and is designed to work with a variety of architectures, ranging from fast multi-core machines to cloud or grid computing environments. Weimplement our technique in the Java PathFinder verification tool-set and evaluate it on six case studies with respect to the performance improvement when exploring a finite symbolic execution tree and performing automatic test generation. We demonstrate speedup in both the analysis time over finite symbolic execution trees and in the time required to generate tests relative to sequential execution, with a maximum analysis time speedup of 90x observed using 128 workers and a maximum test generation speedup of 70x observed using 64 workers. \textcopyright{} 2010 ACM.},
  file = {D\:\\GDrive\\zotero\\Staats\\staats_2010_parallel_symbolic_execution_for_structural_test_generation.pdf},
  isbn = {9781605588230},
  journal = {ISSTA'10 - Proceedings of the 2010 International Symposium on Software Testing and Analysis},
  keywords = {Java Pathfinder,Parallel,Symbolic execution}
}

@techreport{StackSmashingRecent2004,
  title = {Beyond {{Stack Smashing}}: {{Recent Advances}} in {{Exploiting Buffer Overruns}}},
  year = {2004},
  file = {D\:\\GDrive\\zotero\\undefined\\2004_beyond_stack_smashing.pdf}
}

@article{staicuLeakyImagesTargeted2019,
  title = {Leaky Images: {{Targeted}} Privacy Attacks in the Web},
  author = {Staicu, Cristian Alexandru and Pradel, Michael},
  year = {2019},
  pages = {923--939},
  abstract = {Sharing files with specific users is a popular service provided by various widely used websites, e.g., Facebook, Twitter, Google, and Dropbox. A common way to ensure that a shared file can only be accessed by a specific user is to authenticate the user upon a request for the file. This paper shows a novel way of abusing shared image files for targeted privacy attacks. In our attack, called leaky images, an image shared with a particular user reveals whether the user is visiting a specific website. The basic idea is simple yet effective: an attacker-controlled website requests a privately shared image, which will succeed only for the targeted user whose browser is logged into the website through which the image was shared. In addition to targeted privacy attacks aimed at single users, we discuss variants of the attack that allow an attacker to track a group of users and to link user identities across different sites. Leaky images require neither JavaScript nor CSS, exposing even privacy-aware users, who disable scripts in their browser, to the leak. Studying the most popular websites shows that the privacy leak affects at least eight of the 30 most popular websites that allow sharing of images between users, including the three most popular of all sites. We disclosed the problem to the affected sites, and most of them have been fixing the privacy leak in reaction to our reports. In particular, the two most popular affected sites, Facebook and Twitter, have already fixed the leaky images problem. To avoid leaky images, we discuss potential mitigation techniques that address the problem at the level of the browser and of the image sharing website.},
  file = {D\:\\GDrive\\zotero\\Staicu\\staicu_2019_leaky_images.pdf},
  isbn = {9781939133069},
  journal = {Proceedings of the 28th USENIX Security Symposium}
}

@techreport{STALKINGWILYHACKER,
  title = {{{STALKING THE WILY HACKER}}},
  abstract = {An astronomer-turned-sleuth truces a German trespasser on our n\&fury networks, who slipped through operating system security holes and browsed through sensitive databases. Was if espionage? CLIFFORD STOLL In August 1986 a persistent computer intruder attacked the Lawrence Berkeley Laboratory (LBL). Instead of trying to keep the intruder out, we took the novel approach of allowing him access while we printed out his activities and traced him to his source. This trace back was harder than we expected, requiring nearly a year of work and the cooperation of many organizations. This article tells the story of the break-ins and the trace, and sums up what we learned. We approached the problem as a short, scientific exercise in discovery, intending to determine who was breaking into our system and document the exploited weaknesses. It became apparent, however, that rather than innocuously playing around, the intruder was using our computer as a hub to reach many others. His main interest was in computers operated by the military and by defense contractors. Targets and keywords suggested that he was attempting espionage by remotely entering sensitive computers and stealing data; at least he exhibited an unusual interest in a few, spe-cifical!y military topics. Although most attacked computers were at military and defense contractor sites, some were at universities and research organizations. Over the next 10 months, we watched this individual attack about 450 computers and successfully enter more than 30.},
  file = {D\:\\GDrive\\zotero\\undefined\\stalking_the_wily_hacker.pdf}
}

@techreport{stanifordHow0wnInternet,
  title = {How to 0wn the {{Internet}} in {{Your Spare Time}}},
  author = {Staniford, Stuart and Paxson, Vern and Weaver, Nicholas},
  abstract = {The ability of attackers to rapidly gain control of vast numbers of Internet hosts poses an immense risk to the overall security of the Internet. Once subverted, these hosts can not only be used to launch massive denial of service floods, but also to steal or corrupt great quantities of sensitive information, and confuse and disrupt use of the network in more subtle ways. We present an analysis of the magnitude of the threat. We begin with a mathematical model derived from empirical data of the spread of Code Red I in July, 2001. We discuss techniques subsequently employed for achieving greater virulence by Code Red II and Nimda. In this context, we develop and evaluate several new, highly virulent possible techniques: hit-list scanning (which creates a Warhol worm), permutation scanning (which enables self-coordinating scanning), and use of Internet-sized hit-lists (which creates a flash worm). We then turn to the to the threat of surreptitious worms that spread more slowly but in a much harder to detect "contagion" fashion. We demonstrate that such a worm today could arguably subvert upwards of 10,000,000 In-ternet hosts. We also consider robust mechanisms by which attackers can control and update deployed worms. In conclusion, we argue for the pressing need to develop a "Center for Disease Control" analog for virus-and worm-based threats to national cybersecurity, and sketch some of the components that would go into such a Center.},
  file = {D\:\\GDrive\\zotero\\Staniford\\staniford_how_to_0wn_the_internet_in_your_spare_time.pdf}
}

@phdthesis{stanleyLanguageExtensionsPerformanceOriented2003,
  title = {Language {{Extensions}} for {{Performance}}-{{Oriented Programming}}},
  author = {STANLEY, JOEL},
  year = {2003},
  abstract = {Modern software development practices lack portable, precise and powerful mechanisms for describing performance properties of application code. Traditional approaches rely almost solely on performance instrumentation libraries, which have significant drawbacks in certain types (e.g., adaptive) of applications, present the end user with integration challenges and complex APIs, and often pose portability problems of their own. This thesis proposes a small set of C-like language extensions that facilitate the treatment of performance properties as intrinsic properties of application code. The proposed language extensions allow the application developer to encode performance expectations, gather and aggregate various types of performance information, and more, all at the language level. Furthermore, this thesis demonstrates many novel compiler implementation techniques that make the the presented approach possible with an arbitrary (third-party) compiler, and that minimize performance perturbation by enabling compiler optimizations that are commonly inhibited by traditional approaches. This thesis describes the fundamental contribution of language-level performance properties, the language extensions themselves, the implementation of the compilation and runtime system, together with a standard library of widely-used metrics, and demonstrates the role that the extensions and compilation system can play in describing the performance-oriented aspects of both a production-quality raytracing application and a long-running adaptive server code.},
  file = {D\:\\GDrive\\zotero\\STANLEY\\stanley_2003_language_extensions_for_performance-oriented_programming.pdf;D\:\\MEGA\\zotero\\Stanley\\stanley_language_extensions_for_performance-oriented_programming.pdf}
}

@techreport{starkCasePrefetchingPrevalidating,
  title = {The {{Case}} for {{Prefetching}} and {{Prevalidating TLS Server Certificates}}},
  author = {Stark, Emily and Huang, Lin-Shung and Israni, Dinesh and Jackson, Collin and Boneh, Dan},
  abstract = {A key bottleneck in a full TLS handshake is the need to fetch and validate the server certificate before establishing a secure connection. We propose a mechanism by which a browser can prefetch and prevalidate server certificates so that by the time the user clicks on an HTTPS link, the server's certificate is immediately ready to be used. Combining this with a recent proposal called Snap Start reduces the TLS handshake to zero round trips. Prefetching and prevalidating certificates improves web security by making it less costly for websites to enable TLS and by removing time pressure from the certificate validation process. We implemented prefetching and prevalidation and studied the effects of four different prefetching strategies on server performance. Along the way we conducted a study of OCSP, a certificate validation mechanism. This data enabled us to evaluate the effectiveness of prefetching and prevalidating in reducing TLS handshake latency. In some cases we show a factor of four speed-up over a full TLS handshake.},
  file = {D\:\\GDrive\\zotero\\Stark\\stark_the_case_for_prefetching_and_prevalidating_tls_server_certificates.pdf}
}

@techreport{starkREALTIMECHORDRECOGNITION,
  title = {{{REAL}}-{{TIME CHORD RECOGNITION FOR LIVE PERFORMANCE}}},
  author = {Stark, Adam M and Plumbley, Mark D},
  abstract = {This paper describes work aimed at creating an efficient, real-time, robust and high performance chord recognition system for use on a single instrument in a live performance context. An improved chroma calculation method is combined with a classification technique based on masking out expected note positions in the chromagram and minimising the residual energy. We demonstrate that our approach can be used to classify a wide range of chords, in real-time, on a frame by frame basis. We present these analysis techniques as externals for Max/MSP.},
  file = {D\:\\GDrive\\zotero\\Stark\\stark_real-time_chord_recognition_for_live_performance.pdf}
}

@book{StaticSingleAssignment,
  title = {Static {{Single Assignment Book}}},
  file = {D\:\\GDrive\\zotero\\_\\static_single_assignment_book.pdf}
}

@techreport{steinbergerScatterAllocMassivelyParallel,
  title = {{{ScatterAlloc}}: {{Massively Parallel Dynamic Memory Allocation}} for the {{GPU}}},
  author = {Steinberger, Markus and Kenzel, Michael and Kainz, Bernhard and Schmalstieg, Dieter},
  abstract = {In this paper, we analyze the special requirements of a dynamic memory allocator that is designed for massively parallel architec-tures such as Graphics Processing Units (GPUs). We show that traditional strategies, which work well on CPUs, are not well suited for the use on GPUs and present the thorough design of ScatterAl-loc, which can efficiently deal with hundreds of requests in parallel. Our allocator greatly reduces collisions and congestion by scattering memory requests based on hashing. We analyze ScatterAlloc in terms of allocation speed, data access time and fragmentation, and compare it to current state-of-the-art allocators, including the one provided with the NVIDIA CUDA toolkit. Our results show, that ScatterAlloc clearly outperforms these other approaches, yielding speed-ups between 10 to 100.},
  file = {D\:\\GDrive\\zotero\\Steinberger\\steinberger_scatteralloc.pdf},
  keywords = {D42 [Operating Systems]: Storage Management-Allocation / deallocation strategies Keywords dynamic memory allocation,GPU,hashing,massively parallel}
}

@article{stelliosSurveyIotenabledCyberattacks2018,
  title = {A Survey of Iot-Enabled Cyberattacks: {{Assessing}} Attack Paths to Critical Infrastructures and Services},
  author = {Stellios, Ioannis and Kotzanikolaou, Panayiotis and Psarakis, Mihalis and Alcaraz, Cristina and Lopez, Javier},
  year = {2018},
  volume = {20},
  pages = {3453--3495},
  publisher = {{IEEE}},
  issn = {1553877X},
  doi = {10.1109/COMST.2018.2855563},
  abstract = {As the deployment of Internet of Things (IoT) is experiencing an exponential growth, it is no surprise that many recent cyber attacks are IoT-enabled: The attacker initially exploits some vulnerable IoT technology as a first step toward compromising a critical system that is connected, in some way, with the IoT. For some sectors, like industry, smart grids, transportation, and medical services, the significance of such attacks is obvious, since IoT technologies are part of critical back-end systems. However, in sectors where IoT is usually at the end-user side, like smart homes, such attacks can be underestimated, since not all possible attack paths are examined. In this paper, we survey IoT-enabled cyber attacks, found in all application domains since 2010. For each sector, we emphasize on the latest, verified IoT-enabled attacks, based on known real-world incidents and published proof-of-concept attacks. We methodologically analyze representative attacks that demonstrate direct, indirect, and subliminal attack paths against critical targets. Our goal is threefold: 1) to assess IoT-enabled cyber attacks in a risk-like approach, in order to demonstrate their current threat landscape; 2) to identify hidden and subliminal IoT-enabled attack paths against critical infrastructures and services; and 3) to examine mitigation strategies for all application domains.},
  file = {D\:\\GDrive\\zotero\\Stellios\\stellios_2018_a_survey_of_iot-enabled_cyberattacks.pdf},
  journal = {IEEE Communications Surveys and Tutorials},
  keywords = {Critical infrastructures,Cyber attacks,Intelligent transportation systems,Internet of Things,SCADA,Smart grids,Smart home,Smart medical systems},
  number = {4}
}

@book{stepanovNotesProgramming,
  title = {Notes on {{Programming}}},
  author = {Stepanov, Alexander},
  file = {D\:\\GDrive\\zotero\\Stepanov\\stepanov_notes_on_programming.pdf}
}

@article{stephensDrillerAugmentingFuzzing2017,
  title = {Driller: {{Augmenting Fuzzing Through Selective Symbolic Execution}}},
  author = {Stephens, Nick and Grosen, John and Salls, Christopher and Dutcher, Andrew and Wang, Ruoyu and Corbetta, Jacopo and Shoshitaishvili, Yan and Kruegel, Christopher and Vigna, Giovanni},
  year = {2017},
  pages = {21--24},
  doi = {10.14722/ndss.2016.23368},
  abstract = {\textemdash Memory corruption vulnerabilities are an ever-present risk in software, which attackers can exploit to obtain private information or monetary gain. As products with access to sensitive data are becoming more prevalent, the number of potentially exploitable systems is also increasing, resulting in a greater need for automated software vetting tools. DARPA recently funded a competition, with millions of dollars in prize money, to further research of automated vulnerability finding and patching, showing the importance of research in this area. Current techniques for finding potential bugs include static, dynamic, and concolic analysis systems, which each have their own advantages and disadvantages. Systems designed to create inputs which trigger vulnerabilities typically only find shallow bugs and struggle to exercise deeper paths in executables. We present Driller, a hybrid vulnerability excavation tool which leverages fuzzing and selective concolic execution, in a complementary manner, to find deeper bugs. Inexpensive fuzzing is used to exercise compartments of an application, while concolic execution is used to generate inputs which satisfy the complex checks separating the compartments. By combining the strengths of the two techniques, we mitigate their weaknesses, avoiding the path explosion inherent in concolic analysis and the incomplete-ness of fuzzing. Driller uses selective concolic execution to explore only the paths deemed interesting by the instrumented fuzzer and to generate inputs for conditions that the fuzzer could not satisfy. We evaluate Driller on 126 applications released in the qualifying event of the DARPA Cyber Grand Challenge and show its efficacy by identifying the same number of vulnerabilities, in the same time, as the top-scoring team of the qualifying event.},
  file = {D\:\\GDrive\\zotero\\Stephens\\stephens_2017_driller.pdf},
  isbn = {189156241X},
  number = {February}
}

@techreport{stevensFirstCollisionFull2017,
  title = {The First Collision for Full {{SHA}}-1},
  author = {Stevens, Marc and Bursztein, Elie and Karpman, Pierre and Albertini, Ange and Markov, Yarik},
  year = {2017},
  abstract = {SHA-1 is a widely used 1995 NIST cryptographic hash function standard that was officially deprecated by NIST in 2011 due to fundamental security weaknesses demonstrated in various analyses and theoretical attacks. Despite its deprecation, SHA-1 remains widely used in 2017 for document and TLS certificate signatures, and also in many software such as the GIT versioning system for integrity and backup purposes. A key reason behind the reluctance of many industry players to replace SHA-1 with a safer alternative is the fact that finding an actual collision has seemed to be impractical for the past eleven years due to the high complexity and computational cost of the attack. In this paper, we demonstrate that SHA-1 collision attacks have finally become practical by providing the first known instance of a collision. Furthermore, the prefix of the colliding messages was carefully chosen so that they allow an attacker to forge two PDF documents with the same SHA-1 hash yet that display arbitrarily-chosen distinct visual contents. We were able to find this collision by combining many special cryptanalytic techniques in complex ways and improving upon previous work. In total the computational effort spent is equivalent to 2 63.1 SHA-1 compressions and took approximately 6 500 CPU years and 100 GPU years. As a result while the computational power spent on this collision is larger than other public cryptanalytic computations, it is still more than 100 000 times faster than a brute force search.},
  file = {D\:\\GDrive\\zotero\\Stevens et al\\stevens_et_al_2017_the_first_collision_for_full_sha-1.pdf},
  keywords = {collision attack,collision example,cryptanalysis,differential path,hash function}
}

@techreport{stocktongaineseditorOperatingSystemsOptimal1981,
  title = {Operating {{Systems An Optimal Algorithm}} for {{Mutual Exclusion}} in {{Computer Networks}}},
  author = {Stockton Gaines Editor, R and Ricart, Glenn and Agrawala, Ashok K},
  year = {1981},
  abstract = {An algorithm is proposed that creates mutual exclusion in a computer network whose nodes communicate only by messages and do not share memory. The algorithm sends only 2*(N-1) messages, where N is the number of nodes in the network per critical section invocation. This number of messages is at a minimum if parallel, distributed, symmetric control is used; hence, the algorithm is optimal in this respect. The time needed to achieve mutual exclusion is also minimal under some general assumptions. As in Lamport's "bakery algorithm," unbounded sequence numbers are used to provide first-come first-served priority into the critical section. It is shown that the number can be contained in a fixed amount of memory by storing it as the residue of a modulus. The number of messages required to implement the exclusion can be reduced by using sequential node-by-node processing, by using broadcast message techniques, or by sending information through timing channels. The "readers and writers" problem is solved by a simple modification of the algorithm and the modifications necessary to make the algorithm robust are described.},
  file = {D\:\\GDrive\\zotero\\Stockton Gaines Editor\\stockton_gaines_editor_1981_operating_systems_an_optimal_algorithm_for_mutual_exclusion_in_computer_networks.pdf},
  keywords = {433,435,and Phrases: concurrent programming,critical section,distributed algorithm,mutual exclusion,network,synchronization CR Categories: 432}
}

@techreport{stoicaChordScalablePeertopeer2001,
  title = {Chord: {{A Scalable Peer}}-to-Peer {{Lookup Service}} for {{Internet Applications}}},
  author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M Frans and Balakrishnan, Hari},
  year = {2001},
  abstract = {A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.},
  file = {D\:\\GDrive\\zotero\\Stoica\\stoica_2001_chord.pdf},
  keywords = {distributed systems}
}

@article{stoicaFormalVerificationBusiness2016,
  title = {Formal Verification of Business Processes Using Model Checking},
  author = {Stoica, Florin},
  year = {2016},
  pages = {2563--2575},
  abstract = {The solution against the changing business environment is construction of flexible business processes in order to be aligned with actual business needs and requirements. The reliability of a business process can be increased through modelling of the process before the implementation of code, followed by the verification of its correctness. Verification of a business process involves checking whether the process in question behaves as it was designed to behave. Model checking is a technology widely used for the automated system verification and represents a technique for verifying that finite state systems satisfy specifications expressed in the language of temporal logics. Formal verification of business process models aims checking for process correctness and business compliance. This paper presents a suitable approach for automatic verification of a business process using Alternating-time Temporal Logic (ATL). In development of a user-friendly supporting tool, we will use our original ATL model checker.},
  file = {D\:\\GDrive\\zotero\\Stoica\\stoica_2016_formal_verification_of_business_processes_using_model_checking.pdf},
  isbn = {9780986041969},
  journal = {Proceedings of the 27th International Business Information Management Association Conference - Innovation Management and Education Excellence Vision 2020: From Regional Development Sustainability to Global Economic Growth, IBIMA 2016},
  keywords = {ATL model checking,Business process,Formal verification},
  number = {2}
}

@article{stoltingbrodalchrisokasakiOptimalPurelyFunctional1996,
  title = {Optimal {{Purely Functional Priority Queues}}},
  author = {St{\o}lting Brodal Chris Okasaki, Gerth},
  year = {1996},
  issn = {0909-0878},
  file = {D\:\\GDrive\\zotero\\Stølting Brodal Chris Okasaki\\stølting_brodal_chris_okasaki_1996_optimal_purely_functional_priority_queues.pdf}
}

@techreport{stonebrakerOneSizeFits,
  title = {"{{One Size Fits All}}": {{An Idea Whose Time Has Come}} and {{Gone}}},
  author = {Stonebraker, Michael and {\c C}etintemel, U{\u g}ur},
  abstract = {The last 25 years of commercial DBMS development can be summed up in a single phrase: "One size fits all". This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data-warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products.},
  file = {D\:\\GDrive\\zotero\\Stonebraker\\stonebraker_one_size_fits_all.pdf}
}

@article{stoneInformationTheoryTutorial2019,
  title = {Information {{Theory}}: {{A Tutorial Introduction}}},
  shorttitle = {Information {{Theory}}},
  author = {Stone, James V.},
  year = {2019},
  month = jun,
  abstract = {Shannon's mathematical theory of communication defines fundamental limits on how much information can be transmitted between the different components of any man-made or biological system. This paper is an informal but rigorous introduction to the main ideas implicit in Shannon's theory. An annotated reading list is provided for further reading.},
  archiveprefix = {arXiv},
  eprint = {1802.05968},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Stone\\stone_2019_information_theory.pdf;C\:\\Users\\Admin\\Zotero\\storage\\T48FLFS9\\1802.html},
  journal = {arXiv:1802.05968 [cs, math, stat]},
  keywords = {94A05,Computer Science - Information Theory,Statistics - Machine Learning},
  primaryclass = {cs, math, stat}
}

@techreport{stoyanEarlyLISPHistory,
  title = {Early {{LISP History}} (1956-1959)},
  author = {Stoyan, Herbert},
  abstract = {This paper describes the development of LISP from McCarthy's first research in the topic of programming languages for AI until the stage when the LISP1 implementation had developed into a serious program (May 1959). We show the steps that led to LISP and the various proposals for LISP interpreters (between November 1958 and May 1959). The paper contains some correcting details of our book (32).},
  file = {D\:\\GDrive\\zotero\\Stoyan\\stoyan_early_lisp_history_(1956-1959).pdf}
}

@techreport{stracheyHigherOrderSymbolicComputation2000,
  title = {Higher-{{Order}} and {{Symbolic Computation}}},
  author = {Strachey, Christopher},
  year = {2000},
  volume = {13},
  pages = {11--49},
  abstract = {This paper forms the substance of a course of lectures given at the International Summer School in Computer Programming at Copenhagen in August, 1967. The lectures were originally given from notes and the paper was written after the course was finished. In spite of this, and only partly because of the shortage of time, the paper still retains many of the shortcomings of a lecture course. The chief of these are an uncertainty of aim-it is never quite clear what sort of audience there will be for such lectures-and an associated switching from formal to informal modes of presentation which may well be less acceptable in print than it is natural in the lecture room. For these (and other) faults, I apologise to the reader. There are numerous references throughout the course to CPL [1-3]. This is a programming language which has been under development since 1962 at Cambridge and London and Oxford. It has served as a vehicle for research into both programming languages and the design of compilers. Partial implementations exist at Cambridge and London. The language is still evolving so that there is no definitive manual available yet. We hope to reach another resting point in its evolution quite soon and to produce a compiler and reference manuals for this version. The compiler will probably be written in such a way that it is relatively easy to transfer it to another machine, and in the first instance we hope to establish it on three or four machines more or less at the same time. The lack of a precise formulation for CPL should not cause much difficulty in this course, as we are primarily concerned with the ideas and concepts involved rather than with their precise representation in a programming language.},
  file = {D\:\\GDrive\\zotero\\Strachey\\strachey_2000_higher-order_and_symbolic_computation.pdf},
  keywords = {ad hoc polymorphism,binding mechanisms,CPL,foundations of computing,functions as data,L-values,para-meter passing,parametric polymorphism,programming languages,R-values,semantics,type completeness,variable binding}
}

@inproceedings{streibeltExploringEDNSclientsubnetAdopters2013,
  title = {Exploring {{EDNS}}-Client-Subnet Adopters in Your Free Time},
  booktitle = {Proceedings of the {{ACM SIGCOMM Internet Measurement Conference}}, {{IMC}}},
  author = {Streibelt, Florian and B{\"o}ttger, Jan and Chatzis, Nikolaos and Smaragdakis, Georgios and Feldmann, Anja},
  year = {2013},
  pages = {305--311},
  doi = {10.1145/2504730.2504767},
  abstract = {The recently proposedDNSextension, EDNS-Client-Subnet (ECS), has been quickly adopted by major Internet companies such as Google to better assign user requests to their servers and improve end-user experience. In this paper, we show that the adoption of ECS also offers unique, but likely unintended, opportunities to uncover details about these companies' operational practices at almost no cost. A key observation is that ECS allows to resolve domain names of ECS adopters on behalf of any arbitrary IP/prefix in the Internet. In fact, by utilizing only a single residential vantage point and relying solely on publicly available information, we are able to (i) uncover the global footprint of ECS adopters with very little effort, (ii) infer the DNS response cacheability and end-user clustering of ECS adopters for an arbitrary network in the Internet, and (iii) capture snapshots of user to server mappings as practiced by major ECS adopters. While pointing out such new measurement opportunities, our work is also intended to make current and future ECS adopters aware of which operational information gets exposed when utilizing this recent DNS extension.},
  file = {D\:\\GDrive\\zotero\\Streibelt\\streibelt_2013_exploring_edns-client-subnet_adopters_in_your_free_time.pdf},
  isbn = {978-1-4503-1953-9},
  keywords = {CDN,Content delivery,DNS}
}

@book{stroustrupProgrammingLanguage2013,
  title = {The {{C}}++ Programming Language},
  author = {Stroustrup, Bjarne},
  year = {2013},
  edition = {Fourth edition},
  publisher = {{Addison-Wesley}},
  address = {{Upper Saddle River, NJ}},
  isbn = {978-0-321-56384-2},
  keywords = {C++ (Computer program language)},
  lccn = {QA76.73.C153 S77 2013}
}

@techreport{stumpfRobustIntegrityReporting2006,
  title = {A {{Robust Integrity Reporting Protocol}} for {{Remote Attestation}}},
  author = {Stumpf, Frederic and Tafreschi, Omid and R{\"o}der, Patrick and Eckert, Claudia},
  year = {2006},
  abstract = {Trusted Computing Platforms provide the functionality of remote attestation, i.e. attesting the configuration and status of a system to a remote entity. Remote attestation hereby proves integrity and authenticity of system environments. This is crucial for policy enforcement, which in turn is needed in many usage scenarios, e.g., DRM. However, applying remote attestation solely allows masquerading attacks. These attacks are possible since the concept of remote attestation does not provide any means for establishing secured communication channels. In this paper we describe this kind of attacks against protocols for remote attestation and present a protocol for preventing masquerading attacks.},
  file = {D\:\\GDrive\\zotero\\Stumpf et al\\stumpf_et_al_2006_a_robust_integrity_reporting_protocol_for_remote_attestation.pdf}
}

@techreport{stutzSignalCollectGraph,
  title = {Signal/{{Collect}}: {{Graph Algorithms}} for the ({{Semantic}}) {{Web}}},
  author = {Stutz, Philip and Bernstein, Abraham and Cohen, William},
  abstract = {The Semantic Web graph is growing at an incredible pace, enabling opportunities to discover new knowledge by interlinking and analyzing previously unconnected data sets. This confronts researchers with a conundrum: Whilst the data is available the programming models that facilitate scalability and the infrastructure to run various algorithms on the graph are missing. Some use MapReduce-a good solution for many problems. However, even some simple iterative graph algorithms do not map nicely to that programming model requiring programmers to shoehorn their problem to the MapReduce model. This paper presents the Signal/Collect programming model for synchronous and asynchronous graph algorithms. We demonstrate that this abstraction can capture the essence of many algorithms on graphs in a concise and elegant way by giving Signal/Collect adaptations of various relevant algorithms. Furthermore, we built and evaluated a prototype Signal/Collect framework that executes algorithms in our programming model. We empirically show that this prototype transparently scales and that guiding computations by scoring as well as asyn-chronicity can greatly improve the convergence of some example algorithms. We released the framework under the Apache License 2.0 (at},
  file = {D\:\\GDrive\\zotero\\Stutz\\stutz_signal-collect.pdf}
}

@article{submittedTaskOptimizationFramework2004,
  title = {A Task Optimization Framework for Mssp},
  author = {Submitted, Thesis and Science, Computer},
  year = {2004},
  abstract = {The Master/Slave Speculative Parallelization paradigm relies on the use of a highly optimized and mostly correct version of a sequential program, called distilled code, for breaking inter-task dependences. We describe the design and implementation of an optimization framework that can create such distilled code within the context of an MSSP simulator. Our optimizer processes pieces of Alpha machine code called traces and is designed to optimize code while adhering to certain restrictions and requirements imposed by the MSSP paradigm. We describe the specific places where our optimizer is different from that in a compiler and explain how the optimized traces are deployed in the simulator.},
  file = {D\:\\GDrive\\zotero\\Submitted\\submitted_2004_a_task_optimization_framework_for_mssp.pdf},
  journal = {University of Illinois at Urbana-Champaign}
}

@article{sunOATAttestingOperation2019,
  title = {{{OAT}}: {{Attesting Operation Integrity}} of {{Embedded Devices}}},
  shorttitle = {{{OAT}}},
  author = {Sun, Zhichuang and Feng, Bo and Lu, Long and Jha, Somesh},
  year = {2019},
  month = oct,
  abstract = {Due to the wide adoption of IoT/CPS systems, embedded devices(IoT frontends) become increasingly connected and mission-critical, which in turn has attracted advanced attacks (e.g., control-flow hijacks and data-only attacks). Unfortunately, IoT backends are unable to detect if such attacks have happened while receiving data, service requests, or operation status from IoT devices. As a result, currently, IoT backends are forced to blindly trust the IoT devices that they interact with. To fill this void, we first formulate a new security property for embedded devices, called "Operation Execution Integrity" or OEI. We then design and build a system, OAT, that enables remote OEI attestation for ARM-based bare-metal embedded devices. Our formulation of OEI captures the integrity of both control flow and critical data involved in an operation execution. Therefore, satisfying OEI entails that an operation execution is free of unexpected control and data manipulations, which existing attestation methods cannot check. Our design of OAT strikes a balance between prover's constraints (embedded devices' limited computing power and storage) and verifier's requirements(complete verifiability and forensic assistance). OAT uses a new control-flow measurement scheme, which enables light-weight and space-efficient collection of measurements (97\% space reduction from the trace-based approach). OAT performs the remote control-flow verification through abstract execution, which is fast and deterministic. OAT also features lightweight integrity checking for critical data (74\% fewer instrumentation needed than previous work). Our security analysis shows that OAT allows remote verifiers or IoT backends to detect both control-flow hijacks and data-only attacks that affect the execution of operations on IoT devices. In our evaluation using real embedded programs, OAT incurs a runtime overhead of 2.7\%.},
  archiveprefix = {arXiv},
  eprint = {1802.03462},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Sun et al\\sun_et_al_2019_oat.pdf;C\:\\Users\\Admin\\Zotero\\storage\\JE9PT4EM\\1802.html},
  journal = {arXiv:1802.03462 [cs]},
  keywords = {Computer Science - Cryptography and Security},
  primaryclass = {cs}
}

@inproceedings{sunOATAttestingOperation2020,
  title = {{{OAT}}: {{Attesting Operation Integrity}} of {{Embedded Devices}}},
  shorttitle = {{{OAT}}},
  booktitle = {2020 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Sun, Zhichuang and Feng, Bo and Lu, Long and Jha, Somesh},
  year = {2020},
  month = may,
  pages = {1433--1449},
  issn = {2375-1207},
  doi = {10.1109/SP40000.2020.00042},
  abstract = {Due to the wide adoption of IoT/CPS systems, embedded devices (IoT frontends) become increasingly connected and mission-critical, which in turn has attracted advanced attacks (e.g., control-flow hijacks and data-only attacks). Unfortunately, IoT backends (e.g., remote controllers or in-cloud services) are unable to detect if such attacks have happened while receiving data, service requests, or operation status from IoT devices (remotely deployed embedded devices). As a result, currently, IoT backends are forced to blindly trust the IoT devices that they interact with.To fill this void, we first formulate a new security property for embedded devices, called "Operation Execution Integrity" or OEI. We then design and build a system, OAT, that enables remote OEI attestation for ARM-based bare-metal embedded devices. Our formulation of OEI captures the integrity of both control flow and critical data involved in an operation execution. Therefore, satisfying OEI entails that an operation execution is free of unexpected control and data manipulations, which existing attestation methods cannot check. Our design of OAT strikes a balance between prover's constraints (embedded devices' limited computing power and storage) and verifier's requirements (complete verifiability and forensic assistance). OAT uses a new control-flow measurement scheme, which enables lightweight and space-efficient collection of measurements (97\% space reduction from the trace-based approach). OAT performs the remote control-flow verification through abstract execution, which is fast and deterministic. OAT also features lightweight integrity checking for critical data (74\% less instrumentation needed than previous work). Our security analysis shows that OAT allows remote verifiers or IoT backends to detect both controlflow hijacks and data-only attacks that affect the execution of operations on IoT devices. In our evaluation using real embedded programs, OAT incurs a runtime overhead of 2.7\%.},
  file = {D\:\\GDrive\\zotero\\Sun et al\\sun_et_al_2020_oat.pdf},
  keywords = {Data integrity,Instruments,Manipulators,Open area test sites,Performance evaluation}
}

@article{sunRAPTORRoutingAttacks2015,
  title = {{{RAPTOR}} : {{Routing Attacks}} on {{Privacy}} in {{Tor This}} Paper Is Included in the {{Proceedings}} of The},
  author = {Sun, Yixin and Edmundson, Anne and Vanbever, Laurent and Z{\"u}rich, E T H and Li, Oscar and Rexford, Jennifer and Chiang, Mung and Mittal, Prateek and Edmundson, Anne and Vanbever, Laurent and Rexford, Jennifer},
  year = {2015},
  file = {D\:\\GDrive\\zotero\\Sun\\sun_2015_raptor.pdf},
  isbn = {9781931971232},
  journal = {USENIX Security}
}

@techreport{SurveyBlockchainTechnologies2016,
  title = {Survey on {{Blockchain Technologies}} and {{Related Services FY2015 Report}}},
  year = {2016}
}

@techreport{SurveyVirtualLAN2011,
  title = {A {{Survey}} of {{Virtual LAN Usage}} in {{Campus Networks}}},
  year = {2011},
  abstract = {99 incoming link for frames sent by that MAC address and creating a mapping between the MAC address and that port. To connect to the rest of the enterprise network (and the rest of the Internet), the island of Ethernet switches connects to IP routers that forward traffic to and from remote hosts. Each host interface in the LAN has an IP address from a common IP prefix (or set of prefixes). Traffic sent to an IP address in the same subnet stays within the LAN; the sending host uses the Address Resolution Protocol (ARP) to determine the MAC address associated with the destination IP address. For traffic destined to remote IP addresses, the host forwards the packets to the gateway router, which forwards packets further toward their destinations. COMMUNICATION WITHIN A VLAN Administrators use VLANs to construct network segments that behave logically like a conventional LAN but are independent of the physical locations of the hosts; for example, hosts H1 and H3 in Fig. 1 both belong to VLAN1. As in a conventional physical LAN, the switches in a VLAN construct a spanning tree, and use flooding and learning to forward traffic between hosts. For example, the switches S3, S4, and S5 form a spanning tree for VLAN2. Communication between hosts in the same VLAN stays within the VLAN, with the switches forwarding Ethernet frames along the spanning tree to the destination MAC address. For example , hosts H2 and H4 communicate over the 2 spanning tree in VLAN2 based on their MAC addresses. Similarly, hosts H1 and H3 communicate over the spanning tree in VLAN1, where some of the IP routers (e.g., R1, R2, and R2) may also act as switches in the spanning tree; alternatively, a tunnel between R1 and R2 could participate in VLAN1 so the links in the IP backbone do not need to participate in the VLANs. COMMUNICATION BBETWEEN VLANS Each host has an IP address from an IP prefix (or prefixes) associated with its VLAN; IP routers forward packets based on these prefixes, over paths computed in the routing protocol (e.g., Open Shortest Path First [OSPF] or Routing Information Protocol [RIP]). Hence, traffic between hosts in different VLANs must traverse an intermediate IP router. For example, traffic between hosts H3 and H4 would traverse router R2, even though the two hosts connect to the same switch. For example, when sending traffic to H4, host H3 forwards the packets to its gateway router R2, since the destination IP address belongs to a different prefix. R2 would then look up the destination IP address to forward the packet to H4 in VLAN2. If H4 sends an IP packet to H1, then H4's router R3 forwards the packet based on the IP routing protocol toward the router announcing H1's IP prefix, and that router would then forward the packet over the spanning tree for VLAN1.},
  file = {D\:\\GDrive\\zotero\\undefined\\2011_a_survey_of_virtual_lan_usage_in_campus_networks.pdf}
}

@misc{SurvivalGuidePhD,
  title = {A {{Survival Guide}} to a {{PhD}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\SE6NVPGX\\phd.html},
  howpublished = {http://karpathy.github.io/2016/09/07/phd/}
}

@misc{SyllabusEricPhD,
  title = {Syllabus for {{Eric}}'s {{PhD}} Students},
  abstract = {Table of Contents  Author	2 Licence	2 Purpose of this document	3 Acknowledgements	3 Perspective on the PhD	4 How long is it?	4 What will I (your advisor) get out of it?	5 Doing research	6 Selecting problems	6 Quality vs. quantity	6 Staying organized	8 Ideas	9 Where to find ideas	9 Writing papers...},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\V5PIS85X\\edit.html},
  howpublished = {https://docs.google.com/document/d/11D3kHElzS2HQxTwPqcaTnU5HCJ8WGE5brTXI4KLf4dM/edit?usp=embed\_facebook},
  journal = {Google Docs},
  language = {en}
}

@book{syropoulosDigitalTypographyUsing2003,
  title = {Digital Typography Using {{LaTeX}}},
  author = {Syropoulos, Apostolos and Tsolomitis, Antonis and Sofroniou, Nick},
  year = {2003},
  publisher = {{Springer}},
  address = {{New York}},
  file = {D\:\\GDrive\\zotero\\Syropoulos et al\\syropoulos_et_al_2003_digital_typography_using_latex.pdf},
  isbn = {978-0-387-95217-8},
  keywords = {Computerized typesetting,LaTeX (Computer file)},
  language = {en},
  lccn = {Z253.4.L38 S97 2003}
}

@article{syversonCodeInjectionAttacks2008,
  title = {Code {{Injection Attacks}} on {{Harvard}}-{{Architecture Devices}}},
  author = {Syverson, Paul. and Jha, Somesh. and Zhang, Xiaolan. and {Association for Computing Machinery. Special Interest Group on Security}, Audit},
  year = {2008},
  abstract = {"ACM order number 537080"--Title page verso. CCS'08.},
  file = {D\:\\GDrive\\zotero\\Syverson\\syverson_2008_code_injection_attacks_on_harvard-architecture_devices.pdf}
}

@article{szaboApplicationsWeb2019,
  title = {C/{{C}}++ {{Applications}} on the {{Web}}},
  author = {Szab{\'o}, M{\'a}rton and Neh{\'e}z, K{\'a}roly},
  year = {2019},
  volume = {8},
  pages = {69--87},
  issn = {17851270},
  doi = {10.32968/psaie.2019.005},
  abstract = {The JavaScript technology, especially in the latest five years, has been evolving very rapidly. In the world of WWW, of course, other technologies are also available for developers, but JavaScript is one of the best ways to develop applications for both traditional web and the latest mobile environments. The modern web application frameworks unfortunately do not support direct integration of C/C++ technologies. This problem can be solved by various utility software, e.g. the Emscripten compiler, which translates C/C++ codes into JavaScript leveraging LLVM technology as a transitional layer. Thus C/C++ programmers are not omitted from the world of Web and mobile development and they can reuse existing mature codebases, especially in the field of computer graphics. This paper describes theory, practical use and inherent potential deals of Emscripten technology. A native C++ OpenGL application will be used as a real-world example, demonstrating efficiency and flexibility of this technique. Performance evaluation of the JavaScript code compared to the native C/C++ application will also be presented. To promote better understanding, our source code is also available upon request.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\YWMQ62NV\\Szabó and Nehéz - 2019 - CC++ Applications on the Web.pdf;D\:\\GDrive\\zotero\\Szabó_Nehéz\\szabó_nehéz_2019_c-c++_applications_on_the_web.pdf},
  journal = {Production Systems and Information Engineering},
  language = {en}
}

@techreport{szalachowskiShortPaperDeployment,
  title = {Short {{Paper}}: {{On Deployment}} of {{DNS}}-Based {{Security Enhancements}}},
  author = {Szalachowski, Pawel and Perrig ETH Zurich, Adrian},
  abstract = {Although the Domain Name System (DNS) was designed as a naming system, its features have made it appealing to repurpose it for the deployment of novel systems. One important class of such systems are security enhancements, and this work sheds light on their deployment. We show the characteristics of these solutions and measure reliability of DNS in these applications. We investigate the compatibility of these solutions with the Tor network, signal necessary changes, and report on surprising drawbacks in Tor's DNS resolution.},
  file = {D\:\\GDrive\\zotero\\Szalachowski\\szalachowski_short_paper.pdf},
  keywords = {ss}
}

@article{szegedyGoingDeeperConvolutions,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  file = {D\:\\GDrive\\zotero\\Szegedy\\szegedy_going_deeper_with_convolutions.pdf}
}

@techreport{szekeresSoKEternalWar2013,
  title = {{{SoK}}: {{Eternal War}} in {{Memory}}},
  author = {Szekeres, L{\'a}szl{\'o} and Payer, Mathias and Wei, Tao and Song, Dawn},
  year = {2013},
  abstract = {Memory corruption bugs in software written in low-level languages like C or C++ are one of the oldest problems in computer security. The lack of safety in these languages allows attackers to alter the program's behavior or take full control over it by hijacking its control flow. This problem has existed for more than 30 years and a vast number of potential solutions have been proposed, yet memory corruption attacks continue to pose a serious threat. Real world exploits show that all currently deployed protections can be defeated. This paper sheds light on the primary reasons for this by describing attacks that succeed on today's systems. We systematize the current knowledge about various protection techniques by setting up a general model for memory corruption attacks. Using this model we show what policies can stop which attacks. The model identifies weaknesses of currently deployed techniques, as well as other proposed protections enforcing stricter policies. We analyze the reasons why protection mechanisms implementing stricter polices are not deployed. To achieve wide adoption, protection mechanisms must support a multitude of features and must satisfy a host of requirements. Especially important is performance, as experience shows that only solutions whose overhead is in reasonable bounds get deployed. A comparison of different enforceable policies helps designers of new protection mechanisms in finding the balance between effectiveness (security) and efficiency. We identify some open research problems, and provide suggestions on improving the adoption of newer techniques.},
  file = {D\:\\GDrive\\zotero\\Szekeres\\szekeres_sok.pdf}
}

@article{tagliabueAnnotationFrameworkLowLevel2013,
  title = {An {{Annotation Framework}} for {{Low}}-{{Level Virtual Machine Compiler Infrastructure}}},
  author = {Tagliabue, Giacomo},
  year = {2013},
  file = {D\:\\GDrive\\zotero\\Tagliabue\\tagliabue_2013_an_annotation_framework_for_low-level_virtual_machine_compiler_infrastructure.pdf}
}

@article{TailScale2013,
  title = {The Tail at Scale},
  year = {2013},
  doi = {10.1145/2408776.2408794},
  abstract = {sYsteMs that resPoND to user actions quickly (within 100ms) feel more fluid and natural to users than those that take longer. 3 Improvements in Internet connectivity and the rise of warehouse-scale computing systems 2 have enabled Web services that provide fluid responsiveness while consulting multi-terabyte datasets spanning thousands of servers; for example, the Google search system updates query results interactively as the user types, predicting the most likely query based on the prefix typed so far, performing the search and showing the results within a few tens of milliseconds.},
  file = {D\:\\GDrive\\zotero\\undefined\\the_tail_at_scale.pdf}
}

@misc{TakingNotesResearch,
  title = {Taking {{Notes}} from {{Research Reading}} | {{Writing Advice}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\P58UWR2B\\notes-from-research.html},
  language = {en-US}
}

@techreport{tallamConceptAnalysisInspired2005,
  title = {A {{Concept Analysis Inspired Greedy Algorithm}} for {{Test Suite Minimization}}},
  author = {Tallam, Sriraman and Gupta, Neelam},
  year = {2005},
  abstract = {Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible and to ensure that changes to existing software do not break the software. Test suites once developed are reused and updated frequently as the software evolves. As a result, some test cases in the test suite may become redundant as the software is modified over time since the requirements covered by them are also covered by other test cases. Due to the resource and time constraints for re-executing large test suites, it is important to develop techniques to minimize available test suites by removing redundant test cases. In general, the test suite minimization problem is NP complete. In this paper, we present a new greedy heuristic algorithm for selecting a minimal subset of a test suite T that covers all the requirements covered by T. We show how our algorithm was inspired by the concept analysis framework. We conducted experiments to measure the extent of test suite reduction obtained by our algorithm and prior heuristics for test suite minimization. In our experiments, our algorithm always selected same size or smaller size test suite than that selected by prior heuristics and had comparable time performance.},
  file = {D\:\\GDrive\\zotero\\Tallam\\tallam_2005_a_concept_analysis_inspired_greedy_algorithm_for_test_suite_minimization.pdf},
  keywords = {concept analysis,test cases,test suite minimiza-tion,testing requirements}
}

@article{tangCLKSCREWExposingPerils2017,
  title = {{{CLKSCREW}} : {{Exposing}} the {{Perils}} of {{Security}}- {{Oblivious Energy Management This}} Paper Is Included in the {{Proceedings}} of The},
  author = {Tang, Adrian and Sethumadhavan, Simha and Stolfo, Salvatore and Tang, Adrian and Stolfo, Salvatore},
  year = {2017},
  pages = {1057--1074},
  abstract = {The need for power-and energy-efficient computing has resulted in aggressive cooperative hardware-software en-ergy management mechanisms on modern commodity devices. Most systems today, for example, allow soft-ware to control the frequency and voltage of the under-lying hardware at a very fine granularity to extend bat-tery life. Despite their benefits, these software-exposed energy management mechanisms pose grave security im-plications that have not been studied before. In this work, we present the CLKSCREW attack, a new class of fault attacks that exploit the security-obliviousness of energy management mechanisms to break security. A novel benefit for the attackers is that these fault attacks become more accessible since they can now be conducted without the need for physical access to the devices or fault injection equipment. We demonstrate CLKSCREW on commodity ARM/Android devices. We show that a malicious kernel driver (1) can extract secret cryptographic keys from Trustzone, and (2) can escalate its privileges by loading self-signed code into Trustzone. As the first work to show the security ramifications of en-ergy management mechanisms, we urge the community to re-examine these security-oblivious designs.},
  file = {D\:\\GDrive\\zotero\\Tang\\tang_2017_clkscrew.pdf},
  isbn = {9781931971409},
  journal = {\{USENIX\} Security Symposium}
}

@misc{taoWriting2007,
  title = {On Writing},
  author = {Tao, Terence},
  year = {2007},
  month = may,
  abstract = {There are three rules for writing the novel. Unfortunately, no one knows what they are. (W. Somerset Maugham) Everyone has to develop their own writing style, based on their own strengths and weakn\ldots},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\M8DHKM2V\\advice-on-writing-papers.html},
  journal = {What's new},
  language = {en}
}

@inproceedings{tarjanDepthfirstSearchLinear1971,
  title = {Depth-First Search and Linear Graph Algorithms},
  booktitle = {12th {{Annual Symposium}} on {{Switching}} and {{Automata Theory}} (Swat 1971)},
  author = {Tarjan, Robert},
  year = {1971},
  month = oct,
  pages = {114--121},
  issn = {0272-4847},
  doi = {10.1109/SWAT.1971.10},
  abstract = {The value of depth-first search or "backtracking" as a technique for solving graph problems is illustrated by two examples. An algorithm for finding the biconnected components of an undirected graph and an improved version of an algorithm for finding the strongly connected components of a directed graph are presented. The space and time requirements of both algorithms are bounded by k1V + k2E + k3 for some constants k1, k2, and k3, where V is the number of vertices and E is the number of edges of the graph being examined.},
  file = {D\:\\GDrive\\zotero\\Tarjan\\tarjan_1971_depth-first_search_and_linear_graph_algorithms.pdf},
  keywords = {Chemistry,Computer science,Electrical engineering,Erbium,Information retrieval,Sociology,Tail,Tree graphs,TV}
}

@techreport{techtalkChrisLattnerClattner2007,
  title = {Chris {{Lattner}} Clattner@apple.Com {{LLVM}}-2.0 and Beyond!},
  author = {Tech Talk, Google},
  year = {2007},
  file = {D\:\\GDrive\\zotero\\Tech Talk\\tech_talk_2007_chris_lattner_clattner@apple.pdf}
}

@techreport{tennenhouseActiveNetworkArchitecture,
  title = {Towards an {{Active Network Architecture}}},
  author = {Tennenhouse, David L and Wetherall, David J},
  abstract = {Active networks allow their users to inject customized programs into the nodes of the network. An extreme case, in which we are most interested, replaces packets with "capsules"-program fragments that are executed at each network router/switch they traverse. Active architectures permit a massive increase in the sophistication of the computation that is performed within the network. They will enable new applications, especially those based on application-specific multicast, information fusion, and other services that leverage network-based computation and storage. Furthermore, they will accelerate the pace of innovation by decoupling network services from the underlying hardware and allowing new services to be loaded into the infrastructure on demand. In this paper, we describe our vision of an active network architecture, outline our approach to its design, and survey the technologies that can be brought to bear on its implementation. We propose that the research community mount a joint effort to develop and deploy a wide area ActiveNet.},
  file = {D\:\\GDrive\\zotero\\Tennenhouse\\tennenhouse_towards_an_active_network_architecture.pdf}
}

@article{teranPerceptronLearningReuse2016,
  title = {Perceptron Learning for Reuse Prediction},
  author = {Teran, Elvira and Wang, Zhe and Jimenez, Daniel A.},
  year = {2016},
  volume = {2016-Decem},
  issn = {10724451},
  doi = {10.1109/MICRO.2016.7783705},
  abstract = {The disparity between last-level cache and memory latencies motivates the search for efficient cache management policies. Recent work in predicting reuse of cache blocks enables optimizations that significantly improve cache performance and efficiency. However, the accuracy of the prediction mechanisms limits the scope of optimization. This paper proposes perceptron learning for reuse prediction. The proposed predictor greatly improves accuracy over previous work. For multi-programmed workloads, the average false positive rate of the proposed predictor is 3.2\%, while sampling dead block prediction (SDBP) and signature-based hit prediction (SHiP) yield false positive rates above 7\%. The improvement in accuracy translates directly into performance. For single-Thread workloads and a 4MB last-level cache, reuse prediction with perceptron learning enables a replacement and bypass optimization to achieve a geometric mean speedup of 6.1\%, compared with 3.8\% for SHiP and 3.5\% for SDBP on the SPEC CPU 2006 benchmarks. On a memory-intensive subset of SPEC, perceptron learning yields 18.3\% speedup, versus 10.5\% for SHiP and 7.7\% for SDBP. For multi-programmed workloads and a 16MB cache, the proposed technique doubles the efficiency of the cache over LRU and yields a geometric mean normalized weighted speedup of 7.4\%, compared with 4.4\% for SHiP and 4.2\% for SDBP.},
  file = {D\:\\GDrive\\zotero\\Teran\\teran_2016_perceptron_learning_for_reuse_prediction.pdf},
  isbn = {9781509035083},
  journal = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO}
}

@article{Terry1994Session,
  title = {Terry et al. - 1994 - {{Session}} Guarantees for Weakly Consistent Replicate},
  file = {D\:\\GDrive\\zotero\\undefined\\terry_et_al.pdf}
}

@article{ThereBeDragon1992,
  title = {There {{Be Dragon}}},
  year = {1992},
  file = {D\:\\GDrive\\zotero\\undefined\\there_be_dragon.pdf}
}

@article{thiranItStillPossible2011,
  title = {Is It {{Still Possible}} to {{Extend TCP}}?},
  author = {Thiran, Patrick and {ACM Digital Library.} and {ACM-Sigmetrics.} and {ACM Special Interest Group on Data Communication.}},
  year = {2011},
  abstract = {Title from The ACM Digital Library.},
  file = {D\:\\GDrive\\zotero\\Thiran\\thiran_2011_is_it_still_possible_to_extend_tcp.pdf}
}

@article{thisLLVMCodeGeneration2017,
  title = {{{LLVM}} Code Generation for a Wide {{SIMD}} Architecture with Automatic Bypassing},
  author = {This, Disclaimer},
  year = {2017},
  file = {D\:\\GDrive\\zotero\\This\\this_2017_llvm_code_generation_for_a_wide_simd_architecture_with_automatic_bypassing.pdf}
}

@article{thomadakisArchitectureNehalemProcessor2010,
  title = {The {{Architecture}} of the {{Nehalem Processor}} and {{Nehalem}}-{{EP SMP Platforms}}},
  author = {Thomadakis, Michael E and D, Ph},
  year = {2010},
  pages = {1--38},
  abstract = {Nehalem is an implementation of the CISC Intel64 instruction speci cation based on 45nm and high-k + metal gate transistor technology. Nehalem micro-architectures and system platforms employ a number of state-of-the-art technologies which enable high computation rates for scienti c and other demanding workloads. Nehalem based processors incorporate multiple cores, on-chip DDR3 memory controller, a shared Level 3 cache and high-speed Quick-Path Interconnect ports for connectivity with other chips and the I/O sub-system. Each core has super- scalar, out-of-order and speculative execution pipelines and supports 2-way simultaneous multi-threading. Each core o ers multiple functional units which can sustain high instruction level parallelism rates with the assistance of program development tools, compilers or special coding techniques. A prominent feature of Intel64 is the processing of SIMD instructions at a nominal rate of 4 double or 8 single precision oating-point instructions per clock cycle. Nehalem platforms are cc-NUMA shared-memory processor systems. Complex processors and platforms, such as those based on Nehalem, present several challenges to application developers, as well as, system level engineers. Developers are faced with the task of writing ecient code on increasingly complex platforms. System engineers need to understand the system level bottlenecks in order to con gure and tune the system to yield good performance for the application mix of interest. This report discusses technical details of the Nehalem architecture and platforms with an emphasis on inner workings and the cost of instruction execution. The discussion presented here can assist developers and engineers in their respective elds. The rst to produce ecient scalar and parallel code on the Nehalem platform and the latter ones to con gure and tune a system to perform well under complex application workloads.},
  file = {D\:\\GDrive\\zotero\\Thomadakis\\thomadakis_2010_the_architecture_of_the_nehalem_processor_and_nehalem-ep_smp_platforms.pdf},
  isbn = {A research report of Texas A\&M university},
  journal = {Digital Media},
  keywords = {archy,cache memory hier-,cc-numa,core and uncore,global queue,integrated-memory controllers,intel64,local and remote memory,micro-architecture,nehalem,performance,superscalar processors}
}

@book{thomsonCalvinFastDistributed2012,
  title = {Calvin: {{Fast Distributed Transactions}} for {{Partitioned Database Systems}}},
  author = {Thomson, Alexander and Diamond, Thaddeus and Weng, Shu-Chun and Ren, Kun and Shao, Philip and Abadi, Daniel J},
  year = {2012},
  abstract = {Many distributed storage systems achieve high data access through-put via partitioning and replication, each system with its own advantages and tradeoffs. In order to achieve high scalability, however , today's systems generally reduce transactional support, disallowing single transactions from spanning multiple partitions. Calvin is a practical transaction scheduling and data replication layer that uses a deterministic ordering guarantee to significantly reduce the normally prohibitive contention costs associated with distributed transactions. Unlike previous deterministic database system prototypes , Calvin supports disk-based storage, scales near-linearly on a cluster of commodity machines, and has no single point of failure. By replicating transaction inputs rather than effects, Calvin is also able to support multiple consistency levels-including Paxos-based strong consistency across geographically distant replicas-at no cost to transactional throughput.},
  file = {D\:\\GDrive\\zotero\\Thomson\\thomson_2012_calvin.pdf},
  isbn = {978-1-4503-1247-9}
}

@techreport{thurstonProofProgressMathematics1994,
  title = {On {{Proof}} and {{Progress}} in {{Mathematics}}},
  author = {Thurston, William P},
  year = {1994},
  volume = {30},
  pages = {161--177},
  file = {D\:\\GDrive\\zotero\\Thurston\\thurston_1994_on_proof_and_progress_in_mathematics.pdf},
  journal = {APPEARED IN BULLETIN OF THE AMERICAN MATHEMATICAL SOCIETY},
  number = {2}
}

@techreport{thusooHiveAPetabyteScale,
  title = {Hive-{{A Petabyte Scale Data Warehouse Using Hadoop}}},
  author = {Thusoo, Ashish and Sarma, Joydeep Sen and Jain, Namit and Shao, Zheng and Chakka, Prasad and Zhang, Ning and Antony, Suresh and Liu, Hao and Murthy, Raghotham},
  abstract = {The size of data sets being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hadoop [1] is a popular open-source map-reduce implementation which is being used in companies like Yahoo, Facebook etc. to store and process extremely large data sets on commodity hardware. However, the map-reduce programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse. In this paper, we present Hive, an open-source data warehousing solution built on top of Hadoop. Hive supports queries expressed in a SQL-like declarative language-HiveQL, which are compiled into map-reduce jobs that are executed using Hadoop. In addition, HiveQL enables users to plug in custom map-reduce scripts into queries. The language includes a type system with support for tables containing primitive types, collections like arrays and maps, and nested compositions of the same. The underlying IO libraries can be extended to query data in custom formats. Hive also includes a system catalog-Metastore-that contains schemas and statistics, which are useful in data exploration, query optimization and query compilation. In Facebook, the Hive warehouse contains tens of thousands of tables and stores over 700TB of data and is being used extensively for both reporting and ad-hoc analyses by more than 200 users per month.},
  file = {D\:\\GDrive\\zotero\\Thusoo\\thusoo_hive-a_petabyte_scale_data_warehouse_using_hadoop.pdf}
}

@techreport{tianCopyDiscardExecution,
  title = {Copy {{Or Discard Execution Model For Speculative Parallelization On Multicores}}},
  author = {Tian, Chen and Feng, Min and Nagarajan, Vijay and Gupta, Rajiv},
  abstract = {The advent of multicores presents a promising opportunity for speeding up sequential programs via profile-based speculative parallelization of these programs. In this paper we present a novel solution for efficiently supporting software speculation on multicore processors. We propose the Copy or Discard (CorD) execution model in which the state of speculative parallel threads is maintained separately from the non-speculative computation state. If speculation is successful, the results of the speculative computation are committed by copying them into the non-speculative state. If misspeculation is detected, no costly state recovery mechanisms are needed as the speculative state can be simply discarded. Optimizations are proposed to reduce the cost of data copying between non-speculative and speculative state. A lightweight mechanism that maintains version numbers for non-speculative data values enables misspeculation detection. We also present an algorithm for profile-based speculative parallelization that is effective in extracting parallelism from sequential programs. Our experiments show that the combination of CorD and our speculative parallelization algorithm achieves speedups ranging from 3.7 to 7.8 on a Dell PowerEdge 1900 server with two Intel Xeon quad-core processors.},
  file = {D\:\\GDrive\\zotero\\Tian\\tian_copy_or_discard_execution_model_for_speculative_parallelization_on_multicores.pdf},
  keywords = {copy or discard execution model,multicores,speculative parallelization}
}

@article{ticeEnforcingForwardEdgeControlFlow2014,
  title = {Enforcing {{Forward}}-{{Edge Control}}-{{Flow Integrity}} in {{GCC}} \& {{LLVM}}},
  author = {Tice, Caroline and Roeder, Tom and Collingbourne, Peter and Checkoway, Stephen and Erlingsson, {\'U}lfar and Lozano, Luis and Pike, Geoff},
  year = {2014},
  pages = {16},
  abstract = {Constraining dynamic control transfers is a common technique for mitigating software vulnerabilities. This defense has been widely and successfully used to protect return addresses and stack data; hence, current attacks instead typically corrupt vtable and function pointers to subvert a forward edge (an indirect jump or call) in the control-flow graph. Forward edges can be protected using Control-Flow Integrity (CFI) but, to date, CFI implementations have been research prototypes, based on impractical assumptions or ad hoc, heuristic techniques. To be widely adoptable, CFI mechanisms must be integrated into production compilers and be compatible with software-engineering aspects such as incremental compilation and dynamic libraries. This paper presents implementations of fine-grained, forward-edge CFI enforcement and analysis for GCC and LLVM that meet the above requirements. An analysis and evaluation of the security, performance, and resource consumption of these mechanisms applied to the SPEC CPU2006 benchmarks and common benchmarks for the Chromium web browser show the practicality of our approach: these fine-grained CFI mechanisms have significantly lower overhead than recent academic CFI prototypes. Implementing CFI in industrial compiler frameworks has also led to insights into design tradeoffs and practical challenges, such as dynamic loading.},
  file = {D\:\\GDrive\\zotero\\Tice et al\\tice_et_al_2014_enforcing_forward-edge_control-flow_integrity_in_gcc_&_llvm.pdf},
  language = {en}
}

@phdthesis{tikhomirovSecurityPrivacyBlockchain2020,
  title = {Security and {{Privacy}} of {{Blockchain Protocols}} and {{Applications}}},
  author = {Tikhomirov, Sergei},
  year = {2020},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\DGFTFU56\\tikhomirov-thesis.pdf;D\:\\GDrive\\zotero\\Tikhomirov\\tikhomirov_2020_security_and_privacy_of_blockchain_protocols_and_applications.pdf}
}

@article{tillmannPexWhiteBox2018,
  title = {Pex \textendash{} {{White Box Test Generation}} for . {{NET}}},
  author = {Tillmann, Nikolai and Halleux, Jonathan De},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Tillmann\\tillmann_2018_pex_–_white_box_test_generation_for.pdf}
}

@article{tobin-hochstadtExtensiblePatternMatching2011,
  title = {Extensible {{Pattern Matching}} in an {{Extensible Language}}},
  author = {{Tobin-Hochstadt}, Sam},
  year = {2011},
  month = jun,
  abstract = {Pattern matching is a widely used technique in functional languages, especially those in the ML and Haskell traditions, where it is at the core of the semantics. In languages in the Lisp tradition, in contrast, pattern matching it typically provided by libraries built with macros. We present match, a sophisticated pattern matcher for Racket, implemented as language extension. using macros. The system supports novel and widely-useful pattern-matching forms, and is itself extensible. The extensibility of match is implemented via a general technique for creating extensible language extensions.},
  file = {D\:\\GDrive\\zotero\\Tobin-Hochstadt\\tobin-hochstadt_2011_extensible_pattern_matching_in_an_extensible_language.pdf}
}

@techreport{toffaliniScaRRScalableRuntime2019,
  title = {{{ScaRR}}: {{Scalable Runtime Remote Attestation}} for {{Complex Systems}}},
  author = {Toffalini, Flavio and Losiouk, Eleonora and Biondo, Andrea and Zhou, Jianying and Conti, Mauro},
  year = {2019},
  abstract = {The introduction of remote attestation (RA) schemes has allowed academia and industry to enhance the security of their systems. The commercial products currently available enable only the validation of static properties, such as applications fingerprint , and do not handle runtime properties, such as control-flow correctness. This limitation pushed researchers towards the identification of new approaches, called runtime RA. However , those mainly work on embedded devices, which share very few common features with complex systems, such as virtual machines in a cloud. A naive deployment of runtime RA schemes for embedded devices on complex systems faces scalability problems, such as the representation of complex control-flows or slow verification phase. In this work, we present ScaRR: the first Scalable Runtime Remote attestation schema for complex systems. Thanks to its novel control-flow model, ScaRR enables the deployment of runtime RA on any application regardless of its complexity, by also achieving good performance. We implemented ScaRR and tested it on the benchmark suite SPEC CPU 2017. We show that ScaRR can validate on average 2M control-flow events per second, definitely outperforming existing solutions that support runtime RA on complex systems.},
  file = {D\:\\GDrive\\zotero\\Toffalini et al\\toffalini_et_al_2019_scarr.pdf}
}

@article{toffaliniSnakeGXSneakyAttack,
  title = {{{SnakeGX}}: A Sneaky Attack against {{SGX Enclaves}}},
  author = {Toffalini, Flavio and Graziano, Mariano and Conti, Mauro and Zhou, Jianying},
  pages = {30},
  abstract = {Intel Software Guard eXtension (SGX) is a technology to create enclaves (i.e., trusted memory regions) hardware isolated from a compromised operating system. Recently, researchers showed that unprivileged adversaries can mount code-reuse attacks to steal secrets from enclaves. However, modern operating systems can use memory-forensic techniques to detect their traces. To this end, we propose SnakeGX, an approach that allows stealthier attacks with a minimal footprint; SnakeGX is a framework to implant a persistent backdoor in legitimate enclaves. Our solution encompasses a new architecture specifically designed to overcome the challenges SGX environments pose, while preserving their integrity and functionality. We thoroughly evaluate SnakeGX against StealthDB, which demonstrates the feasibility of our approach.},
  file = {D\:\\GDrive\\zotero\\Toﬀalini et al\\toﬀalini_et_al_snakegx.pdf},
  language = {en}
}

@inproceedings{togashiConcurrencyGoJava2014,
  title = {Concurrency in {{Go}} and {{Java}}: {{Performance}} Analysis},
  shorttitle = {Concurrency in {{Go}} and {{Java}}},
  booktitle = {2014 4th {{IEEE International Conference}} on {{Information Science}} and {{Technology}}},
  author = {Togashi, Naohiro and Klyuev, Vitaly},
  year = {2014},
  month = apr,
  pages = {213--216},
  issn = {2164-4357},
  doi = {10.1109/ICIST.2014.6920368},
  abstract = {Go is a new programming language developed by Google. Although it is still young compared to other programming languages, it already has modern and powerful features inherited from existing programming languages, and some of these are similar to Java. Go is designed for quick time development. Concurrency is the one of the main its features. In this paper, we analyze the performance of Go, and compare it with Java from two aspects: compile time and concurrency. There are many studies about the performance analysis and comparison of programming languages, but only a few publications investigate Go. Some of Go performance evaluation are based on the experimental release of Go. To analyze concurrency features, we implement simple matrix multiplication programs in both Go and Java. Java implementation uses Java Thread, and Go implementation uses Gor-outine and Channel. From the experiment, Go derived better performance than Java in both compile time and concurrency. Moreover, Go code shows the ease of concurrent programming. Go is still young, but we are convinced that Go will become the mainstream.},
  file = {D\:\\GDrive\\zotero\\Togashi_Klyuev\\togashi_klyuev_2014_concurrency_in_go_and_java.pdf},
  keywords = {Benchmark testing,Concurrency,Concurrent computing,Educational institutions,Evaluation,Go,Java,Message systems,Performance,Programming}
}

@article{tomasuloEfficientAlgorithmExploiting1967,
  title = {An {{Efficient Algorithm}} for {{Exploiting Multiple Arithmetics Unit}}},
  author = {Tomasulo, Robert M},
  year = {1967},
  volume = {11},
  pages = {25--33},
  file = {D\:\\GDrive\\zotero\\Tomasulo\\tomasulo_1967_an_efficient_algorithm_for_exploiting_multiple_arithmetics_unit.pdf},
  journal = {IBM Journal of research and Development},
  number = {1}
}

@misc{TopMostEffective2013,
  title = {The {{Top}} 3 {{Most Effective Ways}} to {{Take Notes While Reading}}},
  year = {2013},
  month = nov,
  abstract = {The most effective way to take notes while reading is to follow this simple three step process that I've tested and honed on over one thousand books.},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\CGRGPN55\\taking-notes-while-reading.html},
  howpublished = {https://fs.blog/2013/11/taking-notes-while-reading/},
  journal = {Farnam Street},
  language = {en-US}
}

@article{torresDissectingVideoServer2011,
  title = {Dissecting Video Server Selection Strategies in the {{YouTube CDN}}},
  author = {Torres, Ruben and Finamore, Alessandro and Kim, Jin Ryong and Mellia, Marco and Munaf{\`o}, Maurizio M. and Rao, Sanjay},
  year = {2011},
  pages = {248--257},
  publisher = {{IEEE}},
  doi = {10.1109/ICDCS.2011.43},
  abstract = {In this paper, we conduct a detailed study of the YouTube CDN with a view to understanding the mechanisms and policies used to determine which data centers users download video from. Our analysis is conducted using week-long datasets simultaneously collected from the edge of five networks - two university campuses and three ISP networks - located in three different countries. We employ state-of-the-art delay-based geolo-cation techniques to find the geographical location of YouTube servers. A unique aspect of our work is that we perform our analysis on groups of related YouTube flows. This enables us to infer key aspects of the system design that would be difficult to glean by considering individual flows in isolation. Our results reveal that while the RTT between users and data centers plays a role in the video server selection process, a variety of other factors may influence this selection including load-balancing, diurnal effects, variations across DNS servers within a network, limited availability of rarely accessed video, and the need to alleviate hot-spots that may arise due to popular video content. \textcopyright{} 2011 IEEE.},
  file = {D\:\\GDrive\\zotero\\Torres\\torres_2011_dissecting_video_server_selection_strategies_in_the_youtube_cdn.pdf},
  isbn = {9780769543642},
  journal = {Proceedings - International Conference on Distributed Computing Systems},
  keywords = {Content distribution networks,Web and internet services}
}

@inproceedings{toshniwalStormTwitter2014,
  title = {Storm @{{Twitter}}},
  booktitle = {Proceedings of the {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Toshniwal, Ankit and Taneja, Siddarth and Shukla, Amit and Ramasamy, Karthik and Patel, Jignesh M. and Kulkarni, Sanjeev and Jackson, Jason and Gade, Krishna and Fu, Maosong and Donham, Jake and Bhagat, Nikunj and Mittal, Sailesh and Ryaboy, Dmitriy},
  year = {2014},
  pages = {147--156},
  publisher = {{Association for Computing Machinery}},
  issn = {07308078},
  doi = {10.1145/2588555.2595641},
  abstract = {This paper describes the use of Storm at Twitter. Storm is a realtime fault-tolerant and distributed stream data processing system. Storm is currently being used to run various critical computations in Twitter at scale, and in real-time. This paper describes the architecture of Storm and its methods for distributed scale-out and fault-tolerance. This paper also describes how queries (aka. topologies) are executed in Storm, and presents some operational stories based on running Storm at Twitter. We also present results from an empirical evaluation demonstrating the resilience of Storm in dealing with machine failures. Storm is under active development at Twitter and we also present some potential directions for future work. \textcopyright{} 2014 ACM.},
  file = {D\:\\GDrive\\zotero\\Toshniwal\\toshniwal_2014_storm_@twitter.pdf},
  isbn = {978-1-4503-2376-5},
  keywords = {distributed systems}
}

@article{tramerStealingMachineLearning,
  title = {Stealing {{Machine Learning Models}} via {{Prediction APIs}}},
  author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  pages = {19},
  abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (``predictive analytics'') systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
  file = {D\:\\GDrive\\zotero\\Tramer et al\\tramer_et_al_stealing_machine_learning_models_via_prediction_apis.pdf},
  language = {en}
}

@incollection{tranExpressivenessReturnintolibcAttacks2011,
  title = {On the {{Expressiveness}} of {{Return}}-into-Libc {{Attacks}}},
  booktitle = {Recent {{Advances}} in {{Intrusion Detection}}},
  author = {Tran, Minh and Etheridge, Mark and Bletsch, Tyler and Jiang, Xuxian and Freeh, Vincent and Ning, Peng},
  editor = {Sommer, Robin and Balzarotti, Davide and Maier, Gregor},
  year = {2011},
  volume = {6961},
  pages = {121--141},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23644-0_7},
  abstract = {Return-into-libc (RILC) is one of the most common forms of code-reuse attacks. In this attack, an intruder uses a buffer overflow or other exploit to redirect control flow through existing (libc) functions within the legitimate program. While dangerous, it is generally considered limited in its expressive power since it only allows the attacker to execute straight-line code. In other words, RILC attacks are believed to be incapable of arbitrary computation\textemdash they are not Turing complete. Consequently, to address this limitation, researchers have developed other code-reuse techniques, such as return-oriented programming (ROP). In this paper, we make the counterargument and demonstrate that the original RILC technique is indeed Turing complete. Specifically, we present a generalized RILC attack called Turing complete RILC (TC-RILC) that allows for arbitrary computations. We demonstrate that TC-RILC satisfies formal requirements of Turing-completeness. In addition, because it depends on the well-defined semantics of libc functions, we also show that a TC-RILC attack can be portable between different versions (or even different families) of operating systems and naturally has negative implications for some existing anti-ROP defenses. The development of TC-RILC on both Linux and Windows platforms demonstrates the expressiveness and practicality of the generalized RILC attack.},
  file = {D\:\\GDrive\\zotero\\Tran et al\\tran_et_al_2011_on_the_expressiveness_of_return-into-libc_attacks.pdf},
  isbn = {978-3-642-23643-3 978-3-642-23644-0},
  language = {en}
}

@misc{TrendsCryptocurrenciesBlockchain,
  title = {Trends in Cryptocurrencies and Blockchain Technologies: A Monetary Theory and Regulation Perspective},
  file = {D\:\\GDrive\\zotero\\_\\trends_in_cryptocurrencies_and_blockchain_technologies.pdf}
}

@article{tripunitaraFoundationalWorkHarrisonruzzoullman2013,
  title = {The Foundational Work of Harrison-Ruzzo-Ullman Revisited},
  author = {Tripunitara, Mahesh V. and Li, Ninghui},
  year = {2013},
  volume = {10},
  pages = {28--39},
  issn = {15455971},
  doi = {10.1109/TDSC.2012.77},
  abstract = {The work by Harrison, Ruzzo, and Ullman (the HRU paper) on safety in the context of the access matrix model is widely considered to be foundational work in access control. In this paper, we address two errors we have discovered in the HRU paper. To our knowledge, these errors have not been previously reported in the literature. The first error regards a proof that shows that safety analysis for mono-operational HRU systems is in bf NP. The error stems from a faulty assumption that such systems are monotonic for the purpose of safety analysis. We present a corrected proof in this paper. The second error regards a mapping from one version of the safety problem to another that is presented in the HRU paper. We demonstrate that the mapping is not a reduction, and present a reduction that enables us to infer that the second version of safety introduced in the HRU paper is also undecidable for the HRU scheme. These errors lead us to ask whether the notion of safety as defined in the HRU paper is meaningful. We introduce other notions of safety that we argue have more intuitive appeal, and present the corresponding safety analysis results for the HRU scheme. \textcopyright{} 2004-2012 IEEE.},
  file = {D\:\\GDrive\\zotero\\Tripunitara\\tripunitara_2013_the_foundational_work_of_harrison-ruzzo-ullman_revisited.pdf},
  journal = {IEEE Transactions on Dependable and Secure Computing},
  keywords = {Access control,computational complexity,reducibility and completeness},
  number = {1}
}

@techreport{tschorschBitcoinTechnicalSurvey,
  title = {Bitcoin and {{Beyond}}: {{A Technical Survey}} on {{Decentralized Digital Currencies}}},
  author = {Tschorsch, Florian and Scheuermann, Bj{\"o}rn},
  abstract = {Besides attracting a billion dollar economy, Bitcoin revolutionized the field of digital currencies and influenced many adjacent areas. This also induced significant scientific interest. In this survey, we unroll and structure the manyfold results and research directions. We start by introducing the Bitcoin protocol and its building blocks. From there we continue to explore the design space by discussing existing contributions and results. In the process, we deduce the fundamental structures and insights at the core of the Bitcoin protocol and its applications. As we show and discuss, many key ideas are likewise applicable in various other fields, so that their impact reaches far beyond Bitcoin itself.},
  file = {D\:\\GDrive\\zotero\\Tschorsch\\tschorsch_bitcoin_and_beyond.pdf}
}

@techreport{tsipenyukSevenPerniciousKingdoms2005,
  title = {Seven {{Pernicious Kingdoms}}: {{A Taxonomy}} of {{Software Security Errors}}},
  author = {Tsipenyuk, Katrina and Chess, Brian and Mcgraw, Gary},
  year = {2005},
  abstract = {We want to help developers and security practitioners understand common types of coding errors that lead to vulnerabilities. By organizing these errors into a simple taxonomy, we can teach developers to recognize categories of problems that lead to vulnerabilities and identify existing errors as they build software. The information contained in our taxonomy is most effectively enforced via a tool. In fact, all of the errors included in our taxonomy are amenable to automatic identification using static source code analysis techniques. We demonstrate why our taxonomy is not only simpler, but also more comprehensive than other modern taxonomy proposals and vulnerability lists. We provide an in-depth explanation and one or more code-level examples for each of the errors on a companion web site: http://vulncat.fortifysoftware.com.},
  file = {D\:\\GDrive\\zotero\\Tsipenyuk\\tsipenyuk_seven_pernicious_kingdoms.pdf},
  keywords = {authentication,cryptographic controls,D46 [Operating Systems]: Security and Protection-access controls,information flow controls,invasive software,invasive software K65 [Management of Computing and Information Systems]: Security and Protection-authentication,security defects,standardization Keywords Software security,static analysis tools,taxonomy,unauthorized access General Terms Security}
}

@article{tsunooCryptanalysisImplementedComputers2003,
  title = {Cryptanalysis of {{DES}} Implemented on Computers with Cache},
  author = {Tsunoo, Yukiyasu and Saito, Teruo and Suzaki, Tomoyasu and Shigeri, Maki and Miyauchi, Hiroshi},
  year = {2003},
  volume = {2779},
  pages = {62--76},
  issn = {03029743},
  doi = {10.1007/978-3-540-45238-6_6},
  abstract = {This paper presents the results of applying an attack against the Data Encryption Standard (DES) implemented in some applications, using side-channel information based on CPU delay as proposed in [11]. This cryptanalysis technique uses side-channel information on encryption processing to select and collect effective plaintexts for cryptanalysis, and infers the information on the expanded key from the collected plaintexts. On applying this attack, we found that the cipher can be broken with 223 known plaintexts and 224 calculations at a success rate {$>$} 90\%, using a personal computer with 600-MHz Pentium III. We discuss the feasibility of cache attack on ciphers that need many S-box look-ups, through reviewing the results of our experimental attacks on the block ciphers excluding DES, such as AES. \textcopyright{} Springer-Verlag Berlin Heidelberg 2003.},
  file = {D\:\\GDrive\\zotero\\Tsunoo\\tsunoo_2003_cryptanalysis_of_des_implemented_on_computers_with_cache.pdf},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {AES,Cache,Camellia,DES,Side-channel,Timing attacks}
}

@misc{Turingawardlecture,
  title = {Turing-Award-Lecture},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\LB7KV6RN\\turing-award-lecture.ppt}
}

@techreport{TypesProgrammingReasoning,
  title = {Types for {{Programming}} and {{Reasoning}}},
  file = {D\:\\GDrive\\zotero\\undefined\\types_for_programming_and_reasoning.pdf}
}

@techreport{tzanetakisMARSYASFrameworkAudio,
  title = {{{MARSYAS}}: {{A}} Framework for Audio Analysis},
  author = {Tzanetakis, George and Cook, Perry},
  abstract = {Existing audio tools handle the increasing amount of computer audio data inadequately. The typical tape-recorder paradigm for audio interfaces is inflexible and time consuming, especially for large data sets. On the other hand, completely automatic audio analysis and annotation is impossible using current techniques. Alternative solutions are semi-automatic user interfaces that let users interact with sound in flexible ways based on content. This approach offers significant advantages over manual browsing, annotation and retrieval. Furthermore , it can be implemented using existing techniques for audio content analysis in restricted domains. This paper describes MARSYAS, a framework for experimenting, evaluating and integrating such techniques. As a test for the architecture, some recently proposed techniques have been implemented and tested. In addition , a new method for temporal segmentation based on audio texture is described. This method is combined with audio analysis techniques and used for hierarchical browsing, classification and annotation of audio files.},
  file = {D\:\\GDrive\\zotero\\Tzanetakis\\tzanetakis_marsyas.pdf}
}

@article{umerGeneratingInvariantsUsing2020,
  title = {Generating Invariants Using Design and Data-Centric Approaches for Distributed Attack Detection},
  author = {Umer, Muhammad Azmi and Mathur, Aditya and Junejo, Khurum Nazir and Adepu, Sridhar},
  year = {2020},
  volume = {28},
  pages = {100341},
  publisher = {{Elsevier B.V.}},
  issn = {18745482},
  doi = {10.1016/j.ijcip.2020.100341},
  abstract = {A cyber attack launched on a critical infrastructure (CI), such as a power grid or a water treatment plant, could lead to anomalous behavior. There exist several methods to detect such behavior. This paper reports on a study conducted to compare two methods for detecting anomalies in CI. One of these methods, referred to as design-centric, generates invariants from the design of a CI. Another method, referred to as data-centric, generates the invariants from data collected from an operational CI. The key question that motivated the study is ``How do design and data-centric methods compare in the effectiveness of the generated invariants in detecting process anomalies.'' The data-centric approach used Association Rule Mining for generating invariants from operational data. These invariants, and their performance in detecting anomalies, was compared against those generated by a design-centric approach reported in the literature. The entire study was conducted in the context of an operational scaled down version of a water treatment plant.},
  file = {D\:\\GDrive\\zotero\\Umer\\umer_2020_generating_invariants_using_design_and_data-centric_approaches_for_distributed.pdf},
  journal = {International Journal of Critical Infrastructure Protection},
  keywords = {Association rule mining,Critical Infrastructure,Cyber-physical attacks,Distributed attack detection,Machine learning,SCADA security,Water treatment plant}
}

@article{umerIntegratingDesignData2017,
  title = {Integrating Design and Data Centric Approaches to Generate Invariants for Distributed Attack Detection},
  author = {Umer, Muhammad Azmi and Mathur, Aditya and Junejo, Khurum Nazir and Adepu, Sridhar},
  year = {2017},
  pages = {131--136},
  doi = {10.1145/3140241.3140248},
  abstract = {Process anomaly is used for detecting cyber-physical attacks on critical infrastructure such as plants for water treatment and electric power generation. Identification of process anomaly is possible using rules that govern the physical and chemical behavior of the process within a plant. These rules, often referred to as invariants, can be derived either directly from plant design or from the data generated in an operational. However, for operational legacy plants, one might consider a data-centric approach for the derivation of invariants. The study reported here is a comparison of designcentric and data-centric approaches to derive process invariants. The study was conducted using the design of, and the data generated from, an operational water treatment plant. The outcome of the study supports the conjecture that neither approach is adequate in itself, and hence, the two ought to be integrated.},
  file = {D\:\\GDrive\\zotero\\Umer\\umer_2017_integrating_design_and_data_centric_approaches_to_generate_invariants_for.pdf},
  isbn = {9781450353946},
  journal = {CPS-SPC 2017 - Proceedings of the 2017 Workshop on Cyber-Physical Systems Security and PrivaCy, co-located with CCS 2017},
  keywords = {Association rule mining,Critical Infrastructure,Cyber-physical attacks,Distributed attack detection,Machine learning,Water treatment plant}
}

@article{ungarSelfPowerSimplicity1987,
  title = {Self: {{The Power}} of {{Simplicity}}},
  author = {Ungar, David and Smith, Randall B},
  year = {1987},
  pages = {16},
  abstract = {Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scopingin conventional languages. Finally, becauseSelf does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self''s simplicity and expressivenessoffer new insights into object-oriented computation.},
  file = {D\:\\GDrive\\zotero\\Ungar_Smith\\ungar_smith_1987_self.pdf},
  language = {en}
}

@inproceedings{ungerSoKSecureMessaging2015,
  title = {{{SoK}}: {{Secure Messaging}}},
  shorttitle = {{{SoK}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Unger, Nik and Dechand, Sergej and Bonneau, Joseph and Fahl, Sascha and Perl, Henning and Goldberg, Ian and Smith, Matthew},
  year = {2015},
  month = may,
  pages = {232--249},
  publisher = {{IEEE}},
  address = {{San Jose, CA, USA}},
  doi = {10.1109/SP.2015.22},
  abstract = {Motivated by recent revelations of widespread state surveillance of personal communication, many products now claim to offer secure and private messaging. This includes both a large number of new projects and many widely adopted tools that have added security features. The intense pressure in the past two years to deliver solutions quickly has resulted in varying threat models, incomplete objectives, dubious security claims, and a lack of broad perspective on the existing cryptographic literature on secure communication.},
  file = {D\:\\GDrive\\zotero\\Unger et al\\unger_et_al_2015_sok.pdf},
  isbn = {978-1-4673-6949-7},
  language = {en}
}

@techreport{UnreasonableEffectivenessData2009,
  title = {The {{Unreasonable Effectiveness}} of {{Data}}},
  year = {2009},
  abstract = {such as f = ma or e = mc 2. Meanwhile, sciences that involve human beings rather than elementary particles have proven more resistant to elegant mathematics. Economists suffer from physics envy over their inability to neatly model human behavior. An informal, incomplete grammar of the English language runs over 1,700 pages. 2 Perhaps when it comes to natural language processing and related fi elds, we're doomed to complex theories that will never have the elegance of physics equations. But if that's so, we should stop acting as if our goal is to author extremely elegant theories, and instead embrace complexity and make use of the best ally we have: the unreasonable effectiveness of data. One of us, as an undergraduate at Brown University , remembers the excitement of having access to the Brown Corpus, containing one million English words. 3 Since then, our fi eld has seen several notable corpora that are about 100 times larger, and in 2006, Google released a trillion-word corpus with frequency counts for all sequences up to fi ve words long. 4 In some ways this corpus is a step backwards from the Brown Corpus: it's taken from unfi ltered Web pages and thus contains incomplete sentences, spelling errors , grammatical errors, and all sorts of other errors. It's not annotated with carefully hand-corrected part-of-speech tags. But the fact that it's a million times larger than the Brown Corpus outweighs these drawbacks. A trillion-word corpus-along with other Web-derived corpora of millions, billions, or trillions of links, videos, images, tables, and user interactions captures even very rare aspects of human behavior. So, this corpus could serve as the basis of a complete model for certain tasks-if only we knew how to extract the model from the data.},
  file = {D\:\\GDrive\\zotero\\undefined\\2009_the_unreasonable_effectiveness_of_data.pdf}
}

@article{urbinaAttackingFieldbusCommunications2016,
  title = {Attacking Fieldbus Communications in {{ICS}}: {{Applications}} to the {{SWaT}} Testbed},
  author = {Urbina, David and Giraldo, Jairo and Tippenhauer, Nils Ole and Cardenas, Alvaro},
  year = {2016},
  volume = {14},
  pages = {75--89},
  issn = {18798101},
  doi = {10.3233/978-1-61499-617-0-75},
  abstract = {The study of cyber-attacks in industrial control systems is of growing interest among the research community. Nevertheless, restricted access to real industrial control systems that can be used to test attacks has limited the study of their implementation and potential impact. In this work, we discuss practical attacks applied to a room-sized water treatment testbed. The testbed includes a complete physical process, industrial communication systems, and supervisory controls. We implement scenarios in which the attacker manipulates or replaces sensor data as reported from the field devices to the control components. As a result, the attacker can change the system state vector as perceived by the controls, which will cause incorrect control decisions and potential catastrophic failures. We discuss practical challenges in setting up Man-In-The-Middle attacks on fieldbus communications in the industrial EtherNet/IP protocol and topologies such as Ethernet rings using the Device-Level-Ring protocol. We show how the attacker can overcome those challenges, and insert herself into the ring. Once established as a Man-in-the-Middle attacker, we launched a range of attacks to modify sensor measurements and manipulate actuators. We show the efficacy of the proposed methodology in two experimental examples, where an adversary can intelligently design attacks that remain undetected for a typical bad-data detection mechanism.},
  file = {D\:\\GDrive\\zotero\\Urbina\\urbina_2016_attacking_fieldbus_communications_in_ics.pdf},
  isbn = {9781614996163},
  journal = {Cryptology and Information Security Series},
  keywords = {Critical system,Cyber-attacks,Fieldbus,ICS}
}

@article{usenixassociationSpartanDistributedArray2004,
  title = {Spartan: {{A Distributed Array Framework}} with {{Smart Tiling}}},
  author = {{USENIX Association} and {USENIX Annual Technical Conference (2004.06.30-07.02 Boston}, Mass.)},
  year = {2004},
  file = {D\:\\GDrive\\zotero\\USENIX Association\\usenix_association_2004_spartan.pdf}
}

@article{usenixassociationTieredReplicationCosteffective2004,
  title = {Tiered {{Replication}}: {{A Cost}}-Effective {{Alternative}} to {{FullCluster Geo}}-Replication},
  author = {{USENIX Association} and {USENIX Annual Technical Conference (2004.06.30-07.02 Boston}, Mass.)},
  year = {2004},
  file = {D\:\\GDrive\\zotero\\USENIX Association\\usenix_association_2004_tiered_replication.pdf}
}

@techreport{UsingEncryptionAuthentication1978,
  title = {Using {{Encryption}} for {{Authentication}} in {{Large Networks}} of {{Computers}}},
  year = {1978},
  abstract = {R o g e r M. N e e d h a m a n d M i c h a e l D. S c h r o e d e r X e r o x P a l o A l t o R e s e a r c h C e n t e r Use of encryption to achieve authenticated communication in computer networks is discussed. Example protocols are presented for the establishment of authenticated connections, for the management of authenticated mail, and for signature verification and document integrity guarantee. Both conventional and public-key encryption algorithms are considered as the basis for protocols.},
  file = {D\:\\GDrive\\zotero\\undefined\\1978_using_encryption_for_authentication_in_large_networks_of_computers.pdf}
}

@techreport{valentaSearchCurveSwapMeasuring2018,
  title = {In Search of {{CurveSwap}}: {{Measuring}} Elliptic Curve Implementations in the Wild},
  author = {Valenta, Luke and Sullivan, Nick and Sanso, Antonio and Heninger, Nadia},
  year = {2018},
  abstract = {We survey elliptic curve implementations from several vantage points. We perform internet-wide scans for TLS on a large number of ports, as well as SSH and IPsec to measure elliptic curve support and implementation behaviors, and collect passive measurements of client curve support for TLS. We also perform active measurements to estimate server vulnerability to known attacks against elliptic curve implementations, including support for weak curves, invalid curve attacks, and curve twist attacks. We estimate that 0.77\% of HTTPS hosts, 0.04\% of SSH hosts, and 4.04\% of IKEv2 hosts that support elliptic curves do not perform curve validity checks as specified in elliptic curve standards. We describe how such vulnerabilities could be used to construct an elliptic curve parameter downgrade attack called CurveSwap for TLS, and observe that there do not appear to be combinations of weak behaviors we examined enabling a feasible CurveSwap attack in the wild. We also analyze source code for elliptic curve implementations, and find that a number of libraries fail to perform point validation for JSON Web Encryption, and find a flaw in the Java and NSS multiplication algorithms.},
  file = {D\:\\GDrive\\zotero\\Valenta et al\\valenta_et_al_2018_in_search_of_curveswap.pdf}
}

@techreport{vancuijkTimingAttacksRSA2009,
  title = {Timing {{Attacks}} on {{RSA}}},
  author = {Van Cuijk, Mark},
  year = {2009},
  abstract = {In general, timing attacks are used to analyze differences in execution time that result from differences in input parameters of a cryptographic algorithm. These timing differences are often caused by optimizing algorithm implementations, but they may leak information about the input parameters. Using a timing attack, an adversary hopes to find secret information, like bits from a secret RSA exponent. This paper summarizes several algorithms used in RSA implementations and how timing attacks can be used to reconstruct the entire secret RSA exponent.},
  file = {D\:\\GDrive\\zotero\\Van Cuijk\\van_cuijk_2009_timing_attacks_on_rsa.pdf}
}

@inproceedings{vandenhooffVuvuzelaScalablePrivate2015,
  title = {Vuvuzela: Scalable Private Messaging Resistant to Traffic Analysis},
  shorttitle = {Vuvuzela},
  booktitle = {Proceedings of the 25th {{Symposium}} on {{Operating Systems Principles}}},
  author = {{van den Hooff}, Jelle and Lazar, David and Zaharia, Matei and Zeldovich, Nickolai},
  year = {2015},
  month = oct,
  pages = {137--152},
  publisher = {{ACM}},
  address = {{Monterey California}},
  doi = {10.1145/2815400.2815417},
  abstract = {Private messaging over the Internet has proven challenging to implement, because even if message data is encrypted, it is difficult to hide metadata about who is communicating in the face of traffic analysis. Systems that offer strong privacy guarantees, such as Dissent [36], scale to only several thousand clients, because they use techniques with superlinear cost in the number of clients (e.g., each client broadcasts their message to all other clients). On the other hand, scalable systems, such as Tor, do not protect against traffic analysis, making them ineffective in an era of pervasive network monitoring. Vuvuzela is a new scalable messaging system that offers strong privacy guarantees, hiding both message data and metadata. Vuvuzela is secure against adversaries that observe and tamper with all network traffic, and that control all nodes except for one server. Vuvuzela's key insight is to minimize the number of variables observable by an attacker, and to use differential privacy techniques to add noise to all observable variables in a way that provably hides information about which users are communicating. Vuvuzela has a linear cost in the number of clients, and experiments show that it can achieve a throughput of 68,000 messages per second for 1 million users with a 37-second end-to-end latency on commodity servers.},
  file = {D\:\\GDrive\\zotero\\van den Hooff et al\\van_den_hooff_et_al_2015_vuvuzela.pdf},
  isbn = {978-1-4503-3834-9},
  language = {en}
}

@techreport{vanderveenMemoryErrorsPresent2012,
  title = {Memory {{Errors}}: {{The Past}}, the {{Present}}, and the {{Future}}},
  author = {Van Der Veen, Victor and {Dutt-Sharma}, Nitish and Cavallaro, Lorenzo and Bos, Herbert},
  year = {2012},
  abstract = {Memory error exploitations have been around for over 25 years and still rank among the top 3 most dangerous software errors. Why haven't we been able to stop them? Given the host of security measures on modern machines, are we less vulnerable than before, and can we expect to eradicate memory error problems in the near future? In this paper, we present a quarter century worth of memory errors: attacks, defenses, and statistics. A historical overview provides insights in past trends and developments, while an investigation of real-world vulnerabilities and exploits allows us to answer on the significance of memory errors in the foreseeable future.},
  file = {D\:\\GDrive\\zotero\\Van Der Veen et al\\van_der_veen_et_al_2012_memory_errors.pdf}
}

@book{vandongenLaTeXFriends2012,
  title = {{{LaTeX}} and {{Friends}}},
  author = {{van Dongen}, M. R. C.},
  year = {2012},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23816-1},
  file = {D\:\\GDrive\\zotero\\van Dongen\\van_dongen_2012_latex_and_friends.pdf},
  isbn = {978-3-642-23815-4 978-3-642-23816-1},
  language = {en},
  series = {X.Media.Publishing}
}

@inproceedings{vanhoefKeyReinstallationAttacks2017,
  title = {Key Reinstallation Attacks: {{Forcing}} Nonce {{Reuse}} in {{WPA2}}},
  booktitle = {Proceedings of the {{ACM Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Vanhoef, Mathy and Piessens, Frank},
  year = {2017},
  month = oct,
  pages = {1313--1328},
  publisher = {{Association for Computing Machinery}},
  issn = {15437221},
  doi = {10.1145/3133956.3134027},
  abstract = {We introduce the key reinstallation attack. This attack abuses design or implementation flaws in cryptographic protocols to reinstall an already-in-use key. This resets the key's associated parameters such as transmit nonces and receive replay counters. Several types of cryptographic Wi-Fi handshakes are affected by the attack. All protected Wi-Fi networks use the 4-way handshake to gen-erate a fresh session key. So far, this 14-year-old handshake has remained free from attacks, and is even proven secure. However, we show that the 4-way handshake is vulnerable to a key reinstalla-tion attack. Here, the adversary tricks a victim into reinstalling an already-in-use key. This is achieved by manipulating and replaying handshake messages. When reinstalling the key, associated param-eters such as the incremental transmit packet number (nonce) and receive packet number (replay counter) are reset to their initial value. Our key reinstallation attack also breaks the PeerKey, group key, and Fast BSS Transition (FT) handshake. The impact depends on the handshake being attacked, and the data-confidentiality pro-tocol in use. Simplified, against AES-CCMP an adversary can replay and decrypt (but not forge) packets. This makes it possible to hijack TCP streams and inject malicious data into them. Against WPA-TKIP and GCMP the impact is catastrophic: packets can be replayed, decrypted, and forged. Because GCMP uses the same authentication key in both communication directions, it is especially affected. Finally, we confirmed our findings in practice, and found that every Wi-Fi device is vulnerable to some variant of our attacks. Notably, our attack is exceptionally devastating against Android 6.0: it forces the client into using a predictable all-zero encryption key.},
  file = {D\:\\GDrive\\zotero\\Vanhoef\\vanhoef_2017_key_reinstallation_attacks.pdf},
  isbn = {978-1-4503-4946-8},
  keywords = {Attacks,Handshake,Initialization Vector,Key Reinstallation,Network Security,Nonce Reuse,Packet Number,Security Protocols,WPA2}
}

@techreport{vanrenesseByzantineChainReplication,
  title = {Byzantine {{Chain Replication}}},
  author = {Van Renesse, Robbert and Ho, Chi and Schiper, Nicolas},
  abstract = {We present a new class of Byzantine-tolerant State Machine Replication protocols for asynchronous environments that we term Byzan-tine Chain Replication. We demonstrate two implementations that present different trade-offs between performance and security, and compare these with related work. Leveraging an external reconfiguration service, these protocols are not based on Byzantine consensus, do not require majority-based quorums during normal operation, and the set of replicas is easy to reconfigure. One of the implementations is instantiated with t + 1 replicas to tolerate t failures and is useful in situations where perimeter security makes malicious attacks unlikely. Applied to in-memory BerkeleyDB replication, it supports 20,000 transactions per second while a fully Byzantine implementation supports 12,000 transactions per second-about 70\% of the throughput of a non-replicated database.},
  file = {D\:\\GDrive\\zotero\\Van Renesse\\van_renesse_byzantine_chain_replication.pdf}
}

@techreport{vanrenesseChainReplicationSupporting,
  title = {Chain {{Replication}} for {{Supporting High Throughput}} and {{Availability}}},
  author = {Van Renesse, Robbert and Schneider, Fred B},
  abstract = {Chain replication is a new approach to coordinating clusters of fail-stop storage servers. The approach is intended for supporting large-scale storage services that exhibit high throughput and availability without sacrificing strong consistency guarantees. Besides outlining the chain replication protocols themselves , simulation experiments explore the performance characteristics of a prototype implementation. Throughput, availability, and several object-placement strategies (including schemes based on distributed hash table routing) are discussed.},
  file = {D\:\\GDrive\\zotero\\Van Renesse\\van_renesse_chain_replication_for_supporting_high_throughput_and_availability.pdf}
}

@techreport{vanrenesseGossipStyleFailureDetection,
  title = {A {{Gossip}}-{{Style Failure Detection Service}}},
  author = {Van Renesse, Robbert and Minsky, Yaron and Hayden, Mark},
  abstract = {Failure Detection is valuable for system management, replication, load balancing, and other distributed services. To date, Failure Detection Services scale badly in the number of members that are being monitored. This paper describes a new protocol based on gossiping that does scale well and provides timely detection. We analyze the protocol, and then extend it to discover and leverage the underlying network topology for much improved resource utilization. We then combine it with another protocol, based on broadcast, that is used to handle partition failures.},
  file = {D\:\\GDrive\\zotero\\Van Renesse\\van_renesse_a_gossip-style_failure_detection_service.pdf}
}

@techreport{vanrenessePaxosMadeModerately2011,
  title = {Paxos {{Made Moderately Complex}}},
  author = {Van Renesse, Robbert},
  year = {2011},
  abstract = {For anybody who has ever tried to implement it, Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. This paper provides imperative pseudo-code for the full Paxos (or Multi-Paxos) protocol without shying away from discussing various implementation details. The initial description avoids optimizations that complicate comprehension. Next we discuss liveness, and list various optimizations that make the protocol practical .},
  file = {D\:\\GDrive\\zotero\\Van Renesse\\van_renesse_2011_paxos_made_moderately_complex.pdf}
}

@techreport{vanschieIntroductionImplementationResults2008a,
  title = {Introduction {{Implementation Results Conclusion Compiling Haskell}} to {{LLVM}}},
  author = {Van Schie, John},
  year = {2008},
  file = {D\:\\GDrive\\zotero\\Van Schie\\van_schie_2008_introduction_implementation_results_conclusion_compiling_haskell_to_llvm.pdf;D\:\\GDrive\\zotero\\Van Schie\\van_schie_2008_introduction_implementation_results_conclusion_compiling_haskell_to_llvm2.pdf}
}

@techreport{vapnikOverviewStatisticalLearning1999,
  title = {An {{Overview}} of {{Statistical Learning Theory}}},
  author = {Vapnik, Vladimir N},
  year = {1999},
  volume = {10},
  abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).},
  file = {D\:\\GDrive\\zotero\\Vapnik\\vapnik_1999_an_overview_of_statistical_learning_theory.pdf},
  journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS},
  number = {5}
}

@techreport{varianOnlineAdAuctions2009,
  title = {Online {{Ad Auctions}}},
  author = {Varian, Hal R},
  year = {2009},
  abstract = {I describe how search engines sell ad space using an auction. I analyze advertiser behavior in this context using elementary price theory and derive a simple way to estimate the producer surplus generated by online search advertising. It appears that the estimated value of online advertising tends to be between 2 and 2.3 times advertising expenditures. JEL: D44 (Auctions), D21 (Firm Behavior) Keywords: Auctions, Online advertising The ad auctions used by major search engines all have a similar structure. Advertisers enter ad text, keywords and bids into the system. When a user sends a query to the search engine, the system finds a set of ads with keywords that match the query and determines which ads to show and where to show them. When the search results and ads are displayed, the user may click on an ad for further information. In this case, the advertiser pays the search engine an amount determined by the bids of the other competing advertisers. The expected revenue received by the search engine is the price per click times the expected number of clicks. In general, the search engine would like to sell the most prominent positions-those most likely to receive clicks-to those ads that have the highest expected revenue. To accomplish this, the ads are ranked by bid times expected click-through rates and those ads with the highest expected revenue are shown in the most prominent positions. It is natural to suppose that users who have positive experiences from ad clicks will increase their propensity to click on ads in the future, and those with negative experiences will tend to decrease their propensity to click in the future. Hence search engines may also consider various measures of "ad quality" in their choice of which ads to display. I. Auction rules How do search engines determine which ads are shown, where they are shown, and how much they pay per click? We start with some notation. Let a = 1,. .. , A index advertisers and s = 1,. .. , S index slots. Let (v a , b a , p a) be the value, bid, and price per click of advertiser a for a particular keyword. We assume that the expected clickthrough rate of advertiser a in slot s (z as) can be written as the product of an ad-specific effect (e a) and a position-specific effect (x s), so we write z as = e a x s. Other formulas for predicting clicks could be used, but this one leads to particularly simple results. Here are the rules of the Generalized Second Price Auction used by the major search engines. (1) Each advertiser a chooses a bid b a. (2) The advertisers are ordered by bid times predicted clickthrough rate (b a e a). (3) The price that advertiser a pays for a click is the minimum necessary to retain its position. (4) If there are fewer bidders than slots, the last bidder pays a reserve price r.},
  file = {D\:\\GDrive\\zotero\\Varian\\varian_2009_online_ad_auctions.pdf}
}

@techreport{vasinBlackCoinProofofStakeProtocol,
  title = {{{BlackCoin}}'s {{Proof}}-of-{{Stake Protocol}} V2},
  author = {Vasin, Pavel and Co, Blackcoin},
  abstract = {The current Proof of Stake protocol has several potential security issues: coin age can be abused by malicious nodes to gain significant network weight to perform a successful double spend. Additionally, due to coin age, honest nodes can abuse the system by staking only on a periodical basis. This does not secure the network. Lastly: in the current system all components of a stake of proof are predictable enough to allow pre-computation of future proof-of-stakes. In this paper a system is proposed to solve said issues.}
}

@inproceedings{vattikondaEliminatingFineGrained2011,
  title = {Eliminating Fine Grained Timers in {{Xen}}},
  booktitle = {Proceedings of the 3rd {{ACM}} Workshop on {{Cloud}} Computing Security Workshop - {{CCSW}} '11},
  author = {Vattikonda, Bhanu C. and Das, Sambit and Shacham, Hovav},
  year = {2011},
  pages = {41},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/2046660.2046671},
  abstract = {The move to ``infrastructure-as-a-service'' cloud computing brings with it a new risk: cross-virtual machine side channels through shared physical resources such as the L2 cache. One approach to this risk is to rewrite sensitive code to eliminate the signal. In this paper we consider another approach: weakening malicious virtual machines' ability to receive the signal by eliminating fine-grained timers. Such ``fuzzy time'' was implemented in 1991 in the VAX security kernel, but it was not clearly applicabile to modern virtual machine managers such as Xen on platforms such as the x86, which exports a cycle counter through the RDTSC instruction.},
  file = {D\:\\GDrive\\zotero\\Vattikonda et al\\vattikonda_et_al_2011_eliminating_fine_grained_timers_in_xen.pdf},
  isbn = {978-1-4503-1004-8},
  language = {en}
}

@inproceedings{vavilapalliApacheHadoopYARN2013,
  title = {Apache Hadoop {{YARN}}: {{Yet}} Another Resource Negotiator},
  booktitle = {Proceedings of the 4th {{Annual Symposium}} on {{Cloud Computing}}, {{SoCC}} 2013},
  author = {Vavilapalli, Vinod Kumar and Murthy, Arun C. and Douglas, Chris and Agarwal, Sharad and Konar, Mahadev and Evans, Robert and Graves, Thomas and Lowe, Jason and Shah, Hitesh and Seth, Siddharth and Saha, Bikas and Curino, Carlo and O'Malley, Owen and Radia, Sanjay and Reed, Benjamin and Baldeschwieler, Eric},
  year = {2013},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/2523616.2523633},
  abstract = {The initial design of Apache Hadoop [1] was tightly fo-cused on running massive, MapReduce jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agor\'a \textemdash the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its in-tended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the re-source management infrastructure, forcing developers to abuse the MapReduce programming model, and 2) cen-tralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler. In this paper, we summarize the design, development, and current state of deployment of the next genera-tion of Hadoop's compute platform: YARN. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running YARN on production environ-ments (including 100\% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several},
  file = {D\:\\GDrive\\zotero\\Kumar Vavilapalli\\kumar_vavilapalli_2013_apache_hadoop_yarn.pdf;D\:\\GDrive\\zotero\\Vavilapalli\\vavilapalli_2013_apache_hadoop_yarn.pdf},
  isbn = {978-1-4503-2428-1}
}

@inproceedings{vekrisRefinementTypesTypeScript2016,
  title = {Refinement Types for {{TypeScript}}},
  booktitle = {Proceedings of the 37th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Vekris, Panagiotis and Cosman, Benjamin and Jhala, Ranjit},
  year = {2016},
  month = jun,
  pages = {310--325},
  publisher = {{ACM}},
  address = {{Santa Barbara CA USA}},
  doi = {10.1145/2908080.2908110},
  file = {D\:\\GDrive\\zotero\\Vekris et al\\vekris_et_al_2016_refinement_types_for_typescript.pdf;D\:\\GDrive\\zotero\\Vekris et al\\vekris_et_al_2016_refinement_types_for_typescript2.pdf},
  isbn = {978-1-4503-4261-2},
  language = {en}
}

@article{venetPreciseEfficientStatic2004,
  title = {Precise and Efficient Static Array Bound Checking for Large Embedded {{C}} Programs},
  author = {Venet, Arnaud and Brat, Guillaume},
  year = {2004},
  volume = {1},
  pages = {231--242},
  doi = {10.1145/996841.996869},
  abstract = {In this paper we describe the design and implementation of a static array-bound checker for a family of embedded programs: the flight control software of recent Mars missions. These codes are large (up to 280 KLOC), pointer intensive, heavily multithreaded and written in an object-oriented style, which makes their analysis very challenging. We designed a tool called C Global Surveyor (CGS) that can analyze the largest code in a couple of hours with a precision of 80\%. The scalability and precision of the analyzer are achieved by using an incremental framework in which a pointer analysis and a numerical analysis of array indices mutually refine each other. CGS has been designed so that it can distribute the analysis over several processors in a cluster of machines. To the best of our knowledge this is the first distributed implementation of static analysis algorithms. Throughout the paper we will discuss the scalability setbacks that we encountered during the construction of the tool and their impact on the initial design decisions.},
  isbn = {1581138075},
  journal = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  keywords = {Abstract interpretation,Array-bound checking,Difference-bound matrices,Pointer analysis,Program verification}
}

@article{venkatHIPStRHeterogeneousISAProgram2016,
  title = {{{HIPStR}} \textendash{} {{Heterogeneous}}-{{ISA Program State Relocation}}},
  author = {Venkat, Ashish and Shamasunder, Sriskanda and Shacham, Hovav and Tullsen, Dean M},
  year = {2016},
  pages = {15},
  abstract = {Heterogeneous Chip Multiprocessors have been shown to provide significant performance and energy efficiency gains over homogeneous designs. Recent research has expanded the dimensions of heterogeneity to include diverse Instruction Set Architectures, called Heterogeneous-ISA Chip Multiprocessors. This work leverages such an architecture to realize substantial new security benefits, and in particular, to thwart Return-Oriented Programming. This paper proposes a novel security defense called HIPStR \textendash{} Heterogeneous-ISA Program State Relocation \textendash{} that performs dynamic randomization of run-time program state, both within and across ISAs. This technique outperforms the state-of-the-art just-intime code reuse (JIT-ROP) defense by an average of 15.6\%, while simultaneously providing greater security guarantees against classic return-into-libc, ROP, JOP, brute force, JITROP, and several evasive variants.},
  file = {D\:\\GDrive\\zotero\\Venkat et al\\venkat_et_al_2016_hipstr_–_heterogeneous-isa_program_state_relocation.pdf},
  language = {en}
}

@inproceedings{vermaLargescaleClusterManagement2015,
  title = {Large-Scale Cluster Management at {{Google}} with {{Borg}}},
  booktitle = {Proceedings of the 10th {{European Conference}} on {{Computer Systems}}, {{EuroSys}} 2015},
  author = {Verma, Abhishek and Pedrosa, Luis and Korupolu, Madhukar and Oppenheimer, David and Tune, Eric and Wilkes, John},
  year = {2015},
  month = apr,
  publisher = {{Association for Computing Machinery, Inc}},
  doi = {10.1145/2741948.2741964},
  abstract = {Google's Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior. We present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.},
  isbn = {978-1-4503-3238-5}
}

@article{viegaStateEmbeddeddeviceSecurity2012,
  title = {The State of Embedded-Device Security ({{Spoiler Alert}}: {{It}}'s {{Bad}})},
  author = {Viega, John and Thompson, Hugh},
  year = {2012},
  volume = {10},
  pages = {68--70},
  publisher = {{IEEE}},
  issn = {15407993},
  doi = {10.1109/MSP.2012.134},
  abstract = {Embedded-systems security is a mess, and the embedded-software industry needs to start focusing on it. This will involve moving beyond just the technology to rethink our assumptions of how people will actually use and maintain embedded devices. \textcopyright{} 2012 IEEE.},
  file = {D\:\\GDrive\\zotero\\Viega\\viega_2012_the_state_of_embedded-device_security_(spoiler_alert.pdf},
  journal = {IEEE Security and Privacy},
  keywords = {Computer security,Embedded devices,Embedded systems,Embedded-device security,SCADA,Stuxnet},
  number = {5}
}

@article{viglioneDoesGovernanceHave2015,
  title = {Does {{Governance Have}} a {{Role}} in {{Pricing}}? {{Cross}}-{{Country Evidence}} from {{Bitcoin Markets}}},
  author = {Viglione, Robert},
  year = {2015},
  pages = {1--31},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2666243},
  abstract = {I investigate the effects of social technologies related to governance on cross-country differences in Bitcoin prices. Investors pay a persistent premium over global prices in countries with less economic freedom, particularly when there exist foreign exchange and capital controls limiting investment freedom. Using the Heritage Foundation's Economic Freedom Index and associated macroeconomic time-series, I find that a 10 point increase in the index leads to a 7.5 percent decrease in premium. Of the component indices, Financial Freedom has the largest marginal effect in that a 10 point improvement in its value decreases prices by 5.3 percent. From this perspective, Bitcoin can be seen as a disaster asset offering a new channel to evade domestic jurisdiction repression, a process resembling imperfect markets for catastrophe insurance inducing unexpectedly high premiums. Finally, a natural question arises as to whether this finding can be extended to other assets, in other words, whether endogenous social technologies effect systemic risk and manifest in the pricing kernel.},
  file = {D\:\\GDrive\\zotero\\Viglione\\viglione_2015_does_governance_have_a_role_in_pricing.pdf},
  journal = {SSRN Electronic Journal}
}

@techreport{vikramLLVMCompilerLLVM,
  title = {The {{LLVM Compiler The LLVM Compiler Framework}} and {{Infrastructure Framework}} and {{Infrastructure Acknowledgements Acknowledgements}}},
  author = {Vikram, Vikram and Adve, Adve and Chris, Chris and Lattner, Lattner and Lattner, Chris},
  file = {D\:\\GDrive\\zotero\\Vikram\\vikram_the_llvm_compiler_the_llvm_compiler_framework_and_infrastructure_framework_and.pdf}
}

@techreport{villaHANDSONCOMPUTERSECURITY2016,
  title = {{{HANDS}}-{{ON COMPUTER SECURITY WITH A RASPBERRY PI}} *},
  author = {Villa, Adam H},
  year = {2016},
  abstract = {Computer security is a popular topic in today's world, whether it appears in the form of a newspaper headline or a job posting. The need for students to have hands-on experience with computer security topics and software is paramount. Employers are increasingly interested in hiring students with security knowledge, especially those familiar with popular security software packages and low-level security tools. This paper details the use of a small, portable computer called a Raspberry Pi for instructing real-world computer security concepts via hands-on laboratory assignments. The benefits of using these small computers in lieu of a dedicated computer lab include: decreased expenses, isolated sandbox environments, greater flexibility in designing assignments, and utilizing existing classroom spaces.},
  file = {D\:\\GDrive\\zotero\\Villa\\villa_2016_hands-on_computer_security_with_a_raspberry_pi.pdf}
}

@article{vintanHighPerformanceNeural1999,
  title = {Towards a High Performance Neural Branch Predictor},
  author = {Vintan, Lucian N. and Iridon, Mihaela},
  year = {1999},
  volume = {2},
  pages = {868--873},
  doi = {10.1109/ijcnn.1999.831066},
  abstract = {The main aim of this short paper is to propose a new branch prediction approach called by us `neural branch prediction'. We developed a first neural predictor model based on a simple neural learning algorithm, known as Learning Vector Quantization algorithm. Based on a trace driven simulation method we investigated the influences of the learning step, training processes, etc. Also we compared the neural predictor with a powerful classical predictor and we establish that they result in close performances. Therefore, we conclude that in the nearest future it might be necessary to model and simulate other more powerful neural adaptive predictors, based on more efficient neural networks architectures, in order to obtain better prediction accuracies compared with the previous known schemes.},
  file = {D\:\\GDrive\\zotero\\Vintan\\vintan_1999_towards_a_high_performance_neural_branch_predictor.pdf},
  journal = {Proceedings of the International Joint Conference on Neural Networks},
  keywords = {branch prediction,lvq,mii architectures,mlp,neural algorithms,trace driven simulation}
}

@article{visintinSAFEdSelfattestationNetworks2019,
  title = {{{SAFEd Self}}-Attestation for Networks of Heterogeneous Embedded Devices},
  author = {Visintin, Alessandro and Toffalini, Flavio and Conti, Mauro and Zhou, Jianying},
  year = {2019},
  issn = {23318422},
  abstract = {The Internet of Things (IoT) is an emerging paradigm that allows a fine-grained and autonomous control of the surrounding environment. This is achieved through a large number of devices that collaboratively perform complex tasks. To date, IoT networks are used in a variety of critical scenarios and therefore their security has become a primary concern. A robust technique to enhance the integrity of remote devices is called Remote Attestation (RA). However, classic RA schemes require a central and powerful entity, called Verifier, that manages the entire process of attestation. This makes the entire system dependent on an external entity and inevitably introduces a single point of failure. In our work, we present SAFEdthe first concrete solution to self-attest autonomous networks of heterogeneous embedded devices. SAFEd overcomes the limitations of the previous works by spreading the duties of the Verifier among all the devices in a scalable way. In our schema, the information needed for the attestation phase are replicated inside the network, thus raising the bar to accomplish an attack. As a result, the IoT network can self-inspect its integrity and self-recover in case of attack, without the need of an external entity. Our proposal exploits the security guarantees offered by ARM TrustZone chips to perform a decentralized attestation protocol based on an enhanced version of a distributed hash table. We implemented a prototype of SAFEd for the Raspberry Pi platform to evaluate the feasibility and the security properties of our protocol. Moreover, we measured the scalable properties of SAFEd by using a large network of virtual devices. The results show that SAFEd can detect infected devices and recover up to 99.7\% of its initial status in case of faults or attacks. Moreover, we managed to protect 50 devices with a logarithmic overhead on the network and on the devices' memory.},
  file = {D\:\\GDrive\\zotero\\Visintin et al\\visintin_et_al_2019_safed_self-attestation_for_networks_of_heterogeneous_embedded_devices.pdf},
  journal = {arXiv},
  keywords = {all or part of,chord protocol,classroom use is granted,copies are not made,distributed hash table,or,or distributed,or hard copies of,permission to make digital,remote attestation,security,this work for personal,without fee provided that}
}

@article{vixieWhatDNSNot2009,
  title = {What {{DNS}} Is Not},
  author = {Vixie, Paul},
  year = {2009},
  volume = {7},
  pages = {1--6},
  issn = {15427730},
  doi = {10.1145/1647300.1647302},
  abstract = {DNS is many things to many people - perhaps too many things to too many people. \textcopyright{} 2009 ACM.},
  file = {D\:\\GDrive\\zotero\\Vixie\\vixie_2009_what_dns_is_not.pdf},
  journal = {Queue},
  number = {10}
}

@techreport{vmwareComparisonSoftwareHardware,
  title = {A {{Comparison}} of {{Software}} and {{Hardware Techniques}} for X86 {{Virtualization}}},
  author = {Vmware, Keith Adams and Vmware, Ole Agesen},
  abstract = {Until recently, the x86 architecture has not permitted classical trap-and-emulate virtualization. Virtual Machine Monitors for x86, such as VMware R Workstation and Virtual PC, have instead used binary translation of the guest kernel code. However, both Intel and AMD have now introduced architectural extensions to support classical virtualization. We compare an existing software VMM with a new VMM designed for the emerging hardware support. Surprisingly, the hardware VMM often suffers lower performance than the pure software VMM. To determine why, we study architecture-level events such as page table updates, context switches and I/O, and find their costs vastly different among native, software VMM and hardware VMM execution. We find that the hardware support fails to provide an unambigu-ous performance advantage for two primary reasons: first, it offers no support for MMU virtualization; second, it fails to co-exist with existing software techniques for MMU virtualization. We look ahead to emerging techniques for addressing this MMU virtualiza-tion problem in the context of hardware-assisted virtualization.},
  file = {D\:\\GDrive\\zotero\\Vmware\\vmware_a_comparison_of_software_and_hardware_techniques_for_x86_virtualization.pdf}
}

@techreport{voglPersistentDataonlyMalware2014,
  title = {Persistent {{Data}}-Only {{Malware}}: {{Function Hooks}} without {{Code}}},
  author = {Vogl, Sebastian and Pfoh, Jonas and Kittel, Thomas and Eckert, Claudia},
  year = {2014},
  abstract = {As protection mechanisms become increasingly advanced , so too does the malware that seeks to circumvent them. Protection mechanisms such as secure boot, stack protection, heap protection, W X, and address space layout randomization have raised the bar for system security. In turn, attack mechanisms have become increasingly sophisticated. Starting with simple instruction pointer manipulation aimed at executing shellcode on the stack, we are now seeing sophisticated attacks that combine complex heap exploitation with techniques such as return-oriented programming (ROP). ROP belongs to a family of exploitation techniques called data-only exploitation. This class of exploitation and the malware that is built around it makes use solely of data to manipulate the control flow of software without introducing any code. This advanced form of exploitation circumvents many of the modern protection mechanisms presented above, however it has had, until now, one limitation. Due to the fact that it introduces no code, it is very difficult to achieve any sort of persistence. Placing a function hook is straightforward, but where should this hook point to if the malware introduces no code? There are many challenges that must first be overcome if one wishes to answer this question. In this paper, we present the first persistent data-only malware proof of concept in the form of a persistent rootkit. We also present several methods by which one can achieve persistence beyond our proof of concept.},
  file = {D\:\\GDrive\\zotero\\Vogl\\vogl_2014_persistent_data-only_malware.pdf},
  isbn = {1891562355}
}

@techreport{vukolicvukolicQuestScalableBlockchain,
  title = {The {{Quest}} for {{Scalable Blockchain Fabric}}: {{Proof}}-of-{{Work}} vs. {{BFT Replication}}},
  author = {Vukoli{\textasciiacute}cvukoli{\textasciiacute}c, Marko},
  abstract = {Bitcoin cryptocurrency demonstrated the utility of global consensus across thousands of nodes, changing the world of digital transactions forever. In the early days of Bitcoin, the performance of its probabilistic proof-of-work (PoW) based consensus fabric, also known as blockchain, was not a major issue. Bitcoin became a success story, despite its consensus latencies on the order of an hour and the theoretical peak throughput of only up to 7 transactions per second. The situation today is radically different and the poor performance scal-ability of early PoW blockchains no longer makes sense. Specifically, the trend of modern cryptocurrency platforms, such as Ethereum, is to support execution of arbitrary distributed applications on blockchain fabric , needing much better performance. This approach, however, makes cryptocurrency platforms step away from their original purpose and enter the domain of database-replication protocols, notably, the classical state-machine replication, and in particular its Byzantine fault-tolerant (BFT) variants. In this paper, we contrast PoW-based blockchains to those based on BFT state machine replication, focusing on their scalability limits. We also discuss recent proposals to overcoming these scalability limits and outline key outstanding open problems in the quest for the "ultimate" blockchain fabric(s).},
  file = {D\:\\GDrive\\zotero\\Vukoli´cvukoli´c\\vukoli´cvukoli´c_the_quest_for_scalable_blockchain_fabric.pdf},
  keywords = {Bitcoin,blockchain,Byzantine fault tolerance,consensus,proof-of-work,scalability,state machine replication}
}

@phdthesis{vuResilientAttackDetection,
  title = {Resilient {{Attack Detection}} in {{Mission}}-{{Critical Networks}}},
  author = {Vu, Quyen Dinh},
  file = {D\:\\GDrive\\zotero\\Vu\\vu_resilient_attack_detection_in_mission-critical_networks.pdf},
  language = {en}
}

@techreport{wadlerPropositionsTypes,
  title = {Propositions as {{Types}} *},
  author = {Wadler, Philip},
  file = {D\:\\GDrive\\zotero\\Wadler\\wadler_propositions_as_types.pdf}
}

@article{wagnerIntrusionDetectionStatic2001,
  title = {Intrusion Detection via Static Analysis},
  author = {Wagner, David and Dean, Drew},
  year = {2001},
  pages = {156--168},
  issn = {10637109},
  doi = {10.1109/secpri.2001.924296},
  abstract = {One of the primary challenges in intrusion detection is modelling typical application behavior, so that we can recognize attacks by their typical effects without raising too many false alarms. We show how static analysis may be used to automatically derive a model of application behavior. The result is a host-based intrusion detection system with three advantages: a high degree of automation, protection against a broad class of attacks based on corrupted code, and the elimination of false alarms. We report on our experience with a prototype implementation of this technique. \textcopyright{} 2000 IEEE.},
  file = {D\:\\GDrive\\zotero\\Wagner\\wagner_2001_intrusion_detection_via_static_analysis.pdf;D\:\\GDrive\\zotero\\Wagner\\wagner_2001_intrusion_detection_via_static_analysis2.pdf},
  journal = {Proceedings of the IEEE Computer Society Symposium on Research in Security and Privacy}
}

@techreport{wahbeEfficientSoftwareBasedFault,
  title = {Efficient {{Software}}-{{Based Fault Isolation}}},
  author = {Wahbe, Robert and Lucco, Steven and Anderson, Thomas E and Graham, Susan L},
  abstract = {One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead, In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain , a logically separate portion of the application's address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeoff relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.},
  file = {D\:\\GDrive\\zotero\\Wahbe\\wahbe_efficient_software-based_fault_isolation.pdf}
}

@techreport{walcottTraducementModelRecord,
  title = {Traducement: {{A Model}} for {{Record Security}}},
  author = {Walcott, Tom and Bishop, Matt},
  abstract = {Security models generally incorporate elements of both confidentiality and integrity. We examine a case where confidentiality is irrelevant to the process being modeled. In this case, integrity includes not only the authentication of origin and the lack of unauthorized changes to a document, but also the acceptance of all parties that the document is complete, signed by all parties, and cannot be modified further. This is especially critical when the document is recorded, so that it is legally the agreement or statement of record, and any copies of the document have no legal force. We show that current security models do not capture the details of this process. We then present a new security model for this process. This model captures the recordation process, and augments, rather than supplants, existing models. Hence it can also be used with existing security models to describe other situations.},
  file = {D\:\\GDrive\\zotero\\Walcott\\walcott_traducement.pdf},
  keywords = {D20 [Software Engineering]: General-Protection mech-anisms,D46 [Operating Systems]: Security and Protection-Access controls,H27 [Database Management]: Database Administration-Security; integrity; and protection,J1 [Computer Ap-plications]: Administrative Data Processing-Government,K65 [Management of Computing and Information Systems]: Security and Protection-Unauthorized access General Terms: Design; Management; Security; Theory Additional Key Words and Phrases: Integrity; recordation; security policy; traducement}
}

@techreport{waldoNoteDistributedComputing1994,
  title = {A {{Note}} on {{Distributed Computing}}},
  author = {Waldo, Jim and Wyant, Geoff and Wollrath, Ann and Kendall, Sam},
  year = {1994},
  abstract = {We argue that objects that interact in a distributed system need to be dealt with in ways that are intrinsically different from objects that interact in a single address space. These differences are required because distributed systems require that the programmer be aware of latency, have a different model of memory access, and take into account issues of concurrency and partial failure. We look at a number of distributed systems that have attempted to paper over the distinction between local and remote objects, and show that such systems fail to support basic requirements of robustness and reliability. These failures have been masked in the past by the small size of the distributed systems that have been built. In the enterprise-wide distributed systems foreseen in the near future, however, such a masking will be impossible. We conclude by discussing what is required of both systems-level and application-level programmers and designers if one is to take distribution seriously.},
  file = {D\:\\GDrive\\zotero\\Waldo\\waldo_1994_a_note_on_distributed_computing.pdf}
}

@techreport{wallachSurveyPeertoPeerSecurity2003,
  title = {A {{Survey}} of {{Peer}}-to-{{Peer Security Issues}}},
  author = {Wallach, Dan S},
  year = {2003},
  abstract = {Peer-to-peer (p2p) networking technologies have gained popularity as a mechanism for users to share files without the need for centralized servers. A p2p network provides a scalable and fault-tolerant mechanism to locate nodes anywhere on a network without maintaining a large amount of routing state. This allows for a variety of applications beyond simple file sharing. Examples include multicast systems, anonymous communications systems, and web caches. We survey security issues that occur in the underlying p2p routing protocols, as well as fairness and trust issues that occur in file sharing and other p2p applications. We discuss how techniques, ranging from cryptography, to random network probing, to economic incentives, can be used to address these problems.},
  file = {D\:\\GDrive\\zotero\\Wallach\\wallach_a_survey_of_peer-to-peer_security_issues.pdf}
}

@article{wangDeepWebUnderstanding2020,
  title = {Into the {{Deep Web}}: {{Understanding E}}-Commerce {{Fraud}} from {{Autonomous Chat}} with {{Cybercriminals}}},
  author = {Wang, Peng and Liao, Xiaojing and Qin, Yue and Wang, XiaoFeng},
  year = {2020},
  doi = {10.14722/ndss.2020.23071},
  abstract = {E-commerce miscreants heavily rely on instant messaging (IM) to promote their illicit businesses and coordinate their operations. The threat intelligence provided by IM communication, therefore, becomes invaluable for understanding and mitigating the threats of e-commerce frauds. However, such information is hard to obtain since it is usually shared only through one-on-one conversations with the criminals. In this paper, we present the first chatbot, called Aubrey, to actively collect such intelligence through autonomous chats with real-world e-commerce miscreants. Our approach leverages the question-driven conversation pattern of small-time workers, who seek jobs and/or attack resources from e-commerce fraudsters, to model the interaction process as a finite state machine, thereby enabling an autonomous conversation. Aubrey successfully chatted with 470 real-world e-commerce miscreants and gathered a large amount of fraud-related artifacts, including previously-unknown SIM gateways, account trading websites, and attack toolkits, etc. Further, the conversations revealed the supply chain of e-commerce fraudulent activities on the deep web and the complicated relations (e.g., complicity and reselling) among miscreants.},
  file = {D\:\\GDrive\\zotero\\Wang\\wang_2020_into_the_deep_web.pdf},
  isbn = {1891562614},
  number = {February}
}

@article{wangGeneratingAccurateDependencies2013,
  title = {Generating {{Accurate Dependencies}} for {{Large Software}} By},
  author = {Wang, Pei},
  year = {2013},
  file = {D\:\\GDrive\\zotero\\Wang\\wang_2013_generating_accurate_dependencies_for_large_software_by.pdf}
}

@article{wangHyperSafeLightweightApproach2010,
  title = {{{HyperSafe}}: {{A}} Lightweight Approach to Provide Lifetime Hypervisor Control-Flow Integrity},
  author = {Wang, Zhi and Jiang, Xuxian},
  year = {2010},
  pages = {380--395},
  issn = {10816011},
  doi = {10.1109/SP.2010.30},
  abstract = {Virtualization is being widely adopted in today's computing systems. Its unique security advantages in isolating and introspecting commodity OSes as virtual machines (VMs) have enabled a wide spectrum of applications. However, a common, fundamental assumption is the presence of a trustworthy hypervisor. Unfortunately, the large code base of commodity hypervisors and recent successful hypervisor attacks (e.g., VM escape) seriously question the validity of this assumption. In this paper, we present HyperSafe, a lightweight approach that endows existing Type-I bare-metal hypervisors with a unique self-protection capability to provide lifetime controlflow integrity. Specifically, we propose two key techniques. The first one - non-bypassable memory lockdown - reliably protects the hypervisor's code and static data from being compromised even in the presence of exploitable memory corruption bugs (e.g., buffer overflows), therefore successfully providing hypervisor code integrity. The second one - restricted pointer indexing - introduces one layer of indirection to convert the control data into pointer indexes. These pointer indexes are restricted such that the corresponding call/return targets strictly follow the hypervisor control flow graph, hence expanding protection to control-flow integrity. We have built a prototype and used it to protect two open-source Type-I hypervisors: BitVisor and Xen. The experimental results with synthetic hypervisor exploits and benchmarking programs show HyperSafe can reliably enable the hypervisor self-protection and provide the integrity guarantee with a small performance overhead. \textcopyright{} 2010 IEEE.},
  file = {D\:\\GDrive\\zotero\\Wang\\wang_2010_hypersafe.pdf},
  isbn = {9780769540351},
  journal = {Proceedings - IEEE Symposium on Security and Privacy}
}

@techreport{wangOo7LowoverheadDefense2019,
  title = {Oo7: {{Low}}-Overhead {{Defense}} against {{Spectre Attacks}} via {{Program Analysis}}},
  author = {Wang, Guanhua},
  year = {2019},
  abstract = {The Spectre vulnerability in modern processors has been widely reported. The key insight in this vulnerability is that speculative execution in processors can be misused to access the secrets. Subsequently, even though the speculatively executed instructions are squashed, the secret may linger in micro-architectural states such as cache, and can potentially be accessed by an attacker via side channels. In this paper, we propose oo7, a static analysis approach that can mitigate Spectre attacks by detecting potentially vulnerable code snippets in program binaries and protecting them against the attack by patching them. Our key contribution is to balance the concerns of effectiveness, analysis time and run-time overheads. We employ control flow extraction, taint analysis, and address analysis to detect tainted conditional branches and speculative memory accesses. oo7 can detect all fifteen purpose-built Spectre-vulnerable code patterns [1], whereas Microsoft compiler with Spectre mitigation option can only detect two of them. We also report the results of a large-scale study on applying oo7 to over 500 program binaries (average binary size 261 KB) from different real-world projects. We protect programs against Spectre attack by selectively inserting fences only at vulnerable conditional branches to prevent speculative execution. Our approach is experimentally observed to incur around 5.9\% performance overheads on SPECint benchmarks.},
  file = {D\:\\GDrive\\zotero\\Wang\\wang_oo7.pdf}
}

@article{wangOpenScanFullyTransparent2010,
  title = {{{OpenScan}}: {{A Fully Transparent Optical Scan Voting System}}},
  author = {Wang, Kai and Rescorla, Eric and Shacham, Hovav and Belongie, Serge},
  year = {2010},
  pages = {13},
  abstract = {Existing optical scan voting systems depend on the integrity of the scanner. If a compromised \textemdash{} or merely faulty \textemdash{} scanner reports incorrect results, there is no ready mechanism for detecting errors. While methods exist for ameliorating these risks, none of them are entirely satisfactory. We propose an alternative: a radically open system in which any observer can simultaneously and independently count the ballots for himself. Our approach, called OpenScan, combines digital video recordings of ballot sheet feeding with computer vision techniques to allow any observer with a video camera to obtain a series of ballot images that he can then process with ordinary optical scan counting software. Preliminary experimental results indicate that OpenScan produces accurate results at a manageable cost of around \$1000 in hardware plus \$0.0010 per ballot counted.},
  file = {D\:\\GDrive\\zotero\\Wang et al\\wang_et_al_2010_openscan.pdf},
  language = {en}
}

@inproceedings{wangOptimalConcolicTesting2018,
  title = {Towards Optimal Concolic Testing},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}} - {{ICSE}} '18},
  author = {Wang, Xinyu and Sun, Jun and Chen, Zhenbang and Zhang, Peixin and Wang, Jingyi and Lin, Yun},
  year = {2018},
  pages = {291--302},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3180155.3180177},
  abstract = {Concolic testing integrates concrete execution (e.g., random testing) and symbolic execution for test case generation. It is shown to be more cost-effective than random testing or symbolic execution sometimes. A concolic testing strategy is a function which decides when to apply random testing or symbolic execution, and if it is the latter case, which program path to symbolically execute. Many heuristics-based strategies have been proposed. It is still an open problem what is the optimal concolic testing strategy. In this work, we make two contributions towards solving this problem. First, we show the optimal strategy can be defined based on the probability of program paths and the cost of constraint solving. The problem of identifying the optimal strategy is then reduced to a model checking problem of Markov Decision Processes with Costs. Secondly, in view of the complexity in identifying the optimal strategy, we design a greedy algorithm for approximating the optimal strategy. We conduct two sets of experiments. One is based on randomly generated models and the other is based on a set of C programs. The results show that existing heuristics have much room to improve and our greedy algorithm often outperforms existing heuristics.},
  file = {D\:\\GDrive\\zotero\\Wang\\wang_2018_towards_optimal_concolic_testing.pdf},
  isbn = {978-1-4503-5638-1},
  keywords = {concolic,sse}
}

@article{wangTrustedMeasurementModel2018,
  title = {A Trusted Measurement Model Based on Dynamic Policy and Privacy Protection in {{IaaS}} Security Domain},
  author = {Wang, Liangming and Liu, Fagui},
  year = {2018},
  volume = {2018},
  pages = {1--8},
  publisher = {{EURASIP Journal on Information Security}},
  issn = {2510523X},
  doi = {10.1186/s13635-018-0071-1},
  abstract = {In Infrastructure as a Service (IaaS) environments, the user virtual machine is the user's private property. However, in the case of privacy protection, how to ensure the security of files in the user virtual machine and the user virtual machine's behavior does not affect other virtual machines; it is a major challenge. This paper presents a trusted measurement model based on dynamic policy and privacy protection in IaaS security domain, called TMMDP. The model first proposed a measure architecture, where it defines the trusted measurement of the user virtual machine into the trust of files in the virtual machine and trusted network behavior. The trusted measure was detected through the front-end and back-end modules. It then describes in detail the process of the trusted measurement in the two modules. Because the front-end module is in the guest virtual machine, it also describes the protocol to ensure the integrity of the module. Finally, the model proved to address security challenges of the user virtual machine in IaaS environments by a security analysis.},
  file = {D\:\\GDrive\\zotero\\Wang\\wang_2018_a_trusted_measurement_model_based_on_dynamic_policy_and_privacy_protection_in.pdf},
  journal = {Eurasip Journal on Information Security},
  keywords = {Dynamic policy,IaaS,Privacy protection,Trusted measurement},
  number = {1}
}

@techreport{washingtonNetworkIntrusionDetection,
  title = {Network {{Intrusion Detection}}: {{Evasion}},{{Traffic Normalization}}, and {{End}}-to-{{End Protocol Semantics}}},
  author = {Washington, D C and Handley, Mark and Paxson, Vern and Kreibich, Christian},
  abstract = {A fundamental problem for network intrusion detection systems is the ability of a skilled attacker to evade detection by exploiting ambiguities in the traffic stream as seen by the monitor. We discuss the viability of addressing this problem by introducing a new network forwarding element called a traffic normalizer. The normalizer sits directly in the path of traffic into a site and patches up the packet stream to eliminate potential ambiguities before the traffic is seen by the monitor , removing evasion opportunities. We examine a number of tradeoffs in designing a normalizer, emphasizing the important question of the degree to which normalizations undermine end-to-end protocol semantics. We discuss the key practical issues of "cold start" and attacks on the normalizer, and develop a methodology for systematically examining the ambiguities present in a protocol based on walking the protocol's header. We then present norm, a publicly available user-level implementation of a normalizer that can normalize a TCP traffic stream at 100,000 pkts/sec in memory-to-memory copies, suggesting that a kernel implementation using PC hardware could keep pace with a bidirectional 100 Mbps link with sufficient headroom to weather a high-speed flooding attack of small packets.},
  file = {D\:\\GDrive\\zotero\\Washington\\washington_network_intrusion_detection.pdf}
}

@article{watsonNumber876Capability2015,
  title = {Number 876 {{Capability Hardware Enhanced RISC Instructions}}: {{CHERI Instruction}}-{{Set Architecture}}},
  author = {Watson, Robert N M and Neumann, Peter G and Woodruff, Jonathan and Roe, Michael and Anderson, Jonathan and Chisnall, David and Davis, Brooks and Joannou, Alexandre and Laurie, Ben and Moore, Simon W and Murdoch, Steven J and Norton, Robert and Son, Stacey},
  year = {2015},
  issn = {1476-2986},
  file = {D\:\\GDrive\\zotero\\Watson\\watson_2015_number_876_capability_hardware_enhanced_risc_instructions.pdf}
}

@techreport{weaverWorstCaseWorm2004,
  title = {A {{Worst}}-{{Case Worm}} *},
  author = {Weaver, Nicholas and Paxson, Vern},
  year = {2004},
  abstract = {Worms represent a substantial economic threat to the U.S. computing infrastructure. An important question is how much damage might be caused, as this figure can serve as a guide to evaluating how much to spend on defenses. We construct a parameterized worst-case analysis based on a simple damage model, combined with our understanding of what an attack could accomplish. Although our estimates are at best approximations, we speculate that a plausible worst-case worm could cause \$50 billion or more in direct economic damage by attacking widely-used services in Microsoft Windows and carrying a highly destructive payload.},
  file = {D\:\\GDrive\\zotero\\Weaver\\weaver_2004_a_worst-case_worm.pdf}
}

@techreport{WebSearchPlanet2003,
  title = {Web {{Search}} for a {{Planet}}: {{The Google Cluster Architecture}}},
  year = {2003},
  file = {D\:\\GDrive\\zotero\\undefined\\web_search_for_a_planet.pdf}
}

@techreport{weilRADOSScalableReliable,
  title = {{{RADOS}}: {{A Scalable}}, {{Reliable Storage Service}} for {{Petabyte}}-Scale {{Storage Clusters}}},
  author = {Weil, Sage A and Leung, Andrew W and Brandt, Scott A and Maltzahn, Carlos},
  abstract = {Brick and object-based storage architectures have emerged as a means of improving the scalability of storage clusters. However, existing systems continue to treat storage nodes as passive devices, despite their ability to exhibit significant intelligence and autonomy. We present the design and implementation of RADOS, a reliable object storage service that can scales to many thousands of devices by leveraging the intelligence present in individual storage nodes. RADOS preserves consistent data access and strong safety semantics while allowing nodes to act semi-autonomously to self-manage replication, failure detection, and failure recovery through the use of a small cluster map. Our implementation offers excellent performance, reliability, and scalability while providing clients with the illusion of a single logical object store.},
  file = {D\:\\GDrive\\zotero\\Weil\\weil_rados.pdf},
  keywords = {C4 [Performance of Systems]: Reliability; availability; and serviceability,D43 [File Systems Management]: Distributed file systems,D47 [Organization and De-sign]: Distributed systems General Terms design; performance; reliability Keywords clustered storage; petabyte-scale storage; object-based stor-age}
}

@article{weinshelOhPlacesYou2019,
  title = {Oh, the Places You've Been! {{User}} Reactions to Longitudinal Transparency about Third-Party Web Tracking and Inferencing},
  author = {Weinshel, Ben and Wei, Miranda and Mondal, Mainack and Choi, Euirim and Shan, Shawn and Dolin, Claire and Mazurek, Michelle L. and Ur, Blase},
  year = {2019},
  pages = {149--166},
  issn = {15437221},
  doi = {10.1145/3319535.3363200},
  abstract = {Internet companies track users' online activity to make inferences about their interests, which are then used to target ads and personalize their web experience. Prior work has shown that existing privacy-protective tools give users only a limited understanding and incomplete picture of online tracking. We present Tracking Transparency, a privacy-preserving browser extension that visualizes examples of long-term, longitudinal information that third-party trackers could have inferred from users' browsing. The extension uses a client-side topic modeling algorithm to categorize pages that users visit and combines this with data about the web trackers encountered over time to create these visualizations. We conduct a longitudinal field study in which 425 participants use one of six variants of our extension for a week. We find that, after using the extension, participants have more accurate perceptions of the extent of tracking and also intend to take privacy-protecting actions.},
  file = {D\:\\GDrive\\zotero\\Weinshel\\weinshel_2019_oh,_the_places_you've_been.pdf},
  isbn = {9781450367479},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Third-party online tracking,Transparency,Usable privacy,User study}
}

@misc{weirichHowGiveGood,
  title = {How to Give a Good Research Talk},
  author = {Weirich, Stephanie},
  annotation = {https://www.youtube.com/watch?v=sT\_-owjKIbA\&ab\_channel=MicrosoftResearch},
  file = {D\:\\GDrive\\zotero\\Weirich\\weirich_how_to_give_a_good_research_talk.pdf},
  language = {en}
}

@article{weirPasswordCrackingUsing2009,
  title = {Password Cracking Using Probabilistic Context-Free Grammars},
  author = {Weir, Matt and Aggarwal, Sudhir and De Medeiros, Breno and Glodek, Bill},
  year = {2009},
  pages = {391--405},
  issn = {10816011},
  doi = {10.1109/SP.2009.8},
  abstract = {Choosing the most effective word-mangling rules to use when performing a dictionary-based password cracking attack can be a difficult task. In this paper we discuss a new method that generates password structures in highest probability order. We first automatically create a probabilistic context-free grammar based upon a training set of previously disclosed passwords. This grammar then allows us to generate word-mangling rules, and from them, password guesses to be used in password cracking. We will also show that this approach seems to provide a more effective way to crack passwords as compared to traditional methods by testing our tools and techniques on real password sets. In one series of experiments, training on a set of disclosed passwords, our approach was able to crack 28\% to 129\% more passwords than John the Ripper, a publicly available standard password cracking program. \textcopyright{} 2009 IEEE.},
  file = {D\:\\GDrive\\zotero\\Weir\\weir_2009_password_cracking_using_probabilistic_context-free_grammars.pdf},
  isbn = {9780769536330},
  journal = {Proceedings - IEEE Symposium on Security and Privacy},
  keywords = {Computer crime,Computer security,Data security}
}

@techreport{wensleySIFTDesignAnalysis1978,
  title = {{{SIFT}}: {{Design}} and {{Analysis}} of a {{Fault}}-{{Tolerant Computer}} for {{Aircraft Control}}},
  author = {Wensley, John H and Lamport, Leslie and Goldberg, Jack and Weinstock, Charles B and Green, Milton W and Levi'it, Karl N and {Melliar-Smith}, P M and Shostak, Robert E},
  year = {1978},
  volume = {66},
  abstract = {Abstmt-SIFT (Softwue Implemented Fault Tolerance) is an ldtmdme Coreputer f a criticd .ircnlt caltrd appkdom that rhievesf.ulttdenncebytherep\texteuro hthoft\&aamongproedng units. The rmin procesing units are off-the-shelf minicomputers, with sturdud microcomputers serving as the interface to the YO systean. Faultiadrtioniarchievedbyusingrspedrllydes@?dredundantbus system to interconnect the processhrg units. Error detection and analysis and system recontigumtbn are performed by software. Iterative tasks are redundantly executed, and the results of each iteration are voted upon before being d. Thus, any single failure in a processing unit or bus can be tolerated with triplication of t a s k s , and subsequent faihues can be tolerated after remnfigunthn. Independent execution by separate procesors meann that the processors need only be loosely synchronized, and a n d f a u l t-t d m t synchroniution method is d e s c n i. The S U T aoftwue is highly structured and is formally specified using the SRldeveloped SPECIAL I nnsuoge. I h e correctness of SIFT is to be proved using a hienrchy of formal modeis. A Markov model is uaed both to analyze the reliability of the system and to serve aa the formal requirement for the SIFT design. Axioms axe \& e n to duncterize the high-level behavior of the system, from which a correctness statement has been proved. An en-g test version of SIFT is currently being built. T},
  file = {D\:\\GDrive\\zotero\\Wensley\\wensley_1978_sift.pdf},
  journal = {PROCEEDINGS OF THE IEEE},
  number = {10}
}

@article{wernerSoKDecentralizedFinance2021,
  title = {{{SoK}}: {{Decentralized Finance}} ({{DeFi}})},
  shorttitle = {{{SoK}}},
  author = {Werner, Sam M. and Perez, Daniel and Gudgeon, Lewis and {Klages-Mundt}, Ariah and Harz, Dominik and Knottenbelt, William J.},
  year = {2021},
  month = apr,
  abstract = {Decentralized Finance (DeFi), a blockchain powered peer-to-peer financial system, is mushrooming. One year ago the total value locked in DeFi systems was approximately 700m USD, now, as of April 2021, it stands at around 51bn USD. The frenetic evolution of the ecosystem makes it challenging for newcomers to gain an understanding of its basic features. In this Systematization of Knowledge (SoK), we delineate the DeFi ecosystem along its principal axes. First, we provide an overview of the DeFi primitives. Second, we classify DeFi protocols according to the type of operation they provide. We then go on to consider in detail the technical and economic security of DeFi protocols, drawing particular attention to the issues that emerge specifically in the DeFi setting. Finally, we outline the open research challenges in the ecosystem.},
  archiveprefix = {arXiv},
  eprint = {2101.08778},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Werner et al\\werner_et_al_2021_sok.pdf;C\:\\Users\\Admin\\Zotero\\storage\\GK8AASXC\\2101.html},
  journal = {arXiv:2101.08778 [cs, econ, q-fin]},
  keywords = {Computer Science - Cryptography and Security,Economics - General Economics},
  primaryclass = {cs, econ, q-fin}
}

@techreport{wetherallActiveNetworkVision1999,
  title = {Active Network Vision and Reality: Lessons from a Capsule-Based System},
  author = {Wetherall, David},
  year = {1999},
  volume = {34},
  pages = {64--79},
  abstract = {Although active networks have generated much debate in the research community, on the whole there has been little hard evidence to inform this debate. This paper aims to redress the situation by reporting what we have learned by designing , implementing and using the ANTS active network toolkit over the past two years. At this early stage, active networks remain an open research area. However, we believe that we have made substantial progress towards providing a more flexible network layer while at the same time addressing the performance and security concerns raised by the presence of mobile code in the network. In this paper, we argue our progress towards the original vision and the difficulties that we have not yet resolved in three areas that characterize a "pure" active network: the capsule model of programma-bility; the accessibility of that model to all users; and the applications that can be constructed in practice.},
  file = {D\:\\GDrive\\zotero\\Wetherall\\wetherall_1999_active_network_vision_and_reality.pdf},
  journal = {Operating Systems Review},
  number = {5}
}

@techreport{WhatSatoshiDid,
  title = {What {{Satoshi Did Not Know}}},
  file = {D\:\\GDrive\\zotero\\undefined\\what_satoshi_did_not_know.pdf}
}

@techreport{whitesidesWhitesidesGroupWriting2004,
  title = {Whitesides' {{Group}}: {{Writing}} a {{Paper}}},
  author = {Whitesides, George M and Whitesides, G M},
  year = {2004},
  abstract = {A paper is not just an archival device for storing a com\- pleted research program; it is also a structure for planning your research in progress. If you clearly understand the pur\- pose and form of a paper, it can be immensely useful to you in organizing and conducting your research. A good outline for the paper is also a good plan for the research program. You should write and rewrite these plans/outlines throughout the course of the research. At the beginning, you will have mostly plan; at the end, mostly outline. The continuous effort to un\- derstand, analyze, summarize, and reformulate hypotheses on paper will be immensely more efficient for you than a process in which you collect data and only start to organize them when their collection is "complete". 2. Outlines 2.1. The Reason for Outlines I emphasize the central place of an outline in writing papers, preparing seminars, and planning research. I espe\- cially believe that for you, and for me, it is most efficient to write papers from outlines. An outline is a written plan of the organization of a paper, including the data on which it rests. You should, in fact, think of an outline as a carefully orga\- nized and presented set of data, with attendant objectives, hypotheses, and conclusions, rather than an outline of text. An outline itself contains little text. If you and I can agree on the details of the outline (that is, on the data and organiza\- tion), the supporting text can be assembled fairly easily. If we ["'J Prof.},
  file = {D\:\\GDrive\\zotero\\Whitesides\\whitesides_2004_whitesides'_group.pdf},
  keywords = {to-read}
}

@misc{WhyKeepResearch,
  title = {Why {{I Keep}} a {{Research Blog}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\ARPZXX2Y\\why-research-blog.html},
  howpublished = {http://gregorygundersen.com/blog/2020/01/12/why-research-blog/}
}

@techreport{wickhamrstudioJournalStatisticalSoftware2014,
  title = {Journal of {{Statistical Software Tidy Data}}},
  author = {Wickham RStudio, Hadley},
  year = {2014},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of untidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  file = {D\:\\GDrive\\zotero\\Wickham RStudio\\wickham_rstudio_2014_journal_of_statistical_software_tidy_data.pdf},
  keywords = {data cleaning,data tidying,R,relational databases}
}

@phdthesis{wijayaDomainbasedFuzzingEquivalence,
  title = {Domain-Based {{Fuzzing}} with {{Equivalence Partitioning}} for {{Supervised Learning}} of {{Anomaly Detection}} in a {{Cyber Physical System}}},
  author = {Wijaya, Herman Nuradhy},
  file = {D\:\\GDrive\\zotero\\Wijaya\\wijaya_domain-based_fuzzing_with_equivalence_partitioning_for_supervised_learning_of.pdf},
  language = {en}
}

@article{wijnenOpenSourceSyringePump2014,
  title = {Open-{{Source Syringe Pump Library}}},
  author = {Wijnen, Bas and Hunt, Emily J and Anzalone, Gerald C and Pearce, Joshua M},
  year = {2014},
  doi = {10.1371/journal.pone.0107216},
  abstract = {This article explores a new open-source method for developing and manufacturing high-quality scientific equipment suitable for use in virtually any laboratory. A syringe pump was designed using freely available open-source computer aided design (CAD) software and manufactured using an open-source RepRap 3-D printer and readily available parts. The design, bill of materials and assembly instructions are globally available to anyone wishing to use them. Details are provided covering the use of the CAD software and the RepRap 3-D printer. The use of an open-source Rasberry Pi computer as a wireless control device is also illustrated. Performance of the syringe pump was assessed and the methods used for assessment are detailed. The cost of the entire system, including the controller and web-based control interface, is on the order of 5\% or less than one would expect to pay for a commercial syringe pump having similar performance. The design should suit the needs of a given research activity requiring a syringe pump including carefully controlled dosing of reagents, pharmaceuticals, and delivery of viscous 3-D printer media among other applications.},
  file = {D\:\\GDrive\\zotero\\Wijnen\\wijnen_2014_open-source_syringe_pump_library.pdf}
}

@phdthesis{winderixSecurityEnhancedLLVM2018,
  title = {Security {{Enhanced LLVM}}},
  author = {Winderix, Hans},
  year = {2018},
  file = {D\:\\GDrive\\zotero\\Winderix\\winderix_2018_security_enhanced_llvm.pdf}
}

@article{winterTrustedComputingBuilding2008,
  title = {Trusted {{Computing Building Blocks}} for {{Embedded Linux}}-Based {{ARM TrustZone Platforms}}},
  author = {Winter, Johannes},
  year = {2008},
  abstract = {Security is an emerging topic in the field of mobile and embedded platforms. The Trusted Computing Group (TCG) has outlined one possible approach to mobile platform security by recently extending their set of Trusted Computing specifications with Mobile Trusted Modules (MTMs). The MTM specification [13] published by the TCG is a platform independent approach to Trusted Computing explicitly allowing for a wide range of potential implementations. ARM follows a different approach to mobile platform security , by extending platforms with hardware supported ARM TrustZone security [3] mechanisms. This paper outlines an approach to merge TCG-style Trusted Computing concepts with ARM TrustZone technology in order to build an open Linux-based embedded trusted computing platform.},
  file = {D\:\\GDrive\\zotero\\Winter\\winter_2008_trusted_computing_building_blocks_for_embedded_linux-based_arm_trustzone.pdf},
  keywords = {D20 [Software Engineering]: Protection mechanisms; C3 [Computer Systems Organization]: Special purpose and application based systems General Terms Design Keywords ARM TrustZone,Linux,Mobile Trusted Computing,Virtu-alisation}
}

@techreport{wischikDesignImplementationEvaluation,
  title = {Design, Implementation and Evaluation of Congestion Control for Multipath {{TCP}}},
  author = {Wischik, Damon and Raiciu, Costin and Greenhalgh, Adam and Handley, Mark},
  abstract = {Multipath TCP, as proposed by the IETF working group mptcp, allows a single data stream to be split across multiple paths. This has obvious benefits for reliability, and it can also lead to more efficient use of networked resources. We describe the design of a multipath congestion control algorithm, we implement it in Linux, and we evaluate it for multihomed servers, data centers and mobile clients. We show that some 'obvious' solutions for multipath congestion control can be harmful, but that our algorithm improves throughput and fairness compared to single-path TCP. Our algorithm is a drop-in replacement for TCP, and we believe it is safe to deploy.},
  file = {D\:\\GDrive\\zotero\\Wischik\\wischik_design,_implementation_and_evaluation_of_congestion_control_for_multipath_tcp.pdf}
}

@article{wrightUncoveringSpokenPhrases2010,
  title = {Uncovering Spoken Phrases in Encrypted Voice over {{IP}} Conversations},
  author = {Wright, C V and Ballard, L and Coull, S E and Monrose, F and Masson, G M},
  year = {2010},
  volume = {13},
  doi = {10.1145/1880022.1880029},
  abstract = {Although Voice over IP (VoIP) is rapidly being adopted, its security implications are not yet fully understood. Since VoIP calls may traverse untrusted networks, packets should be encrypted to ensure confidentiality. However, we show that it is possible to identify the phrases spoken within encrypted VoIP calls when the audio is encoded using variable bit rate codecs. To do so, we train a hidden Markov model using only knowledge of the phonetic pronunciations of words, such as those provided by a dictionary, and search packet sequences for instances of specified phrases. Our approach does not require examples of the speaker's voice, or even example recordings of the words that make up the target phrase. We evaluate our techniques on a standard speech recognition corpus containing over 2,000 phonetically rich phrases spoken by 630 distinct speakers from across the continental United States. Our results indicate that we can identify phrases within encrypted calls with an average accuracy of 50\%, and with accuracy greater than 90\% for some phrases. Clearly, such an attack calls into question the efficacy of current VoIP encryption standards. In addition, we examine the impact of various features of the underlying audio on our performance and discuss methods for mitigation.},
  journal = {ACM Transactions on Information and System Security},
  keywords = {side-channel,ss},
  number = {4}
}

@misc{WritingLiteratureReview,
  title = {Writing a {{Literature Review}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\PUHGX6UI\\Lit Review Guide.pdf}
}

@misc{WritingTechnicalArticles,
  title = {Writing {{Technical Articles}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\5MDJMUF7\\writing-style.html},
  howpublished = {https://www.cs.columbia.edu/\textasciitilde hgs/etc/writing-style.html}
}

@techreport{wurthingerIncrementalGarbageCollection,
  title = {Incremental {{Garbage Collection}}: {{The Train Algorithm}}},
  author = {W{\"u}rthinger, Thomas},
  abstract = {Modern programming languages use an automated way of getting rid of unused objects called garbage collection. This paper describes the use of incremental garbage collection and discusses the Train Algorithm in particular. At first the approach to split memory into blocks is described and then the reason why trains are needed is revealed. Furthermore another not very obvious problem of the algorithm and its solution is shown. Optimization possibilities and some implementation details as well as a discussion of the Train Algorithm in distributed systems conclude the article. This paper is the last part of a three paper series about incremental garbage collection algorithms. The first two parts ([8], [6]) give an overview of different algorithms to achieve incrementality, whereas this paper concentrates on the Train Algorithm.},
  file = {D\:\\GDrive\\zotero\\Würthinger\\würthinger_incremental_garbage_collection.pdf}
}

@inproceedings{wurthingerOneVMRule2013,
  title = {One {{VM}} to Rule Them All},
  booktitle = {{{SPLASH Indianapolis}} 2013: {{Onward}}! 2013 - {{Proceedings}} of the 2013 {{International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} and {{Software}}},
  author = {W{\"u}rthinger, Thomas and Wimmer, Christian and W{\"o}{\ss}, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
  year = {2013},
  pages = {187--204},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/2509578.2509581},
  abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efficient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new high-performance language implementations can be obtained by writing little more than a stylized interpreter.},
  file = {D\:\\GDrive\\zotero\\Würthinger\\würthinger_2013_one_vm_to_rule_them_all.pdf},
  isbn = {978-1-4503-2472-4},
  keywords = {Dynamic languages,Java,Javascript,Language implementation,Optimization,Virtual machine}
}

@techreport{wustrowTelexAnticensorshipNetwork,
  title = {Telex: {{Anticensorship}} in the {{Network Infrastructure}}},
  author = {Wustrow, Eric and Wolchok, Scott and Goldberg, Ian and Halderman, J Alex},
  abstract = {In this paper, we present Telex, a new approach to resisting state-level Internet censorship. Rather than attempting to win the cat-and-mouse game of finding open proxies, we leverage censors' unwillingness to completely block day-today Internet access. In effect, Telex converts innocuous, unblocked websites into proxies, without their explicit collaboration. We envision that friendly ISPs would deploy Telex stations on paths between censors' networks and popular, uncensored Internet destinations. Telex stations would monitor seemingly innocuous flows for a special "tag" and transparently divert them to a forbidden website or service instead. We propose a new cryptographic scheme based on elliptic curves for tagging TLS handshakes such that the tag is visible to a Telex station but not to a censor. In addition, we use our tagging scheme to build a protocol that allows clients to connect to Telex stations while resisting both passive and active attacks. We also present a proof-of-concept implementation that demonstrates the feasibility of our system.},
  file = {D\:\\GDrive\\zotero\\Wustrow\\wustrow_telex.pdf}
}

@article{xiaSurveySoftwareDefinedNetworking2015,
  title = {A {{Survey}} on {{Software}}-{{Defined Networking}}},
  author = {Xia, Wenfeng and Wen, Yonggang and Member, Senior and Heng Foh, Chuan and Niyato, Dusit and Xie, Haiyong and Foh, C H},
  year = {2015},
  volume = {17},
  pages = {27},
  doi = {10.1109/COMST.2014.2330903},
  abstract = {Emerging mega-trends (e.g., mobile, social, cloud, and big data) in information and communication technologies (ICT) are commanding new challenges to future Internet, for which ubiquitous accessibility, high bandwidth, and dynamic management are crucial. However, traditional approaches based on manual configuration of proprietary devices are cumbersome and error-prone, and they cannot fully utilize the capability of physical network infrastructure. Recently, software-defined networking (SDN) has been touted as one of the most promising solutions for future Internet. SDN is characterized by its two distinguished features, including decoupling the control plane from the data plane and providing programmability for network application development. As a result, SDN is positioned to provide more efficient configuration, better performance, and higher flexibility to accommodate innovative network designs. This paper surveys latest developments in this active research area of SDN. We first present a generally accepted definition for SDN with the afore-mentioned two characteristic features and potential benefits of SDN. We then dwell on its three-layer architecture, including an infrastructure layer, a control layer, and an application layer, and substantiate each layer with existing research efforts and its related research areas. We follow that with an overview of the de facto SDN implementation (i.e., OpenFlow). Finally, we conclude this survey paper with some suggested open research challenges.},
  file = {D\:\\GDrive\\zotero\\Xia\\xia_2015_a_survey_on_software-defined_networking.pdf},
  number = {1}
}

@book{xieFitnessGuidedPathExploration,
  title = {Fitness-{{Guided Path Exploration}} in {{Dynamic Symbolic Execution}}},
  author = {Xie, Tao and Tillmann, Nikolai and De Halleux, Jonathan and Schulte, Wolfram},
  abstract = {Dynamic symbolic execution is a structural testing technique that systematically explores feasible paths of the program under test by running the program with different test inputs to improve code coverage. To address the space-explosion issue in path exploration, we propose a novel approach called Fitnex, a search strategy that uses state-dependent fitness values (computed through a fitness function) to guide path exploration. The fitness function measures how close an already discovered feasible path is to a particular test target (e.g., covering a not-yet-covered branch). Our new fitness-guided search strategy is integrated with other strategies that are effective for exploration problems where the fitness heuristic fails. We implemented the new approach in Pex, an automated structural testing tool developed at Microsoft Research. We evaluated our new approach by comparing it with existing search strategies. The empirical results show that our approach is effective since it consistently achieves high code coverage faster than existing search strategies.},
  file = {D\:\\GDrive\\zotero\\Xie et al\\xie_et_al_fitness-guided_path_exploration_in_dynamic_symbolic_execution.pdf},
  isbn = {978-1-4244-4421-2}
}

@techreport{xinGraphXUnifyingDataParallel,
  title = {{{GraphX}}: {{Unifying Data}}-{{Parallel}} and {{Graph}}-{{Parallel Analytics}}},
  author = {Xin, Reynold S and Crankshaw, Daniel and Dave, Ankur and Gonzalez, Joseph E and Franklin, Michael J},
  abstract = {From social networks to language modeling, the growing scale and importance of graph data has driven the development of numerous new graph-parallel systems (e.g., Pregel, GraphLab). By restricting the computation that can be expressed and introducing new techniques to partition and distribute the graph, these systems can efficiently execute iterative graph algorithms orders of magnitude faster than more general data-parallel systems. However, the same restrictions that enable the performance gains also make it difficult to express many of the important stages in a typical graph-analytics pipeline: constructing the graph, modifying its structure, or expressing computation that spans multiple graphs. As a consequence, existing graph analytics pipelines compose graph-parallel and data-parallel systems using external storage systems, leading to extensive data movement and complicated programming model. To address these challenges we introduce GraphX, a distributed graph computation framework that unifies graph-parallel and data-parallel computation. GraphX provides a small, core set of graph-parallel operators expressive enough to implement the Pregel and PowerGraph abstractions, yet simple enough to be cast in relational algebra. GraphX uses a collection of query optimization techniques such as automatic join rewrites to efficiently implement these graph-parallel operators. We evaluate GraphX on real-world graphs and workloads and demonstrate that GraphX achieves comparable performance as specialized graph computation systems, while outper-forming them in end-to-end graph pipelines. Moreover, GraphX achieves a balance between expressiveness, performance, and ease of use.},
  file = {D\:\\GDrive\\zotero\\Xin\\xin_graphx.pdf}
}

@article{xinSharkSQLRich,
  title = {Shark: {{SQL}} and Rich Analytics at Scale},
  author = {Xin, Reynold S and Rosen, Josh and Zaharia, Matei and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  pages = {12},
  abstract = {Shark is a new data analysis system that marries query processing with complex analytics on large clusters. It leverages a novel distributed memory abstraction to provide a unified engine that can run SQL queries and sophisticated analytics functions (e.g., iterative machine learning) at scale, and efficiently recovers from failures mid-query. This allows Shark to run SQL queries up to 100\texttimes{} faster than Apache Hive, and machine learning programs more than 100\texttimes{} faster than Hadoop. Unlike previous systems, Shark shows that it is possible to achieve these speedups while retaining a MapReducelike execution engine, and the fine-grained fault tolerance properties that such engine provides. It extends such an engine in several ways, including column-oriented in-memory storage and dynamic mid-query replanning, to effectively execute SQL. The result is a system that matches the speedups reported for MPP analytic databases over MapReduce, while offering fault tolerance properties and complex analytics capabilities that they lack.},
  file = {D\:\\GDrive\\zotero\\Xin et al\\xin_et_al_shark.pdf},
  language = {en}
}

@techreport{xuMIROMultipathInterdomain2006,
  title = {{{MIRO}}: {{Multi}}-Path {{Interdomain ROuting}}},
  author = {Xu, Wen and Rexford, Jennifer},
  year = {2006},
  abstract = {The Internet consists of thousands of independent domains with different, and sometimes competing, business interests. However, the current interdomain routing protocol (BGP) limits each router to using a single route for each destination prefix, which may not satisfy the diverse requirements of end users. Recent proposals for source routing offer an alternative where end hosts or edge routers select the end-to-end paths. However, source routing leaves transit domains with very little control and introduces difficult scalability and security challenges. In this paper, we present a multi-path inter-domain routing protocol called MIRO that offers substantial flexibility , while giving transit domains control over the flow of traffic through their infrastructure and avoiding state explosion in disseminating reachability information. In MIRO, routers learn default routes through the existing BGP protocol, and arbitrary pairs of domains can negotiate the use of additional paths (bound to tunnels in the data plane) tailored to their special needs. MIRO retains the simplicity of BGP for most traffic, and remains backwards compatible with BGP to allow for incremental deployability. Experiments with Internet topology and routing data illustrate that MIRO offers tremendous flexibility for path selection with reasonable overhead.},
  file = {D\:\\GDrive\\zotero\\Xu\\xu_2006_miro.pdf},
  keywords = {C26 [Communication Networks]: Internetworking General Terms: Design,Experimentation Keywords: BGP,flexibility,inter-domain routing,multipath rout-ing,scalability}
}

@article{xuSoKDecentralizedExchanges2021,
  title = {{{SoK}}: {{Decentralized Exchanges}} ({{DEX}}) with {{Automated Market Maker}} ({{AMM}}) Protocols},
  shorttitle = {{{SoK}}},
  author = {Xu, Jiahua and Vavryk, Nazariy and Paruch, Krzysztof and Cousaert, Simon},
  year = {2021},
  month = apr,
  abstract = {As an integral part of the Decentralized Finance (DeFi) ecosystem, Automated Market Maker (AMM) based Decentralized Exchanges (DEXs) have gained massive traction with the revived interest in blockchain and distributed ledger technology in general. Most prominently, the top six AMMs -- Uniswap, Balancer, Curve, DODO, Bancor and Sushiswap -- hold in aggregate 15 billion USD worth of crypto-assets as of March 2021. Instead of matching the buy and sell sides, AMMs employ a peer-to-pool method and determine asset price algorithmically through a so-called conservation function. Compared to centralized exchanges, AMMs exhibit the apparent advantage of decentralization, automation and continuous liquidity. Nonetheless, AMMs typically feature drawbacks such as high slippage for traders and divergence loss for liquidity providers. This work establishes a general AMM framework describing the economics and formalizing the system's state-space representation. We employ our framework to systematically compare the top AMM protocols' mechanics, deriving their slippage and divergence loss functions. We further discuss security and privacy concerns associated with AMM DEXs, and conduct a comprehensive literature review on related work covering both DeFi and conventional market microstructure.},
  archiveprefix = {arXiv},
  eprint = {2103.12732},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Xu et al\\xu_et_al_2021_sok.pdf;C\:\\Users\\Admin\\Zotero\\storage\\3QEP8NBL\\2103.html},
  journal = {arXiv:2103.12732 [cs, q-fin]},
  keywords = {Computer Science - Computer Science and Game Theory,Quantitative Finance - Trading and Market Microstructure},
  primaryclass = {cs, q-fin}
}

@techreport{xuTaintEnhancedPolicyEnforcement,
  title = {Taint-{{Enhanced Policy Enforcement}}: {{A Practical Approach}} to {{Defeat}} a {{Wide Range}} of {{Attacks}}},
  author = {Xu, Wei and Bhatkar, Sandeep and Sekar, R},
  abstract = {Policy-based confinement, employed in SELinux and specification-based intrusion detection systems, is a popular approach for defending against exploitation of vul-nerabilities in benign software. Conventional access control policies employed in these approaches are effective in detecting privilege escalation attacks. However, they are unable to detect attacks that "hijack" legitimate access privileges granted to a program, e.g., an attack that subverts an FTP server to download the password file. (Note that an FTP server would normally need to access the password file for performing user authentica-tion.) Some of the common attack types reported today, such as SQL injection and cross-site scripting, involve such subversion of legitimate access privileges. In this paper, we present a new approach to strengthen policy enforcement by augmenting security policies with information about the trustworthiness of data used in security-sensitive operations. We evaluated this technique using 9 available exploits involving several popular software packages containing the above types of vulnerabilities. Our technique sucessfully defeated these exploits.},
  file = {D\:\\GDrive\\zotero\\Xu\\xu_taint-enhanced_policy_enforcement.pdf}
}

@techreport{yaarSIFFStatelessInternet,
  title = {{{SIFF}}: {{A Stateless Internet Flow Filter}} to {{Mitigate DDoS Flooding Attacks}} *},
  author = {Yaar, Abraham and Perrig, Adrian and Song, Dawn},
  abstract = {One of the fundamental limitations of the Internet is the inability of a packet flow recipient to halt disruptive flows before they consume the recipient's network link resources. Critical infrastructures and businesses alike are vulnerable to DoS attacks or flash-crowds that can incapacitate their networks with traffic floods. Unfortunately, current mechanisms require per-flow state at routers, ISP collaboration, or the deployment of an overlay infrastructure to defend against these events. In this paper, we present SIFF, a Stateless Internet Flow Filter, which allows an end-host to selectively stop individual flows from reaching its network, without any of the common assumptions listed above. We divide all network traffic into two classes, privileged (prioritized packets subject to recipient control) and unprivileged (legacy traffic). Privileged channels are established through a capability exchange handshake. Capabilities are dynamic and verified statelessly by the routers in the network, and can be revoked by quenching update messages to an offending host. SIFF is transparent to legacy clients and servers, but only updated hosts will enjoy the benefits of it.},
  file = {D\:\\GDrive\\zotero\\Yaar\\yaar_siff.pdf}
}

@article{yangUsingModelChecking2004,
  title = {Using Model Checking to Find Serious File System Errors},
  author = {Yang, Junfeng and Twohey, Paul and Engler, Dawson and Musuvathi, Madanlal},
  year = {2004},
  pages = {273--287},
  abstract = {This paper shows how to use model checking to find serious errors in file systems. Model checking is a formal verification technique tuned for finding corner-case errors by comprehensively exploring the state spaces defined by a system. File systems have two dynamics that make them attractive for such an approach. First, their errors are some of the most serious, since they can destroy persistent data and lead to unrecoverable corruption. Second, traditional testing needs an impractical, exponential number of test cases to check that the system will recover if it crashes at any point during execution. Model checking employs a variety of state-reducing techniques that allow it to explore such vast state spaces efficiently. We built a system, FiSC, for model checking file systems. We applied it to three widely-used, heavily-tested file systems: ext3 [13], JFS [21], and ReiserFS [27]. We found serious bugs in all of them, 32 in total. Most have led to patches within a day of diagnosis. For each file system, FiSC found demonstrable events leading to the unrecoverable destruction of metadata and entire directories, including the file system root directory ``/''.},
  file = {D\:\\GDrive\\zotero\\Yang\\yang_2004_using_model_checking_to_find_serious_file_system_errors.pdf},
  journal = {OSDI 2004 - 6th Symposium on Operating Systems Design and Implementation}
}

@inproceedings{yangWaitfreeQueueFast2016,
  title = {A Wait-Free Queue as Fast as Fetch-and-Add},
  booktitle = {Proceedings of the {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}, {{PPOPP}}},
  author = {Yang, Chaoran and {Mellor-Crummey}, John},
  year = {2016},
  month = feb,
  volume = {12-16-March-2016},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/2851141.2851168},
  abstract = {Concurrent data structures that have fast and predictable performanceareofcriticalimportanceforharnessingthepowerofmulticoreprocessors,whicharenowubiquitous.Althoughwait-freeobjects, whose operations complete in a bounded number of steps, weredevisedmorethantwodecadesago,wait-freeobjectsthatcan deliverscalablehighperformancearestillrare. Inthispaper,wepresentthefirstwait-freeFIFOqueuebasedon fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention.AlongwithFAA,ourqueueusesacustomepoch-based scheme to reclaim memory; on x86 architectures, it requires no extramemoryfencesonouralgorithm'stypicalexecutionpath.An empiricalstudyofournewFAA-basedwait-freeFIFOqueueunder highcontentiononfourdifferentarchitectureswithmanyhardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention,thethroughputofourqueueisoftenashighasthatofa microbenchmarkthatonlyperformsFAA.Asaresult,ourfastwaitfree queueimplementation isuseful inpractice onmost multi-core systems today. We believe that our design can serve as an example ofhowtoconstructotherfastwait-freeobjects.},
  file = {D\:\\GDrive\\zotero\\Yang\\yang_2016_a_wait-free_queue_as_fast_as_fetch-and-add.pdf},
  isbn = {978-1-4503-4092-2},
  keywords = {Fast-path-slow-path,Non-blocking queue,Wait-free}
}

@article{yaoJOPalarmDetectingJumporiented2013,
  title = {{{JOP}}-Alarm: {{Detecting}} Jump-Oriented Programming-Based Anomalies in Applications},
  author = {Yao, Fan and Chen, Jie and Venkataramani, Guru},
  year = {2013},
  pages = {467--470},
  publisher = {{IEEE}},
  doi = {10.1109/ICCD.2013.6657084},
  abstract = {Code Reuse-based Attacks (popularly known as CRA) are becoming increasingly notorious because of their ability to reuse existing code, and evade the guarding mechanisms in place to prevent code injection-based attacks. Among the recent code reuse-based exploits, Jump Oriented Programming (JOP) captures short sequences of existing code ending in indirect jumps or calls (known as gadgets), and utilizes them to cause harmful, unintended program behavior. In this work, we propose a novel, easily implementable algorithm, called JOP-alarm, that computes a score value to assess the potential for JOP attack, and detects possibly harmful program behavior. We demonstrate the effectiveness of our algorithm using published JOP code, and test the false positive alarm rate using several unmodified SPEC2006 benchmarks. \textcopyright{} 2013 IEEE.},
  file = {D\:\\GDrive\\zotero\\Yao\\yao_2013_jop-alarm.pdf},
  isbn = {9781479929870},
  journal = {2013 IEEE 31st International Conference on Computer Design, ICCD 2013},
  keywords = {Code reuse attack,Detection algorithm,Jump-oriented programming}
}

@article{yaoVulnerabilitiesIOT2019,
  title = {Vulnerabilities in {{IOT}}},
  author = {Yao, Yao and Zhou, Wei and Jia, Yan and Zhu, Lipeng and Liu, Peng},
  year = {2019},
  volume = {2},
  pages = {638--657},
  doi = {10.1007/978-3-030-29959-0},
  file = {D\:\\GDrive\\zotero\\Yao\\yao_2019_vulnerabilities_in_iot.pdf},
  isbn = {9783030299590},
  keywords = {firmware analysis,Firmware analysis,internet of things,Internet of Things,Privilege sep,privilege separation},
  number = {September 2019}
}

@article{yaromFLUSHRELOADHigh2014,
  title = {{{FLUSH}}+{{RELOAD}}: {{A}} High Resolution, Low Noise, {{L3}} Cache Side-Channel Attack},
  author = {Yarom, Yuval and Falkner, Katrina},
  year = {2014},
  pages = {719--732},
  abstract = {Sharing memory pages between non-trusting processes is a common method of reducing the memory footprint of multi-tenanted systems. In this paper we demonstrate that, due to a weakness in the Intel X86 processors, page sharing exposes processes to information leaks. We present FLUSH+RELOAD, a cache side-channel attack technique that exploits this weakness to monitor access to memory lines in shared pages. Unlike previous cache side-channel attacks, FLUSH+RELOAD targets the LastLevel Cache (i.e. L3 on processors with three cache levels). Consequently, the attack program and the victim do not need to share the execution core. We demonstrate the efficacy of the FLUSH+RELOAD attack by using it to extract the private encryption keys from a victim program running GnuPG 1.4.13. We tested the attack both between two unrelated processes in a single operating system and between processes running in separate virtual machines. On average, the attack is able to recover 96.7\% of the bits of the secret key by observing a single signfile:///Users/lamida/Downloads/Kocher1996\_Chapter\_TimingAttacksOnImplementations.pdf file:///Users/lamida/Downloads/Last-Level Cache Side-Channel Attacks are Practical.pdf file:///Users/lamida/Downloads/1801.04084.pdf file:///Users/lamida/Downloads/keydrown.pdf file:///Users/lamida/Downloads/ndss17\_maurice.pdf file:///Users/lamida/Downloads/A\_Primer\_on\_Memory\_Consistency\_and\_Cache\_Coherence.pdf file:///Users/lamida/Downloads/Tomasulo67-ibmj-An\_Efficient\_Algorithm\_for\_Exploiting\_Multiple\_Arithmetic\_Units.pdf file:///Users/lamida/Downloads/USA.pdf file:///Users/lamida/Downloads/123465.123475.pdf file:///Users/lamida/Downloads/2660267.2660356.pdf file:///Users/lamida/Downloads/3195638.3195641.pdf file:///Users/lamida/Downloads/06547110.pdf file:///Users/lamida/Downloads/2014-435.pdf file:///Users/lamida/Downloads/2976749.2978321.pdf file:///Users/lamida/Downloads/US7444498.pdf file:///Users/lamida/Downloads/sec17-gruss.pdf file:///Users/lamida/Downloads/Benger2014\_Chapter\_OohAahJustALittleBitASmallAmou.pdf file:///Users/lamida/Downloads/terry\_chen.pdf ature or decryption round.},
  file = {D\:\\GDrive\\zotero\\Yarom\\yarom_2014_flush+reload.pdf},
  isbn = {9781931971157},
  journal = {Proceedings of the 23rd USENIX Security Symposium}
}

@article{yehTwoLevelAdaptivePredict1991,
  title = {Two-{{Level Adaptive Predict}} Ion {{Two}}-{{Level Branch Adaptive Prediction Training Two}}-{{Level}}},
  author = {Yeh, Tse-Yu and Patt, Yale N.},
  year = {1991},
  pages = {51--61},
  file = {D\:\\GDrive\\zotero\\Yeh\\yeh_1991_two-level_adaptive_predict_ion_two-level_branch_adaptive_prediction_training.pdf},
  isbn = {0897914600},
  journal = {Proceeding MICRO 24 Proceedings of the 24th annual international symposium on Microarchitecture}
}

@inproceedings{yilekWhenPrivateKeys2009,
  title = {When Private Keys Are Public: Results from the 2008 {{Debian OpenSSL}} Vulnerability},
  shorttitle = {When Private Keys Are Public},
  booktitle = {Proceedings of the 9th {{ACM SIGCOMM}} Conference on {{Internet}} Measurement Conference - {{IMC}} '09},
  author = {Yilek, Scott and Rescorla, Eric and Shacham, Hovav and Enright, Brandon and Savage, Stefan},
  year = {2009},
  pages = {15},
  publisher = {{ACM Press}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1644893.1644896},
  abstract = {We report on the aftermath of the discovery of a severe vulnerability in the Debian Linux version of OpenSSL. Systems affected by the bug generated predictable random numbers, most importantly public/private keypairs. To study user response to this vulnerability, we collected a novel dataset of daily remote scans of over 50,000 SSL/TLS-enabled Web servers, of which 751 displayed vulnerable certificates. We report three primary results. First, as expected from previous work, we find an extremely slow rate of fixing, with 30\% of the hosts vulnerable when we began our survey on day 4 after disclosure still vulnerable almost six months later. However, unlike conventional vulnerabilities, which typically show a short, fast fixing phase, we observe a much flatter curve with fixing extending six months after the announcement. Second, we identify some predictive factors for the rate of upgrading. Third, we find that certificate authorities continued to issue certificates to servers with weak keys long after the vulnerability was disclosed.},
  file = {D\:\\GDrive\\zotero\\Yilek et al\\yilek_et_al_2009_when_private_keys_are_public.pdf},
  isbn = {978-1-60558-771-4},
  language = {en}
}

@techreport{yinHookFinderIdentifyingUnderstanding,
  title = {{{HookFinder}}: {{Identifying}} and {{Understanding Malware Hooking Behaviors}}},
  author = {Yin, Heng and Liang, Zhenkai},
  abstract = {Installing various hooks into the victim system is an important attacking strategy employed by malware, including spyware, rootkits, stealth backdoors, and others. In order to defeat existing hook detectors, malware writers keep exploring new hooking mechanisms. However, the current malware analysis procedure is painstaking , mostly manual and error-prone. In this paper, we propose the first systematic approach for automatically identifying hooks and extracting hooking mechanisms. We propose a unified approach, fine-grained impact analysis, to identify malware hooking behaviors. Our approach does not rely on any prior knowledge of hooking mechanisms, and thus can identify novel hooking mechanisms. Moreover, we devise a method using semantics-aware impact dependency analysis to provide a succinct and intuitive graph representation to illustrate hooking mechanisms. We have developed a prototype , HookFinder, and conducted extensive experiments using representative malware samples from various categories. We have demonstrated that HookFinder can correctly identify the hooking behaviors of all samples, and provide accurate insights about their hooking mechanisms .},
  file = {D\:\\GDrive\\zotero\\Yin\\yin_hookfinder.pdf}
}

@article{ymchlwFrameworkUnderstandingData1396,
  title = {A Framework for Understanding Data Dependences},
  author = {یامچلو, طاهره شفایی and ابیلی, خدایار and قراملکی, احد فرامرز},
  year = {1396},
  file = {D\:\\GDrive\\zotero\\یامچلو\\یامچلو_1396_a_framework_for_understanding_data_dependences.pdf},
  journal = {فصل نامه علمی پژوهشی آموزش عالی ایران، سال هشتم، شماره چهارم، زمستان 1395}
}

@techreport{yochelsonLISPGarbageCollectorVirtualMemory,
  title = {A {{LISP Garbage}}-{{Collector}} for {{Virtual}}-{{Memory Computer Systems}}},
  author = {Yochelson, Jerome C},
  abstract = {In this paper a garbage-collection algorithm for list-processing systems which operate within very large virtual memo, ies is described. The object of the algorithm is more the compaction of active storage than the discovery of free storage. Because free storage is never really exhausted, the decision to garbage collect is not easily made; therefore, various criteria of this decision are discussed.},
  file = {D\:\\GDrive\\zotero\\Yochelson\\yochelson_a_lisp_garbage-collector_for_virtual-memory_computer_systems.pdf}
}

@article{yongPointerAnalysisPrograms1999,
  title = {Pointer {{Analysis}} for {{Programs}} with {{Structures}} and {{Casting}}},
  author = {Yong, Suan Hsi},
  year = {1999},
  file = {D\:\\GDrive\\zotero\\Yong\\yong_pointer_analysis_for_programs_with_structures_and_casting.pdf}
}

@book{yongyuanBlockchainbasedIntelligentTransportation,
  title = {Towards {{Blockchain}}-Based {{Intelligent Transportation Systems}}},
  author = {{Yong Yuan} and {Fei-Yue Wang}},
  abstract = {Conference held at Windsor Oceanico Hotel, Rio de Janeiro, Brazil, November 1-4, 2016. Annotation The IEEE Intelligent Transportation Systems Conference is the annual flagship conference of the IEEE Intelligent Transportation Systems Society IEEE ITSC 2016 welcomes articles in the field of Intelligent Transportation Systems, dealing with new developments in theory, analytical and numerical simulation and modeling, experimentation, demonstration, advanced deployment and case studies, results of laboratory or field operational tests, under the general theme of Intelligent Transportation for Smarter Societies.},
  file = {D\:\\GDrive\\zotero\\Yong Yuan\\yong_yuan_towards_blockchain-based_intelligent_transportation_systems.pdf},
  isbn = {978-1-5090-1889-5}
}

@article{yoongFrameworkContinuousSystem2019,
  title = {Framework for {{Continuous System Security Protection}} in {{SWaT}}},
  author = {Yoong, Cheah Huei and Heng, Jonathan},
  year = {2019},
  doi = {10.1145/3386164.3387297},
  abstract = {Researchers implemented algorithms and attack techniques in programmable logic controllers of cyber physical systems like water treatment testbeds and power testbeds. However, in a reallife water plant such methods are almost impossible to be realised because the public utility company will not risk the damages may cause to the existing system by the software changes as the plant is actively producing water for the consumers. A reduction or stoppage of water due to system modifications will affect the daily life of many people. Thus, this paper focuses on the architecture framework to generate, run, and test research techniques particularly machine learning invariants in Secure Water Treatment (SWaT) that can be used in a real-life water treatment plant through a non-intrusive method. This framework has been thoroughly tested in SWaT using single or multiple invariants. The software in this framework allows substantial code reuse of data structures and algorithms. The programs to generate, run, and test the invariants are written in Python. The supervised machine learning invariants can detect anomalies without any false alarms for continuous systems in SWaT through physical device attacks and software generated attacks. This framework is also applicable to other cyber physical systems like power and gas testbeds with certain modifications such as the access interfaces and invariant designs. The future direction of this research is to provide a wider coverage protection solution framework to detect anomalies for discrete and continuous systems in cyber physical systems.},
  file = {D\:\\GDrive\\zotero\\Yoong\\yoong_2019_framework_for_continuous_system_security_protection_in_swat.pdf},
  isbn = {9781450376617},
  journal = {ACM International Conference Proceeding Series},
  keywords = {Architecture Framework,Attacks,Code Reuse of Data Structures and Algorithms,Continuous Systems,Machine Learning Invariants,Swat}
}

@article{younanCodeInjectionSurvey2004,
  title = {Code {{Injection}} in \{\vphantom\}{{C}}\vphantom\{\} and \{\vphantom\}{{C}}++\vphantom\{\} : \{\vphantom\}{{A}}\vphantom\{\} {{Survey}} of {{Vulnerabilities}} and {{Countermeasures}}},
  author = {Younan, Yves and Joosen, Wouter and Piessens, Frank},
  year = {2004},
  abstract = {Implementation errors relating to memory-safety are the most common vulnerabilities used by attackers to gain control over the execution-flow of an application. By carefully crafting an exploit for these vulnerabilities, attackers can make an application transfer execution-flow to code that they have injected. Such code injection attacks are among the most powerful and common attacks against software applications. This report documents possible vulnerabilities in C and C++ applications that could lead to situations that allow for code injection and describes the techniques generally used by attackers to exploit them. A fairly large number of defense techniques have been described in literature. An important goal of this report is to give a comprehensive survey of all available preventive and defensive countermeasures that either attempt to eliminate specific vulnerabilities entirely or attempt to combat their exploitation. Finally, the report presents a synthesis of this survey that allows the reader to weigh the advantages and disadvantages of using a specific countermeasure as opposed to using another more easily.},
  file = {D\:\\GDrive\\zotero\\Younan\\younan_2004_code_injection_in_ c _and_ c++ .pdf},
  number = {CW386}
}

@article{younanRuntimeCountermeasuresCode2012,
  title = {Runtime Countermeasures for Code Injection Attacks against {{C}} and {{C}}++ Programs},
  author = {Younan, Yves and Joosen, Wouter and Piessens, Frank},
  year = {2012},
  month = jun,
  volume = {44},
  pages = {1--28},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/2187671.2187679},
  abstract = {The lack of memory safety in C/C++ often leads to vulnerabilities.               Code injection attacks               exploit these vulnerabilities to gain control over the execution flow of applications. These attacks have played a key role in many major security incidents. Consequently, a huge body of research on countermeasures exists. We provide a comprehensive and structured survey of vulnerabilities and countermeasures that operate at runtime. These countermeasures make different trade-offs in terms of performance, effectivity, compatibility, etc., making it hard to evaluate and compare countermeasures in a given context. We define a classification and evaluation framework on the basis of which countermeasures can be assessed.},
  file = {D\:\\GDrive\\zotero\\Younan et al\\younan_et_al_2012_runtime_countermeasures_for_code_injection_attacks_against_c_and_c++_programs.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {3}
}

@techreport{YourCoffeeShop2005,
  title = {Your {{Coffee Shop Doesn}}'t {{Use Two}}-{{Phase Commit}}},
  year = {2005},
  abstract = {h e d b y t h e I E E E C o m p u t e r S o c i e t y 0 7 4 0-7 4 5 9 / 0 5 / \$ 2 0. 0 0 \textcopyright{} 2 0 0 5 I E E E design E d i t o r : M a r t i n F o w l e r \ding{110} T h o u g h t W o r k s \ding{110} f o w l e r @ a c m. o r g Y ou know you're a geek when going to the coffee shop gets you thinking about interaction patterns between loosely coupled systems. This happened to me on a recent trip to Japan. One of the more familiar sights in Tokyo is the numerous Starbucks coffee shops, especially around Shinjuku and Roppongi. While waiting for my "Hotto Cocoa," I started thinking about how a coffee shop processes customer orders. As a business, the coffee shop is naturally interested in maximizing order throughput, because more fulfilled orders mean more revenue. Interestingly, the optimization for throughput results in a concurrent and asynchronous processing model: when you place your order, the cashier marks a coffee cup with your order and places it into a queue. This queue is literally a line of coffee cups on top of the espresso machine. The queue decouples the cashier and barista, letting the cashier continue to take orders even when the barista is backed up. It also allows multiple baristas to start servicing the queue if the store gets busy, without impacting the cashier. Asynchronous processing models can be highly efficient but are not without challenges. If the real world writes the best stories, then maybe we can learn something from Starbucks about designing successful asynchronous mes-saging solutions.},
  file = {D\:\\GDrive\\zotero\\_\\2005_your_coffee_shop_doesn’t_use_two-phase_commit.pdf}
}

@misc{YouYourResearch,
  title = {You and {{Your Research}}},
  file = {C\:\\Users\\Admin\\Zotero\\storage\\IGC27ELH\\YouAndYourResearch.html},
  howpublished = {https://www.cs.virginia.edu/\textasciitilde robins/YouAndYourResearch.html}
}

@techreport{yuanFIREMANToolkitFIREwall2006,
  title = {{{FIREMAN}}: {{A Toolkit}} for {{FIREwall Modeling}} and {{ANalysis}}},
  author = {Yuan, Lihua and Mai, Jianning and Su, Zhendong and Chen, Hao and Chuah, Chen-Nee and Mohapatra, Prasant},
  year = {2006},
  abstract = {Security concerns are becoming increasingly critical in networked systems. Firewalls provide important defense for network security. However, misconfigurations in firewalls are very common and significantly weaken the desired security. This paper introduces FIREMAN, a static analysis toolkit for firewall modeling and analysis. By treating fire-wall configurations as specialized programs, FIREMAN applies static analysis techniques to check misconfigurations, such as policy violations, inconsistencies, and inefficiencies , in individual firewalls as well as among distributed firewalls. FIREMAN performs symbolic model checking of the firewall configurations for all possible IP packets and along all possible data paths. It is both sound and complete because of the finite state nature of firewall configurations. FIREMAN is implemented by modeling firewall rules using binary decision diagrams (BDDs), which have been used successfully in hardware verification and model checking. We have experimented with FIREMAN and used it to uncover several real misconfigurations in enterprise networks, some of which have been subsequently confirmed and corrected by the administrators of these networks.},
  file = {D\:\\GDrive\\zotero\\Yuan\\yuan_2006_fireman.pdf}
}

@book{yuManufacturingCompromiseEmergence,
  title = {Manufacturing {{Compromise}}: {{The Emergence}} of {{Exploit}}-as-a-{{Service}}},
  author = {Yu, Ting and {Association for Computing Machinery. Special Interest Group on Security}, Audit and {National Science Foundation (U.S.)} and {Association for Computing Machinery} and {ACM Digital Library.}},
  file = {D\:\\GDrive\\zotero\\Yu\\yu_manufacturing_compromise.pdf},
  isbn = {978-1-4503-1651-4}
}

@article{yurichevSATSMTExample2019,
  title = {{{SAT}}/{{SMT}} by {{Example}}},
  author = {Yurichev, Dennis},
  year = {2019},
  volume = {10},
  pages = {2--3},
  file = {D\:\\GDrive\\zotero\\Yurichev\\yurichev_2019_sat-smt_by_example.pdf}
}

@article{zacchialunStateArtCyberphysical2019,
  title = {State of the Art of Cyber-Physical Systems Security: {{An}} Automatic Control Perspective},
  author = {Zacchia Lun, Yuriy and D'Innocenzo, Alessandro and Smarra, Francesco and Malavolta, Ivano and Di Benedetto, Maria Domenica},
  year = {2019},
  volume = {149},
  pages = {174--216},
  publisher = {{Elsevier Inc.}},
  issn = {01641212},
  doi = {10.1016/j.jss.2018.12.006},
  abstract = {Cyber-physical systems are integrations of computation, networking, and physical processes. Due to the tight cyber-physical coupling and to the potentially disrupting consequences of failures, security here is one of the primary concerns. Our systematic mapping study sheds light on how security is actually addressed when dealing with cyber-physical systems from an automatic control perspective. The provided map of 138 selected studies is defined empirically and is based on, for instance, application fields, various system components, related algorithms and models, attacks characteristics and defense strategies. It presents a powerful comparison framework for existing and future research on this hot topic, important for both industry and academia.},
  journal = {Journal of Systems and Software},
  keywords = {Cyber-physical systems,Security,Systematic mapping study}
}

@inproceedings{zahariaDelaySchedulingSimple2010,
  title = {Delay Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling},
  shorttitle = {Delay Scheduling},
  booktitle = {Proceedings of the 5th {{European}} Conference on {{Computer}} Systems - {{EuroSys}} '10},
  author = {Zaharia, Matei and Borthakur, Dhruba and Sen Sarma, Joydeep and Elmeleegy, Khaled and Shenker, Scott and Stoica, Ion},
  year = {2010},
  pages = {265},
  publisher = {{ACM Press}},
  address = {{Paris, France}},
  doi = {10.1145/1755913.1755940},
  abstract = {As organizations start to use data-intensive cluster computing systems like Hadoop and Dryad for more applications, there is a growing need to share clusters between users. However, there is a conflict between fairness in scheduling and data locality (placing tasks on nodes that contain their input data). We illustrate this problem through our experience designing a fair scheduler for a 600-node Hadoop cluster at Facebook. To address the conflict between locality and fairness, we propose a simple algorithm called delay scheduling: when the job that should be scheduled next according to fairness cannot launch a local task, it waits for a small amount of time, letting other jobs launch tasks instead. We find that delay scheduling achieves nearly optimal data locality in a variety of workloads and can increase throughput by up to 2x while preserving fairness. In addition, the simplicity of delay scheduling makes it applicable under a wide variety of scheduling policies beyond fair sharing.},
  file = {D\:\\GDrive\\zotero\\Zaharia et al\\zaharia_et_al_2010_delay_scheduling.pdf},
  isbn = {978-1-60558-577-2},
  language = {en}
}

@techreport{zahariaDiscretizedStreamsEfficient,
  title = {Discretized {{Streams}}: {{An Efficient}} and {{Fault}}-{{Tolerant Model}} for {{Stream Processing}} on {{Large Clusters}}},
  author = {Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Shenker, Scott},
  abstract = {Many important "big data" applications need to process data arriving in real time. However, current programming models for distributed stream processing are relatively low-level, often leaving the user to worry about consistency of state across the system and fault recovery. Furthermore, the models that provide fault recovery do so in an expensive manner, requiring either hot repli-cation or long recovery times. We propose a new programming model, discretized streams (D-Streams), that offers a high-level functional programming API, strong consistency, and efficient fault recovery. D-Streams support a new recovery mechanism that improves efficiency over the traditional replication and upstream backup solutions in streaming databases: parallel recovery of lost state across the cluster. We have prototyped D-Streams in an extension to the Spark cluster computing framework called Spark Streaming, which lets users seamlessly in-termix streaming, batch and interactive queries.},
  file = {D\:\\GDrive\\zotero\\Zaharia\\zaharia_discretized_streams.pdf}
}

@inproceedings{zahariaDiscretizedStreamsFaulttolerant2013,
  title = {Discretized Streams: {{Fault}}-Tolerant Streaming Computation at Scale},
  booktitle = {{{SOSP}} 2013 - {{Proceedings}} of the 24th {{ACM Symposium}} on {{Operating Systems Principles}}},
  author = {Zaharia, Matei and Das, Tathagata and Li, Haoyuan and Hunter, Timothy and Shenker, Scott and Stoica, Ion},
  year = {2013},
  pages = {423--438},
  doi = {10.1145/2517349.2522737},
  abstract = {Many ``big data'' applications must act on data in real time. Running these applications at ever-larger scales re- quires parallel platforms that automatically handle faults and stragglers. Unfortunately, current distributed stream processing models provide fault recovery in an expen- sive manner, requiring hot replication or long recovery times, and do not handle stragglers. We propose a new processing model, discretized streams (D-Streams), that overcomes these challenges. D-Streams enable a par- allel recovery mechanism that improves efficiency over traditional replication and backup schemes, and tolerates stragglers.We show that they support a rich set of oper- ators while attaining high per-node throughput similar to single-node systems, linear scaling to 100 nodes, sub- second latency, and sub-second fault recovery. Finally, D-Streams can easily be composed with batch and in- teractive query models like MapReduce, enabling rich applications that combine these modes. We implement D-Streams in a system called Spark Streaming.},
  file = {D\:\\GDrive\\zotero\\Zaharia\\zaharia_2013_discretized_streams.pdf},
  isbn = {978-1-4503-2388-8}
}

@article{zahariaFasterMoreAccurate2011,
  title = {Faster and {{More Accurate Sequence Alignment}} with {{SNAP}}},
  author = {Zaharia, Matei and Bolosky, William J. and Curtis, Kristal and Fox, Armando and Patterson, David and Shenker, Scott and Stoica, Ion and Karp, Richard M. and Sittler, Taylor},
  year = {2011},
  month = nov,
  abstract = {We present the Scalable Nucleotide Alignment Program (SNAP), a new short and long read aligner that is both more accurate (i.e., aligns more reads with fewer errors) and 10-100x faster than state-of-the-art tools such as BWA. Unlike recent aligners based on the Burrows-Wheeler transform, SNAP uses a simple hash index of short seed sequences from the genome, similar to BLAST's. However, SNAP greatly reduces the number and cost of local alignment checks performed through several measures: it uses longer seeds to reduce the false positive locations considered, leverages larger memory capacities to speed index lookup, and excludes most candidate locations without fully computing their edit distance to the read. The result is an algorithm that scales well for reads from one hundred to thousands of bases long and provides a rich error model that can match classes of mutations (e.g., longer indels) that today's fast aligners ignore. We calculate that SNAP can align a dataset with 30x coverage of a human genome in less than an hour for a cost of \$2 on Amazon EC2, with higher accuracy than BWA. Finally, we describe ongoing work to further improve SNAP.},
  archiveprefix = {arXiv},
  eprint = {1111.5572},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Zaharia et al\\zaharia_et_al_2011_faster_and_more_accurate_sequence_alignment_with_snap.pdf;C\:\\Users\\Admin\\Zotero\\storage\\EDBFA2FM\\1111.html},
  journal = {arXiv:1111.5572 [cs, q-bio]},
  keywords = {Computer Science - Data Structures and Algorithms,Quantitative Biology - Genomics},
  primaryclass = {cs, q-bio}
}

@article{zahariaImprovingMapReducePerformance,
  title = {Improving {{MapReduce Performance}} in {{Heterogeneous Environments}}},
  author = {Zaharia, Matei and Konwinski, Andy and Joseph, Anthony D and Katz, Randy and Stoica, Ion},
  pages = {14},
  abstract = {MapReduce is emerging as an important programming model for large-scale data-parallel applications such as web indexing, data mining, and scientific simulation. Hadoop is an open-source implementation of MapReduce enjoying wide adoption and is often used for short jobs where low response time is critical. Hadoop's performance is closely tied to its task scheduler, which implicitly assumes that cluster nodes are homogeneous and tasks make progress linearly, and uses these assumptions to decide when to speculatively re-execute tasks that appear to be stragglers. In practice, the homogeneity assumptions do not always hold. An especially compelling setting where this occurs is a virtualized data center, such as Amazon's Elastic Compute Cloud (EC2). We show that Hadoop's scheduler can cause severe performance degradation in heterogeneous environments. We design a new scheduling algorithm, Longest Approximate Time to End (LATE), that is highly robust to heterogeneity. LATE can improve Hadoop response times by a factor of 2 in clusters of 200 virtual machines on EC2.},
  file = {D\:\\GDrive\\zotero\\Zaharia et al\\zaharia_et_al_improving_mapreduce_performance_in_heterogeneous_environments.pdf},
  language = {en}
}

@techreport{zahariaResilientDistributedDatasets,
  title = {Resilient {{Distributed Datasets}}: {{A Fault}}-{{Tolerant Abstraction}} for {{In}}-{{Memory Cluster Computing}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and Mccauley, Murphy and Franklin, Michael J and Shenker, Scott},
  abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  file = {D\:\\GDrive\\zotero\\Zaharia\\zaharia_resilient_distributed_datasets.pdf}
}

@techreport{zahariaSparkClusterComputing,
  title = {Spark: {{Cluster Computing}} with {{Working Sets}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper fo-cuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.},
  file = {D\:\\GDrive\\zotero\\Zaharia\\zaharia_spark.pdf},
  keywords = {distributed systems}
}

@inproceedings{zakaiEmscriptenLLVMtoJavaScriptCompiler2011,
  title = {Emscripten: An {{LLVM}}-to-{{JavaScript}} Compiler},
  shorttitle = {Emscripten},
  booktitle = {Proceedings of the {{ACM}} International Conference Companion on {{Object}} Oriented Programming Systems Languages and Applications Companion - {{SPLASH}} '11},
  author = {Zakai, Alon},
  year = {2011},
  pages = {301},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/2048147.2048224},
  abstract = {We present Emscripten, a compiler from LLVM (Low Level Virtual Machine) assembly to JavaScript. This opens up two avenues for running code written in languages other than JavaScript on the web: (1) Compile code directly into LLVM assembly, and then compile that into JavaScript using Emscripten, or (2) Compile a language's entire runtime into LLVM and then JavaScript, as in the previous approach, and then use the compiled runtime to run code written in that language. For example, the former approach can work for C and C++, while the latter can work for Python; all three examples open up new opportunities for running code on the web.},
  file = {D\:\\GDrive\\zotero\\Zakai\\zakai_2011_emscripten.pdf},
  isbn = {978-1-4503-0942-4},
  language = {en}
}

@techreport{zaksCoVaCCompilerValidation2008,
  title = {{{CoVaC}}: {{Compiler Validation}} by {{Program Analysis}} of the {{Cross}}-{{Product}}},
  author = {Zaks, Anna and Pnueli, Amir},
  year = {2008},
  abstract = {The paper presents a deductive framework for proving program equivalence and its application to automatic verification of transformations performed by optimizing compilers. To leverage existing program analysis techniques, we reduce the equivalence checking problem to analysis of one system-a cross-product of the two input programs. We show how the approach can be effectively used for checking equivalence of consonant (i.e., structurally similar) programs. Finally, we report on the prototype tool that applies the developed methodology to verify that a compiler optimization run preserves the program semantics. Unlike existing frameworks, CoVaC accommodates absence of compiler annotations and handles most of the classical intraprocedural optimizations such as constant folding, reassociation, common subexpression elimination, code motion, dead code elimination, branch optimizations, and others.},
  file = {D\:\\GDrive\\zotero\\Zaks\\zaks_covac.pdf}
}

@article{zaksEnsuringCorrectnessCompiled2009,
  title = {Ensuring {{Correctness}} of {{Compiled Code}}},
  author = {Zaks, Ganna and Pnueli, Amir and Alex, To},
  year = {2009},
  file = {D\:\\GDrive\\zotero\\Zaks\\zaks_2009_ensuring_correctness_of_compiled_code.pdf;D\:\\GDrive\\zotero\\Zaks\\zaks_2009_ensuring_correctness_of_compiled_code2.pdf}
}

@book{zaksProgramAnalysisCompiler2008,
  title = {Program {{Analysis}} for {{Compiler Validation}} *},
  author = {Zaks, Anna and Pnueli, Amir},
  year = {2008},
  abstract = {Translation Validation is an approach of ensuring compilation correctness in which each compiler run is followed by a validation pass that proves that the target code produced by the compiler is a correct translation (implementation) of the source code. It has been previously shown that the problem of translation validation can be reduced to checking if a single system-the corss-product of the source and target, satisfies a specific property. In this paper, we show how to adapt the existing program analysis techniques in the setting of translation validation. In addition, we present a novel invariant generation algorithm which strengthens our analysis when the input programs contain dynamically allocated data structures. Finally, we report on the prototype tool that applies the developed methodology to verification of the LLVM compiler. The tool handles many of the classical intraprocedural compiler optimizations such as constant folding, reassociation, common subexpression elimination, code motion, dead code elimination, and others.},
  file = {D\:\\GDrive\\zotero\\Zaks\\zaks_2008_program_analysis_for_compiler_validation.pdf},
  isbn = {978-1-60558-382-2}
}

@techreport{zaksVerifyingMultithreadedPrograms2008,
  title = {Verifying {{Multi}}-Threaded {{C Programs}} with {{SPIN}}},
  author = {Zaks, Anna and Joshi, Rajeev},
  year = {2008},
  abstract = {A key challenge in model checking software is the difficulty of verifying properties of implementation code, as opposed to checking an abstract algorithmic description. We describe a tool for verifying multi-threaded C programs that uses the SPIN model checker. Our tool works by compiling a multi-threaded C program into a typed bytecode format , and then using a virtual machine that interprets the bytecode and computes new program states under the direction of SPIN. Our virtual machine is compatible with most of SPIN's search options and optimization flags, such as bitstate hashing and multi-core checking. It provides support for dynamic memory allocation (the malloc and free family of functions), and for the pthread library, which provides primitives often used by multi-threaded C programs. A feature of our approach is that it can check code after compiler optimizations, which can sometimes introduce race conditions. We describe how our tool addresses the state space explosion problem by allowing users to define data abstraction functions and to constrain the number of allowed context switches. We also describe a reduction method that reduces context switches using dynamic knowledge computed on-the-fly, while being sound for both safety and liveness properties. Finally, we present initial experimental results with our tool on some small examples.},
  file = {D\:\\GDrive\\zotero\\Zaks\\zaks_verifying_multi-threaded_c_programs_with_spin.pdf}
}

@techreport{zaveDesignSpaceNetwork2013,
  title = {The {{Design Space}} of {{Network Mobility}}},
  author = {Zave, P and Rexford, J},
  year = {2013},
  pages = {379--412},
  abstract = {While the Internet is increasingly mobile, seamless mobility is difficult to implement at Internet scale. Over the years, standards bodies and the research community have introduced a large and confusing collection of mobility proposals that are difficult to compare. In this tutorial, we present these mobility proposals in a uniform framework, called the geomorphic view of networking. The geomorphic view shows that there are two distinct patterns for implementing mobility, each with its own range of design choices and cost-benefit trade-offs. We use these patterns to classify and explain a representative sample of mobility mechanisms, abstractly yet precisely. The patterns also serve as a basis for evaluating properties of these mechanisms such as resource costs and scalability, and for considering composition of mobility mechanisms.},
  file = {D\:\\GDrive\\zotero\\Zave\\zave_2013_the_design_space_of_network_mobility.pdf}
}

@techreport{zeilerADADELTAADAPTIVELEARNING,
  title = {{{ADADELTA}}: {{AN ADAPTIVE LEARNING RATE METHOD}}},
  author = {Zeiler, Matthew D},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochas-tic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information , different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  file = {D\:\\GDrive\\zotero\\Zeiler\\zeiler_adadelta.pdf},
  keywords = {Gradient Descent,Index Terms-Adaptive Learning Rates,Machine Learn-ing,Neural Networks}
}

@article{zeitouniATRIUMRuntimeAttestation2017,
  title = {{{ATRIUM}}: {{Runtime}} Attestation Resilient under Memory Attacks},
  author = {Zeitouni, Shaza and Dessouky, Ghada and Arias, Orlando and Sullivan, Dean and Ibrahim, Ahmad and Jin, Yier and Sadeghi, Ahmad Reza},
  year = {2017},
  volume = {2017-Novem},
  pages = {384--391},
  issn = {10923152},
  doi = {10.1109/ICCAD.2017.8203803},
  abstract = {Remote attestation is an important security service that allows a trusted party (verifier) to verify the integrity of a software running on a remote and potentially compromised device (prover). The security of existing remote attestation schemes relies on the assumption that attacks are software-only and that the prover's code cannot be modified at runtime. However, in practice, these schemes can be bypassed in a stronger and more realistic adversary model that is hereby capable of controlling and modifying code memory to attest benign code but execute malicious code instead - leaving the underlying system vulnerable to Time of Check Time of Use (TOCTOU) attacks. In this work, we first demonstrate TOCTOU attacks on recently proposed attestation schemes by exploiting physical access to prover's memory. Then we present the design and proof-of-concept implementation of ATRIUM, a runtime remote attestation system that securely attests both the code's binary and its execution behavior under memory attacks. ATRIUM provides resilience against both software- and hardware-based TOCTOU attacks, while incurring minimal area and performance overhead.},
  file = {D\:\\GDrive\\zotero\\Zeitouni\\zeitouni_2017_atrium.pdf},
  isbn = {9781538630938},
  journal = {IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
  keywords = {Attestation,Memory attacks,Runtime}
}

@article{zengCombiningControlFlowIntegrity2011,
  title = {Combining {{Control}}-{{Flow Integrity}} and Static Analysis for Efficient and Validated Data Sandboxing},
  author = {Zeng, Bin and Tan, Gang and Morrisett, Greg},
  year = {2011},
  pages = {29--39},
  issn = {15437221},
  doi = {10.1145/2046707.2046713},
  abstract = {In many software attacks, inducing an illegal control-flow transfer in the target system is one common step. Control-Flow Integrity (CFI [1]) protects a software system by enforcing a pre-determined control-flow graph. In addition to providing strong security, CFI enables static analysis on low-level code. This paper evaluates whether CFI-enabled static analysis can help build efficient and validated data sandboxing. Previous systems generally sandbox memory writes for integrity, but avoid protecting confidentiality due to the high overhead of sandboxing memory reads. To reduce overhead, we have implemented a series of optimizations that remove sandboxing instructions if they are proven unnecessary by static analysis. On top of CFI, our system adds only 2.7\% runtime overhead on SPECint2000 for sandboxing memory writes and adds modest 19\% for sandboxing both reads and writes. We have also built a principled data-sandboxing verifier based on range analysis. The verifier checks the safety of the results of the optimizer, which removes the need to trust the rewriter and optimizer. Our results show that the combination of CFI and static analysis has the potential of bringing down the cost of general inlined reference monitors, while maintaining strong security. \textcopyright{} 2011 ACM.},
  file = {D\:\\GDrive\\zotero\\Zeng\\zeng_2011_combining_control-flow_integrity_and_static_analysis_for_efficient_and.pdf;D\:\\GDrive\\zotero\\Zeng\\zeng_2011_combining_control-flow_integrity_and_static_analysis_for_efficient_and2.pdf},
  isbn = {9781450310758},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Binary rewriting,Control-flow integrity,Inlined reference monitors,Static analysis}
}

@article{zhangAllYourClicks2019,
  title = {All Your Clicks Belong to Me: {{Investigating}} Click Interception on the Web},
  author = {Zhang, Mingxue and Meng, Wei and Lee, Sangho and Lee, Byoungyoung and Xing, Xinyu},
  year = {2019},
  pages = {941--957},
  abstract = {Click is the prominent way that users interact with web applications. For example, we click hyperlinks to navigate among different pages on the Web, click form submission buttons to send data to websites, and click player controls to tune video playback. Clicks are also critical in online advertising, which fuels the revenue of billions of websites. Because of the critical role of clicks in the Web ecosystem, attackers aim to intercept genuine user clicks to either send malicious commands to another application on behalf of the user or fabricate realistic ad click traffic. However, existing studies mainly consider one type of click interceptions in the cross-origin settings via iframes, i.e., clickjacking. This does not comprehensively represent various types of click interceptions that can be launched by malicious third-party JavaScript code. In this paper, we therefore systematically investigate the click interception practices on the Web. We developed a browser-based analysis framework, OBSERVER, to collect and analyze click related behaviors. Using OBSERVER, we identified three different techniques to intercept user clicks on the Alexa top 250K websites, and detected 437 third-party scripts that intercepted user clicks on 613 websites, which in total receive around 43 million visits on a daily basis. We revealed that some websites collude with third-party scripts to hijack user clicks for monetization. In particular, our analysis demonstrated that more than 36\% of the 3,251 unique click interception URLs were related to online advertising, which is the primary monetization approach on the Web. Further, we discovered that users can be exposed to malicious contents such as scamware through click interceptions. Our research demonstrated that click interception has become an emerging threat to web users.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2019_all_your_clicks_belong_to_me.pdf},
  isbn = {9781939133069},
  journal = {Proceedings of the 28th USENIX Security Symposium}
}

@techreport{zhangAttributeBasedAccessMatrix2005,
  title = {An {{Attribute}}-{{Based Access Matrix Model}}},
  author = {Zhang, Xinwen and Li, Yingjiu and Nalla, Divya},
  year = {2005},
  abstract = {In traditional access control models like MAC, DAC, and RBAC, authorization decisions are determined according to identities of subjects and objects, which are authenticated by a system completely. Modern access control practices, such as DRM, trust management , and usage control, require flexible authorization policies. In such systems, a subject may be only partially authenticated according to one or more attributes. In this paper we propose an attribute-based access matrix model, named ABAM, which extends the access matrix model. We show that ABAM enhances the expressive power of the access matrix model by supporting attribute-based authorizations. Specifically, ABAM is comprehensive enough to encompass traditional access control models as well as some usage control concepts and specifications. On the other side, expressive power and safety are two fundamental but conflictive objectives in an access control model. We study the safety property of ABAM and conclude that the safety problem is decidable for a restricted case where attribute relationships allow no cycles. The restricted case is shown to be reasonable enough to model practical systems.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2005_an_attribute-based_access_matrix_model.pdf},
  keywords = {access matrix model,D46 [Operating Systems]: Security and Protection-Access con-trols; K65 [Management of Computing and Information Sys-tems]: Security and Protection-Unauthorized access General Terms Security Keywords access control,decidability,safety analysis}
}

@article{zhangControlFlowIntegrity2013,
  title = {Control {{Flow Integrity}} for {{COTS Binaries Control Flow Integrity}} for {{COTS Binaries}}},
  author = {Zhang, Mingwei and Sekar, R},
  year = {2013},
  abstract = {Control-Flow Integrity (CFI) has been recognized as an important low-level security property. Its enforcement can defeat most injected and existing code attacks, including those based on Return-Oriented Programming (ROP). Previous implementations of CFI have required compiler support or the presence of relocation or debug information in the binary. In contrast, we present a technique for applying CFI to stripped binaries on x86/Linux. Ours is the first work to apply CFI to complex shared libraries such as glibc. Through experimental evaluation , we demonstrate that our CFI implementation is effective against control-flow hijack attacks, and eliminates the vast majority of ROP gadgets. To achieve this result, we have developed robust techniques for disas-sembly, static analysis, and transformation of large bina-ries. Our techniques have been tested on over 300MB of binaries (executables and shared libraries).},
  file = {D\:\\GDrive\\zotero\\Zhang_Sekar\\zhang_sekar_2013_control_flow_integrity_for_cots_binaries_control_flow_integrity_for_cots.pdf}
}

@article{zhangCrosstenantSidechannelAttacks2014,
  title = {Cross-Tenant Side-Channel Attacks in {{PaaS}} Clouds},
  author = {Zhang, Yinqian and Juels, Ari and Reiter, Michael K. and Ristenpart, Thomas},
  year = {2014},
  pages = {990--1003},
  issn = {15437221},
  doi = {10.1145/2660267.2660356},
  abstract = {We present a new attack framework for conducting cache- based side-channel attacks and demonstrate this framework in attacks between tenants on commercial Platform-as-a-Service (PaaS) clouds. Our framework uses the Flush- Reload attack of Gullasch et al. as a primitive, and ex- tends this work by leveraging it within an automaton-driven strategy for tracing a victim's execution. We leverage our framework first to confirm co-location of tenants and then to extract secrets across tenant boundaries. We specifically demonstrate attacks to collect potentially sensitive application data (e.g., the number of items in a shopping cart), to hijack user accounts, and to break SAML single sign-on. To the best of our knowledge, our attacks are the first granular, cross-tenant, side-channel attacks successfully demonstrated on state-of-the-art commercial clouds, PaaS or otherwise. Copyright is held by the author/owner(s).},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2014_cross-tenant_side-channel_attacks_in_paas_clouds.pdf},
  isbn = {9781450329576},
  journal = {Proceedings of the ACM Conference on Computer and Communications Security},
  keywords = {Cache-based side channels,Cloud security,Platform-as-a-Service,Side-channel attacks}
}

@article{zhangCrossVMSideChannels2012,
  title = {Cross-{{VM Side Channels}} and {{Their Use}} to {{Extract Private Keys}}},
  author = {Zhang, Yinqian and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  year = {2012},
  doi = {10.1145/2382196.2382230},
  abstract = {This paper details the construction of an access-driven side-channel attack by which a malicious virtual machine (VM) extracts fine-grained information from a victim VM running on the same physical computer. This attack is the first such attack demonstrated on a symmetric multiprocessing system virtualized using a modern VMM (Xen). Such systems are very common today, ranging from desktops that use vir-tualization to sandbox application or OS compromises, to clouds that co-locate the workloads of mutually distrust-ful customers. Constructing such a side-channel requires overcoming challenges including core migration, numerous sources of channel noise, and the difficulty of preempting the victim with sufficient frequency to extract fine-grained information from it. This paper addresses these challenges and demonstrates the attack in a lab setting by extracting an ElGamal decryption key from a victim using the most recent version of the libgcrypt cryptographic library.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2012_cross-vm_side_channels_and_their_use_to_extract_private_keys.pdf;D\:\\GDrive\\zotero\\Zhang\\zhang_2012_cross-vm_side_channels_and_their_use_to_extract_private_keys2.pdf},
  isbn = {9781450316514},
  keywords = {Cache-based side channel,Cross-VMside channel,side-channel,Side-channel attack,ss}
}

@article{zhangIoTElectricBusiness2015,
  title = {An {{IoT}} Electric Business Model Based on the Protocol of Bitcoin},
  author = {Zhang, Yu and Wen, Jiangtao},
  year = {2015},
  pages = {184--191},
  publisher = {{IEEE}},
  doi = {10.1109/ICIN.2015.7073830},
  abstract = {Nowadays, the development of traditional business models become more and more mature that people use them to guide various kinds of E-business activities. Internet of things(IoT), being an innovative revolution over the Internet, becomes a new platform for E-business. However, old business models could hardly fit for the E-business on the IoT. In this article, we 1) propose an IoT E-business model, which is specially designed for the IoT E-business; 2) redesign many elements in traditional E-business models; 3) realize the transaction of smart property and paid data on the IoT with the help of P2P trade based on the Blockchain and smart contract. We also experiment our design and make a comprehensive discuss.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2015_an_iot_electric_business_model_based_on_the_protocol_of_bitcoin.pdf},
  isbn = {9781479918669},
  journal = {2015 18th International Conference on Intelligence in Next Generation Networks, ICIN 2015},
  keywords = {bitcoin,E-business model,Internet of things}
}

@article{zhangPracticalControlFlow2013,
  title = {Practical {{Control Flow Integrity}} \& {{Randomization}} for {{Binary Executables}}},
  author = {Zhang, Chao and Wei, Tao and Chen, Zhaofeng and Duan, Lei and Szekeres, Laszlo and McCamant, Stephen and Song, Dawn and Zou, Wei},
  year = {2013},
  pages = {15},
  abstract = {Control Flow Integrity (CFI) provides a strong protection against modern control-flow hijacking attacks. However, performance and compatibility issues limit its adoption.},
  file = {D\:\\GDrive\\zotero\\Zhang et al\\zhang_et_al_practical_control_flow_integrity_&_randomization_for_binary_executables.pdf},
  language = {en}
}

@techreport{zhangSCIONScalabilityControl,
  title = {{{SCION}}: {{Scalability}}, {{Control}}, and {{Isolation On Next}}-{{Generation Networks}}},
  author = {Zhang, Xin and Hsiao, Hsu-Chun and Hasker, Geoffrey and Chan, Haowen and Perrig, Adrian and Andersen, David G},
  abstract = {We present the first Internet architecture designed to provide route control, failure isolation, and explicit trust information for end-to-end communications. SCION separates ASes into groups of independent routing sub-planes, called trust domains, which then interconnect to form complete routes. Trust domains provide natural isolation of routing failures and human misconfiguration, give endpoints strong control for both inbound and outbound traffic, provide meaningful and enforceable trust, and enable scalable routing updates with high path freshness. As a result, our architecture provides strong resilience and security properties as an intrinsic consequence of good design principles , avoiding piecemeal add-on protocols as security patches. Meanwhile, SCION only assumes that a few top-tier ISPs in the trust domain are trusted for providing reliable end-to-end communications, thus achieving a small Trusted Computing Base. Both our security analysis and evaluation results show that SCION naturally prevents numerous attacks and provides a high level of resilience, scalability, control, and isolation.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_scion.pdf}
}

@article{zhangSoKStudyUsing2016,
  title = {{{SoK}}: {{A Study}} of {{Using Hardware}}-Assisted {{Isolated Execution Environments}} for {{Security}}},
  author = {Zhang, Fengwei and Zhang, Hongwei},
  year = {2016},
  doi = {10.1145/2948618.2948621},
  abstract = {Hardware-assisted Isolated Execution Environments (HIEEs) have been widely adopted to build effective and efficient defensive tools for securing systems. Hardware vendors have introduced a variety of HIEEs including system management mode, Intel management engine, ARM TrustZone, and Intel software guard extensions. This SoK paper presents a comprehensive study of existing HIEEs and compares their features from the security perspective. Additionally, we explore both defensive and offensive use scenarios of HIEEs and discuss the attacks against HIEE-based systems. Overall, this paper aims to give an essential checkpoint of the state-of-the-art systems that use HIEEs for trustworthy computing.},
  file = {D\:\\GDrive\\zotero\\Zhang_Zhang\\zhang_zhang_2016_sok.pdf},
  isbn = {9781450347693},
  keywords = {Hardware,Isolated execution environments,Security,ss}
}

@article{zhangTweetScoreScoringTweets2019,
  title = {{{TweetScore}}: {{Scoring}} Tweets via Social Attribute Relationships for Twitter Spammer Detection},
  author = {Zhang, Yihe and Zhang, Hao and Yuan, Xu and Tzeng, Nian Feng},
  year = {2019},
  pages = {379--390},
  doi = {10.1145/3321705.3329836},
  abstract = {The spammers have been grossly detrimental since the inception of Twitter social networks and keep polluting social environments by hiding themselves among a large amount of normal users. In this paper, we aim to address two challenges existing in the spammer detection problem: 1) monitoring tweets that have a higher probability of including spam messages; 2) providing an accurate solution for spam classification. To address these two challenges, we first propose a pseudo-honeypot framework for efficient tweets monitoring and collection. By taking advantage of users' diversity and selecting normal users as the parasitic body, the pseudo-honeypot can harness normal users with features having much more potentials of attracting spammers. This lets the pseudo-honeypot collect tweets that are far more likely to include spam messages. Furthermore, we design a novel spam classification solution called TweetScore by exploring both the intrinsic attributes' and users' relationships in social networks. TweetScore quantifies such relationships into a vector of numerical values to represent each tweet's score, reflecting the associated user's behaviors. The neural network is then employed to take these vectors as input to classify spams and spammers. Through extensive experiments, we demonstrate the efficiency of the pseudo-honeypot system on spam monitoring and the accuracy of TweetScore on spam classification. Specifically, the spam and spammer ratios collected by our pseudo-honeypot system are four times as much as those of a non pseudo-honeypot counterpart while the TweetScore can achieve, on an average, 93.5\% accuracy, 93.71\% precision, and 1.52\% false positive in online spam classification.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2019_tweetscore.pdf},
  isbn = {9781450367523},
  journal = {AsiaCCS 2019 - Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security},
  keywords = {Neural network,Spam detection,Twitter social networks}
}

@article{zhangUnreelingXunleiKankan2015,
  title = {Unreeling {{Xunlei Kankan}}: {{Understanding Hybrid CDN}}-{{P2P}} Video-on-Demand Streaming},
  author = {Zhang, Ge and Liu, Wei and Hei, Xiaojun and Cheng, Wenqing},
  year = {2015},
  volume = {17},
  pages = {229--242},
  publisher = {{IEEE}},
  issn = {15209210},
  doi = {10.1109/TMM.2014.2383617},
  abstract = {The hybrid architecture of content distribution network (CDN) and peer to peer (P2P) is promising in providing online streaming media services. In this paper, we conducted a comprehensive measurement study on Kankan, one of the leading VoD streaming service providers in China that is based on a hybrid CDN-P2P architecture. Our measurements are multi-fold, as follows. 1) Kankan adopts a loosely-coupled hybrid architecture , in which the user requests are handled by its CDN and P2P network independently. 2) Kankan deploys a small-scale CDN densely in three geographic clusters in China. It adopts specific redirection servers to dispatch the nationwide requests. 3) Kankan adopts a dual-server mechanism to enhance start-up video streaming. It also provides the CDN acceleration in case of inefficient P2P streaming performance. 4) According to our studies on the peer cache lists, the video contents stored in Kankan peers update quite slowly. The average lifetime of cached videos is longer than one week. Our results show that, by utilizing the slow-varying contents cached in peers and deploying various CDN enhancement mechanisms , Kankan provides a large-scale VoD streaming service with a small-scale fixed infrastructure. Insights obtained in this study will be valuable for the development and deployment of future hybrid CDN-P2P VoD streaming systems.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2015_unreeling_xunlei_kankan.pdf},
  journal = {IEEE Transactions on Multimedia},
  keywords = {Content distribution network (CDN),hybrid content distribution network peer to peer (,peer-to-peer (P2P) network,video on demand (VoD)},
  number = {2}
}

@article{zhangUsingTestCase2014,
  title = {Using Test Case Reduction and Prioritization to Improve Symbolic Execution},
  author = {Zhang, Chaoqiang and Groce, Alex and Alipour, Mohammad Amin},
  year = {2014},
  pages = {160--170},
  doi = {10.1145/2610384.2610392},
  abstract = {Scaling symbolic execution to large programs or programs with complex inputs remains difficult due to path explosion and complex constraints, as well as external method calls. Additionally, creating an effective test structure with symbolic inputs can be difficult. A popular symbolic execution strategy in practice is to perform symbolic execution not "from scratch" but based on existing test cases. This paper proposes that the effectiveness of this approach to symbolic execution can be enhanced by (1) reducing the size of seed test cases and (2) prioritizing seed test cases to maximize exploration efficiency. The proposed test case reduction strategy is based on a recently introduced generalization of deltadebugging, and our prioritization techniques include novel methods that, for this purpose, can outperform some traditional regression testing algorithms. We show that applying these methods can significantly improve the effectiveness of symbolic execution based on existing test cases.},
  file = {D\:\\GDrive\\zotero\\Zhang\\zhang_2014_using_test_case_reduction_and_prioritization_to_improve_symbolic_execution.pdf},
  isbn = {9781450326452},
  journal = {2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
  keywords = {Symbolic execution,Test case reduction,Test prioritization}
}

@book{zhangWhenChinesePhD2016,
  title = {When a {{Chinese PhD Student Meets}} a {{German Supervisor Tips}} for {{PhD Beginners}}},
  author = {Zhang, Keshun and Song, Shuang},
  year = {2016},
  file = {D\:\\GDrive\\zotero\\Zhang_Song\\zhang_song_2016_when_a_chinese_phd_student_meets_a_german_supervisor_tips_for_phd_beginners.pdf}
}

@article{zhaoFormalizingSsaBasedCompiler2013,
  title = {Formalizing the {{Ssa}}-{{Based Compiler}} for {{Verified}}},
  author = {Zhao, Jianzhou},
  year = {2013},
  file = {D\:\\GDrive\\zotero\\Zhao\\zhao_2013_formalizing_the_ssa-based_compiler_for_verified.pdf}
}

@techreport{zhengBlockchainChallengesOpportunities2018,
  title = {Blockchain Challenges and Opportunities: {{A}} Survey},
  author = {Zheng, Zibin and Xie, Shaoan and Dai, Hong Ning and Chen, Xiangping and Wang, Huaimin},
  year = {2018},
  volume = {14},
  pages = {352--375},
  issn = {17411114},
  doi = {10.1504/IJWGS.2018.095647},
  abstract = {Blockchain has numerous benefits such as decentralisation, persistency, anonymity and auditability. There is a wide spectrum of blockchain applications ranging from cryptocurrency, financial services, risk management, internet of things (IoT) to public and social services. Although a number of studies focus on using the blockchain technology in various application aspects, there is no comprehensive survey on the blockchain technology in both technological and application perspectives. To fill this gap, we conduct a comprehensive survey on the blockchain technology. In particular, this paper gives the blockchain taxonomy, introduces typical blockchain consensus algorithms, reviews blockchain applications and discusses technical challenges as well as recent advances in tackling the challenges. Moreover, this paper also points out the future directions in the blockchain technology.},
  file = {D\:\\GDrive\\zotero\\Zheng\\zheng_2018_blockchain_challenges_and_opportunities.pdf},
  journal = {International Journal of Web and Grid Services},
  keywords = {Blockchain,Consensus algorithms,Cryptocurrency,Internet of things,IoT,Smart contract},
  number = {4}
}

@article{zhengOverviewBlockchainTechnology2017,
  title = {An {{Overview}} of {{Blockchain Technology}}: {{Architecture}}, {{Consensus}}, and {{Future Trends}}},
  author = {Zheng, Zibin and Xie, Shaoan and Dai, Hongning and Chen, Xiangping and Wang, Huaimin},
  year = {2017},
  pages = {557--564},
  doi = {10.1109/BigDataCongress.2017.85},
  abstract = {Blockchain, the foundation of Bitcoin, has received extensive attentions recently. Blockchain serves as an immutable ledger which allows transactions take place in a decentralized manner. Blockchain-based applications are springing up, covering numerous fields including financial services, reputation system and Internet of Things (IoT), and so on. However, there are still many challenges of blockchain technology such as scalability and security problems waiting to be overcome. This paper presents a comprehensive overview on blockchain technology. We provide an overview of blockchain architechture firstly and compare some typical consensus algorithms used in different blockchains. Furthermore, technical challenges and recent advances are briefly listed. We also lay out possible future trends for blockchain.},
  file = {D\:\\GDrive\\zotero\\Zheng\\zheng_2017_an_overview_of_blockchain_technology.pdf},
  isbn = {9781538619964},
  journal = {Proceedings - 2017 IEEE 6th International Congress on Big Data, BigData Congress 2017},
  keywords = {Blockchain,consensus,decentralization,scalability}
}

@article{zhouEmpiricalStudyOptimization2021,
  title = {An Empirical Study of Optimization Bugs in {{GCC}} and {{LLVM}}},
  author = {Zhou, Zhide and Ren, Zhilei and Gao, Guojun and Jiang, He},
  year = {2021},
  month = apr,
  volume = {174},
  pages = {110884},
  issn = {01641212},
  doi = {10.1016/j.jss.2020.110884},
  abstract = {Optimizations are the fundamental component of compilers. Bugs in optimizations have significant impacts, and can cause unintended application behavior and disasters, especially for safety-critical domains. Thus, an in-depth analysis of optimization bugs should be conducted to help developers understand and test the optimizations in compilers. To this end, we conduct an empirical study to investigate the characteristics of optimization bugs in two mainstream compilers, GCC and LLVM. We collect about 57K and 22K bugs of GCC and LLVM, and then exhaustively examine 8,771 and 1,564 optimization bugs of the two compilers, respectively. The results reveal the following five characteristics of optimization bugs: (1) Optimizations are the buggiest component in both compilers except for the C++ component; (2) the value range propagation optimization and the instruction combine optimization are the buggiest optimizations in GCC and LLVM, respectively; the loop optimizations in both GCC and LLVM are more bug-prone than other optimizations; (3) most of the optimization bugs in both GCC and LLVM are misoptimization bugs, accounting for 57.21\% and 61.38\% respectively; (4) on average, the optimization bugs live over five months, and developers take 11.16 months for GCC and 13.55 months for LLVM to fix an optimization bug; in both GCC and LLVM, many confirmed optimization bugs have lived for a long time; (5) the bug fixes of optimization bugs involve no more than two files and three functions on average in both compilers, and around 99\% of them modify no more than 100 lines of code, while 90\% less than 50 lines of code.},
  file = {D\:\\GDrive\\zotero\\Zhou et al\\zhou_et_al_2021_an_empirical_study_of_optimization_bugs_in_gcc_and_llvm.pdf},
  journal = {Journal of Systems and Software},
  language = {en}
}

@phdthesis{zihongWirelessSignalEmulation,
  title = {Wireless {{Signal Emulation}} via {{AI}}-Based {{PHY Design}}},
  author = {Zihong, Shen},
  file = {D\:\\GDrive\\zotero\\Zihong\\zihong_wireless_signal_emulation_via_ai-based_phy_design.pdf}
}

@phdthesis{zimmermanProfiledirectedIfConversionSuperscalar2005,
  title = {Profile-Directed {{If}}-{{Conversion}} in {{Superscalar Microprocessors}}},
  author = {Zimmerman, Eric},
  year = {2005},
  abstract = {Conventional superscalar microprocessors have improved tremendously in performance, but still meet several limitations in achieving maximum performance. First, a load that misses in the data cache will wait many cycles for the value to arrive. Along with its dependent instructions, it will tie up critical resources including physical registers and scheduler entries. Even though there is often independent work that can be overlapped with the memory latency, many times the processor must stall due to a blocked reorder buffer (ROB). Karkhanis and Smith report about 90\% of cache misses to memory result in a blocked ROB [1]. Second, accurate branch prediction remains essential to keeping superscalar pipelines running efficiently. Modern branch predictors are very accurate for most branches, but long pipelines result in many instructions having to be squashed in the unfortunate case of a misspeculation. Aragon et al. report that ``an average 8-wide processor spends 47\% of its total cycles fetching wrong-path instructions'' [2]. Proposed future architectures like Continual Flow Pipelines (CFP) [3] effectively provide a large instruction window that is able to tolerate long-latency cache misses. CFP introduces a non- blocking register file and scheduler that off-loads miss-dependent instructions to a Slice Processing Unit while the memory request is satisfied. As a result of this design, a single cache miss will not likely cause a structural hazard, but instead, issuing can proceed with instructions that are independent of the cache miss. This allows a CFP machine to service multiple cache misses in parallel, achieving the desirable property of memory-level paral lelism (MLP) [4]. When processors with a high degree of memory-level parallelism eliminate pipeline stalls due to cache misses, branch misspeculations will become a more acute limiter of system performance. \textbackslash nBranch misspeculations are also responsible for significant energy and power costs in modern microprocessors. Before a misprediction is resolved, many wrong-path instructions are fetched, issued, and executed. These wasted instructions offer some benefit for prefetching future data values and instructions, but the instructions themselves are eventually flushed from the pipeline. Energy consumption of a processor is in part proportional to the number of instructions fetched, so wrong-path instructions represent a significant waste of power and energy. Indeed, Aragon et al. state ``branch mispredictions are responsible for around 28\% of the power dissipated by a typical processor due to the useless activities performed by instructions that are squashed'' [2]. We believe that CFP-style processors' heightened misspeculation penalty in terms of power and performance can be reduced through predicated execution. Predicated execution is a mechanism by which microprocessors fetch and execute instructions but conditionally commit the results based on a predicate value that evaluates to true or false. Pred- icated execution is often used to eliminate conditional branches in a process called if-conversion. Predicated execution is commonly studied in the context of EPIC architectures where it is largely utilized to increase scheduling scope for statically scheduled machines like Intel's Itanium 2 [5]. Dynamic predication, proposed by Klauser, et al. is an interesting exception for superscalar ar- chitectures because it requires no support in the instruction set architecture (ISA) for predicated instructions [6]. Instead, the predicated execution is performed dynamically by the register renamer and pipeline back-end. This method is limited to the if-conversion of simple branch hammocks, which are used to implement if-then-else statements. In this thesis, our goal is to identify the control structures associated with frequently mis- predicting branches in a modern out-of-order microprocessor. In Chapter 2, we classify these frequently mispredicting branches into several categories, and suggest possible ways to improve their predictability or remove the need for speculation entirely through predication. In Chapter 3, we suggest a series of profile-guided heuristics for selecting favorable branches to if-convert in a modern out-of-order processor. In Chapter 4, we implement several profile-directed compiler transformations to illustrate the predication mechanisms and our selection heuristics for decid- ing when to employ them. Using microprocessor simulations, we measure the effectiveness of the \textbackslash ntransformations in reducing the misspeculation penalty in terms of energy and performance. In Chapter 5, we summarize our results and discuss how predicated execution fits with future trends in microprocessor design.},
  file = {D\:\\GDrive\\zotero\\Zimmerman\\zimmerman_2005_profile-directed_if-conversion_in_superscalar_microprocessors.pdf;D\:\\GDrive\\zotero\\Zimmerman\\zimmerman_2005_profile-directed_if-conversion_in_superscalar_microprocessors2.pdf;D\:\\GDrive\\zotero\\Zimmerman\\zimmerman_2005_profile-directed_if-conversion_in_superscalar_microprocessors3.pdf}
}

@article{zinkCharacteristicsYouTubeNetwork2008,
  title = {Characteristics of {{YouTube}} Network Traffic at a Campus Network \textendash{} {{Measurements}}, Models, and Implications},
  author = {Zink, Michael and Suh, Kyoungwon and Gu, Yu and Kurose, Jim},
  year = {2008},
  volume = {53},
  pages = {501--514},
  doi = {10.1016/j.comnet.2008.09.022},
  abstract = {User-Generated Content has become very popular since new web services such as YouTube allow for the distribution of user-produced media content. YouTube-like services are different from existing traditional VoD services in that the service provider has only limited control over the creation of new content. We analyze how content distribution in YouTube is realized and then conduct a measurement study of YouTube traffic in a large university campus network. Based on these measurements, we analyzed the duration and the data rate of streaming sessions, the popularity of videos, and access patterns for video clips from the clients in the campus network. The analysis of the traffic shows that trace statistics are relatively stable over short-term periods while long-term trends can be observed. We demonstrate how synthetic traces can be generated from the measured traces and show how these synthetic traces can be used as inputs to trace-driven simulations. We also analyze the benefits of alternative distribution infrastructures to improve the performance of a YouTube-like VoD service. The results of these simulations show that P2P-based distribution and proxy caching can reduce network traffic significantly and allow for faster access to video clips.},
  journal = {Computer Networks}
}

@article{ziogasProductivityPortabilityPerformance2021,
  title = {Productivity, {{Portability}}, {{Performance}}: {{Data}}-{{Centric Python}}},
  shorttitle = {Productivity, {{Portability}}, {{Performance}}},
  author = {Ziogas, Alexandros Nikolaos and Schneider, Timo and {Ben-Nun}, Tal and Calotoiu, Alexandru and De Matteis, Tiziano and Licht, Johannes de Fine and Lavarini, Luca and Hoefler, Torsten},
  year = {2021},
  month = jul,
  doi = {10.1145/1122445.1122456},
  abstract = {Python has become the de facto language for scientific computing. Programming in Python is highly productive, mainly due to its rich science-oriented software ecosystem built around the NumPy module. As a result, the demand for Python support in High Performance Computing (HPC) has skyrocketed. However, the Python language itself does not necessarily offer high performance. In this work, we present a workflow that retains Python's high productivity while achieving portable performance across different architectures. The workflow's key features are HPC-oriented language extensions and a set of automatic optimizations powered by a data-centric intermediate representation. We show performance results and scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated Python, and up to 93.16\% scaling efficiency on 512 nodes.},
  archiveprefix = {arXiv},
  eprint = {2107.00555},
  eprinttype = {arxiv},
  file = {D\:\\GDrive\\zotero\\Ziogas et al\\ziogas_et_al_2021_productivity,_portability,_performance.pdf;C\:\\Users\\Admin\\Zotero\\storage\\D2QUK5L9\\2107.html},
  journal = {arXiv:2107.00555 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Performance,Computer Science - Programming Languages},
  primaryclass = {cs}
}

@book{zobelWritingComputerScience2014a,
  title = {Writing for {{Computer Science}}},
  author = {Zobel, Justin},
  year = {2014},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-6639-9},
  file = {D\:\\GDrive\\zotero\\Zobel\\zobel_2014_writing_for_computer_science2.pdf},
  isbn = {978-1-4471-6638-2 978-1-4471-6639-9},
  language = {en}
}


